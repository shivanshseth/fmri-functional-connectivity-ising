{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20effbb9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "from scripts.ising_simulation import IsingSimulation\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6353c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bold = np.loadtxt('../data/50003_timeseries.txt')\n",
    "bold_bin = np.zeros(bold.shape)\n",
    "bold_bin[np.where(bold>=0)] = 1\n",
    "bold_bin[np.where(bold<0)] = -1\n",
    "n_rois = bold_bin.shape[1]\n",
    "n_timesteps = bold_bin.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb039127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb7f84b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196, 116)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = 0.1\n",
    "state = bold_bin\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c7713c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(J, s, b):\n",
    "#     print(s.shape)\n",
    "    J = np.reshape(J, (n_rois, n_rois))\n",
    "#     print(J.shape)\n",
    "    term1 = 0\n",
    "    term2 = 0\n",
    "    for t in range(n_timesteps):\n",
    "        C = b * J @ s[t].T\n",
    "        term1 += C @ s[t].T\n",
    "        term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))\n",
    "    return (term1+term2)/n_timesteps\n",
    "\n",
    "def gradient(J, s, b):\n",
    "    J = np.reshape(J, (n_rois, n_rois))\n",
    "    grad = np.zeros((n_rois, n_rois))\n",
    "    for t in range(n_timesteps):\n",
    "        C = b * J @ s[t].T\n",
    "        grad += np.outer(s[t], s[t].T) - np.outer(np.tanh(C).T, s[t])\n",
    "    grad = grad * b/n_timesteps\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f3e2352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_ascent(max_iterations,w_init,\n",
    "                     obj_func,grad_func,extra_param = (),\n",
    "                     learning_rate=0.05,momentum=0.8, threshold=0.001):\n",
    "    \n",
    "    w = w_init\n",
    "    w_history = [w]\n",
    "    f_history = [obj_func(w,*extra_param)]\n",
    "    delta_w = np.zeros(w.shape)\n",
    "    i = 0\n",
    "    diff = 1.0e10\n",
    "    \n",
    "    while i<max_iterations and diff > threshold:\n",
    "        grad = grad_func(w,*extra_param)\n",
    "        # print(\"from func\", grad.shape)\n",
    "        grad = np.reshape(grad, (n_rois, n_rois))\n",
    "        # print(grad.shape)\n",
    "        delta_w = learning_rate*grad\n",
    "        w = w+delta_w\n",
    "        w_history.append(w)\n",
    "        f_history.append(obj_func(w,*extra_param))\n",
    "        if i%10 == 0: \n",
    "            print(f\"iteration: {i} loss: {f_history[-1]} grad: {np.sum(grad)}\")\n",
    "        i+=1\n",
    "        diff = np.absolute(f_history[-1]-f_history[-2])\n",
    "    \n",
    "    return w_history,f_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d9e2205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_timesteps = 20\n",
    "eq_steps = 1000     #  number of MC sweeps for equilibration\n",
    "σ = 10000\n",
    "α = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e3964ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "J = np.random.uniform(0, 1, size=(n_rois, n_rois))\n",
    "J = (J + J.T)/2 # making it symmetric\n",
    "np.fill_diagonal(J, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "86a73b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = 1/n_timesteps * bold_bin.T @ bold_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c436d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "J_hist, f_hist = gradient_ascent(σ, J, loss, gradient, extra_param=(bold_bin, beta), learning_rate=α)\n",
    "J_max = J_hist[f_hist.index(max(f_hist))]\n",
    "sim = IsingSimulation(n_rois, beta, coupling_mat = True, J=J_max)\n",
    "for i in range(eq_steps):\n",
    "    sim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d9fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = optimize.fmin_cg(loss, x0=J.flatten(), fprime=gradient, args=(bold_bin, ), disp=True, callback=lambda x: print(f'loss: {loss(x, bold_bin)}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "932180c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = np.ones(n_rois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecf136bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "sim = IsingSimulation(n_rois, beta, coupling_mat = True, J=J_max, initial_state=True, state=init_state)\n",
    "print(sim.state)\n",
    "M = []\n",
    "E = []\n",
    "for i in range(eq_steps):\n",
    "    sim.step()\n",
    "    M.append(sim.calcMag())\n",
    "    E.append(sim.calcEnergy())\n",
    "print(sim.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7f004c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "18f7a7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1760.0\n"
     ]
    }
   ],
   "source": [
    "time_series = np.zeros((n_timesteps, n_rois))\n",
    "state = sim.state[:]\n",
    "for i in range(n_timesteps):\n",
    "    state = sim.step(False, state)\n",
    "    time_series[i] = state\n",
    "print(np.sum(time_series))\n",
    "sim_fc = 1/n_timesteps * time_series.T @ time_series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ce445ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_analysis(beta, fc):\n",
    "    print(beta)\n",
    "    J = np.random.uniform(0, 1, size=(n_rois, n_rois))\n",
    "    J = (J + J.T)/2 # making it symmetric\n",
    "    np.fill_diagonal(J, 0)\n",
    "    J_hist, f_hist = gradient_ascent(σ, J, loss, gradient, extra_param=(bold_bin, beta), learning_rate=α)\n",
    "    J_max = J_hist[f_hist.index(max(f_hist))]\n",
    "    sim = IsingSimulation(n_rois, beta, coupling_mat = True, J=J_max)\n",
    "    for i in range(eq_steps):\n",
    "        sim.step()\n",
    "    _, sim_fc = sim.getTimeseries(n_timesteps)\n",
    "    c = np.corrcoef(np.triu(fc).flatten(), np.triu(sim_fc).flatten())[0, 1]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "35494d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "iteration: 0 loss: -1164.2871777895325 grad: -2203.7876849029717\n",
      "iteration: 10 loss: -1154.9696422055001 grad: -2203.744943207862\n",
      "iteration: 20 loss: -1145.6702551765302 grad: -2203.683843239097\n",
      "iteration: 30 loss: -1136.3892568584101 grad: -2203.6041471224\n",
      "iteration: 40 loss: -1127.1268731453847 grad: -2203.5056843388593\n",
      "iteration: 50 loss: -1117.8833035932125 grad: -2203.3883154700266\n",
      "iteration: 60 loss: -1108.6587168897722 grad: -2203.251878318165\n",
      "iteration: 70 loss: -1099.453254652123 grad: -2203.096139375947\n",
      "iteration: 80 loss: -1090.2670417060103 grad: -2202.9207651434654\n",
      "iteration: 90 loss: -1081.1002001269653 grad: -2202.7253197214595\n",
      "iteration: 100 loss: -1071.952863568424 grad: -2202.509290247609\n",
      "iteration: 110 loss: -1062.8251878954936 grad: -2202.272130331612\n",
      "iteration: 120 loss: -1053.7173552696167 grad: -2202.013298393136\n",
      "iteration: 130 loss: -1044.6295716205652 grad: -2201.732266057872\n",
      "iteration: 140 loss: -1035.5620605222543 grad: -2201.428485211554\n",
      "iteration: 150 loss: -1026.5150580849104 grad: -2201.1013245285426\n",
      "iteration: 160 loss: -1017.4888121725604 grad: -2200.7500057226907\n",
      "iteration: 170 loss: -1008.4835854663712 grad: -2200.3735720342065\n",
      "iteration: 180 loss: -999.4996585347453 grad: -2199.9709007960305\n",
      "iteration: 190 loss: -990.5373290536518 grad: -2199.5407427024065\n",
      "iteration: 200 loss: -981.5969064833756 grad: -2199.081755577217\n",
      "iteration: 210 loss: -972.6787050160074 grad: -2198.5925090460037\n",
      "iteration: 220 loss: -963.7830388629461 grad: -2198.0714586233507\n",
      "iteration: 230 loss: -954.9102225350067 grad: -2197.516906379946\n",
      "iteration: 240 loss: -946.0605761318958 grad: -2196.9269699432416\n",
      "iteration: 250 loss: -937.2344335417066 grad: -2196.299573298676\n",
      "iteration: 260 loss: -928.4321506863124 grad: -2195.6324611959617\n",
      "iteration: 270 loss: -919.6541113695121 grad: -2194.923236400209\n",
      "iteration: 280 loss: -910.9007291036892 grad: -2194.169436751603\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3654/647150058.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorr_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3654/1582602160.py\u001b[0m in \u001b[0;36mcorr_analysis\u001b[0;34m(beta, fc)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mJ\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;31m# making it symmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_diagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mJ_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_ascent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mσ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbold_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mα\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mJ_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJ_hist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_hist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIsingSimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_rois\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoupling_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mJ_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3654/2923190657.py\u001b[0m in \u001b[0;36mgradient_ascent\u001b[0;34m(max_iterations, w_init, obj_func, grad_func, extra_param, learning_rate, momentum, threshold)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdelta_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mw_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mf_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mextra_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"iteration: {i} loss: {f_history[-1]} grad: {np.sum(grad)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3654/1735410691.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(J, s, b)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mJ\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mterm1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mterm2\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mterm1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mterm2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corr_analysis(1, fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d70fa735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 loss: 141.27549065899245 grad: 187.34407103352828\n",
      "iteration: 10 loss: 231.97767175052803 grad: 197.9661688473745\n",
      "iteration: 20 loss: 336.429713280668 grad: 195.4346220286714\n",
      "iteration: 30 loss: 449.1200361318037 grad: 188.94741888370348\n",
      "iteration: 40 loss: 567.590242249011 grad: 181.41872080772123\n",
      "iteration: 50 loss: 690.6059553467371 grad: 173.45831422899073\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3654/2162592219.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorr_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcorr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3654/3461162818.py\u001b[0m in \u001b[0;36mcorr_analysis\u001b[0;34m(beta, fc)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mJ\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;31m# making it symmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_diagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mJ_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_ascent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mσ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbold_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mα\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mJ_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJ_hist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_hist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIsingSimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_rois\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoupling_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mJ_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3654/2232348269.py\u001b[0m in \u001b[0;36mgradient_ascent\u001b[0;34m(max_iterations, w_init, obj_func, grad_func, extra_param, learning_rate, momentum, threshold)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mmax_iterations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mextra_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m# print(\"from func\", grad.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_rois\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rois\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3654/168530359.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(J, s)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mJ\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mouter\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/brain/lib/python3.9/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mouter\u001b[0;34m(a, b, out)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corr = []\n",
    "for i in np.linspace(0, 1, 20):\n",
    "    c = corr_analysis(i, fc)\n",
    "    corr.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5301b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f658b688eb0>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(0, 1, 20), corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "370cbdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 loss: 140.6381637810415 grad: 187.75364391612715\n",
      "iteration: 0 loss: 141.40100387700136 grad: 187.61166022558484\n",
      "iteration: 0 loss: 139.70721897456104 grad: 186.7486256554929\n",
      "iteration: 0 loss: 139.9839880798173 grad: 186.80171770995156iteration: 0 loss: 140.82678844377128 grad: 187.09036078539933\n",
      "iteration: 0 loss: 140.36581272523478 grad: 187.23697325481737\n",
      "iteration: 0 loss: 140.84347369310015 grad: 187.27855739293508\n",
      "\n",
      "iteration: 0 loss: 141.4215465440448 grad: 187.41536192253528\n",
      "iteration: 10 loss: 230.49381508739557 grad: 198.98158638077194\n",
      "iteration: 10 loss: 230.95880006286566 grad: 198.24586809482528\n",
      "iteration: 10 loss: 231.98424362734949 grad: 198.15409309226038\n",
      "iteration: 10 loss: 231.21110936698062 grad: 198.2027169881814\n",
      "iteration: 10 loss: 229.6410560050476 grad: 198.08588739582578\n",
      "iteration: 10 loss: 231.60690613955106 grad: 198.65067914284532\n",
      "iteration: 10 loss: 230.4200074597521 grad: 198.3630715289915\n",
      "iteration: 10 loss: 230.2746446488316 grad: 197.93234132886877\n",
      "iteration: 20 loss: 333.92037725429844 grad: 197.15860438691954\n",
      "iteration: 20 loss: 334.82203616222387 grad: 196.17389321476998\n",
      "iteration: 20 loss: 335.29122953919864 grad: 196.0194685863916\n",
      "iteration: 20 loss: 336.25875003563164 grad: 195.79141661234323\n",
      "iteration: 20 loss: 333.5051516970986 grad: 195.9471669847693\n",
      "iteration: 20 loss: 335.43843823391717 grad: 196.62106307398605\n",
      "iteration: 20 loss: 334.229239565218 grad: 196.25281251254205\n",
      "iteration: 20 loss: 334.43653365828993 grad: 195.6445501033215\n",
      "iteration: 30 loss: 446.932965392299 grad: 189.96007393329091\n",
      "iteration: 30 loss: 447.5773729230854 grad: 189.8595724888463\n",
      "iteration: 30 loss: 445.709640558226 grad: 189.70379146450279\n",
      "iteration: 30 loss: 448.766673364944 grad: 189.38723485145314\n",
      "iteration: 30 loss: 445.51839310851506 grad: 191.22273287463048\n",
      "iteration: 30 loss: 447.3915265194693 grad: 190.43454975218012\n",
      "iteration: 30 loss: 446.2898893178941 grad: 190.10343322270725\n",
      "iteration: 30 loss: 446.8663232522347 grad: 189.3575028121836\n",
      "iteration: 40 loss: 564.8531043966817 grad: 182.62456237879974\n",
      "iteration: 40 loss: 567.1300211734281 grad: 181.77352741440848\n",
      "iteration: 40 loss: 565.5667767516702 grad: 182.718551782138\n",
      "iteration: 40 loss: 563.7330354755949 grad: 182.3265362086189\n",
      "iteration: 40 loss: 562.8355725510008 grad: 184.19150313710972\n",
      "iteration: 40 loss: 565.1637993679395 grad: 183.00839450073175\n",
      "iteration: 40 loss: 565.0575045435576 grad: 182.06819508318904\n",
      "iteration: 40 loss: 564.0853217290995 grad: 182.98797537089735\n",
      "iteration: 50 loss: 687.3443720364066 grad: 174.77965347627503\n",
      "iteration: 50 loss: 690.1731289181704 grad: 173.7773995228073\n",
      "iteration: 50 loss: 686.3227136193118 grad: 174.5614382647087\n",
      "iteration: 50 loss: 687.9677968624794 grad: 175.2318205246705\n",
      "iteration: 50 loss: 684.5759359091495 grad: 176.71245134297865\n",
      "iteration: 50 loss: 687.5342095403395 grad: 175.29817552404052\n",
      "iteration: 50 loss: 686.2441248224378 grad: 175.55376376213354\n",
      "iteration: 50 loss: 687.7255192872816 grad: 174.45395955376608\n",
      "iteration: 60 loss: 817.2966184449533 grad: 165.39599898878302\n",
      "iteration: 60 loss: 813.8620850179317 grad: 166.6631556915928\n",
      "iteration: 60 loss: 812.8793504064602 grad: 166.42566703967498\n",
      "iteration: 60 loss: 814.2023601442828 grad: 167.37987088062016\n",
      "iteration: 60 loss: 810.179254435631 grad: 168.87102052651812\n",
      "iteration: 60 loss: 813.7981816364145 grad: 167.60676422187072\n",
      "iteration: 60 loss: 812.1884199637444 grad: 167.61235840745755\n",
      "iteration: 60 loss: 814.2255276001298 grad: 166.72756554595483\n",
      "iteration: 70 loss: 948.4388686017309 grad: 156.3253744636391\n",
      "iteration: 70 loss: 944.211958285136 grad: 157.9036128904905\n",
      "iteration: 70 loss: 943.3538395028993 grad: 157.3830032929475\n",
      "iteration: 70 loss: 944.1327630662814 grad: 159.11870906174403\n",
      "iteration: 70 loss: 939.485740289105 grad: 160.5706886852215\n",
      "iteration: 70 loss: 943.5899164125537 grad: 159.55881564337614\n",
      "iteration: 70 loss: 941.8982500040346 grad: 158.91403818656312\n",
      "iteration: 80 loss: 1083.9096156829876 grad: 146.44717811024339\n",
      "iteration: 80 loss: 1078.8318791197328 grad: 147.8956238827057\n",
      "iteration: 70 loss: 944.2878610346952 grad: 158.6165956238237\n",
      "iteration: 80 loss: 1078.306242241053 grad: 147.0464271712703\n",
      "iteration: 80 loss: 1077.8519060703686 grad: 150.21553316414634\n",
      "iteration: 80 loss: 1077.0972904620762 grad: 150.36113586932566\n",
      "iteration: 80 loss: 1072.6547037617458 grad: 151.75740613409945\n",
      "iteration: 90 loss: 1224.0701364827355 grad: 136.3271854623725\n",
      "iteration: 90 loss: 1218.489498696328 grad: 137.16434177828393\n",
      "iteration: 80 loss: 1075.636012714336 grad: 149.65657622367786\n",
      "iteration: 90 loss: 1218.498372096538 grad: 136.0389545576421\n",
      "iteration: 80 loss: 1078.1400666891882 grad: 149.43968491965177\n",
      "iteration: 90 loss: 1215.9026705640056 grad: 140.008090570994\n",
      "iteration: 100 loss: 1369.354660654127 grad: 125.36320743328179\n",
      "iteration: 90 loss: 1215.063432479326 grad: 139.83675628911618\n",
      "iteration: 100 loss: 1363.4883682955383 grad: 126.21883368317813\n",
      "iteration: 100 loss: 1364.4147364244197 grad: 125.16869956538758\n",
      "iteration: 90 loss: 1210.1631886188961 grad: 141.7498486181961\n",
      "iteration: 90 loss: 1213.8786104342232 grad: 139.4234927219046\n",
      "iteration: 90 loss: 1216.385945876648 grad: 139.39559199906728\n",
      "iteration: 100 loss: 1359.2047485780163 grad: 128.7919603164887\n",
      "iteration: 110 loss: 1520.1105781579893 grad: 114.83623740717674\n",
      "iteration: 110 loss: 1514.3992319842114 grad: 115.10788779244989\n",
      "iteration: 100 loss: 1358.5456134812719 grad: 128.0304995028293\n",
      "iteration: 110 loss: 1515.8604235975354 grad: 115.07783383712922\n",
      "iteration: 100 loss: 1353.002472612427 grad: 130.61618306142304\n",
      "iteration: 110 loss: 1508.3517078463494 grad: 117.65933319585838\n",
      "iteration: 100 loss: 1357.4239595192455 grad: 128.41179378423843\n",
      "iteration: 100 loss: 1359.714772896288 grad: 128.30519449768207\n",
      "iteration: 120 loss: 1675.9623087203754 grad: 105.1697097929482\n",
      "iteration: 120 loss: 1670.783703839951 grad: 105.8339164210929\n",
      "iteration: 120 loss: 1672.1875407728676 grad: 106.10147344401675\n",
      "iteration: 120 loss: 1662.997616664677 grad: 108.02040807654052\n",
      "iteration: 110 loss: 1507.9922208438218 grad: 116.83316509293533\n",
      "iteration: 110 loss: 1501.3533711466869 grad: 120.19509598944562\n",
      "iteration: 110 loss: 1506.551129951269 grad: 117.2255094139704\n",
      "iteration: 130 loss: 1836.6541943817049 grad: 96.16158920597314\n",
      "iteration: 130 loss: 1831.8397075371533 grad: 97.03958415777986\n",
      "iteration: 110 loss: 1508.4866567742597 grad: 118.17362801936576\n",
      "iteration: 130 loss: 1832.969238407292 grad: 97.68991171479009\n",
      "iteration: 130 loss: 1822.4126056610312 grad: 99.06504873521695\n",
      "iteration: 120 loss: 1663.0906938294113 grad: 106.50310198534974\n",
      "iteration: 120 loss: 1654.8405370700655 grad: 110.77283945838911\n",
      "iteration: 140 loss: 2002.093112007479 grad: 87.42119613173074\n",
      "iteration: 120 loss: 1661.482329230678 grad: 106.51899213537766\n",
      "iteration: 140 loss: 1997.261647109278 grad: 89.06521360103017\n",
      "iteration: 140 loss: 1998.0844255371983 grad: 90.03000477110302\n",
      "iteration: 120 loss: 1662.1057008294765 grad: 109.17737604865792\n",
      "iteration: 140 loss: 1986.384910984943 grad: 90.97200063490158\n",
      "iteration: 130 loss: 1823.738619129195 grad: 96.82109084787317\n",
      "iteration: 130 loss: 1812.8771752690868 grad: 102.33001112429599\n",
      "iteration: 150 loss: 2172.2070118600454 grad: 79.0466025113484\n",
      "iteration: 130 loss: 1821.6911049325518 grad: 97.49070992828717\n",
      "iteration: 150 loss: 2166.66749055024 grad: 81.62649322144672\n",
      "iteration: 150 loss: 2166.8908375275946 grad: 83.13211752044568\n",
      "iteration: 130 loss: 1820.3485857823143 grad: 100.5284826364404\n",
      "iteration: 150 loss: 2154.2484874802403 grad: 83.76703941759153\n",
      "iteration: 160 loss: 2346.743809133298 grad: 71.25908766684424\n",
      "iteration: 140 loss: 1975.0455171254257 grad: 94.488939298828\n",
      "iteration: 140 loss: 1989.2564056681424 grad: 88.6470612970907\n",
      "iteration: 160 loss: 2339.9426557280526 grad: 74.50474376477119\n",
      "iteration: 160 loss: 2339.193184294177 grad: 76.43662812278093\n",
      "iteration: 140 loss: 1986.472968274295 grad: 89.09702558579858\n",
      "iteration: 140 loss: 1983.046599680614 grad: 92.25642350321877\n",
      "iteration: 160 loss: 2326.0008127463616 grad: 76.1917769376765\n",
      "iteration: 170 loss: 2525.3441869220796 grad: 64.52876944294962\n",
      "iteration: 170 loss: 2516.9837034222587 grad: 67.53243600687608\n",
      "iteration: 170 loss: 2514.566549885142 grad: 70.4457687402992\n",
      "iteration: 150 loss: 2159.025863475586 grad: 81.01504144898064\n",
      "iteration: 150 loss: 2141.238250860605 grad: 86.61870579220405\n",
      "iteration: 170 loss: 2501.7486938267652 grad: 69.27433014106578\n",
      "iteration: 150 loss: 2150.322573027521 grad: 83.92489481309252\n",
      "iteration: 150 loss: 2155.631692368854 grad: 81.10116257188491\n",
      "iteration: 180 loss: 2707.2341238740305 grad: 59.17577827377885\n",
      "iteration: 180 loss: 2697.5415979521395 grad: 61.538796362869654\n",
      "iteration: 180 loss: 2692.904710669786 grad: 64.37222764023949\n",
      "iteration: 160 loss: 2332.5567870794603 grad: 74.5234812591383\n",
      "iteration: 180 loss: 2681.1020064894615 grad: 62.87647818807173\n",
      "iteration: 160 loss: 2321.9120439790995 grad: 76.47931110290321\n",
      "iteration: 160 loss: 2311.490571680779 grad: 79.08191092356651\n",
      "iteration: 160 loss: 2328.8471593222007 grad: 73.6426551864192\n",
      "iteration: 190 loss: 2891.7553963146042 grad: 54.1855405309396\n",
      "iteration: 190 loss: 2881.016794421474 grad: 56.56974817743556\n",
      "iteration: 190 loss: 2874.2401498000368 grad: 58.67242701439318\n",
      "iteration: 170 loss: 2509.3288420841423 grad: 68.26341450838484\n",
      "iteration: 190 loss: 2863.599872206321 grad: 57.64126372629619\n",
      "iteration: 170 loss: 2497.0462324263244 grad: 70.25914919643046\n",
      "iteration: 170 loss: 2485.711845206079 grad: 71.49726226412642\n",
      "iteration: 200 loss: 3078.8071414988644 grad: 49.22661621538475\n",
      "iteration: 170 loss: 2506.0292429274136 grad: 66.26492714438984\n",
      "iteration: 200 loss: 3066.7960571243098 grad: 52.242249161274756\n",
      "iteration: 200 loss: 3058.521329782246 grad: 52.971090918451225\n",
      "iteration: 180 loss: 2689.3861401326044 grad: 62.07028615174435\n",
      "iteration: 200 loss: 3048.4858600096627 grad: 53.2366490932657\n",
      "iteration: 180 loss: 2675.0787347530163 grad: 64.52639949203332\n",
      "iteration: 180 loss: 2663.7735073332183 grad: 64.75197970113942\n",
      "iteration: 210 loss: 3268.3261558647014 grad: 44.174051417654496\n",
      "iteration: 180 loss: 2686.9399041970105 grad: 59.541938439613524\n",
      "iteration: 210 loss: 3254.7956145192206 grad: 47.83051265667409\n",
      "iteration: 210 loss: 3245.86966443636 grad: 47.359856577703944\n",
      "iteration: 190 loss: 2872.737045990823 grad: 55.83101166273162\n",
      "iteration: 210 loss: 3235.556680616975 grad: 48.65881948695908\n",
      "iteration: 190 loss: 2856.0373545559005 grad: 58.73592468439283\n",
      "iteration: 190 loss: 2845.1747615361996 grad: 58.859341067113455\n",
      "iteration: 220 loss: 3460.2538936360934 grad: 39.687094834498595\n",
      "iteration: 220 loss: 3444.914936704552 grad: 43.538724127360815\n",
      "iteration: 190 loss: 2871.202826422433 grad: 53.77605535594542\n",
      "iteration: 220 loss: 3435.8942865785602 grad: 42.71833323471366\n",
      "iteration: 200 loss: 3059.148191865987 grad: 50.433304434128644\n",
      "iteration: 220 loss: 3424.8852752095886 grad: 44.284355517548526\n",
      "iteration: 200 loss: 3039.6753087801076 grad: 53.72528010642743\n",
      "iteration: 200 loss: 3029.513552283113 grad: 53.342516993604136\n",
      "iteration: 230 loss: 3654.3618785064564 grad: 35.36428988175886\n",
      "iteration: 230 loss: 3637.195548964526 grad: 39.15571163522313\n",
      "iteration: 230 loss: 3628.19326674774 grad: 38.46215357904009\n",
      "iteration: 200 loss: 3058.4406067955833 grad: 48.26232556161118\n",
      "iteration: 210 loss: 3248.1640825436702 grad: 45.578321972099985\n",
      "iteration: 230 loss: 3616.3526995625866 grad: 40.16243031049082\n",
      "iteration: 210 loss: 3225.888136721367 grad: 48.457619137303965\n",
      "iteration: 240 loss: 3850.6698264812007 grad: 31.16174983564517\n",
      "iteration: 210 loss: 3216.6781229998437 grad: 48.11512530815068\n",
      "iteration: 240 loss: 3831.5648875253437 grad: 35.2117615090371\n",
      "iteration: 240 loss: 3822.5262573180817 grad: 34.58128403162372\n",
      "iteration: 210 loss: 3248.7679006279363 grad: 42.80686237504965\n",
      "iteration: 220 loss: 3439.641438863991 grad: 41.08275229751436\n",
      "iteration: 240 loss: 3809.9389514188224 grad: 36.21064298676573\n",
      "iteration: 220 loss: 3414.786718296401 grad: 43.446156898793724\n",
      "iteration: 220 loss: 3406.408396293276 grad: 43.485863289234246iteration: 250 loss: 4049.1375344974363 grad: 27.254481687290088\n",
      "\n",
      "iteration: 250 loss: 4027.7543291479296 grad: 31.882984686459032\n",
      "iteration: 250 loss: 4018.804995791439 grad: 30.731105331026605\n",
      "iteration: 220 loss: 3441.8509244803986 grad: 38.232208611162065\n",
      "iteration: 230 loss: 3633.2630087284397 grad: 37.05640757458667\n",
      "iteration: 250 loss: 4005.48726979296 grad: 32.63835463155134\n",
      "iteration: 230 loss: 3606.0861791671655 grad: 38.94624760054556\n",
      "iteration: 230 loss: 3598.4577831185943 grad: 39.08579845750127\n",
      "iteration: 260 loss: 4249.638215283769 grad: 23.476001765331958\n",
      "iteration: 260 loss: 4225.561402128533 grad: 28.410810716112195\n",
      "iteration: 260 loss: 4217.032880655369 grad: 26.871559473533402\n",
      "iteration: 230 loss: 3637.430506301142 grad: 33.82426551606598\n",
      "iteration: 240 loss: 3828.892318373663 grad: 33.29975806330187\n",
      "iteration: 260 loss: 4202.811489013826 grad: 29.303625581067443\n",
      "iteration: 240 loss: 3799.619247786128 grad: 34.82088705691717\n",
      "iteration: 240 loss: 3792.686602627437 grad: 34.85323561296704\n",
      "iteration: 270 loss: 4452.143888152261 grad: 19.912890155607943\n",
      "iteration: 270 loss: 4425.180497350261 grad: 24.676698884972275\n",
      "iteration: 270 loss: 4417.260162810367 grad: 23.218762373228433\n",
      "iteration: 250 loss: 4026.305638608606 grad: 30.09897057263375\n",
      "iteration: 270 loss: 4401.694999556243 grad: 26.208687557701506\n",
      "iteration: 240 loss: 3835.317046435306 grad: 30.098297135028602\n",
      "iteration: 250 loss: 3995.394622627557 grad: 30.34434741579729\n",
      "iteration: 250 loss: 3989.040263141873 grad: 31.06174576125738\n",
      "iteration: 280 loss: 4656.391819896347 grad: 16.798755964889818\n",
      "iteration: 280 loss: 4626.605738447786 grad: 21.264033849271733\n",
      "iteration: 280 loss: 4619.256459648516 grad: 20.227578047368866\n",
      "iteration: 280 loss: 4602.142533777742 grad: 22.90853965414926\n",
      "iteration: 260 loss: 4225.2930659172625 grad: 27.117856655355954\n",
      "iteration: 250 loss: 4035.0020630959266 grad: 27.172677073487147\n",
      "iteration: 260 loss: 4193.669543363645 grad: 25.788509763585875\n",
      "iteration: 260 loss: 4187.232880452926 grad: 27.83887066629785\n",
      "iteration: 290 loss: 4862.243808251304 grad: 13.919996336235133\n",
      "iteration: 290 loss: 4829.687773987617 grad: 18.303798256095472\n",
      "iteration: 290 loss: 4822.770909260108 grad: 17.498727283261214\n",
      "iteration: 270 loss: 4425.831862213663 grad: 23.91613264033404\n",
      "iteration: 290 loss: 4804.220306567552 grad: 19.7154903418774\n",
      "iteration: 270 loss: 4394.1389353838895 grad: 22.052263387427157\n",
      "iteration: 260 loss: 4236.202161305184 grad: 24.450352510197252\n",
      "iteration: 270 loss: 4387.123587227367 grad: 24.839489973975972\n",
      "iteration: 300 loss: 5069.410759819931 grad: 11.47305892679991\n",
      "iteration: 300 loss: 5034.252278002738 grad: 15.454100781060964\n",
      "iteration: 300 loss: 5027.611473671957 grad: 15.048810369735813\n",
      "iteration: 300 loss: 5007.853023792484 grad: 16.668382034794075\n",
      "iteration: 280 loss: 4628.047315138176 grad: 20.712542869708628\n",
      "iteration: 270 loss: 4438.9108454167035 grad: 21.614305186163854\n",
      "iteration: 280 loss: 4588.545090577886 grad: 21.98981041919728\n",
      "iteration: 280 loss: 4596.527071551347 grad: 18.656370829979856\n",
      "iteration: 310 loss: 5240.313914207963 grad: 12.695093319700858\n",
      "iteration: 310 loss: 5277.757662109864 grad: 9.181419319261796\n",
      "iteration: 310 loss: 5233.6188616025 grad: 12.774321010183485\n",
      "iteration: 310 loss: 5212.975705835309 grad: 13.858862471583503\n",
      "iteration: 290 loss: 4831.838083798989 grad: 17.72334895010936\n",
      "iteration: 280 loss: 4643.091002035577 grad: 19.0254733710794\n",
      "iteration: 290 loss: 4791.48338744525 grad: 19.00156038756458\n",
      "iteration: 320 loss: 5447.7282359563 grad: 10.088531984140385\n",
      "iteration: 320 loss: 5419.400007747587 grad: 11.517514692585952\n",
      "iteration: 320 loss: 5487.091058243856 grad: 7.2868516015743285\n",
      "iteration: 290 loss: 4800.630883075248 grad: 15.647271154282006\n",
      "iteration: 320 loss: 5440.694439274815 grad: 10.646781757167274\n",
      "iteration: 300 loss: 4996.037094600651 grad: 15.997865947618301\n",
      "iteration: 290 loss: 4848.485189976613 grad: 16.822214545641035\n",
      "iteration: 330 loss: 5656.405507447374 grad: 7.702397030073412\n",
      "iteration: 300 loss: 5037.151635878386 grad: 14.845699552060008\n",
      "iteration: 330 loss: 5626.992285676734 grad: 9.171828555971926\n",
      "iteration: 330 loss: 5697.359559802199 grad: 5.270175740970759\n",
      "iteration: 300 loss: 5006.182736172655 grad: 13.04696569726\n",
      "iteration: 330 loss: 5648.773660874995 grad: 8.512335421600195\n",
      "iteration: 310 loss: 5202.075886348724 grad: 13.401507783591239\n",
      "iteration: 300 loss: 5054.918571715869 grad: 14.74475125790343\n",
      "iteration: 340 loss: 5866.2127999451 grad: 5.580469234400521\n",
      "iteration: 310 loss: 5243.911031667851 grad: 12.274968943079188\n",
      "iteration: 340 loss: 5835.666040841207 grad: 7.22979581945922\n",
      "iteration: 340 loss: 5908.660010184445 grad: 3.151174149782821\n",
      "iteration: 340 loss: 5857.894142829728 grad: 6.407171726625967\n",
      "iteration: 310 loss: 5213.057602607204 grad: 10.641987096316804\n",
      "iteration: 320 loss: 5409.422207440074 grad: 10.883555436678481\n",
      "iteration: 310 loss: 5262.327468366164 grad: 12.641096116514042\n",
      "iteration: 350 loss: 6077.057552700881 grad: 3.5551605914643227\n",
      "iteration: 320 loss: 5451.914789317611 grad: 10.147266057778646\n",
      "iteration: 350 loss: 6045.239696009555 grad: 5.46162168630103\n",
      "iteration: 350 loss: 6120.9753648488795 grad: 1.1389347689087757\n",
      "iteration: 330 loss: 5617.96754139131 grad: 8.573465479817283\n",
      "iteration: 320 loss: 5470.7337860820435 grad: 10.374230425063255\n",
      "iteration: 320 loss: 5421.073514980838 grad: 8.467755972635203\n",
      "iteration: 350 loss: 6067.998885168722 grad: 4.3362961570768315\n",
      "iteration: 360 loss: 6288.885873069225 grad: 1.6821684324590782\n",
      "iteration: 360 loss: 6255.705170390956 grad: 3.6681703400586114\n",
      "iteration: 330 loss: 5660.959094299749 grad: 8.167881993395124\n",
      "iteration: 360 loss: 6334.14138357169 grad: -0.5422852742356614\n",
      "iteration: 340 loss: 5827.62219535176 grad: 6.413846406742655\n",
      "iteration: 330 loss: 5680.151536876778 grad: 8.168713896601284\n",
      "iteration: 330 loss: 5630.140905762999 grad: 6.3991434987853575\n",
      "iteration: 360 loss: 6279.131144303391 grad: 2.271296525210076\n",
      "iteration: 370 loss: 6501.572640121225 grad: 0.004398724662111775\n",
      "iteration: 340 loss: 5871.008084764525 grad: 6.172094862655863\n",
      "iteration: 370 loss: 6467.076766044448 grad: 1.8548430070774478\n",
      "iteration: 370 loss: 6548.061072177673 grad: -2.1442647996004123\n",
      "iteration: 350 loss: 6038.321922018848 grad: 4.432818264711424\n",
      "iteration: 340 loss: 5890.582032477169 grad: 6.039388206790806\n",
      "iteration: 340 loss: 5840.169098670428 grad: 4.27962955590392\n",
      "iteration: 370 loss: 6491.266101312465 grad: 0.4265169526274546\n",
      "iteration: 380 loss: 6715.00840499674 grad: -1.5196766229842957\n",
      "iteration: 350 loss: 6081.987850440455 grad: 4.307385808828993\n",
      "iteration: 380 loss: 6679.348672006121 grad: 0.12473715786504869\n",
      "iteration: 380 loss: 6762.718855607176 grad: -3.7396618945925333\n",
      "iteration: 350 loss: 6101.88414438627 grad: 4.27747668724423\n",
      "iteration: 360 loss: 6250.047126599029 grad: 2.4669958240438676\n",
      "iteration: 350 loss: 6051.295853065214 grad: 1.9865279613007036\n",
      "iteration: 380 loss: 6704.198894961101 grad: -1.0804029470032024\n",
      "iteration: 390 loss: 6929.133786458942 grad: -2.930779478435543\n",
      "iteration: 390 loss: 6892.459172916013 grad: -1.5558666047616247\n",
      "iteration: 360 loss: 6293.834968273606 grad: 2.504168744620788\n",
      "iteration: 390 loss: 6978.077983275871 grad: -5.170324367932597\n",
      "iteration: 370 loss: 6462.614935596124 grad: 0.8805337791639909\n",
      "iteration: 360 loss: 6313.985198746968 grad: 2.594780843882768\n",
      "iteration: 390 loss: 6917.848799690326 grad: -2.676591663725075\n",
      "iteration: 360 loss: 6263.458697519703 grad: -0.0054805245946445424\n",
      "iteration: 400 loss: 7143.841066501053 grad: -4.208936179087392\n",
      "iteration: 400 loss: 7106.273378410532 grad: -2.9746736560710545\n",
      "iteration: 370 loss: 6506.490863503684 grad: 0.9291338709136268\n",
      "iteration: 400 loss: 7194.0434397147965 grad: -6.526759805561131\n",
      "iteration: 380 loss: 6675.956044632523 grad: -0.7657132069908714\n",
      "iteration: 370 loss: 6526.858133267064 grad: 1.0581128235061872\n",
      "iteration: 400 loss: 7132.290718822873 grad: -4.355732333625507\n",
      "iteration: 370 loss: 6476.5107982986065 grad: -1.7939769964175634\n",
      "iteration: 410 loss: 7320.7559916811815 grad: -4.42222998326747\n",
      "iteration: 410 loss: 7359.078532031278 grad: -5.467439360654675\n",
      "iteration: 380 loss: 6719.849933176017 grad: -0.5436009140792706\n",
      "iteration: 410 loss: 7410.667304366168 grad: -7.971745270908885\n",
      "iteration: 390 loss: 6890.145997810278 grad: -2.4355786999552222\n",
      "iteration: 380 loss: 6740.439657205223 grad: -0.36472334568301257\n",
      "iteration: 410 loss: 7347.547586004002 grad: -6.0323335049507065\n",
      "iteration: 380 loss: 6690.518344194605 grad: -3.799483400141\n",
      "iteration: 420 loss: 7535.881349175413 grad: -5.803504805345007\n",
      "iteration: 390 loss: 6933.902291790139 grad: -2.0699417389055705\n",
      "iteration: 420 loss: 7574.84879651991 grad: -6.736171990768843\n",
      "iteration: 420 loss: 7627.922619859347 grad: -9.327064190501535\n",
      "iteration: 400 loss: 7105.179426496848 grad: -3.987549041727876\n",
      "iteration: 390 loss: 6954.629780880442 grad: -1.599683750518416\n",
      "iteration: 420 loss: 7563.525966517308 grad: -7.443090323366443\n",
      "iteration: 390 loss: 6905.49061644292 grad: -5.468863402639873\n",
      "iteration: 430 loss: 7751.672949647441 grad: -7.34575210223588\n",
      "iteration: 400 loss: 7148.619178906329 grad: -3.4540309631262405\n",
      "iteration: 430 loss: 7845.740953458969 grad: -10.566729467661713\n",
      "iteration: 430 loss: 7791.183136526888 grad: -8.01850643040821\n",
      "iteration: 410 loss: 7320.915884941211 grad: -5.253854393976104\n",
      "iteration: 400 loss: 7169.395308277285 grad: -2.906238196651411\n",
      "iteration: 430 loss: 7780.088728774203 grad: -8.691580682873603\n",
      "iteration: 440 loss: 7968.160849258967 grad: -8.809605746840216\n",
      "iteration: 410 loss: 7363.930137924651 grad: -4.726463492503431\n",
      "iteration: 400 loss: 7121.184322510182 grad: -6.966474895378205\n",
      "iteration: 440 loss: 8064.124177639204 grad: -11.816029024486363\n",
      "iteration: 440 loss: 8008.1104727396205 grad: -9.349628175712866\n",
      "iteration: 420 loss: 7537.214492257693 grad: -6.361911615950724\n",
      "iteration: 410 loss: 7384.794865350566 grad: -4.281136388064882\n",
      "iteration: 440 loss: 7997.227080251854 grad: -10.00140265371219\n",
      "iteration: 450 loss: 8185.271664826125 grad: -10.158137136089806\n",
      "iteration: 420 loss: 7579.814521077424 grad: -6.0060458991602\n",
      "iteration: 410 loss: 7337.585961019397 grad: -8.49667792422554\n",
      "iteration: 450 loss: 8283.108246621101 grad: -13.086813257690054\n",
      "iteration: 430 loss: 7754.011130111512 grad: -7.492293821912901\n",
      "iteration: 420 loss: 7600.867350527616 grad: -5.647794389075834\n",
      "iteration: 450 loss: 8225.643291380247 grad: -10.6381351478644\n",
      "iteration: 450 loss: 8214.998584981708 grad: -11.381856937382187\n",
      "iteration: 460 loss: 8403.005073404507 grad: -11.54165797365594\n",
      "iteration: 430 loss: 7796.251671857299 grad: -7.194888799189222\n",
      "iteration: 420 loss: 7554.733997463175 grad: -10.075893788713168\n",
      "iteration: 460 loss: 8502.695705710175 grad: -14.299052556377651\n",
      "iteration: 440 loss: 7971.403285850407 grad: -8.83675113876366\n",
      "iteration: 430 loss: 7817.563633214557 grad: -6.862132029669324\n",
      "iteration: 460 loss: 8443.74697959336 grad: -11.816867789846874\n",
      "iteration: 460 loss: 8433.41971490326 grad: -12.703487382470769\n",
      "iteration: 430 loss: 7772.636134748858 grad: -11.614819815214489\n",
      "iteration: 440 loss: 8013.242936094895 grad: -8.498848514089936\n",
      "iteration: 470 loss: 8621.371308688438 grad: -12.929656157927411\n",
      "iteration: 450 loss: 8189.403952712189 grad: -10.097067699545686\n",
      "iteration: 470 loss: 8722.796807615949 grad: -15.343551471022382\n",
      "iteration: 440 loss: 8034.746444627231 grad: -7.949245103971783\n",
      "iteration: 470 loss: 8652.443130069289 grad: -13.993304104116568\n",
      "iteration: 470 loss: 8662.371685364851 grad: -12.953549991573286\n",
      "iteration: 440 loss: 7991.245128903491 grad: -12.957637349340866\n",
      "iteration: 450 loss: 8230.807508962911 grad: -9.721624885232158\n",
      "iteration: 480 loss: 8840.35382567349 grad: -14.288797823406936\n",
      "iteration: 460 loss: 8407.916811985642 grad: -11.17777016954303\n",
      "iteration: 480 loss: 8943.372151663325 grad: -16.378869113497434\n",
      "iteration: 450 loss: 8252.385849835677 grad: -9.02160362723055\n",
      "iteration: 480 loss: 8871.981008263087 grad: -15.102150715796283\n",
      "iteration: 480 loss: 8881.514293846372 grad: -14.066298486190908\n",
      "iteration: 460 loss: 8448.88232033196 grad: -10.892645529293537\n",
      "iteration: 450 loss: 8210.473797941493 grad: -14.173149527283227\n",
      "iteration: 490 loss: 9059.949687354061 grad: -15.563755701054877\n",
      "iteration: 470 loss: 8626.893411666471 grad: -12.217243141216613\n",
      "iteration: 490 loss: 9164.38912362964 grad: -17.434864835307806\n",
      "iteration: 460 loss: 8470.4427438352 grad: -10.038410419384444\n",
      "iteration: 490 loss: 9091.942587701069 grad: -16.14108799037617\n",
      "iteration: 490 loss: 9101.131530942925 grad: -15.099705740992457\n",
      "iteration: 500 loss: 9280.08472349199 grad: -16.61761595170314\n",
      "iteration: 480 loss: 8846.32148847785 grad: -13.192651771665165\n",
      "iteration: 500 loss: 9385.876603372775 grad: -18.55919379752576\n",
      "iteration: 470 loss: 8667.465659481255 grad: -12.132418484339631\n",
      "iteration: 460 loss: 8430.280954139447 grad: -15.379510991537979\n",
      "iteration: 470 loss: 8688.930839377814 grad: -11.167933778950623\n",
      "iteration: 500 loss: 9312.375413544041 grad: -17.329694567852563\n",
      "iteration: 500 loss: 9321.235193415318 grad: -16.15377808968286\n",
      "iteration: 490 loss: 9066.169734453788 grad: -14.147158075611067\n",
      "iteration: 510 loss: 9500.695121735942 grad: -17.64758381534324\n",
      "iteration: 510 loss: 9607.851694714625 grad: -19.74141787138735\n",
      "iteration: 480 loss: 8886.625836588979 grad: -13.345169973506328\n",
      "iteration: 470 loss: 8650.605222133387 grad: -16.384280119174523\n",
      "iteration: 480 loss: 8907.872125011423 grad: -12.24072028401476\n",
      "iteration: 510 loss: 9533.321656491304 grad: -18.58710202573962\n",
      "iteration: 510 loss: 9541.815203028886 grad: -17.246563389275465\n",
      "iteration: 500 loss: 9286.413112734203 grad: -15.060314885772765\n",
      "iteration: 520 loss: 9721.825073554586 grad: -18.749647343118145\n",
      "iteration: 520 loss: 9830.34036359088 grad: -20.949534044310766\n",
      "iteration: 490 loss: 9127.196233605177 grad: -13.168687738094789\n",
      "iteration: 480 loss: 8871.35805087232 grad: -17.27988233830458\n",
      "iteration: 490 loss: 9106.269502419666 grad: -14.390525012448714\n",
      "iteration: 520 loss: 9754.744800757304 grad: -19.651571523810937\n",
      "iteration: 520 loss: 9762.889630660608 grad: -18.42539390084882iteration: 510 loss: 9507.05183269346 grad: -16.0213165326638\n",
      "\n",
      "iteration: 530 loss: 9943.429405586321 grad: -19.727590961237986\n",
      "iteration: 530 loss: 10053.33995212011 grad: -22.090995262897387\n",
      "iteration: 500 loss: 9346.84428220542 grad: -14.100306122038909\n",
      "iteration: 490 loss: 9092.50130627942 grad: -18.12975296381819\n",
      "iteration: 500 loss: 9326.342278675176 grad: -15.372382937987057\n",
      "iteration: 530 loss: 9976.562973254 grad: -20.54375985658121\n",
      "iteration: 540 loss: 10165.437299069734 grad: -20.68307296905855\n",
      "iteration: 540 loss: 10276.768721797302 grad: -23.036938050945093\n",
      "iteration: 510 loss: 9566.835637254588 grad: -15.036436937049146\n",
      "iteration: 520 loss: 9728.098704734748 grad: -17.008004314040086\n",
      "iteration: 530 loss: 9984.39296377615 grad: -19.400625366753737\n",
      "iteration: 500 loss: 9314.031185018255 grad: -19.12123941232748\n",
      "iteration: 510 loss: 9546.82841320513 grad: -16.40610357030817\n",
      "iteration: 540 loss: 10198.741102763319 grad: -21.42024156945738\n",
      "iteration: 550 loss: 10500.532431013757 grad: -23.90312268344227\n",
      "iteration: 550 loss: 10387.846487964169 grad: -21.62356212439499\n",
      "iteration: 520 loss: 9787.183771722119 grad: -16.032641629542944\n",
      "iteration: 530 loss: 9949.518374039673 grad: -17.932629487765887\n",
      "iteration: 510 loss: 9535.999504537765 grad: -20.112053730329173\n",
      "iteration: 540 loss: 10206.281522539643 grad: -20.389499630743753\n",
      "iteration: 520 loss: 9767.747314414873 grad: -17.37424355798982\n",
      "iteration: 550 loss: 10421.281081018664 grad: -22.31324441031574\n",
      "iteration: 560 loss: 10724.653282012272 grad: -24.860202179401192\n",
      "iteration: 560 loss: 10610.68277810407 grad: -22.69190445962269\n",
      "iteration: 530 loss: 10007.903625888546 grad: -17.034748230651815\n",
      "iteration: 540 loss: 10171.277615104056 grad: -18.85078727567365\n",
      "iteration: 520 loss: 9758.387094535132 grad: -21.054576523831575\n",
      "iteration: 530 loss: 9989.031538657508 grad: -18.207362973017606\n",
      "iteration: 550 loss: 10428.599005977941 grad: -21.448647436143656\n",
      "iteration: 560 loss: 10644.222543143938 grad: -23.33275497175164\n",
      "iteration: 570 loss: 10949.210024177533 grad: -25.89264477401509\n",
      "iteration: 540 loss: 10229.048525233673 grad: -18.09886793989439\n",
      "iteration: 550 loss: 10393.382833463444 grad: -19.81382397960019\n",
      "iteration: 570 loss: 10834.02288337541 grad: -23.803178137018968\n",
      "iteration: 530 loss: 9981.184307714684 grad: -22.054019785597752\n",
      "iteration: 540 loss: 10210.629384092992 grad: -18.994949151487297\n",
      "iteration: 570 loss: 10867.606699621863 grad: -24.426722437787674\n",
      "iteration: 560 loss: 10651.380922172737 grad: -22.43402706466513\n",
      "iteration: 580 loss: 11174.160473314807 grad: -26.838372911528943\n",
      "iteration: 550 loss: 10450.592322561022 grad: -19.042195420227834\n",
      "iteration: 580 loss: 11057.80640684189 grad: -24.73437481037527\n",
      "iteration: 540 loss: 10204.39605178792 grad: -23.02390303353163\n",
      "iteration: 560 loss: 10615.865818227829 grad: -20.779061809386988\n",
      "iteration: 550 loss: 10432.516227203594 grad: -19.75930975345883\n",
      "iteration: 580 loss: 11091.431531586668 grad: -25.54856434789245\n",
      "iteration: 570 loss: 10874.536632273199 grad: -23.229243157253237\n",
      "iteration: 590 loss: 11399.456194949966 grad: -27.75042380557381\n",
      "iteration: 560 loss: 10672.465607375918 grad: -19.906029305050115\n",
      "iteration: 590 loss: 11281.950243238802 grad: -25.6391500179345\n",
      "iteration: 550 loss: 10428.056176679853 grad: -24.058222969542165\n",
      "iteration: 560 loss: 10654.709082866864 grad: -20.69392135446827\n",
      "iteration: 570 loss: 10838.761600483658 grad: -21.806254148362687\n",
      "iteration: 590 loss: 11315.663976667449 grad: -26.594119521607524\n",
      "iteration: 580 loss: 11097.996691971171 grad: -24.00628034774831\n",
      "iteration: 600 loss: 11625.116357533423 grad: -28.666074495536023\n",
      "iteration: 570 loss: 10894.65908404799 grad: -20.733454242117947\n",
      "iteration: 600 loss: 11506.506854489317 grad: -26.62526963395102\n",
      "iteration: 560 loss: 10652.134508024476 grad: -24.96189074732205\n",
      "iteration: 570 loss: 10877.292431577902 grad: -21.732456095659874\n",
      "iteration: 600 loss: 11540.332284479315 grad: -27.63977819784156\n",
      "iteration: 580 loss: 11062.070778162182 grad: -22.739118992452898\n",
      "iteration: 590 loss: 11321.760418593418 grad: -24.805015672943853\n",
      "iteration: 610 loss: 11851.13921639355 grad: -29.510029130417934\n",
      "iteration: 610 loss: 11731.485189979534 grad: -27.483526624555513\n",
      "iteration: 580 loss: 11117.165516441546 grad: -21.52580038963335\n",
      "iteration: 570 loss: 10876.602091813687 grad: -25.928157827101362\n",
      "iteration: 580 loss: 11100.250040906123 grad: -22.70657779943999\n",
      "iteration: 610 loss: 11765.404358174965 grad: -28.588223800464597\n",
      "iteration: 590 loss: 11285.73312948355 grad: -23.581782131879905\n",
      "iteration: 600 loss: 11545.844877088342 grad: -25.648959246771426\n",
      "iteration: 620 loss: 12077.485929074985 grad: -30.30734908611395\n",
      "iteration: 620 loss: 11956.793814110493 grad: -28.195063578820985\n",
      "iteration: 590 loss: 11339.967147273586 grad: -22.272660950194236\n",
      "iteration: 580 loss: 11101.469526396908 grad: -26.832828279483277\n",
      "iteration: 590 loss: 11323.586621788929 grad: -23.730675578058293\n",
      "iteration: 620 loss: 11990.844618973242 grad: -29.534190265603655\n",
      "iteration: 610 loss: 11770.264199699517 grad: -26.44481597627111\n",
      "iteration: 600 loss: 11509.727784703933 grad: -24.48186206191799\n",
      "iteration: 630 loss: 12304.144411163124 grad: -31.150763704216573iteration: 630 loss: 12182.405610761472 grad: -28.95344117241973\n",
      "\n",
      "iteration: 600 loss: 11563.069750101275 grad: -23.065544499308388\n",
      "iteration: 590 loss: 11326.660990124734 grad: -27.595520720037413\n",
      "iteration: 600 loss: 11547.366757111315 grad: -24.82960442588563\n",
      "iteration: 630 loss: 12216.678311834401 grad: -30.450463066915127\n",
      "iteration: 620 loss: 11995.000950416768 grad: -27.23992341530996\n",
      "iteration: 610 loss: 11734.049474571659 grad: -25.292005745300635\n",
      "iteration: 640 loss: 12531.120449044352 grad: -31.973498613887628\n",
      "iteration: 640 loss: 12408.372172240555 grad: -29.854010802341712\n",
      "iteration: 610 loss: 11786.538638627546 grad: -24.0459811152438\n",
      "iteration: 600 loss: 11552.169144464184 grad: -28.45474040211181\n",
      "iteration: 640 loss: 12442.863992006993 grad: -31.243569046850098\n",
      "iteration: 610 loss: 11771.589630228736 grad: -25.79027792172144\n",
      "iteration: 620 loss: 11958.65353822834 grad: -26.01072016317103\n",
      "iteration: 630 loss: 12220.020664006244 grad: -27.929746752785093\n",
      "iteration: 650 loss: 12758.410163792109 grad: -32.71043823028192\n",
      "iteration: 650 loss: 12634.70292622117 grad: -30.64314557028562\n",
      "iteration: 620 loss: 12010.410721939339 grad: -24.959196257193874\n",
      "iteration: 610 loss: 11778.043535330064 grad: -29.311251606517825\n",
      "iteration: 620 loss: 11996.170008417956 grad: -26.666315718088413\n",
      "iteration: 650 loss: 12669.387557098904 grad: -31.99467161957834\n",
      "iteration: 660 loss: 12861.357994107706 grad: -31.367800192717127\n",
      "iteration: 640 loss: 12445.262403467734 grad: -28.56611081843151\n",
      "iteration: 660 loss: 12985.979617346211 grad: -33.40960495455695\n",
      "iteration: 630 loss: 12183.543983427395 grad: -26.752219583255563\n",
      "iteration: 630 loss: 12234.593356860933 grad: -25.717901273847072\n",
      "iteration: 620 loss: 12004.217186458238 grad: -29.968986312025724\n",
      "iteration: 660 loss: 12896.22900684691 grad: -32.69565901482533\n",
      "iteration: 630 loss: 12221.085041390726 grad: -27.52897116748784\n",
      "iteration: 670 loss: 13088.308967235127 grad: -32.12502951324484\n",
      "iteration: 650 loss: 12670.750857842044 grad: -29.30794395405586\n",
      "iteration: 670 loss: 13213.809345610083 grad: -34.09149328092368\n",
      "iteration: 640 loss: 12459.065905858562 grad: -26.526038862628628\n",
      "iteration: 640 loss: 12408.731729535504 grad: -27.48739471911192\n",
      "iteration: 630 loss: 12230.665998154842 grad: -30.747675722793506\n",
      "iteration: 670 loss: 13123.382575827349 grad: -33.407449035482564\n",
      "iteration: 640 loss: 12446.316065429344 grad: -28.377367175524473\n",
      "iteration: 660 loss: 12896.575431843228 grad: -30.243728987688378\n",
      "iteration: 680 loss: 13315.543143676421 grad: -32.840685825235994\n",
      "iteration: 650 loss: 12683.862543556454 grad: -27.34672530972827\n",
      "iteration: 640 loss: 12457.439345516965 grad: -31.584412392818905\n",
      "iteration: 680 loss: 13441.88361453789 grad: -34.72549412829818\n",
      "iteration: 650 loss: 12634.217549866979 grad: -28.193119469830318\n",
      "iteration: 680 loss: 13350.890534340477 grad: -34.19649604701476\n",
      "iteration: 650 loss: 12671.892352261702 grad: -29.315219454425666\n",
      "iteration: 670 loss: 13122.776741024842 grad: -31.161459152268343\n",
      "iteration: 690 loss: 13543.047571181505 grad: -33.525840356200206\n",
      "iteration: 660 loss: 12908.975580183533 grad: -28.11022109820459\n",
      "iteration: 650 loss: 12684.528946789984 grad: -32.3853799511229\n",
      "iteration: 690 loss: 13670.194484536123 grad: -35.34386800588124\n",
      "iteration: 660 loss: 12859.982619149714 grad: -28.851869562804602\n",
      "iteration: 690 loss: 13578.73061830337 grad: -34.87814818671549\n",
      "iteration: 660 loss: 12897.845578750419 grad: -30.16461771779276\n",
      "iteration: 680 loss: 13349.293969185923 grad: -31.91357134572974\n",
      "iteration: 700 loss: 13770.862606416618 grad: -34.28742784145355\n",
      "iteration: 670 loss: 13134.35608461885 grad: -28.75432836467094\n",
      "iteration: 660 loss: 12911.907736378793 grad: -33.085180202754394\n",
      "iteration: 700 loss: 13898.77018617223 grad: -36.04423842905767\n",
      "iteration: 670 loss: 13086.004503062712 grad: -29.522165010265525\n",
      "iteration: 700 loss: 13806.849398888375 grad: -35.50366631244097\n",
      "iteration: 670 loss: 13124.070650412683 grad: -30.74038471193819\n",
      "iteration: 690 loss: 13576.0713481008 grad: -32.57902709116108\n",
      "iteration: 710 loss: 13998.992153390463 grad: -34.959284165394216\n",
      "iteration: 680 loss: 13359.969812478259 grad: -29.41611517022897\n",
      "iteration: 670 loss: 13139.503832001556 grad: -33.64829590081874\n",
      "iteration: 680 loss: 13312.306033525769 grad: -30.269364749120697\n",
      "iteration: 710 loss: 14127.67535831928 grad: -36.74775200396682\n",
      "iteration: 710 loss: 14035.208319200854 grad: -36.047522297786365\n",
      "iteration: 680 loss: 13350.493258764733 grad: -31.282573825826677\n",
      "iteration: 700 loss: 13803.137385497163 grad: -33.34985105656929\n",
      "iteration: 720 loss: 14227.405934938439 grad: -35.60158864186161\n",
      "iteration: 690 loss: 13585.840962605296 grad: -30.096395756473356\n",
      "iteration: 680 loss: 13367.334482843486 grad: -34.354488838014326\n",
      "iteration: 690 loss: 13538.870909377627 grad: -30.958783591328952\n",
      "iteration: 720 loss: 14356.863884190845 grad: -37.354232635368874\n",
      "iteration: 720 loss: 14263.808238212596 grad: -36.649880157854724\n",
      "iteration: 710 loss: 14030.568144938778 grad: -34.21044670938855\n",
      "iteration: 730 loss: 14456.12605021095 grad: -36.34182428112593\n",
      "iteration: 700 loss: 13811.970133555593 grad: -30.739685967483567\n",
      "iteration: 690 loss: 13595.480170789855 grad: -35.1428984358689\n",
      "iteration: 690 loss: 13577.138054472074 grad: -31.9010792284474\n",
      "iteration: 700 loss: 13765.660575689368 grad: -31.54397319906302\n",
      "iteration: 730 loss: 14492.721622168294 grad: -37.402336527741156\n",
      "iteration: 730 loss: 14586.29255086951 grad: -37.91424719908306\n",
      "iteration: 720 loss: 14258.30895242645 grad: -34.90116973246299\n",
      "iteration: 740 loss: 14685.159544388802 grad: -37.07120548913912\n",
      "iteration: 710 loss: 14038.343334833577 grad: -31.339674448612435\n",
      "iteration: 700 loss: 13823.941065142673 grad: -35.94612010620261\n",
      "iteration: 700 loss: 13804.049742100467 grad: -32.556041378697884\n",
      "iteration: 710 loss: 13992.645503954587 grad: -32.08253059264179\n",
      "iteration: 740 loss: 14721.998062192743 grad: -38.18754320118012\n",
      "iteration: 740 loss: 14815.977771120679 grad: -38.568329375623904\n",
      "iteration: 730 loss: 14486.334996665713 grad: -35.622558655770575\n",
      "iteration: 750 loss: 14914.491841249612 grad: -37.70600064760524\n",
      "iteration: 720 loss: 14264.943283214016 grad: -31.884911349913203\n",
      "iteration: 710 loss: 14052.758083283601 grad: -36.808312165589015\n",
      "iteration: 710 loss: 14031.226030362259 grad: -33.220841923140846\n",
      "iteration: 720 loss: 14219.835685491647 grad: -32.62449263744451\n",
      "iteration: 750 loss: 15045.92992342025 grad: -39.1409567495775\n",
      "iteration: 740 loss: 14714.643988159627 grad: -36.29098881914586\n",
      "iteration: 760 loss: 15144.062018043607 grad: -38.259806865334525\n",
      "iteration: 730 loss: 14491.76042790089 grad: -32.42013621342527\n",
      "iteration: 720 loss: 14281.912471102898 grad: -37.55677633180339\n",
      "iteration: 750 loss: 14951.574183128952 grad: -38.84967298029028\n",
      "iteration: 720 loss: 14258.669804444799 grad: -33.96764304290139\n",
      "iteration: 760 loss: 15276.105636817822 grad: -39.65861084161976\n",
      "iteration: 730 loss: 14447.235871500183 grad: -33.157753080491645\n",
      "iteration: 750 loss: 14943.209179073448 grad: -36.956333961603555\n",
      "iteration: 770 loss: 15373.829334979628 grad: -38.808523229634154\n",
      "iteration: 740 loss: 14718.788058919035 grad: -32.944282473770556\n",
      "iteration: 730 loss: 14511.34597485982 grad: -38.20168492299629\n",
      "iteration: 760 loss: 15181.383382777845 grad: -39.44314943478089\n",
      "iteration: 730 loss: 14486.44539783905 grad: -34.78996119471914\n",
      "iteration: 740 loss: 14674.84951527537 grad: -33.72606127031993\n",
      "iteration: 770 loss: 15506.50321082035 grad: -40.203278585822545\n",
      "iteration: 760 loss: 15172.057701761265 grad: -37.76146602404337\n",
      "iteration: 780 loss: 15603.803272801648 grad: -39.35160811233581\n",
      "iteration: 750 loss: 14946.029721625579 grad: -33.5139700305983\n",
      "iteration: 740 loss: 14741.004461707602 grad: -38.735122577066335\n",
      "iteration: 740 loss: 14714.560270586402 grad: -35.60900097862282\n",
      "iteration: 770 loss: 15411.45889611578 grad: -40.119143206088594\n",
      "iteration: 750 loss: 14902.659169913075 grad: -34.219844751474184\n",
      "iteration: 780 loss: 15737.139966623801 grad: -40.773369214857695\n",
      "iteration: 770 loss: 15401.22948306088 grad: -38.55062304214134\n",
      "iteration: 790 loss: 15833.96449945822 grad: -39.803558039018995\n",
      "iteration: 760 loss: 15173.524295521815 grad: -34.14407368133679\n",
      "iteration: 750 loss: 14970.888884153786 grad: -39.328259067414464\n",
      "iteration: 750 loss: 14942.98765159813 grad: -36.28720343273105\n",
      "iteration: 780 loss: 15641.777183726708 grad: -40.71695615184977\n",
      "iteration: 760 loss: 15130.648477606132 grad: -34.747938008655076\n",
      "iteration: 790 loss: 15967.99401442402 grad: -41.34565765898182\n",
      "iteration: 800 loss: 16064.307546001588 grad: -40.28561006313525\n",
      "iteration: 780 loss: 15630.679061076915 grad: -39.21730870572296\n",
      "iteration: 760 loss: 15200.999089335373 grad: -39.90351273018142\n",
      "iteration: 770 loss: 15401.296080164257 grad: -34.76621971093001\n",
      "iteration: 760 loss: 15171.696518074401 grad: -36.96880050904562\n",
      "iteration: 790 loss: 15872.297186082666 grad: -41.25501171224336\n",
      "iteration: 770 loss: 15358.870276057118 grad: -35.43961911778254\n",
      "iteration: 800 loss: 16199.039275872008 grad: -41.86717511092173\n",
      "iteration: 810 loss: 16294.870816153536 grad: -40.81892962964574\n",
      "iteration: 790 loss: 15860.374682904167 grad: -39.78621006615651\n",
      "iteration: 770 loss: 15431.348517698903 grad: -40.55003096905445\n",
      "iteration: 780 loss: 15629.32769641676 grad: -35.412887615494526\n",
      "iteration: 770 loss: 15400.722190530192 grad: -37.64037967409641\n",
      "iteration: 800 loss: 16103.000839295804 grad: -41.74286579183959\n",
      "iteration: 780 loss: 15587.39792739611 grad: -36.23525008937377\n",
      "iteration: 810 loss: 16430.279825115405 grad: -42.41077616159157\n",
      "iteration: 820 loss: 16525.65475880637 grad: -41.29881541427277\n",
      "iteration: 800 loss: 16090.29654815825 grad: -40.385654856813524\n",
      "iteration: 780 loss: 15661.944645826761 grad: -41.09525242705975\n",
      "iteration: 790 loss: 15857.616419702612 grad: -36.025480870848696\n",
      "iteration: 780 loss: 15630.016574973451 grad: -38.21710448680362\n",
      "iteration: 790 loss: 15816.244427435613 grad: -37.02030566073833\n",
      "iteration: 820 loss: 16661.71615438655 grad: -42.92152513782011\n",
      "iteration: 830 loss: 16756.63306737416 grad: -41.714674700845805\n",
      "iteration: 790 loss: 15892.740334057611 grad: -41.61615232753772\n",
      "iteration: 810 loss: 16320.492086324803 grad: -41.07753398846157\n",
      "iteration: 800 loss: 16086.11560974665 grad: -36.519399876299175\n",
      "iteration: 810 loss: 16333.864341771277 grad: -42.19165203450771\n",
      "iteration: 790 loss: 15859.531220285602 grad: -38.754514111219954\n",
      "iteration: 800 loss: 16045.389367791686 grad: -37.740240405324066\n",
      "iteration: 840 loss: 16987.77236787214 grad: -42.13658671237488\n",
      "iteration: 800 loss: 16123.757011136506 grad: -42.21861376665604\n",
      "iteration: 830 loss: 16893.313629241402 grad: -43.36496752908863\n",
      "iteration: 820 loss: 16550.96470829051 grad: -41.74296300578469\n",
      "iteration: 810 loss: 16314.784475122462 grad: -36.975208417165476\n",
      "iteration: 820 loss: 16564.897365685 grad: -42.719026797433564\n",
      "iteration: 800 loss: 16089.262193335244 grad: -39.308496058341916\n",
      "iteration: 810 loss: 16274.81710495632 grad: -38.431850670293215\n",
      "iteration: 850 loss: 17219.11730416701 grad: -42.73106673457939\n",
      "iteration: 810 loss: 16355.020746968185 grad: -42.82222626296225\n",
      "iteration: 840 loss: 17125.07162027041 grad: -43.88571455092942\n",
      "iteration: 830 loss: 16781.734432423345 grad: -42.445723040691604\n",
      "iteration: 820 loss: 16543.619741730323 grad: -37.463673016816955\n",
      "iteration: 830 loss: 16796.13841382478 grad: -43.26039521720597\n",
      "iteration: 810 loss: 16319.235215879848 grad: -39.902124361362624\n",
      "iteration: 820 loss: 16586.495522482026 grad: -43.355873597717284\n",
      "iteration: 860 loss: 17450.70707119733 grad: -43.27122386285633\n",
      "iteration: 820 loss: 16504.49420750856 grad: -39.019501328231385\n",
      "iteration: 830 loss: 16772.646779587252 grad: -37.97897821475206\n",
      "iteration: 840 loss: 17012.808010845536 grad: -43.06652591232519\n",
      "iteration: 850 loss: 17357.035934770567 grad: -44.477398489952186\n",
      "iteration: 840 loss: 17027.562897605174 grad: -43.70788154074293\n",
      "iteration: 820 loss: 16549.4301007399 grad: -40.42042225120818\n",
      "iteration: 830 loss: 16818.15364425636 grad: -43.866750331826196\n",
      "iteration: 830 loss: 16734.397968345846 grad: -39.605600323731295\n",
      "iteration: 840 loss: 17001.860073629534 grad: -38.4131253162648\n",
      "iteration: 870 loss: 17682.488157231466 grad: -43.72162296308524\n",
      "iteration: 860 loss: 17589.208181669404 grad: -45.00913979547132\n",
      "iteration: 850 loss: 17244.120348821038 grad: -43.58865516256962\n",
      "iteration: 850 loss: 17259.144350733844 grad: -44.11149202819503\n",
      "iteration: 830 loss: 16779.834326977543 grad: -40.94506354456986\n",
      "iteration: 840 loss: 17050.0120779211 grad: -44.4009702980362\n",
      "iteration: 850 loss: 17231.25284735343 grad: -38.871999418425844\n",
      "iteration: 840 loss: 16964.5513200927 grad: -40.207223203208486\n",
      "iteration: 880 loss: 17914.433520619732 grad: -44.16896213684835\n",
      "iteration: 870 loss: 17821.56594655148 grad: -45.4747457331996\n",
      "iteration: 860 loss: 17475.644152108543 grad: -44.116823800521345\n",
      "iteration: 860 loss: 17490.891013615605 grad: -44.52180618916755\n",
      "iteration: 840 loss: 17010.43950901201 grad: -41.4180849422647\n",
      "iteration: 850 loss: 17282.07441578997 grad: -44.87940960914028\n",
      "iteration: 860 loss: 17460.84727907449 grad: -39.4448747875777\n",
      "iteration: 890 loss: 18146.536171049 grad: -44.60344740586143\n",
      "iteration: 880 loss: 18054.072602631877 grad: -45.89195892798794\n",
      "iteration: 850 loss: 17194.93397788908 grad: -40.754428923434105\n",
      "iteration: 870 loss: 17707.403176521017 grad: -44.75238871011305\n",
      "iteration: 870 loss: 17722.79502927693 grad: -44.92417270967092\n",
      "iteration: 850 loss: 17241.21991837268 grad: -41.86528561226031\n",
      "iteration: 860 loss: 17514.307402396913 grad: -45.333086201768175\n",
      "iteration: 870 loss: 17690.683332404198 grad: -40.05299394541634\n",
      "iteration: 890 loss: 18286.72934806119 grad: -46.26450180341429\n",
      "iteration: 860 loss: 17425.544110596144 grad: -41.30561038343028\n",
      "iteration: 900 loss: 18378.786850235854 grad: -45.04456632427963\n",
      "iteration: 880 loss: 17954.881391225565 grad: -45.395742267260545\n",
      "iteration: 880 loss: 17939.431716575375 grad: -45.38210014105857\n",
      "iteration: 870 loss: 17746.71002255229 grad: -45.79279005032372\n",
      "iteration: 860 loss: 17472.2273004913 grad: -42.43975615718537\n",
      "iteration: 880 loss: 17920.72638228788 grad: -40.5226759407699\n",
      "iteration: 900 loss: 18519.49058070181 grad: -46.53547533815482\n",
      "iteration: 910 loss: 18611.201363536493 grad: -45.50809402227436\n",
      "iteration: 870 loss: 17656.38192702052 grad: -41.92040309825398\n",
      "iteration: 890 loss: 18187.17183326059 grad: -45.89185268492437\n",
      "iteration: 880 loss: 17979.275138981546 grad: -46.24126362947865\n",
      "iteration: 890 loss: 18171.695285828 grad: -45.89685636174572\n",
      "iteration: 870 loss: 17703.501542108545 grad: -43.04727133172807\n",
      "iteration: 890 loss: 18150.94368814446 grad: -40.929445757415095\n",
      "iteration: 910 loss: 18752.34692590606 grad: -46.85270315195063\n",
      "iteration: 920 loss: 18843.766975235594 grad: -45.92063914517566iteration: 880 loss: 17887.449372140058 grad: -42.50508590666453\n",
      "\n",
      "iteration: 900 loss: 18419.679908521754 grad: -46.414777563045604\n",
      "iteration: 900 loss: 18404.11838531235 grad: -46.288474994599\n",
      "iteration: 890 loss: 18212.006171056484 grad: -46.75233641921339\n",
      "iteration: 880 loss: 17934.9930271571 grad: -43.51749119788025\n",
      "iteration: 900 loss: 18381.34040958948 grad: -41.35741661490603\n",
      "iteration: 920 loss: 18985.331021014037 grad: -47.226374106515436\n",
      "iteration: 890 loss: 18118.702257218403 grad: -43.019516379499066\n",
      "iteration: 930 loss: 19076.463468246002 grad: -46.34220156027506\n",
      "iteration: 910 loss: 18652.397675758697 grad: -46.94309143636407\n",
      "iteration: 900 loss: 18444.948580307355 grad: -47.32873403925332\n",
      "iteration: 910 loss: 18636.660486525783 grad: -46.64440434541004\n",
      "iteration: 890 loss: 18166.637908253262 grad: -43.925408126520715\n",
      "iteration: 910 loss: 18611.90076581182 grad: -41.71240128298008\n",
      "iteration: 930 loss: 19218.439232057586 grad: -47.56274752262036\n",
      "iteration: 900 loss: 18350.1617250297 grad: -43.574199874665595\n",
      "iteration: 940 loss: 19309.32593719897 grad: -46.80226165133902\n",
      "iteration: 920 loss: 18885.31181285358 grad: -47.442228427532314\n",
      "iteration: 910 loss: 18678.100601613878 grad: -47.86183399557753\n",
      "iteration: 920 loss: 18869.318493962688 grad: -46.99727499530955\n",
      "iteration: 920 loss: 18842.57828445585 grad: -42.04551823347525\n",
      "iteration: 900 loss: 18398.436597205167 grad: -44.34684454010156\n",
      "iteration: 940 loss: 19451.679748285707 grad: -47.96185115324925\n",
      "iteration: 910 loss: 18581.81181836317 grad: -44.0345385200049\n",
      "iteration: 950 loss: 19542.352368252672 grad: -47.211417396607374\n",
      "iteration: 930 loss: 19118.400624954604 grad: -47.874935420798835\n",
      "iteration: 920 loss: 18911.44752615991 grad: -48.38223289867033\n",
      "iteration: 930 loss: 19102.106042611922 grad: -47.36154852035069\n",
      "iteration: 930 loss: 19073.40524665737 grad: -42.459200103053874\n",
      "iteration: 950 loss: 19685.07590938412 grad: -48.38459449319272\n",
      "iteration: 920 loss: 18813.620997806698 grad: -44.46268471369073\n",
      "iteration: 910 loss: 18630.3925706157 grad: -44.74418576181873\n",
      "iteration: 960 loss: 19775.523705997657 grad: -47.58817530594871\n",
      "iteration: 940 loss: 19351.665167347415 grad: -48.32212138145824\n",
      "iteration: 930 loss: 19144.99255702471 grad: -48.93943201932804\n",
      "iteration: 940 loss: 19304.409174224005 grad: -42.90958353095195\n",
      "iteration: 940 loss: 19335.010449113004 grad: -47.66151267403841\n",
      "iteration: 960 loss: 19918.63734572377 grad: -48.807392629062804\n",
      "iteration: 930 loss: 19045.607260808378 grad: -44.914208319329006\n",
      "iteration: 970 loss: 20008.817105852064 grad: -47.910112938972674\n",
      "iteration: 920 loss: 18862.508733630595 grad: -45.18573802827258\n",
      "iteration: 950 loss: 19585.105073704064 grad: -48.80562185231444\n",
      "iteration: 940 loss: 19378.765868490464 grad: -49.48055882632171\n",
      "iteration: 950 loss: 19568.017800920083 grad: -47.97496447178382\n",
      "iteration: 970 loss: 20152.335079206 grad: -49.13230853996009\n",
      "iteration: 940 loss: 19277.78137656152 grad: -45.39080471671262\n",
      "iteration: 950 loss: 19535.582184594154 grad: -43.408511935269374\n",
      "iteration: 930 loss: 19094.80248231387 grad: -45.61181139542893\n",
      "iteration: 980 loss: 20242.21437596519 grad: -48.248122350086554\n",
      "iteration: 960 loss: 19818.731153968216 grad: -49.33642112366165\n",
      "iteration: 950 loss: 19612.720764291364 grad: -49.879534996385594\n",
      "iteration: 950 loss: 19510.141907486486 grad: -45.877084606857224\n",
      "iteration: 980 loss: 20386.185102986106 grad: -49.60752910950915\n",
      "iteration: 960 loss: 19801.14218754065 grad: -48.299850897880404iteration: 990 loss: 20475.73001838128 grad: -48.61402294945495\n",
      "\n",
      "iteration: 960 loss: 19766.928862019944 grad: -43.862988990349535\n",
      "iteration: 940 loss: 19327.24792879168 grad: -46.01971776325783\n",
      "iteration: 970 loss: 20052.5837881341 grad: -49.97186245716383\n",
      "iteration: 960 loss: 19846.838730569 grad: -50.27204116067916\n",
      "iteration: 960 loss: 19742.69063672697 grad: -46.41785927730838\n",
      "iteration: 990 loss: 20620.244513743557 grad: -50.12565413971173\n",
      "iteration: 970 loss: 19998.43285903997 grad: -44.26806760986446\n",
      "iteration: 1000 loss: 20709.375498084642 grad: -48.994510666593285\n",
      "iteration: 970 loss: 20034.383690124017 grad: -48.65038528347536\n",
      "iteration: 950 loss: 19559.852354732746 grad: -46.519209126210775\n",
      "iteration: 980 loss: 20286.679716885883 grad: -50.52234310071239\n",
      "iteration: 970 loss: 20081.107253741804 grad: -50.605608544182324\n",
      "iteration: 970 loss: 19975.461804547063 grad: -46.99334905020547\n",
      "iteration: 980 loss: 20230.085464220214 grad: -44.61088168825435\n",
      "iteration: 1000 loss: 20854.468014343685 grad: -50.51217909668188\n",
      "iteration: 1010 loss: 20943.173588542893 grad: -49.43825129481863\n",
      "iteration: 960 loss: 19792.664453390833 grad: -47.11013760342556\n",
      "iteration: 980 loss: 20267.764598006874 grad: -49.06521604782223\n",
      "iteration: 980 loss: 20315.50724956042 grad: -50.94471057649241\n",
      "iteration: 990 loss: 20520.9719468373 grad: -51.027174906369986\n",
      "iteration: 980 loss: 20208.452501964992 grad: -47.472110310828796\n",
      "iteration: 1010 loss: 21088.826796880337 grad: -50.90441409970505\n",
      "iteration: 1020 loss: 21177.16156734492 grad: -49.9092960391046\n",
      "iteration: 990 loss: 20461.871934299837 grad: -44.953162606261444\n",
      "iteration: 970 loss: 20025.67863158854 grad: -47.634005135539724\n",
      "iteration: 990 loss: 20501.297787519412 grad: -49.49417297613868\n",
      "iteration: 1000 loss: 20755.443745106935 grad: -51.467344882436436\n",
      "iteration: 990 loss: 20550.047891265196 grad: -51.329654749936054\n",
      "iteration: 990 loss: 20441.630149399407 grad: -47.87240294562564\n",
      "iteration: 1030 loss: 21411.303245518156 grad: -50.2967173553822\n",
      "iteration: 1020 loss: 21323.34201451746 grad: -51.347327122219085\n",
      "iteration: 1000 loss: 20693.814548147984 grad: -45.32847982965178\n",
      "iteration: 980 loss: 20258.881022608548 grad: -48.113299006021926\n",
      "iteration: 1000 loss: 20734.968211875235 grad: -49.86620176572511\n",
      "iteration: 1000 loss: 20784.76102111384 grad: -51.84859746186552\n",
      "iteration: 1010 loss: 20990.082048491804 grad: -51.92156822871572\n",
      "iteration: 1000 loss: 20674.98353465138 grad: -48.260768347691496\n",
      "iteration: 1040 loss: 21645.594454220078 grad: -50.77259790704148\n",
      "iteration: 1010 loss: 20925.891266412316 grad: -45.69179148657589\n",
      "iteration: 1030 loss: 21558.020380888804 grad: -51.80743786321197\n",
      "iteration: 990 loss: 20492.248148713486 grad: -48.528067980694644\n",
      "iteration: 1010 loss: 21019.68863833062 grad: -52.40349944737252\n",
      "iteration: 1010 loss: 20968.773839012058 grad: -50.25726019901062\n",
      "iteration: 1020 loss: 21224.898713180824 grad: -52.35555294788763\n",
      "iteration: 1010 loss: 20908.502711468296 grad: -48.65410507542522\n",
      "iteration: 1050 loss: 21880.080819124258 grad: -51.312939907924104\n",
      "iteration: 1020 loss: 21158.107864654048 grad: -46.13064447256902\n",
      "iteration: 1020 loss: 21254.810414605483 grad: -52.92155784937832\n",
      "iteration: 1000 loss: 20725.754769330597 grad: -48.92792378551684iteration: 1040 loss: 21792.8699455665 grad: -52.26761029363951\n",
      "\n",
      "iteration: 1020 loss: 21202.720743160375 grad: -50.63271979702081\n",
      "iteration: 1030 loss: 21459.84806657888 grad: -52.6670723906434\n",
      "iteration: 1020 loss: 21142.174115057067 grad: -48.995450083351066\n",
      "iteration: 1060 loss: 22114.75680392771 grad: -51.79323773243958\n",
      "iteration: 1030 loss: 21390.489760805325 grad: -46.62635969075531\n",
      "iteration: 1030 loss: 21490.112798496986 grad: -53.414280340809356\n",
      "iteration: 1050 loss: 22027.86532971144 grad: -52.65538539353214\n",
      "iteration: 1030 loss: 21436.804301946955 grad: -51.0369827808264\n",
      "iteration: 1040 loss: 21694.890231131416 grad: -52.93965573362007\n",
      "iteration: 1010 loss: 20959.425221652422 grad: -49.40895635561919\n",
      "iteration: 1030 loss: 21375.965834129132 grad: -49.30632273850243\n",
      "iteration: 1070 loss: 22349.605142408793 grad: -52.2286816192191\n",
      "iteration: 1040 loss: 21623.04678258472 grad: -47.109600821711\n",
      "iteration: 1040 loss: 21725.57335539209 grad: -53.83033467836588\n",
      "iteration: 1060 loss: 22263.0116001473 grad: -53.07599706552074\n",
      "iteration: 1040 loss: 21671.034205283006 grad: -51.43275665720712\n",
      "iteration: 1050 loss: 21930.028410031442 grad: -53.2557788573322\n",
      "iteration: 1040 loss: 21609.886357887077 grad: -49.6574623821153\n",
      "iteration: 1080 loss: 22584.621691845627 grad: -52.6095591879007\n",
      "iteration: 1020 loss: 21193.28828866103 grad: -49.899756639844554\n",
      "iteration: 1050 loss: 21855.745747037854 grad: -47.46820166814866\n",
      "iteration: 1050 loss: 21961.153303745876 grad: -54.19132675187494\n",
      "iteration: 1070 loss: 22498.305693613158 grad: -53.43633015461312\n",
      "iteration: 1060 loss: 22165.285355473887 grad: -53.627793232768596iteration: 1050 loss: 21905.421526169623 grad: -51.87087060833274\n",
      "\n",
      "iteration: 1030 loss: 21427.335905228258 grad: -50.35773348524496\n",
      "iteration: 1090 loss: 22819.79296057412 grad: -52.99222098106769\n",
      "iteration: 1060 loss: 22088.54373486855 grad: -47.79338390536479\n",
      "iteration: 1050 loss: 21843.971469261734 grad: -50.118687276787725\n",
      "iteration: 1060 loss: 22196.86280485269 grad: -54.58568435110668\n",
      "iteration: 1070 loss: 22400.679482230546 grad: -53.995247673815584\n",
      "iteration: 1060 loss: 22140.008152311802 grad: -52.38779130431877\n",
      "iteration: 1080 loss: 22733.733136264695 grad: -53.806584114442835\n",
      "iteration: 1040 loss: 21661.526246721143 grad: -50.77193009110752\n",
      "iteration: 1100 loss: 23055.10015895861 grad: -53.306782991062335\n",
      "iteration: 1070 loss: 22321.450160681918 grad: -48.19458394051442\n",
      "iteration: 1060 loss: 22078.267463362507 grad: -50.56144712028519\n",
      "iteration: 1070 loss: 22432.70631430605 grad: -54.98106206911369\n",
      "iteration: 1070 loss: 22374.80231901703 grad: -52.89757204492307\n",
      "iteration: 1090 loss: 22969.28802438819 grad: -54.15854612794993\n",
      "iteration: 1050 loss: 21895.85110544114 grad: -51.18443026849161\n",
      "iteration: 1110 loss: 23290.50578946587 grad: -53.63068288226343\n",
      "iteration: 1080 loss: 22636.219345095036 grad: -54.40537308350817\n",
      "iteration: 1080 loss: 22554.490133896827 grad: -48.635106704253765\n",
      "iteration: 1070 loss: 22312.701998262062 grad: -50.88398203189749\n",
      "iteration: 1080 loss: 22668.682694854495 grad: -55.32648592848682\n",
      "iteration: 1080 loss: 22609.76145772875 grad: -53.313190702723176\n",
      "iteration: 1060 loss: 22130.32205600436 grad: -51.609227108557064\n",
      "iteration: 1100 loss: 23204.950105005788 grad: -54.47327666779046\n",
      "iteration: 1090 loss: 22871.957609988163 grad: -54.93115288364919\n",
      "iteration: 1120 loss: 23526.01885534629 grad: -53.97933265318825\n",
      "iteration: 1090 loss: 22787.677450537107 grad: -49.073821698662655\n",
      "iteration: 1080 loss: 22547.25727488322 grad: -51.25594689699267\n",
      "iteration: 1090 loss: 22904.76828255036 grad: -55.633519469070066\n",
      "iteration: 1090 loss: 22844.854215336127 grad: -53.67192381417655\n",
      "iteration: 1070 loss: 22364.94232501522 grad: -52.00072039749065\n",
      "iteration: 1110 loss: 23440.723726994045 grad: -54.859842979175326\n",
      "iteration: 1100 loss: 23107.916096679313 grad: -55.467285021691936\n",
      "iteration: 1100 loss: 23021.021528664198 grad: -49.47787949255954\n",
      "iteration: 1130 loss: 23761.639926055417 grad: -54.335558225662425\n",
      "iteration: 1100 loss: 23140.96461222134 grad: -55.98730901977906\n",
      "iteration: 1090 loss: 22781.95331719491 grad: -51.66173670543339\n",
      "iteration: 1100 loss: 23080.074459392625 grad: -54.08235475468296\n",
      "iteration: 1120 loss: 23676.649146283464 grad: -55.31946408549412\n",
      "iteration: 1110 loss: 23344.053981914545 grad: -55.90610119535752\n",
      "iteration: 1110 loss: 23254.497074699448 grad: -49.833547096307406\n",
      "iteration: 1140 loss: 23997.3884769967 grad: -54.804154914349894\n",
      "iteration: 1080 loss: 22599.718764380363 grad: -52.41772519585568\n",
      "iteration: 1110 loss: 23377.295423381605 grad: -56.373005642917605\n",
      "iteration: 1100 loss: 23016.79946922893 grad: -52.06925171009025\n",
      "iteration: 1110 loss: 23315.48309925848 grad: -54.66218606789731\n",
      "iteration: 1130 loss: 23912.731653776827 grad: -55.757986235267026\n",
      "iteration: 1120 loss: 23488.095397097622 grad: -50.177957625470185\n",
      "iteration: 1120 loss: 23580.360193816017 grad: -56.31759917849463\n",
      "iteration: 1150 loss: 24233.318385489918 grad: -55.383835730171526\n",
      "iteration: 1090 loss: 22834.67112513908 grad: -52.89155096598584\n",
      "iteration: 1120 loss: 23613.76178965593 grad: -56.76028112126288\n",
      "iteration: 1110 loss: 23251.788624054476 grad: -52.37327781864778\n",
      "iteration: 1120 loss: 23551.086559680476 grad: -55.182942945390884\n",
      "iteration: 1130 loss: 23721.81280445753 grad: -50.531304616370164iteration: 1140 loss: 24148.958257886734 grad: -56.16981386293934\n",
      "\n",
      "iteration: 1130 loss: 23816.819021461364 grad: -56.65031799890009\n",
      "iteration: 1160 loss: 24469.436515025984 grad: -55.883504933796004\n",
      "iteration: 1100 loss: 23069.791024028586 grad: -53.352656749979936\n",
      "iteration: 1130 loss: 23850.374202162606 grad: -57.17882842056436\n",
      "iteration: 1120 loss: 23486.88843627942 grad: -52.6392128069598\n",
      "iteration: 1130 loss: 23786.852602964962 grad: -55.631877192390526\n",
      "iteration: 1140 loss: 23955.652849708702 grad: -50.90715852208289\n",
      "iteration: 1150 loss: 24385.34521984129 grad: -56.62431801663225\n",
      "iteration: 1170 loss: 24705.701188894684 grad: -56.281736439811134\n",
      "iteration: 1140 loss: 24053.393405906314 grad: -56.93944596449471\n",
      "iteration: 1140 loss: 24087.13998827427 grad: -57.58032763817367\n",
      "iteration: 1110 loss: 23305.07162067869 grad: -53.785242474562615\n",
      "iteration: 1130 loss: 23722.099587013043 grad: -52.89999345096298\n",
      "iteration: 1140 loss: 24022.769023351695 grad: -56.046430228397156\n",
      "iteration: 1150 loss: 24189.608852316247 grad: -51.238603234822364\n",
      "iteration: 1160 loss: 24621.912161084496 grad: -57.05780053546124\n",
      "iteration: 1150 loss: 24290.07767831099 grad: -57.21044258213686\n",
      "iteration: 1180 loss: 24942.10206979676 grad: -56.72203987063311\n",
      "iteration: 1150 loss: 24324.069841076867 grad: -58.02257100686111\n",
      "iteration: 1120 loss: 23540.51693690817 grad: -54.210254232327216\n",
      "iteration: 1150 loss: 24258.830246662008 grad: -56.45096975920539\n",
      "iteration: 1140 loss: 23957.423158445938 grad: -53.14885855144623\n",
      "iteration: 1160 loss: 24423.67511818961 grad: -51.56363960302142\n",
      "iteration: 1170 loss: 24858.629254604017 grad: -57.42322637161185\n",
      "iteration: 1190 loss: 25178.655896221408 grad: -57.173600608709506iteration: 1160 loss: 24526.862099364665 grad: -57.46468746093828\n",
      "\n",
      "iteration: 1160 loss: 24561.178197827125 grad: -58.50186777124006\n",
      "iteration: 1130 loss: 23776.11188877199 grad: -54.58572091104793\n",
      "iteration: 1160 loss: 24495.037851016863 grad: -56.87120026767215\n",
      "iteration: 1170 loss: 24657.864041458182 grad: -51.92029940179613\n",
      "iteration: 1200 loss: 25415.353952761976 grad: -57.58177768393608\n",
      "iteration: 1150 loss: 24192.850370590833 grad: -53.4542437549193\n",
      "iteration: 1180 loss: 25095.479311043193 grad: -57.778650611950134\n",
      "iteration: 1170 loss: 24763.73244627311 grad: -57.70996499641935\n",
      "iteration: 1170 loss: 24798.461960433662 grad: -58.93735705415226\n",
      "iteration: 1140 loss: 24011.831108833994 grad: -54.93121311749967\n",
      "iteration: 1170 loss: 24731.391705625974 grad: -57.25572757670284\n",
      "iteration: 1210 loss: 25652.18446825005 grad: -57.93796961533204\n",
      "iteration: 1160 loss: 24428.391691537392 grad: -53.78857259127315\n",
      "iteration: 1190 loss: 25332.464034785382 grad: -58.11638287231567\n",
      "iteration: 1180 loss: 25000.707754657335 grad: -58.02046622998664\n",
      "iteration: 1180 loss: 24892.21651580855 grad: -52.340060753635925\n",
      "iteration: 1180 loss: 25035.8948533605 grad: -59.30028741316453\n",
      "iteration: 1150 loss: 24247.678459466522 grad: -55.313491648393224\n",
      "iteration: 1180 loss: 24967.884856845983 grad: -57.64511357091163\n",
      "iteration: 1220 loss: 25889.132163432783 grad: -58.22916909643715\n",
      "iteration: 1200 loss: 25569.583622004324 grad: -58.48328327303961\n",
      "iteration: 1170 loss: 24664.033175810026 grad: -54.071534833144106\n",
      "iteration: 1190 loss: 25237.810009774457 grad: -58.3555154603338\n",
      "iteration: 1190 loss: 25273.45926272413 grad: -59.62627326597742\n",
      "iteration: 1190 loss: 25126.72240491154 grad: -52.72597559702844\n",
      "iteration: 1160 loss: 24483.683546478223 grad: -55.716772024291316\n",
      "iteration: 1190 loss: 25204.52652067506 grad: -58.06672682445749\n",
      "iteration: 1230 loss: 26126.182045267295 grad: -58.48467385679838\n",
      "iteration: 1210 loss: 25806.84565755112 grad: -58.87188127235781iteration: 1200 loss: 25475.02118035313 grad: -58.64083952432451\n",
      "\n",
      "iteration: 1200 loss: 25511.13411136324 grad: -59.911904779685315\n",
      "iteration: 1180 loss: 24899.758425845685 grad: -54.350067273459345\n",
      "iteration: 1200 loss: 25361.348758247088 grad: -53.05942481197333\n",
      "iteration: 1200 loss: 25441.309558408982 grad: -58.44259555522949\n",
      "iteration: 1240 loss: 26363.328292095986 grad: -58.74035230630693\n",
      "iteration: 1170 loss: 24719.85043892123 grad: -56.05771875377944\n",
      "iteration: 1210 loss: 25748.89975812581 grad: -60.17605978171875\n",
      "iteration: 1210 loss: 25596.09940502892 grad: -53.41618480547268\n",
      "iteration: 1210 loss: 25712.331275831068 grad: -58.93690873428454\n",
      "iteration: 1220 loss: 26044.229664661714 grad: -59.201715102601575\n",
      "iteration: 1190 loss: 25135.579593826333 grad: -54.653387647852476\n",
      "iteration: 1210 loss: 25678.224816365728 grad: -58.7977177614651\n",
      "iteration: 1180 loss: 24956.149781980363 grad: -56.40538501727877\n",
      "iteration: 1220 loss: 25986.748974289992 grad: -60.426881886858006\n",
      "iteration: 1250 loss: 26600.591098247296 grad: -59.043393736141006\n",
      "iteration: 1220 loss: 25830.995823851674 grad: -53.81486731352652\n",
      "iteration: 1220 loss: 25949.753456171245 grad: -59.25952011332916\n",
      "iteration: 1230 loss: 26281.729948091663 grad: -59.54199594698094\n",
      "iteration: 1200 loss: 25371.493316133085 grad: -54.933464400556076\n",
      "iteration: 1220 loss: 25915.274350307587 grad: -59.16013763189528\n",
      "iteration: 1230 loss: 26224.672952739977 grad: -60.64766156956343\n",
      "iteration: 1190 loss: 25192.582636582414 grad: -56.74381947570029\n",
      "iteration: 1260 loss: 26837.993729749287 grad: -59.457151924037206\n",
      "iteration: 1230 loss: 26187.288505346936 grad: -59.545904377329855\n",
      "iteration: 1240 loss: 26519.367788780062 grad: -59.89384336400731\n",
      "iteration: 1230 loss: 26066.05468249801 grad: -54.27312011563495\n",
      "iteration: 1210 loss: 25607.494815512837 grad: -55.21741126456227\n",
      "iteration: 1230 loss: 26152.47119879943 grad: -59.56548196773409\n",
      "iteration: 1240 loss: 26462.67524035044 grad: -60.89391258517915\n",
      "iteration: 1200 loss: 25429.155052614828 grad: -57.16019693046763iteration: 1270 loss: 27075.578113306598 grad: -59.95749286400735\n",
      "\n",
      "iteration: 1240 loss: 26424.909890087038 grad: -59.77350679127686\n",
      "iteration: 1240 loss: 26301.28354102432 grad: -54.722493677977546\n",
      "iteration: 1250 loss: 26757.134559923696 grad: -60.22502146638158\n",
      "iteration: 1220 loss: 25843.595423750077 grad: -55.53309080178817\n",
      "iteration: 1240 loss: 26389.81068473294 grad: -59.94290837176383\n",
      "iteration: 1280 loss: 27313.339622822503 grad: -60.37131540803314\n",
      "iteration: 1250 loss: 26700.777603147115 grad: -61.18321121937693\n",
      "iteration: 1210 loss: 25665.90946699056 grad: -57.66789165564401\n",
      "iteration: 1250 loss: 26536.665415955238 grad: -55.11465721715075\n",
      "iteration: 1250 loss: 26662.622091533776 grad: -60.02437974186471\n",
      "iteration: 1260 loss: 26995.030615989574 grad: -60.569325136787995\n",
      "iteration: 1230 loss: 26079.82364017147 grad: -55.93999637876208\n",
      "iteration: 1250 loss: 26627.28566362389 grad: -60.34386691092726\n",
      "iteration: 1290 loss: 27551.246899802125 grad: -60.69897004659585\n",
      "iteration: 1220 loss: 25902.849210911223 grad: -58.12531312072274\n",
      "iteration: 1260 loss: 26938.993317641074 grad: -61.46564808775362\n",
      "iteration: 1260 loss: 26772.181324116245 grad: -55.455762165952535\n",
      "iteration: 1260 loss: 26900.44321747933 grad: -60.29916276512729\n",
      "iteration: 1270 loss: 27233.05923870546 grad: -60.90213208540315\n",
      "iteration: 1240 loss: 26316.201886421375 grad: -56.33924258708501\n",
      "iteration: 1260 loss: 26864.920790460703 grad: -60.75372900670173\n",
      "iteration: 1300 loss: 27789.291619662272 grad: -61.037830796771786\n",
      "iteration: 1230 loss: 26139.95094251111 grad: -58.5481194192233\n",
      "iteration: 1270 loss: 27177.302398065523 grad: -61.694789782136624\n",
      "iteration: 1270 loss: 27007.811337169736 grad: -55.73662494638044\n",
      "iteration: 1270 loss: 27138.36516777313 grad: -60.562678620243844\n",
      "iteration: 1280 loss: 27471.211541869518 grad: -61.177615711404954\n",
      "iteration: 1250 loss: 26552.70547994232 grad: -56.66798770130251\n",
      "iteration: 1270 loss: 27102.713346756158 grad: -61.13266705307964\n",
      "iteration: 1310 loss: 28027.466067259153 grad: -61.34359471671148\n",
      "iteration: 1240 loss: 26377.21260533441 grad: -58.96623625949563\n",
      "iteration: 1280 loss: 27415.700542078925 grad: -61.9514045382994\n",
      "iteration: 1280 loss: 27243.535980276116 grad: -56.00499822078348\n",
      "iteration: 1280 loss: 27376.379246362405 grad: -60.87293555224943\n",
      "iteration: 1290 loss: 27709.46359226274 grad: -61.411435835620985\n",
      "iteration: 1260 loss: 26789.319987983454 grad: -56.994612150778416\n",
      "iteration: 1280 loss: 27340.665503038308 grad: -61.54863786037901\n",
      "iteration: 1290 loss: 27654.212779845024 grad: -62.271816112404636\n",
      "iteration: 1250 loss: 26614.64468868027 grad: -59.384348667873766\n",
      "iteration: 1290 loss: 27479.385933715195 grad: -56.38333782929904\n",
      "iteration: 1320 loss: 28265.751724861428 grad: -61.66280736020485\n",
      "iteration: 1290 loss: 27614.506095649383 grad: -61.224391646468035\n",
      "iteration: 1300 loss: 27947.805224307038 grad: -61.63564787525161\n",
      "iteration: 1270 loss: 27026.06023055331 grad: -57.33271752894565\n",
      "iteration: 1290 loss: 27578.787262242353 grad: -61.97453734398633\n",
      "iteration: 1300 loss: 27892.860764085577 grad: -62.64145619853438\n",
      "iteration: 1300 loss: 27715.39749638626 grad: -56.766731460758066\n",
      "iteration: 1260 loss: 26852.265921758382 grad: -59.81891782914789\n",
      "iteration: 1300 loss: 27852.761698326187 grad: -61.62103035209955\n",
      "iteration: 1330 loss: 28504.159925553253 grad: -62.022602963666486\n",
      "iteration: 1310 loss: 28186.235866233277 grad: -61.89688371075637\n",
      "iteration: 1300 loss: 27817.052200049857 grad: -62.30338566221535\n",
      "iteration: 1280 loss: 27262.927165785597 grad: -57.62157846137093\n",
      "iteration: 1310 loss: 28131.65772104824 grad: -63.05408952516875\n",
      "iteration: 1310 loss: 27951.54964983558 grad: -57.10271131212105\n",
      "iteration: 1270 loss: 27090.06186637614 grad: -60.23792182397827\n",
      "iteration: 1310 loss: 28091.168482045756 grad: -62.06860541707684\n",
      "iteration: 1320 loss: 28424.779943447407 grad: -62.26117233045494\n",
      "iteration: 1340 loss: 28742.688834881104 grad: -62.360307385602304\n",
      "iteration: 1310 loss: 28055.43568946893 grad: -62.60965146487625\n",
      "iteration: 1290 loss: 27499.907335869295 grad: -57.88280575544911\n",
      "iteration: 1320 loss: 28370.62540567298 grad: -63.464258871135684\n",
      "iteration: 1320 loss: 28187.827071215008 grad: -57.39855208736121\n",
      "iteration: 1280 loss: 27328.021859601733 grad: -60.59094817045228\n",
      "iteration: 1320 loss: 28329.728144637542 grad: -62.474698608811195\n",
      "iteration: 1330 loss: 28663.489974541455 grad: -62.735736994838234\n",
      "iteration: 1350 loss: 28981.33115853441 grad: -62.681786892238065\n",
      "iteration: 1320 loss: 28293.9344426693 grad: -62.895575286429164\n",
      "iteration: 1330 loss: 28609.736223675198 grad: -63.76755423045033\n",
      "iteration: 1330 loss: 28424.20442394759 grad: -57.63638384557787\n",
      "iteration: 1300 loss: 27737.013738147165 grad: -58.16579148687703\n",
      "iteration: 1290 loss: 27566.0978020601 grad: -60.91788806451624\n",
      "iteration: 1330 loss: 28568.410267541847 grad: -62.80366194708447\n",
      "iteration: 1360 loss: 29220.10285443348 grad: -63.00508280714925\n",
      "iteration: 1340 loss: 28902.373252352958 grad: -63.109728549776335\n",
      "iteration: 1330 loss: 28532.552999832933 grad: -63.24094544423739\n",
      "iteration: 1340 loss: 28848.942720489835 grad: -64.03296106060179\n",
      "iteration: 1340 loss: 28660.67144098003 grad: -57.86873967057336\n",
      "iteration: 1340 loss: 28807.195741346295 grad: -63.15107463829913\n",
      "iteration: 1370 loss: 29458.99212080992 grad: -63.317040672972\n",
      "iteration: 1350 loss: 29141.37236360411 grad: -63.39081298183308\n",
      "iteration: 1310 loss: 27974.242529861996 grad: -58.44658890262918\n",
      "iteration: 1300 loss: 27804.307357251153 grad: -61.32727194954174\n",
      "iteration: 1340 loss: 28771.32160494546 grad: -63.72088735295883\n",
      "iteration: 1350 loss: 29088.24175898201 grad: -64.29188394243151\n",
      "iteration: 1350 loss: 28897.24122500798 grad: -58.16863732359641\n",
      "iteration: 1350 loss: 29046.121667162828 grad: -63.64185696468371\n",
      "iteration: 1380 loss: 29697.985699439363 grad: -63.5871866210934\n",
      "iteration: 1320 loss: 28211.572151497618 grad: -58.69428154511315\n",
      "iteration: 1310 loss: 28042.67274537343 grad: -61.74375170193495\n",
      "iteration: 1360 loss: 29380.481764231954 grad: -63.67057877020359\n",
      "iteration: 1350 loss: 29010.30041859092 grad: -64.27183681687856\n",
      "iteration: 1360 loss: 29327.625391086658 grad: -64.5011275065659\n",
      "iteration: 1360 loss: 29133.935528658967 grad: -58.50411664060846\n",
      "iteration: 1360 loss: 29285.233415870724 grad: -64.1464970011765\n",
      "iteration: 1390 loss: 29937.074734253503 grad: -63.845812602049364\n",
      "iteration: 1370 loss: 29619.69606384842 grad: -63.93073273363694\n",
      "iteration: 1330 loss: 28448.986552809518 grad: -58.95001770657128\n",
      "iteration: 1320 loss: 28281.19906910048 grad: -62.121564608553264\n",
      "iteration: 1360 loss: 29249.482047348836 grad: -64.70638314878488\n",
      "iteration: 1370 loss: 29567.075906489026 grad: -64.70787104730063\n",
      "iteration: 1370 loss: 29370.738425852778 grad: -58.7794281656983\n",
      "iteration: 1370 loss: 29524.52138190488 grad: -64.54956657533864\n",
      "iteration: 1400 loss: 30176.26415447331 grad: -64.10613646402263\n",
      "iteration: 1380 loss: 29859.01912946253 grad: -64.27363848522423\n",
      "iteration: 1330 loss: 28519.848484093724 grad: -62.39490883585478\n",
      "iteration: 1340 loss: 28686.517458210725 grad: -59.34690598757575\n",
      "iteration: 1380 loss: 29806.61574969992 grad: -65.0226211793532\n",
      "iteration: 1370 loss: 29488.837172389452 grad: -65.13032227283311\n",
      "iteration: 1380 loss: 29607.63993621365 grad: -59.06728396253389\n",
      "iteration: 1410 loss: 30415.56775731118 grad: -64.42316801356107\n",
      "iteration: 1380 loss: 29763.952024565097 grad: -64.88934576969577\n",
      "iteration: 1390 loss: 30098.470014029102 grad: -64.62524131122842\n",
      "iteration: 1390 loss: 30046.270073204287 grad: -65.35093197961126iteration: 1340 loss: 28758.593563892144 grad: -62.644173665880416\n",
      "\n",
      "iteration: 1350 loss: 28924.21975836377 grad: -59.82252783214014\n",
      "iteration: 1380 loss: 29728.382334891536 grad: -65.61925443434265\n",
      "iteration: 1390 loss: 29844.640492734736 grad: -59.3590371238639\n",
      "iteration: 1420 loss: 30655.009456711443 grad: -64.77716614190413\n",
      "iteration: 1390 loss: 30003.511332798793 grad: -65.1755882052516\n",
      "iteration: 1400 loss: 30338.046866399232 grad: -64.97512883745472\n",
      "iteration: 1400 loss: 30286.0180150514 grad: -65.59196031298887\n",
      "iteration: 1390 loss: 29968.11884582019 grad: -66.13242289447814\n",
      "iteration: 1400 loss: 30081.736160679302 grad: -59.655844709879204\n",
      "iteration: 1360 loss: 29162.08518291136 grad: -60.234569118626695\n",
      "iteration: 1350 loss: 28997.443562821303 grad: -62.90162868250798\n",
      "iteration: 1430 loss: 30894.585271649183 grad: -65.11189688487565\n",
      "iteration: 1400 loss: 30243.181062624863 grad: -65.44560797143421\n",
      "iteration: 1410 loss: 30525.831466696443 grad: -65.76299402499785\n",
      "iteration: 1400 loss: 30208.065967111117 grad: -66.66528272998053\n",
      "iteration: 1410 loss: 30318.93430172306 grad: -59.92740849242463\n",
      "iteration: 1410 loss: 30577.750816379008 grad: -65.31828468468667\n",
      "iteration: 1360 loss: 29236.42774636041 grad: -63.241814507151155\n",
      "iteration: 1370 loss: 29400.100210054974 grad: -60.65221986559705\n",
      "iteration: 1440 loss: 31134.295021030786 grad: -65.4426238412405\n",
      "iteration: 1410 loss: 30482.96523590109 grad: -65.78537976768645\n",
      "iteration: 1420 loss: 30817.59720815672 grad: -65.71222762475594\n",
      "iteration: 1420 loss: 30765.70427061174 grad: -65.92554626516126\n",
      "iteration: 1410 loss: 30448.194349348298 grad: -67.07470470731428\n",
      "iteration: 1420 loss: 30556.221268287813 grad: -60.158737320002686\n",
      "iteration: 1370 loss: 29475.564981552627 grad: -63.57956533090176\n",
      "iteration: 1380 loss: 29638.270702734124 grad: -61.08235186343665\n",
      "iteration: 1450 loss: 31374.142500853308 grad: -65.80575292694752\n",
      "iteration: 1420 loss: 30722.89802907238 grad: -66.21600403096623\n",
      "iteration: 1430 loss: 31005.634601180784 grad: -66.09064604336953\n",
      "iteration: 1430 loss: 31057.596987168945 grad: -66.08201686318081\n",
      "iteration: 1420 loss: 30688.460615894586 grad: -67.41470336373325\n",
      "iteration: 1430 loss: 30793.597260004535 grad: -60.42298553903767\n",
      "iteration: 1460 loss: 31614.171926018706 grad: -66.25396969483083\n",
      "iteration: 1380 loss: 29714.852609763188 grad: -63.952175822463914\n",
      "iteration: 1390 loss: 29876.61632064558 grad: -61.48661360889146\n",
      "iteration: 1430 loss: 30963.00615671237 grad: -66.64425695752146\n",
      "iteration: 1440 loss: 31245.625983653656 grad: -66.28741725012502\n",
      "iteration: 1440 loss: 31297.73963101117 grad: -66.45968313142777\n",
      "iteration: 1430 loss: 30928.862674763717 grad: -67.7892599347848\n",
      "iteration: 1440 loss: 31031.08020888325 grad: -60.75011650677415\n",
      "iteration: 1470 loss: 31854.392907097157 grad: -66.7196700879139\n",
      "iteration: 1390 loss: 29954.329701289516 grad: -64.39742032505703\n",
      "iteration: 1400 loss: 30115.103642898684 grad: -61.828298045160814\n",
      "iteration: 1440 loss: 31203.26356709624 grad: -66.9997607061924\n",
      "iteration: 1450 loss: 31485.696904164924 grad: -66.5511643310173\n",
      "iteration: 1450 loss: 31538.014615725908 grad: -66.82431128269806\n",
      "iteration: 1440 loss: 31169.406656234867 grad: -68.09995396774906\n",
      "iteration: 1450 loss: 31268.687494477268 grad: -61.11735900522106\n",
      "iteration: 1480 loss: 32094.766809168406 grad: -67.05993232889735\n",
      "iteration: 1400 loss: 30194.002850047153 grad: -64.80282159203686\n",
      "iteration: 1410 loss: 30353.717228639074 grad: -62.146682160004474\n",
      "iteration: 1450 loss: 31443.63772595061 grad: -67.27916465354897\n",
      "iteration: 1460 loss: 31725.883207366413 grad: -66.87890271397018\n",
      "iteration: 1460 loss: 31778.415402766306 grad: -67.18477927970773\n",
      "iteration: 1450 loss: 31410.065220702007 grad: -68.37786292803294\n",
      "iteration: 1460 loss: 31506.4375825569 grad: -61.512229488672645\n",
      "iteration: 1490 loss: 32335.263104427075 grad: -67.4056333617948\n",
      "iteration: 1410 loss: 30433.838530273762 grad: -65.18497205277754\n",
      "iteration: 1470 loss: 31966.178139690062 grad: -67.11126734023716\n",
      "iteration: 1460 loss: 31684.0991568538 grad: -67.5136205665404\n",
      "iteration: 1470 loss: 32018.954116056844 grad: -67.53945429663958\n",
      "iteration: 1420 loss: 30592.447756253925 grad: -62.46541436200262\n",
      "iteration: 1460 loss: 31650.858958692603 grad: -68.71496747233503\n",
      "iteration: 1470 loss: 31744.340174162 grad: -61.955126822911296\n",
      "iteration: 1500 loss: 32575.914291247336 grad: -67.7959701032463\n",
      "iteration: 1420 loss: 30673.81173826206 grad: -65.48035377462634\n",
      "iteration: 1480 loss: 32206.54827791749 grad: -67.28099603709305\n",
      "iteration: 1470 loss: 31924.646687636814 grad: -67.75641055496237\n",
      "iteration: 1480 loss: 32259.62407278849 grad: -67.80697936123275\n",
      "iteration: 1470 loss: 31891.81879204625 grad: -69.11657606476135\n",
      "iteration: 1480 loss: 31982.401646205995 grad: -62.34216641867677\n",
      "iteration: 1430 loss: 30831.30400212994 grad: -62.83789959378561\n",
      "iteration: 1510 loss: 32816.69050348303 grad: -68.05825665415105\n",
      "iteration: 1430 loss: 30913.910301840217 grad: -65.82187842167014\n",
      "iteration: 1490 loss: 32446.992251221633 grad: -67.48636226092648\n",
      "iteration: 1480 loss: 32165.288178667368 grad: -67.99782460728065\n",
      "iteration: 1480 loss: 32132.975463740586 grad: -69.53496925661632\n",
      "iteration: 1490 loss: 32500.415262525716 grad: -68.12281981455823\n",
      "iteration: 1490 loss: 32220.58770994969 grad: -62.66506241656633\n",
      "iteration: 1440 loss: 31070.285713032372 grad: -63.13192795287019\n",
      "iteration: 1520 loss: 33057.56447238369 grad: -68.29017439560451\n",
      "iteration: 1440 loss: 31154.16151800073 grad: -66.20667106830143\n",
      "iteration: 1500 loss: 32687.53536184415 grad: -67.73814078531646\n",
      "iteration: 1500 loss: 32458.88693496043 grad: -62.98208200606163\n",
      "iteration: 1490 loss: 32406.032071403606 grad: -68.27173742800998\n",
      "iteration: 1490 loss: 32374.319559406835 grad: -69.90769757261378\n",
      "iteration: 1500 loss: 32741.34476047243 grad: -68.47714195441462\n",
      "iteration: 1530 loss: 33298.53770874183 grad: -68.52299837334957\n",
      "iteration: 1450 loss: 31309.37285152663 grad: -63.42307680352018\n",
      "iteration: 1510 loss: 32928.18283366499 grad: -68.00295335923566\n",
      "iteration: 1450 loss: 31394.582039057466 grad: -66.56854825171536\n",
      "iteration: 1510 loss: 32697.29902784056 grad: -63.32658555628602\n",
      "iteration: 1500 loss: 32615.8180765548 grad: -70.21530348659377\n",
      "iteration: 1500 loss: 32646.887250995416 grad: -68.53001578697837\n",
      "iteration: 1510 loss: 32982.43355900319 grad: -68.87544839710459\n",
      "iteration: 1540 loss: 33539.647459255335 grad: -68.88651743046961\n",
      "iteration: 1460 loss: 31548.589985051523 grad: -63.76208905821233\n",
      "iteration: 1520 loss: 32935.830275608365 grad: -63.65721956927575\n",
      "iteration: 1460 loss: 31635.14592089471 grad: -66.86979244521373\n",
      "iteration: 1510 loss: 32857.4572682458 grad: -70.49554231754107\n",
      "iteration: 1510 loss: 32887.843850837235 grad: -68.72555578699175\n",
      "iteration: 1520 loss: 33168.93299208294 grad: -68.29788397804234\n",
      "iteration: 1520 loss: 33223.70603607428 grad: -69.30356697815421\n",
      "iteration: 1550 loss: 33780.930218188005 grad: -69.2090816880399\n",
      "iteration: 1470 loss: 31787.948583977886 grad: -64.09989910711974\n",
      "iteration: 1530 loss: 33174.469161212546 grad: -63.98048986723091\n",
      "iteration: 1520 loss: 33099.20531201891 grad: -70.71148752972368\n",
      "iteration: 1520 loss: 33128.87435003402 grad: -68.89284945584674\n",
      "iteration: 1530 loss: 33409.79701772717 grad: -68.62681702455106\n",
      "iteration: 1530 loss: 33465.13668028026 grad: -69.58727852822564\n",
      "iteration: 1470 loss: 31875.826670426304 grad: -67.11823630372447\n",
      "iteration: 1560 loss: 34022.32761300699 grad: -69.49397585931543\n",
      "iteration: 1480 loss: 32027.461988035375 grad: -64.47390708208998\n",
      "iteration: 1540 loss: 33413.2463663214 grad: -64.3579821656458\n",
      "iteration: 1530 loss: 33341.050786247375 grad: -70.95673591712674\n",
      "iteration: 1540 loss: 33650.79996018327 grad: -68.97555502681006\n",
      "iteration: 1530 loss: 33369.98346812813 grad: -69.10510343132364\n",
      "iteration: 1540 loss: 33706.65279981011 grad: -69.74909717116208\n",
      "iteration: 1480 loss: 32116.602998649 grad: -67.37005586393954\n",
      "iteration: 1570 loss: 34263.85322685401 grad: -69.85659090105594\n",
      "iteration: 1490 loss: 32267.148499345483 grad: -64.84942606530483\n",
      "iteration: 1550 loss: 33652.152100853 grad: -64.65076294470842\n",
      "iteration: 1540 loss: 33583.00593468096 grad: -71.19029684444556\n",
      "iteration: 1550 loss: 33891.91847701039 grad: -69.19625603897013\n",
      "iteration: 1540 loss: 33611.194440439824 grad: -69.41196229101945\n",
      "iteration: 1550 loss: 33948.23139808138 grad: -69.88784355021478\n",
      "iteration: 1580 loss: 34505.52939647896 grad: -70.1783293147689iteration: 1490 loss: 32357.48432381575 grad: -67.65056026381365\n",
      "\n",
      "iteration: 1500 loss: 32506.986817892797 grad: -65.18678044496161\n",
      "iteration: 1560 loss: 33891.153045274994 grad: -64.88252686336499\n",
      "iteration: 1560 loss: 34133.11247491496 grad: -69.36242484885643\n",
      "iteration: 1550 loss: 33852.543687998084 grad: -69.73462254579476\n",
      "iteration: 1550 loss: 33825.0581685023 grad: -71.4761011695701\n",
      "iteration: 1560 loss: 34189.88461982057 grad: -70.06349000916344\n",
      "iteration: 1590 loss: 34747.33220885314 grad: -70.47005472842056\n",
      "iteration: 1500 loss: 32598.489231082196 grad: -67.9498183260078\n",
      "iteration: 1510 loss: 32746.93913958597 grad: -65.45722819497739\n",
      "iteration: 1570 loss: 34130.22608204318 grad: -65.06964766430369\n",
      "iteration: 1560 loss: 34067.23647553436 grad: -71.7539057865169\n",
      "iteration: 1570 loss: 34374.380854417286 grad: -69.5672179760885\n",
      "iteration: 1560 loss: 34094.00879711542 grad: -70.03324707796091\n",
      "iteration: 1570 loss: 34431.62377275629 grad: -70.27197548006689\n",
      "iteration: 1600 loss: 34989.239896630424 grad: -70.71706027342128\n",
      "iteration: 1580 loss: 34369.359514595046 grad: -65.24210386669591\n",
      "iteration: 1570 loss: 34309.52341737206 grad: -72.03786341979864\n",
      "iteration: 1580 loss: 34615.720034089216 grad: -69.74170589990787\n",
      "iteration: 1510 loss: 32839.62199842996 grad: -68.24894757181485\n",
      "iteration: 1520 loss: 32986.994581108374 grad: -65.76309972372917\n",
      "iteration: 1570 loss: 34335.59827561589 grad: -70.3590861699244\n",
      "iteration: 1580 loss: 34673.46028293915 grad: -70.4975951034487\n",
      "iteration: 1610 loss: 35231.23484164581 grad: -70.94094516145377\n",
      "iteration: 1590 loss: 34608.546712951975 grad: -65.39211900669198\n",
      "iteration: 1590 loss: 34857.11833946109 grad: -69.9228343463376\n",
      "iteration: 1590 loss: 34915.39643216516 grad: -70.7462324303527\n",
      "iteration: 1530 loss: 33227.17082198758 grad: -66.1057062745864\n",
      "iteration: 1580 loss: 34577.29838108423 grad: -70.63800764451224\n",
      "iteration: 1580 loss: 34551.93332773817 grad: -72.30593130016908\n",
      "iteration: 1520 loss: 33080.87853322393 grad: -68.52850863612433\n",
      "iteration: 1620 loss: 35473.33234774684 grad: -71.1711026186917\n",
      "iteration: 1600 loss: 34847.79673893768 grad: -65.58690785615784\n",
      "iteration: 1600 loss: 35098.60336257178 grad: -70.19636232237056\n",
      "iteration: 1600 loss: 35157.457005199314 grad: -71.0120572150793\n",
      "iteration: 1590 loss: 34819.12196178278 grad: -70.98944705164925\n",
      "iteration: 1530 loss: 33322.2752422619 grad: -68.90865051178972\n",
      "iteration: 1540 loss: 33467.457520130265 grad: -66.36783329718313\n",
      "iteration: 1630 loss: 35715.51990926136 grad: -71.36879768045607\n",
      "iteration: 1590 loss: 34794.452813061594 grad: -72.53914193817886\n",
      "iteration: 1610 loss: 35087.12348520761 grad: -65.77877680728389\n",
      "iteration: 1610 loss: 35340.22226884268 grad: -70.53008427241005\n",
      "iteration: 1610 loss: 35399.62306871353 grad: -71.22574761774679\n",
      "iteration: 1600 loss: 35061.09520709906 grad: -71.31064026943167\n",
      "iteration: 1640 loss: 35957.796451630486 grad: -71.62282141011158\n",
      "iteration: 1540 loss: 33563.825617399874 grad: -69.27210039057226\n",
      "iteration: 1550 loss: 33707.831500940214 grad: -66.57120614750977\n",
      "iteration: 1600 loss: 35037.071534854986 grad: -72.7537745268011\n",
      "iteration: 1620 loss: 35326.51459699352 grad: -65.95597183633973\n",
      "iteration: 1620 loss: 35581.95108601875 grad: -70.75348561800513\n",
      "iteration: 1620 loss: 35641.8836542569 grad: -71.43741253496259\n",
      "iteration: 1610 loss: 35303.18518894069 grad: -71.56837849866474\n",
      "iteration: 1650 loss: 36200.17285039509 grad: -71.86221357224272\n",
      "iteration: 1550 loss: 33805.48557866789 grad: -69.56174829656842\n",
      "iteration: 1560 loss: 33948.264773326155 grad: -66.72890487400798\n",
      "iteration: 1610 loss: 35279.77633195582 grad: -72.9484699984719\n",
      "iteration: 1630 loss: 35823.7513156379 grad: -70.94403794997604\n",
      "iteration: 1630 loss: 35884.24486676446 grad: -71.71119351750417\n",
      "iteration: 1630 loss: 35565.980067856945 grad: -66.16049577453724\n",
      "iteration: 1660 loss: 36442.66644718812 grad: -72.1451541045258\n",
      "iteration: 1620 loss: 35545.38060736243 grad: -71.8201693105994\n",
      "iteration: 1560 loss: 34047.223474950195 grad: -69.79178406446093\n",
      "iteration: 1570 loss: 34188.77189402322 grad: -66.96707228841623\n",
      "iteration: 1620 loss: 35522.58084388984 grad: -73.19467531404668\n",
      "iteration: 1640 loss: 36065.6408707115 grad: -71.2171913906475\n",
      "iteration: 1640 loss: 36126.73908427757 grad: -72.03269279929619\n",
      "iteration: 1670 loss: 36685.30548806069 grad: -72.41388648171633\n",
      "iteration: 1640 loss: 35805.538405347295 grad: -66.44315318330001\n",
      "iteration: 1630 loss: 35787.66959230243 grad: -72.03650250597241\n",
      "iteration: 1580 loss: 34429.38854174098 grad: -67.25236258311806\n",
      "iteration: 1570 loss: 34289.03654713772 grad: -70.05285447500559\n",
      "iteration: 1650 loss: 36307.63362531188 grad: -71.46614016997914\n",
      "iteration: 1630 loss: 35765.50321133774 grad: -73.47219124438213\n",
      "iteration: 1650 loss: 36369.36131815741 grad: -72.28974795709439\n",
      "iteration: 1680 loss: 36928.052655550826 grad: -72.62749492332993\n",
      "iteration: 1650 loss: 36045.216947250694 grad: -66.76294742000655\n",
      "iteration: 1640 loss: 36030.04484075133 grad: -72.28646384107672\n",
      "iteration: 1580 loss: 34530.95395233928 grad: -70.36403721095269\n",
      "iteration: 1590 loss: 34670.116385551926 grad: -67.51788783526413\n",
      "iteration: 1660 loss: 36549.70911720344 grad: -71.70653761669912\n",
      "iteration: 1640 loss: 36008.544270808954 grad: -73.72666058972202\n",
      "iteration: 1660 loss: 36612.070185965335 grad: -72.5010782761122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3654/168530359.py:10: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_3654/2232348269.py:24: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1690 loss: 37170.874827719956 grad: -72.81129206429566iteration: 1660 loss: 36285.023572144346 grad: -67.09872450219254\n",
      "\n",
      "iteration: 1650 loss: 36272.55402945307 grad: -72.61352396676284\n",
      "iteration: 1590 loss: 34772.9783174922 grad: -70.63133838508284\n",
      "iteration: 1600 loss: 34910.93931370389 grad: -67.72703618592769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3654/168530359.py:10: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_3654/168530359.py:10: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_3654/2232348269.py:24: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/tmp/ipykernel_3654/2232348269.py:24: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1670 loss: inf grad: -71.98881462681493\n",
      "iteration: 1670 loss: inf grad: -72.74121574531688\n",
      "iteration: 1650 loss: 36251.694053815765 grad: -73.96222478001556\n",
      "iteration: 1670 loss: 36524.96763231589 grad: -67.42047950832442\n",
      "iteration: 1700 loss: inf grad: -72.99870677135536\n",
      "iteration: 1660 loss: 36515.18061903607 grad: -72.8054925534875\n",
      "iteration: 1600 loss: 35015.08046566887 grad: -70.83326672097444\n",
      "iteration: 1680 loss: inf grad: -72.23234560455796\n",
      "iteration: 1610 loss: 35151.84356187225 grad: -67.93508356386823\n",
      "iteration: 1680 loss: inf grad: -72.95995448345963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3654/168530359.py:10: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_3654/168530359.py:10: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_3654/2232348269.py:24: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/tmp/ipykernel_3654/2232348269.py:24: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1680 loss: inf grad: -67.73648925652631\n",
      "iteration: 1710 loss: inf grad: -73.18551478550323\n",
      "iteration: 1660 loss: 36494.9434536624 grad: -74.20339038055911\n",
      "iteration: 1670 loss: inf grad: -72.95440068161031\n",
      "iteration: 1610 loss: 35257.24087224637 grad: -70.99412786160006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3654/168530359.py:10: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_3654/2232348269.py:24: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1690 loss: inf grad: -72.43437386492961\n",
      "iteration: 1620 loss: 35392.83769746254 grad: -68.14050933723802\n",
      "iteration: 1690 loss: inf grad: -73.19824827248387\n",
      "iteration: 1690 loss: inf grad: -68.07315886617957\n",
      "iteration: 1720 loss: inf grad: -73.36337097760895\n",
      "iteration: 1670 loss: inf grad: -74.44735995607857\n",
      "iteration: 1700 loss: inf grad: -72.65473958237946\n",
      "iteration: 1630 loss: 35633.921606289245 grad: -68.36713797457767\n",
      "iteration: 1680 loss: inf grad: -73.12159202025126\n",
      "iteration: 1700 loss: inf grad: -73.38585683905072\n",
      "iteration: 1620 loss: 35499.460680351745 grad: -71.18868579850931\n",
      "iteration: 1700 loss: inf grad: -68.37286407488908\n",
      "iteration: 1680 loss: inf grad: -74.66567592620132\n",
      "iteration: 1730 loss: inf grad: -73.58413227970377\n",
      "iteration: 1710 loss: inf grad: -72.8914368979514\n",
      "iteration: 1690 loss: inf grad: -73.36238007282651\n",
      "iteration: 1640 loss: 35875.10058416932 grad: -68.63510451121735\n",
      "iteration: 1710 loss: inf grad: -73.57047984281434\n",
      "iteration: 1630 loss: 35741.760389560906 grad: -71.41485999361231\n",
      "iteration: 1710 loss: inf grad: -68.60069073008975\n",
      "iteration: 1740 loss: inf grad: -73.85097020860907\n",
      "iteration: 1690 loss: inf grad: -74.91199261022851\n",
      "iteration: 1720 loss: inf grad: -73.13121187405716\n",
      "iteration: 1700 loss: inf grad: -73.58315843086174\n",
      "iteration: 1650 loss: 36116.41499926945 grad: -69.00324963301213\n",
      "iteration: 1720 loss: inf grad: -73.75723092582042\n",
      "iteration: 1640 loss: 35984.150421343846 grad: -71.67245890913034\n",
      "iteration: 1720 loss: inf grad: -68.79826979164876\n",
      "iteration: 1750 loss: inf grad: -74.09775451725181\n",
      "iteration: 1700 loss: inf grad: -75.13975432664343\n",
      "iteration: 1730 loss: inf grad: -73.34686447489949\n",
      "iteration: 1710 loss: inf grad: -73.75564521800618\n",
      "iteration: 1660 loss: 36357.8770415719 grad: -69.30322089378677\n",
      "iteration: 1730 loss: inf grad: -73.96840212937225\n",
      "iteration: 1650 loss: 36226.6503080055 grad: -71.97111777850073\n",
      "iteration: 1730 loss: inf grad: -69.00081986879027\n",
      "iteration: 1760 loss: inf grad: -74.2971111387019\n",
      "iteration: 1710 loss: inf grad: -75.31439061618237\n",
      "iteration: 1740 loss: inf grad: -73.53566170625483\n",
      "iteration: 1720 loss: inf grad: -73.92089393449712\n",
      "iteration: 1670 loss: 36599.43516589776 grad: -69.51200877919781\n",
      "iteration: 1740 loss: inf grad: -74.27729044474597\n",
      "iteration: 1660 loss: 36469.251602833436 grad: -72.2181277516323\n",
      "iteration: 1740 loss: inf grad: -69.21592562346777\n",
      "iteration: 1770 loss: inf grad: -74.44979406288343\n",
      "iteration: 1750 loss: inf grad: -73.71708028801177\n",
      "iteration: 1730 loss: inf grad: -74.11309279101422\n",
      "iteration: 1720 loss: inf grad: -75.5478222802939\n",
      "iteration: 1680 loss: 36841.08238378799 grad: -69.77608807462886\n",
      "iteration: 1750 loss: inf grad: -74.60590422893915\n",
      "iteration: 1670 loss: 36711.93455704742 grad: -72.44898258896393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3654/168530359.py:10: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_3654/2232348269.py:24: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1750 loss: inf grad: -69.37433063844306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3654/168530359.py:10: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_3654/2232348269.py:24: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1760 loss: inf grad: -73.89419218088565\n",
      "iteration: 1740 loss: inf grad: -74.31666928293572\n",
      "iteration: 1780 loss: inf grad: -74.58921269897535\n",
      "iteration: 1760 loss: inf grad: -74.83832189136751\n",
      "iteration: 1690 loss: inf grad: -70.05550706092744\n",
      "iteration: 1730 loss: inf grad: -75.75870407460769\n",
      "iteration: 1680 loss: inf grad: -72.72097724548729\n",
      "iteration: 1760 loss: inf grad: -69.49823913202096\n",
      "iteration: 1770 loss: inf grad: -74.08987119726669\n",
      "iteration: 1750 loss: inf grad: -74.63497325064552\n",
      "iteration: 1790 loss: inf grad: -74.7760255477976iteration: 1770 loss: inf grad: -75.02055772167341\n",
      "\n",
      "iteration: 1700 loss: inf grad: -70.36126875003634\n",
      "iteration: 1740 loss: inf grad: -75.94720255565518\n",
      "iteration: 1770 loss: inf grad: -69.65157593704404\n",
      "iteration: 1690 loss: inf grad: -72.97144724255436\n",
      "iteration: 1780 loss: inf grad: -74.29366479149155\n",
      "iteration: 1760 loss: inf grad: -74.9297993313602\n",
      "iteration: 1780 loss: inf grad: -75.1750938128207\n",
      "iteration: 1800 loss: inf grad: -75.0787711308388\n",
      "iteration: 1710 loss: inf grad: -70.62343561153567\n",
      "iteration: 1750 loss: inf grad: -76.1264224991665\n",
      "iteration: 1780 loss: inf grad: -69.8012771372606\n",
      "iteration: 1790 loss: inf grad: -74.46932130370176\n",
      "iteration: 1770 loss: inf grad: -75.19974572768716\n",
      "iteration: 1700 loss: inf grad: -73.2326113734396\n",
      "iteration: 1790 loss: inf grad: -75.31085574397997\n",
      "iteration: 1790 loss: inf grad: -69.96574165234958\n",
      "iteration: 1810 loss: inf grad: -75.43479141457453\n",
      "iteration: 1720 loss: inf grad: -70.84866642393743\n",
      "iteration: 1760 loss: inf grad: -76.29479485423377\n",
      "iteration: 1800 loss: inf grad: -74.63722622223507\n",
      "iteration: 1780 loss: inf grad: -75.48644369660497\n",
      "iteration: 1800 loss: inf grad: -75.45592181770208\n",
      "iteration: 1710 loss: inf grad: -73.5561865903964\n",
      "iteration: 1800 loss: inf grad: -70.21322272845926\n",
      "iteration: 1730 loss: inf grad: -71.03136102080819\n",
      "iteration: 1820 loss: inf grad: -75.73178196980203\n",
      "iteration: 1810 loss: inf grad: -74.81886712685763\n",
      "iteration: 1790 loss: inf grad: -75.68469768351511\n",
      "iteration: 1770 loss: inf grad: -76.45886166339233\n",
      "iteration: 1810 loss: inf grad: -75.66582629538476\n",
      "iteration: 1720 loss: inf grad: -73.83403385467409\n",
      "iteration: 1810 loss: inf grad: -70.49654394201315\n",
      "iteration: 1830 loss: inf grad: -75.92226955158681\n",
      "iteration: 1740 loss: inf grad: -71.22684161637966\n",
      "iteration: 1820 loss: inf grad: -75.01172378145615\n",
      "iteration: 1800 loss: inf grad: -75.81047604807597\n",
      "iteration: 1780 loss: inf grad: -76.61361710555697\n",
      "iteration: 1820 loss: inf grad: -75.90443043860954\n",
      "iteration: 1730 loss: inf grad: -74.08873111660328\n",
      "iteration: 1820 loss: inf grad: -70.72030362834195\n",
      "iteration: 1840 loss: inf grad: -76.08158403814994\n",
      "iteration: 1750 loss: inf grad: -71.43888045069735\n",
      "iteration: 1810 loss: inf grad: -75.91554998446618\n",
      "iteration: 1830 loss: inf grad: -75.2333539182564\n",
      "iteration: 1790 loss: inf grad: -76.76869411584534\n",
      "iteration: 1830 loss: inf grad: -76.10993261860409\n",
      "iteration: 1740 loss: inf grad: -74.34781857850678\n",
      "iteration: 1830 loss: inf grad: -70.92164396295333\n",
      "iteration: 1850 loss: inf grad: -76.26390590009098\n",
      "iteration: 1760 loss: inf grad: -71.64634743999022\n",
      "iteration: 1840 loss: inf grad: -75.4811288039102\n",
      "iteration: 1820 loss: inf grad: -76.03016593640851\n",
      "iteration: 1840 loss: inf grad: -76.31103366945109\n",
      "iteration: 1800 loss: inf grad: -76.94923265925772\n",
      "iteration: 1750 loss: inf grad: -74.6088609838472\n",
      "iteration: 1840 loss: inf grad: -71.14498948795182\n",
      "iteration: 1860 loss: inf grad: -76.4562408794377\n",
      "iteration: 1770 loss: inf grad: -71.92274152315404\n",
      "iteration: 1850 loss: inf grad: -75.74079587032003\n",
      "iteration: 1830 loss: inf grad: -76.19285795154599\n",
      "iteration: 1850 loss: inf grad: -76.50538406879409\n",
      "iteration: 1810 loss: inf grad: -77.13887924600891\n",
      "iteration: 1850 loss: inf grad: -71.39926804382728\n",
      "iteration: 1760 loss: inf grad: -74.83916064005494\n",
      "iteration: 1870 loss: inf grad: -76.63089564325485\n",
      "iteration: 1780 loss: inf grad: -72.21633187474127\n",
      "iteration: 1860 loss: inf grad: -76.03635135136219\n",
      "iteration: 1840 loss: inf grad: -76.36505532765369\n",
      "iteration: 1860 loss: inf grad: -76.66209528983148\n",
      "iteration: 1820 loss: inf grad: -77.28830399873831\n",
      "iteration: 1860 loss: inf grad: -71.65356124586613\n",
      "iteration: 1770 loss: inf grad: -75.0874323807777\n",
      "iteration: 1880 loss: inf grad: -76.81925304981618\n",
      "iteration: 1790 loss: inf grad: -72.45988644471288\n",
      "iteration: 1870 loss: inf grad: -76.25031085022779\n",
      "iteration: 1850 loss: inf grad: -76.5450653808128\n",
      "iteration: 1870 loss: inf grad: -76.83497988760506\n",
      "iteration: 1830 loss: inf grad: -77.4473732626222\n",
      "iteration: 1870 loss: inf grad: -71.89292057954705\n",
      "iteration: 1780 loss: inf grad: -75.31222823013535\n",
      "iteration: 1800 loss: inf grad: -72.6786738948569\n",
      "iteration: 1890 loss: inf grad: -77.01823526636697\n",
      "iteration: 1860 loss: inf grad: -76.7298539969214\n",
      "iteration: 1880 loss: inf grad: -76.3993587149057\n",
      "iteration: 1880 loss: inf grad: -77.09922774101496\n",
      "iteration: 1840 loss: inf grad: -77.6281968117069\n",
      "iteration: 1880 loss: inf grad: -72.13224785733553\n",
      "iteration: 1790 loss: inf grad: -75.50631733907161\n",
      "iteration: 1890 loss: inf grad: -76.57735405068561\n",
      "iteration: 1900 loss: inf grad: -77.22222873705303\n",
      "iteration: 1870 loss: inf grad: -76.93976934791579\n",
      "iteration: 1810 loss: inf grad: -72.94224824326903\n",
      "iteration: 1890 loss: inf grad: -77.39561623054601\n",
      "iteration: 1890 loss: inf grad: -72.37294545339151\n",
      "iteration: 1850 loss: inf grad: -77.844869650271\n",
      "iteration: 1800 loss: inf grad: -75.66394815959261\n",
      "iteration: 1900 loss: inf grad: -76.77948501328612\n",
      "iteration: 1880 loss: inf grad: -77.12997284726603\n",
      "iteration: 1910 loss: inf grad: -77.35896536803995\n",
      "iteration: 1900 loss: inf grad: -77.66359364871232\n",
      "iteration: 1820 loss: inf grad: -73.20655325561071\n",
      "iteration: 1900 loss: inf grad: -72.605427700826\n",
      "iteration: 1860 loss: inf grad: -78.13108819990214\n",
      "iteration: 1810 loss: inf grad: -75.8355732803187\n",
      "iteration: 1910 loss: inf grad: -77.0540704941996\n",
      "iteration: 1890 loss: inf grad: -77.35794733896299\n",
      "iteration: 1920 loss: inf grad: -77.45791680473971\n",
      "iteration: 1830 loss: inf grad: -73.42843783472844\n",
      "iteration: 1910 loss: inf grad: -77.93804271124444\n",
      "iteration: 1910 loss: inf grad: -72.80283625697152\n",
      "iteration: 1820 loss: inf grad: -76.06964996520225\n",
      "iteration: 1870 loss: inf grad: -78.38023082836926\n",
      "iteration: 1920 loss: inf grad: -77.33298296665475\n",
      "iteration: 1900 loss: inf grad: -77.64655471091976\n",
      "iteration: 1930 loss: inf grad: -77.54066408194811\n",
      "iteration: 1840 loss: inf grad: -73.65192612715839\n",
      "iteration: 1920 loss: inf grad: -78.24339300913181\n",
      "iteration: 1920 loss: inf grad: -73.0391681639538\n",
      "iteration: 1830 loss: inf grad: -76.30289236781526\n",
      "iteration: 1930 loss: inf grad: -77.57448390554262\n",
      "iteration: 1880 loss: inf grad: -78.56788907271999iteration: 1910 loss: inf grad: -77.96703298538941\n",
      "\n",
      "iteration: 1940 loss: inf grad: -77.61861083617703\n",
      "iteration: 1850 loss: inf grad: -73.86811250254524\n",
      "iteration: 1930 loss: inf grad: -78.52325513836178\n",
      "iteration: 1930 loss: inf grad: -73.31964979939036\n",
      "iteration: 1840 loss: inf grad: -76.50990477876081\n",
      "iteration: 1920 loss: inf grad: -78.30807447759443\n",
      "iteration: 1940 loss: inf grad: -77.73805796618825\n",
      "iteration: 1890 loss: inf grad: -78.7326127810795\n",
      "iteration: 1950 loss: inf grad: -77.73297726742518\n",
      "iteration: 1860 loss: inf grad: -74.05078385941272\n",
      "iteration: 1940 loss: inf grad: -78.76228744379358\n",
      "iteration: 1940 loss: inf grad: -73.59081643473803\n",
      "iteration: 1850 loss: inf grad: -76.72807563869054\n",
      "iteration: 1930 loss: inf grad: -78.60668647768105\n",
      "iteration: 1950 loss: inf grad: -77.8660341952446\n",
      "iteration: 1900 loss: inf grad: -78.9265608561334\n",
      "iteration: 1960 loss: inf grad: -77.8399089499414\n",
      "iteration: 1870 loss: inf grad: -74.17403820619379\n",
      "iteration: 1950 loss: inf grad: -78.9702382625131\n",
      "iteration: 1950 loss: inf grad: -73.8565321206984\n",
      "iteration: 1860 loss: inf grad: -76.9499685587869\n",
      "iteration: 1940 loss: inf grad: -78.8896777065826\n",
      "iteration: 1960 loss: inf grad: -78.08006535776482\n",
      "iteration: 1970 loss: inf grad: -77.96672082810323\n",
      "iteration: 1910 loss: inf grad: -79.16531710817989\n",
      "iteration: 1880 loss: inf grad: -74.28116349169699\n",
      "iteration: 1960 loss: inf grad: -79.15660242366033\n",
      "iteration: 1960 loss: inf grad: -74.0777920076504\n",
      "iteration: 1870 loss: inf grad: -77.09800647651943\n",
      "iteration: 1970 loss: inf grad: -78.29593573768938\n",
      "iteration: 1950 loss: inf grad: -79.14182460928191\n",
      "iteration: 1980 loss: inf grad: -78.1594039233976\n",
      "iteration: 1890 loss: inf grad: -74.42877954259528\n",
      "iteration: 1970 loss: inf grad: -79.27731993181825\n",
      "iteration: 1920 loss: inf grad: -79.40246152405523\n",
      "iteration: 1970 loss: inf grad: -74.23963413768872\n",
      "iteration: 1980 loss: inf grad: -78.50011161149288\n",
      "iteration: 1960 loss: inf grad: -79.42846319369802\n",
      "iteration: 1880 loss: inf grad: -77.25755767525843\n",
      "iteration: 1990 loss: inf grad: -78.36067236852992\n",
      "iteration: 1980 loss: inf grad: -79.37250417135232\n",
      "iteration: 1900 loss: inf grad: -74.58941607510576\n",
      "iteration: 1980 loss: inf grad: -74.36690417906959\n",
      "iteration: 1930 loss: inf grad: -79.55030285270776\n",
      "iteration: 1990 loss: inf grad: -78.68937978176271\n",
      "iteration: 1970 loss: inf grad: -79.65557135241266\n",
      "iteration: 1990 loss: inf grad: -79.5043324491821\n",
      "iteration: 1890 loss: inf grad: -77.39750272193028\n",
      "iteration: 1990 loss: inf grad: -74.53683155158076\n",
      "iteration: 2000 loss: inf grad: -78.5379674355281\n",
      "iteration: 1910 loss: inf grad: -74.79411941908384\n",
      "iteration: 2000 loss: inf grad: -78.9162117901842\n",
      "iteration: 1980 loss: inf grad: -79.83751538892432\n",
      "iteration: 1940 loss: inf grad: -79.68948645580826\n",
      "iteration: 2000 loss: inf grad: -79.72115150381975\n",
      "iteration: 1900 loss: inf grad: -77.50950475814415\n",
      "iteration: 2000 loss: inf grad: -74.71645187851647\n",
      "iteration: 2010 loss: inf grad: -78.6700668490337\n",
      "iteration: 1920 loss: inf grad: -75.00217152686521\n",
      "iteration: 2010 loss: inf grad: -79.19777055086696\n",
      "iteration: 1990 loss: inf grad: -80.0443074620652\n",
      "iteration: 1950 loss: inf grad: -79.85984024275301\n",
      "iteration: 2010 loss: inf grad: -79.99050184785321\n",
      "iteration: 2010 loss: inf grad: -74.89204180549291iteration: 1910 loss: inf grad: -77.61269218708009\n",
      "\n",
      "iteration: 2020 loss: inf grad: -78.7870934058115\n",
      "iteration: 1930 loss: inf grad: -75.14120924450755\n",
      "iteration: 2020 loss: inf grad: -79.47324570446318\n",
      "iteration: 2000 loss: inf grad: -80.28708020743574\n",
      "iteration: 2020 loss: inf grad: -80.2218414812161\n",
      "iteration: 1960 loss: inf grad: -80.01093959286405\n",
      "iteration: 2020 loss: inf grad: -75.01843922589245\n",
      "iteration: 1920 loss: inf grad: -77.70341978039548\n",
      "iteration: 2030 loss: inf grad: -78.90301319324374\n",
      "iteration: 1940 loss: inf grad: -75.22341467270266\n",
      "iteration: 2030 loss: inf grad: -79.6908037572841\n",
      "iteration: 2010 loss: inf grad: -80.47045191311842\n",
      "iteration: 2030 loss: inf grad: -80.40787023162471\n",
      "iteration: 1970 loss: inf grad: -80.14899628652216\n",
      "iteration: 2030 loss: inf grad: -75.16395703939395\n",
      "iteration: 1930 loss: inf grad: -77.78719402026718\n",
      "iteration: 2040 loss: inf grad: -79.01545042809312\n",
      "iteration: 2040 loss: inf grad: -79.86448437658763\n",
      "iteration: 1950 loss: inf grad: -75.3139041599814\n",
      "iteration: 2020 loss: inf grad: -80.64695965037367\n",
      "iteration: 2040 loss: inf grad: -80.60783896996426\n",
      "iteration: 2040 loss: inf grad: -75.31097288142232\n",
      "iteration: 1980 loss: inf grad: -80.29648938467163\n",
      "iteration: 1940 loss: inf grad: -77.90642076648012\n",
      "iteration: 2050 loss: inf grad: -79.15411717586642\n",
      "iteration: 2050 loss: inf grad: -80.02926072338327\n",
      "iteration: 1960 loss: inf grad: -75.46208531940633\n",
      "iteration: 2030 loss: inf grad: -80.83784724005483\n",
      "iteration: 2050 loss: inf grad: -80.82760469350683\n",
      "iteration: 2050 loss: inf grad: -75.462497233486\n",
      "iteration: 1950 loss: inf grad: -78.07869491546317\n",
      "iteration: 1990 loss: inf grad: -80.53938255000199\n",
      "iteration: 2060 loss: inf grad: -79.30554492547077\n",
      "iteration: 1970 loss: inf grad: -75.6549916562722\n",
      "iteration: 2060 loss: inf grad: -80.2143491981231\n",
      "iteration: 2040 loss: inf grad: -81.01385014002251\n",
      "iteration: 2060 loss: inf grad: -81.03036655085384\n",
      "iteration: 2060 loss: inf grad: -75.58881329178027\n",
      "iteration: 1960 loss: inf grad: -78.23040341863626\n",
      "iteration: 2000 loss: inf grad: -80.81992113853718\n",
      "iteration: 2070 loss: inf grad: -79.55274152516938\n",
      "iteration: 2070 loss: inf grad: -80.3996303473673\n",
      "iteration: 1980 loss: inf grad: -75.871994035359\n",
      "iteration: 2050 loss: inf grad: -81.19980329148848\n",
      "iteration: 2070 loss: inf grad: -81.24680411574852\n",
      "iteration: 2070 loss: inf grad: -75.7676694627507\n",
      "iteration: 1970 loss: inf grad: -78.33621430108451\n",
      "iteration: 2080 loss: inf grad: -80.58270887453384\n",
      "iteration: 2010 loss: inf grad: -81.02009314314304\n",
      "iteration: 2080 loss: inf grad: -79.82333187575492iteration: 2060 loss: inf grad: -81.41466792674491\n",
      "\n",
      "iteration: 1990 loss: inf grad: -76.0619078069797\n",
      "iteration: 2080 loss: inf grad: -81.51478686909331\n",
      "iteration: 2080 loss: inf grad: -75.97474798528471\n",
      "iteration: 1980 loss: inf grad: -78.47281761303155\n",
      "iteration: 2020 loss: inf grad: -81.16509087841685\n",
      "iteration: 2090 loss: inf grad: -80.79669546672632iteration: 2070 loss: inf grad: -81.61697554325887\n",
      "\n",
      "iteration: 2090 loss: inf grad: -80.00439621606762\n",
      "iteration: 2000 loss: inf grad: -76.28637172705146\n",
      "iteration: 2090 loss: inf grad: -81.77666064988449\n",
      "iteration: 2090 loss: inf grad: -76.15356701513039\n",
      "iteration: 1990 loss: inf grad: -78.63733511643113\n",
      "iteration: 2080 loss: inf grad: -81.76939957412027\n",
      "iteration: 2030 loss: inf grad: -81.32383742500141\n",
      "iteration: 2100 loss: inf grad: -81.01815577386448\n",
      "iteration: 2100 loss: inf grad: -80.15293715648652\n",
      "iteration: 2010 loss: inf grad: -76.51361122916722\n",
      "iteration: 2100 loss: inf grad: -82.04784376919152\n",
      "iteration: 2100 loss: inf grad: -76.30546280839208\n",
      "iteration: 2000 loss: inf grad: -78.77135392419882\n",
      "iteration: 2090 loss: inf grad: -81.88683409199928\n",
      "iteration: 2110 loss: inf grad: -81.24779316358513\n",
      "iteration: 2020 loss: inf grad: -76.63873869238262\n",
      "iteration: 2110 loss: inf grad: -80.298207083356\n",
      "iteration: 2040 loss: inf grad: -81.4554076165846\n",
      "iteration: 2110 loss: inf grad: -82.26061787761597\n",
      "iteration: 2110 loss: inf grad: -76.51155224818785\n",
      "iteration: 2010 loss: inf grad: -78.8898853196684\n",
      "iteration: 2100 loss: inf grad: -82.02554291969594\n",
      "iteration: 2120 loss: inf grad: -81.47473027084372\n",
      "iteration: 2030 loss: inf grad: -76.7629252997358\n",
      "iteration: 2120 loss: inf grad: -80.48915835577577\n",
      "iteration: 2050 loss: inf grad: -81.55067989198459\n",
      "iteration: 2120 loss: inf grad: -82.44028371813\n",
      "iteration: 2120 loss: inf grad: -76.78338013161559\n",
      "iteration: 2020 loss: inf grad: -78.98607195879447\n",
      "iteration: 2130 loss: inf grad: -81.75127837184311\n",
      "iteration: 2110 loss: inf grad: -82.21696687147188\n",
      "iteration: 2040 loss: inf grad: -76.88910044635958\n",
      "iteration: 2130 loss: inf grad: -80.70089004663265\n",
      "iteration: 2060 loss: inf grad: -81.64127157423792\n",
      "iteration: 2130 loss: inf grad: -77.00009006068268\n",
      "iteration: 2130 loss: inf grad: -82.57183896433742\n",
      "iteration: 2030 loss: inf grad: -79.08645162606632\n",
      "iteration: 2140 loss: inf grad: -82.02455047781021\n",
      "iteration: 2120 loss: inf grad: -82.44598196420318\n",
      "iteration: 2050 loss: inf grad: -77.00478063252962\n",
      "iteration: 2140 loss: inf grad: -80.89562843119137\n",
      "iteration: 2140 loss: inf grad: -77.15318492364753\n",
      "iteration: 2140 loss: inf grad: -82.68387398449761\n",
      "iteration: 2070 loss: inf grad: -81.75443288355547\n",
      "iteration: 2040 loss: inf grad: -79.22656968657009\n",
      "iteration: 2130 loss: inf grad: -82.65095326284082\n",
      "iteration: 2150 loss: inf grad: -82.26946912700916\n",
      "iteration: 2060 loss: inf grad: -77.17688336910564\n",
      "iteration: 2150 loss: inf grad: -81.04083114724989\n",
      "iteration: 2150 loss: inf grad: -77.29334425714907\n",
      "iteration: 2150 loss: inf grad: -82.85878892347989\n",
      "iteration: 2080 loss: inf grad: -81.96965053316318\n",
      "iteration: 2050 loss: inf grad: -79.38328512023114\n",
      "iteration: 2140 loss: inf grad: -82.88247868178021\n",
      "iteration: 2160 loss: inf grad: -82.49611270915283\n",
      "iteration: 2070 loss: inf grad: -77.33525306802747\n",
      "iteration: 2160 loss: inf grad: -81.24555410104304\n",
      "iteration: 2160 loss: inf grad: -77.43647026213844\n",
      "iteration: 2160 loss: inf grad: -83.03851365107639\n",
      "iteration: 2090 loss: inf grad: -82.20091182779365\n",
      "iteration: 2060 loss: inf grad: -79.50878854878334\n",
      "iteration: 2150 loss: inf grad: -83.11618565991401\n",
      "iteration: 2170 loss: inf grad: -82.7166184565127\n",
      "iteration: 2170 loss: inf grad: -81.41578831953638\n",
      "iteration: 2170 loss: inf grad: -77.60168094667145\n",
      "iteration: 2170 loss: inf grad: -83.21916549643944\n",
      "iteration: 2080 loss: inf grad: -77.47056721400745\n",
      "iteration: 2100 loss: inf grad: -82.37176908156414\n",
      "iteration: 2160 loss: inf grad: -83.36903249915312\n",
      "iteration: 2180 loss: inf grad: -82.91227010953659\n",
      "iteration: 2070 loss: inf grad: -79.59431294621163\n",
      "iteration: 2180 loss: inf grad: -77.78167753249456\n",
      "iteration: 2180 loss: inf grad: -83.4352793140801\n",
      "iteration: 2180 loss: inf grad: -81.54647228849927\n",
      "iteration: 2090 loss: inf grad: -77.62076802368523\n",
      "iteration: 2190 loss: inf grad: -83.08692222587965\n",
      "iteration: 2170 loss: inf grad: -83.56369476365239\n",
      "iteration: 2110 loss: inf grad: -82.48442570799561\n",
      "iteration: 2190 loss: inf grad: -77.9898344647006\n",
      "iteration: 2190 loss: inf grad: -83.65511699141516\n",
      "iteration: 2080 loss: inf grad: -79.64854876520315\n",
      "iteration: 2190 loss: inf grad: -81.68675584070394\n",
      "iteration: 2100 loss: inf grad: -77.7936708858033\n",
      "iteration: 2200 loss: inf grad: -83.30651470649799\n",
      "iteration: 2180 loss: inf grad: -83.73897838811028\n",
      "iteration: 2120 loss: inf grad: -82.57761731246588\n",
      "iteration: 2200 loss: inf grad: -78.16251536162358\n",
      "iteration: 2200 loss: inf grad: -83.88558521290007\n",
      "iteration: 2090 loss: inf grad: -79.70620028918836\n",
      "iteration: 2200 loss: inf grad: -81.81039017329823\n",
      "iteration: 2110 loss: inf grad: -77.97482430651087\n",
      "iteration: 2210 loss: inf grad: -83.49818137002978\n",
      "iteration: 2190 loss: inf grad: -83.89249203546096\n",
      "iteration: 2210 loss: inf grad: -84.0420097502551\n",
      "iteration: 2210 loss: inf grad: -78.28795350823218\n",
      "iteration: 2130 loss: inf grad: -82.67948425273036\n",
      "iteration: 2100 loss: inf grad: -79.77300363693386\n",
      "iteration: 2210 loss: inf grad: -81.92146681598697\n",
      "iteration: 2120 loss: inf grad: -78.10383612355503\n",
      "iteration: 2220 loss: inf grad: -83.66784255469598\n",
      "iteration: 2200 loss: inf grad: -84.07958508614243\n",
      "iteration: 2220 loss: inf grad: -84.18582080180047\n",
      "iteration: 2220 loss: inf grad: -78.43943835473897\n",
      "iteration: 2140 loss: inf grad: -82.82523935164963\n",
      "iteration: 2110 loss: inf grad: -79.90482345486497\n",
      "iteration: 2220 loss: inf grad: -82.03074974108127\n",
      "iteration: 2130 loss: inf grad: -78.2210693883745\n",
      "iteration: 2230 loss: inf grad: -83.8503655574463\n",
      "iteration: 2210 loss: inf grad: -84.2669758001685\n",
      "iteration: 2230 loss: inf grad: -78.65304132831793\n",
      "iteration: 2230 loss: inf grad: -84.35758962126103\n",
      "iteration: 2150 loss: inf grad: -82.99919254326316\n",
      "iteration: 2120 loss: inf grad: -80.01031223460447\n",
      "iteration: 2240 loss: inf grad: -84.02961656046392\n",
      "iteration: 2140 loss: inf grad: -78.34757726646714\n",
      "iteration: 2220 loss: inf grad: -84.39893894884983\n",
      "iteration: 2230 loss: inf grad: -82.15015347713793\n",
      "iteration: 2240 loss: inf grad: -84.57217212049716\n",
      "iteration: 2240 loss: inf grad: -78.8569701990054\n",
      "iteration: 2130 loss: inf grad: -80.127689537152\n",
      "iteration: 2160 loss: inf grad: -83.20563130437084\n",
      "iteration: 2250 loss: inf grad: -84.21088282324052\n",
      "iteration: 2150 loss: inf grad: -78.49587751952177\n",
      "iteration: 2230 loss: inf grad: -84.50001029574486\n",
      "iteration: 2240 loss: inf grad: -82.29085957040117\n",
      "iteration: 2250 loss: inf grad: -84.85738624582444\n",
      "iteration: 2250 loss: inf grad: -79.05647732809867\n",
      "iteration: 2140 loss: inf grad: -80.27175924991502\n",
      "iteration: 2170 loss: inf grad: -83.40409110723954\n",
      "iteration: 2260 loss: inf grad: -84.40331053549559\n",
      "iteration: 2160 loss: inf grad: -78.65740100323316\n",
      "iteration: 2240 loss: inf grad: -84.58914407207567\n",
      "iteration: 2250 loss: inf grad: -82.46941915044678\n",
      "iteration: 2260 loss: inf grad: -85.07743117579328\n",
      "iteration: 2260 loss: inf grad: -79.32188570960429\n",
      "iteration: 2150 loss: inf grad: -80.38354275431385\n",
      "iteration: 2180 loss: inf grad: -83.59713649811407\n",
      "iteration: 2270 loss: inf grad: -84.60841313287324\n",
      "iteration: 2250 loss: inf grad: -84.67833936166491\n",
      "iteration: 2170 loss: inf grad: -78.82056445341601\n",
      "iteration: 2260 loss: inf grad: -82.66627296855611\n",
      "iteration: 2270 loss: inf grad: -85.23501806766551\n",
      "iteration: 2270 loss: inf grad: -79.54442375589943\n",
      "iteration: 2160 loss: inf grad: -80.50068416138717\n",
      "iteration: 2190 loss: inf grad: -83.7891891319362\n",
      "iteration: 2280 loss: inf grad: -84.86737081196765\n",
      "iteration: 2260 loss: inf grad: -84.77238406210006\n",
      "iteration: 2270 loss: inf grad: -82.81254726050167\n",
      "iteration: 2180 loss: inf grad: -78.98303940317147\n",
      "iteration: 2280 loss: inf grad: -85.39225315739314\n",
      "iteration: 2280 loss: inf grad: -79.71792873588572\n",
      "iteration: 2170 loss: inf grad: -80.61499341494581\n",
      "iteration: 2290 loss: inf grad: -85.07445880081052\n",
      "iteration: 2200 loss: inf grad: -83.95108357315485\n",
      "iteration: 2270 loss: inf grad: -84.86106818457529\n",
      "iteration: 2280 loss: inf grad: -82.90886692326553\n",
      "iteration: 2190 loss: inf grad: -79.14190742950389\n",
      "iteration: 2290 loss: inf grad: -85.57660681737849\n",
      "iteration: 2290 loss: inf grad: -79.88011839877129\n",
      "iteration: 2180 loss: inf grad: -80.76138933463334\n",
      "iteration: 2300 loss: inf grad: -85.25687900568046\n",
      "iteration: 2280 loss: inf grad: -84.94871140017725\n",
      "iteration: 2210 loss: inf grad: -84.0556770887975\n",
      "iteration: 2290 loss: inf grad: -82.98562572124314\n",
      "iteration: 2200 loss: inf grad: -79.29196846013531\n",
      "iteration: 2300 loss: inf grad: -85.75226731273884\n",
      "iteration: 2300 loss: inf grad: -80.03574969717582\n",
      "iteration: 2190 loss: inf grad: -80.92992172078542\n",
      "iteration: 2310 loss: inf grad: -85.47297509735021\n",
      "iteration: 2290 loss: inf grad: -85.05058860924152\n",
      "iteration: 2300 loss: inf grad: -83.08787972481639\n",
      "iteration: 2220 loss: inf grad: -84.17156432612995\n",
      "iteration: 2210 loss: inf grad: -79.44472789076802\n",
      "iteration: 2310 loss: inf grad: -85.89496532227248\n",
      "iteration: 2310 loss: inf grad: -80.25731384656507\n",
      "iteration: 2200 loss: inf grad: -81.04793209735928\n",
      "iteration: 2320 loss: inf grad: -85.64753094038144\n",
      "iteration: 2300 loss: inf grad: -85.1746000319788\n",
      "iteration: 2220 loss: inf grad: -79.60011164766466\n",
      "iteration: 2320 loss: inf grad: -86.05375645256424\n",
      "iteration: 2310 loss: inf grad: -83.2239955754084\n",
      "iteration: 2320 loss: inf grad: -80.51501803764275\n",
      "iteration: 2230 loss: inf grad: -84.30109142024376\n",
      "iteration: 2210 loss: inf grad: -81.1536354100801\n",
      "iteration: 2330 loss: inf grad: -85.80103482020718\n",
      "iteration: 2310 loss: inf grad: -85.31728765407453\n",
      "iteration: 2330 loss: inf grad: -86.21499656185955\n",
      "iteration: 2230 loss: inf grad: -79.77058260602774\n",
      "iteration: 2330 loss: inf grad: -80.7028500024009\n",
      "iteration: 2320 loss: inf grad: -83.34808693502544\n",
      "iteration: 2240 loss: inf grad: -84.43195794655936\n",
      "iteration: 2340 loss: inf grad: -85.97039373993931\n",
      "iteration: 2220 loss: inf grad: -81.27801313559924\n",
      "iteration: 2320 loss: inf grad: -85.45159745205376\n",
      "iteration: 2340 loss: inf grad: -86.3963306563108\n",
      "iteration: 2240 loss: inf grad: -79.94559710758051\n",
      "iteration: 2340 loss: inf grad: -80.85831402757186\n",
      "iteration: 2330 loss: inf grad: -83.44783898081491\n",
      "iteration: 2250 loss: inf grad: -84.57247322596155\n",
      "iteration: 2350 loss: inf grad: -86.17526982341673\n",
      "iteration: 2230 loss: inf grad: -81.39417943903707\n",
      "iteration: 2330 loss: inf grad: -85.58661958877101\n",
      "iteration: 2350 loss: inf grad: -86.63733946720079\n",
      "iteration: 2250 loss: inf grad: -80.05740924063466\n",
      "iteration: 2350 loss: inf grad: -81.01772375587176\n",
      "iteration: 2340 loss: inf grad: -83.58021837093693\n",
      "iteration: 2260 loss: inf grad: -84.69186131459378\n",
      "iteration: 2360 loss: inf grad: -86.36084081605219\n",
      "iteration: 2240 loss: inf grad: -81.50144518959891\n",
      "iteration: 2340 loss: inf grad: -85.69902521450061\n",
      "iteration: 2360 loss: inf grad: -86.85287250056629\n",
      "iteration: 2360 loss: inf grad: -81.1616623763642\n",
      "iteration: 2260 loss: inf grad: -80.14425914705402\n",
      "iteration: 2350 loss: inf grad: -83.7763159354973\n",
      "iteration: 2370 loss: inf grad: -86.51943026166185\n",
      "iteration: 2270 loss: inf grad: -84.77630715663156\n",
      "iteration: 2350 loss: inf grad: -85.83147449651545\n",
      "iteration: 2250 loss: inf grad: -81.64178140185025\n",
      "iteration: 2370 loss: inf grad: -87.10317525179782\n",
      "iteration: 2370 loss: inf grad: -81.34578656259689\n",
      "iteration: 2360 loss: inf grad: -83.98548818029931\n",
      "iteration: 2270 loss: inf grad: -80.23434547753001\n",
      "iteration: 2380 loss: inf grad: -86.64995583654539\n",
      "iteration: 2360 loss: inf grad: -85.95693335908805\n",
      "iteration: 2380 loss: inf grad: -87.38939190908235\n",
      "iteration: 2380 loss: inf grad: -81.59909968099767\n",
      "iteration: 2280 loss: inf grad: -84.86368309464177\n",
      "iteration: 2260 loss: inf grad: -81.80288774140033\n",
      "iteration: 2390 loss: inf grad: -86.73787086187409\n",
      "iteration: 2370 loss: inf grad: -84.16483761169727\n",
      "iteration: 2370 loss: inf grad: -86.06395717738366\n",
      "iteration: 2280 loss: inf grad: -80.33938539264192\n",
      "iteration: 2390 loss: inf grad: -87.57116587575385\n",
      "iteration: 2390 loss: inf grad: -81.83146597547147\n",
      "iteration: 2290 loss: inf grad: -85.00553271285594\n",
      "iteration: 2270 loss: inf grad: -81.95721223270226\n",
      "iteration: 2400 loss: inf grad: -86.84864746581187\n",
      "iteration: 2380 loss: inf grad: -84.28700802625532\n",
      "iteration: 2380 loss: inf grad: -86.18775792784757\n",
      "iteration: 2400 loss: inf grad: -87.74015216321473\n",
      "iteration: 2400 loss: inf grad: -82.06700960302766\n",
      "iteration: 2290 loss: inf grad: -80.46048724475742\n",
      "iteration: 2300 loss: inf grad: -85.17728478102487\n",
      "iteration: 2280 loss: inf grad: -82.0874420943064\n",
      "iteration: 2410 loss: inf grad: -87.03124191820913\n",
      "iteration: 2390 loss: inf grad: -84.41951512889895\n",
      "iteration: 2390 loss: inf grad: -86.3030501578715\n",
      "iteration: 2410 loss: inf grad: -82.27966627910033\n",
      "iteration: 2410 loss: inf grad: -87.90856433969876\n",
      "iteration: 2300 loss: inf grad: -80.60192059696196\n",
      "iteration: 2310 loss: inf grad: -85.36173774229717\n",
      "iteration: 2290 loss: inf grad: -82.22671745374643\n",
      "iteration: 2420 loss: inf grad: -87.22038960604979\n",
      "iteration: 2400 loss: inf grad: -84.58213006478707\n",
      "iteration: 2400 loss: inf grad: -86.40312002796898\n",
      "iteration: 2420 loss: inf grad: -82.51584341452111\n",
      "iteration: 2420 loss: inf grad: -88.02875621686007\n",
      "iteration: 2310 loss: inf grad: -80.75651821145128\n",
      "iteration: 2300 loss: inf grad: -82.39413373018647\n",
      "iteration: 2320 loss: inf grad: -85.54309778603235\n",
      "iteration: 2430 loss: inf grad: -87.33404623156187\n",
      "iteration: 2410 loss: inf grad: -84.75055969766164\n",
      "iteration: 2410 loss: inf grad: -86.50931501701439\n",
      "iteration: 2430 loss: inf grad: -82.70574316762588\n",
      "iteration: 2430 loss: inf grad: -88.11655337068893\n",
      "iteration: 2310 loss: inf grad: -82.55240754640973\n",
      "iteration: 2320 loss: inf grad: -80.93055339587204\n",
      "iteration: 2330 loss: inf grad: -85.71089818157093\n",
      "iteration: 2440 loss: inf grad: -87.4806379873655\n",
      "iteration: 2420 loss: inf grad: -84.8718977555285\n",
      "iteration: 2420 loss: inf grad: -86.61485678113138\n",
      "iteration: 2440 loss: inf grad: -82.87879481265742\n",
      "iteration: 2440 loss: inf grad: -88.19324152863639\n",
      "iteration: 2320 loss: inf grad: -82.65947992708706\n",
      "iteration: 2450 loss: inf grad: -87.63895375270167\n",
      "iteration: 2340 loss: inf grad: -85.84580638472036\n",
      "iteration: 2330 loss: inf grad: -81.09343105228767\n",
      "iteration: 2430 loss: inf grad: -86.74789900457921\n",
      "iteration: 2430 loss: inf grad: -84.95656055520064\n",
      "iteration: 2450 loss: inf grad: -83.03376788353134\n",
      "iteration: 2450 loss: inf grad: -88.29318382213044\n",
      "iteration: 2330 loss: inf grad: -82.73193247879524\n",
      "iteration: 2460 loss: inf grad: -87.78292592873541\n",
      "iteration: 2340 loss: inf grad: -81.24496147315747\n",
      "iteration: 2350 loss: inf grad: -85.96115397669715\n",
      "iteration: 2440 loss: inf grad: -86.88020480658875\n",
      "iteration: 2440 loss: inf grad: -85.03231136292308\n",
      "iteration: 2460 loss: inf grad: -83.20483570807403\n",
      "iteration: 2460 loss: inf grad: -88.41968021170959\n",
      "iteration: 2340 loss: inf grad: -82.81397375547009\n",
      "iteration: 2470 loss: inf grad: -87.90317113331096\n",
      "iteration: 2350 loss: inf grad: -81.40872603688507\n",
      "iteration: 2360 loss: inf grad: -86.08744698024395\n",
      "iteration: 2450 loss: inf grad: -87.01562531557032\n",
      "iteration: 2470 loss: inf grad: -83.36522151847996\n",
      "iteration: 2450 loss: inf grad: -85.1218360337133\n",
      "iteration: 2470 loss: inf grad: -88.55484177395155\n",
      "iteration: 2350 loss: inf grad: -82.948257464449\n",
      "iteration: 2480 loss: inf grad: -88.03176549669217\n",
      "iteration: 2360 loss: inf grad: -81.60645752187605\n",
      "iteration: 2460 loss: inf grad: -87.18163717717816\n",
      "iteration: 2370 loss: inf grad: -86.21604463547827\n",
      "iteration: 2480 loss: inf grad: -83.52821633892687\n",
      "iteration: 2480 loss: inf grad: -88.69522998848309\n",
      "iteration: 2460 loss: inf grad: -85.21376990656705\n",
      "iteration: 2360 loss: inf grad: -83.11548092739372\n",
      "iteration: 2490 loss: inf grad: -88.10827378336919\n",
      "iteration: 2470 loss: inf grad: -87.356277694502\n",
      "iteration: 2370 loss: inf grad: -81.77399430964675\n",
      "iteration: 2380 loss: inf grad: -86.33458617646829\n",
      "iteration: 2490 loss: inf grad: -83.64363798916118\n",
      "iteration: 2490 loss: inf grad: -88.85356395856374\n",
      "iteration: 2470 loss: inf grad: -85.28003359306118\n",
      "iteration: 2500 loss: inf grad: -88.18466252053008\n",
      "iteration: 2370 loss: inf grad: -83.29268883183289\n",
      "iteration: 2380 loss: inf grad: -81.9052977354389iteration: 2480 loss: inf grad: -87.5513562655196\n",
      "\n",
      "iteration: 2500 loss: inf grad: -83.74945114538433\n",
      "iteration: 2390 loss: inf grad: -86.43320312672702\n",
      "iteration: 2500 loss: inf grad: -89.02204806289066\n",
      "iteration: 2480 loss: inf grad: -85.36327909678775\n",
      "iteration: 2510 loss: inf grad: -88.29120423690952\n",
      "iteration: 2380 loss: inf grad: -83.46588080952836\n",
      "iteration: 2490 loss: inf grad: -87.7376166846322\n",
      "iteration: 2390 loss: inf grad: -82.05094791350652\n",
      "iteration: 2510 loss: inf grad: -83.91981780035889\n",
      "iteration: 2510 loss: inf grad: -89.19663690517669\n",
      "iteration: 2400 loss: inf grad: -86.57224916579229\n",
      "iteration: 2490 loss: inf grad: -85.45465128464858\n",
      "iteration: 2520 loss: inf grad: -88.45545102520481\n",
      "iteration: 2390 loss: inf grad: -83.62128410638687\n",
      "iteration: 2520 loss: inf grad: -84.11925038661933\n",
      "iteration: 2400 loss: inf grad: -82.2241072708405\n",
      "iteration: 2500 loss: inf grad: -87.89201170011347\n",
      "iteration: 2520 loss: inf grad: -89.30791904111231\n",
      "iteration: 2500 loss: inf grad: -85.57340101591399\n",
      "iteration: 2410 loss: inf grad: -86.77684489790292\n",
      "iteration: 2530 loss: inf grad: -88.61075719711005\n",
      "iteration: 2400 loss: inf grad: -83.78422612391643\n",
      "iteration: 2530 loss: inf grad: -84.40430488174741\n",
      "iteration: 2410 loss: inf grad: -82.3856915758717\n",
      "iteration: 2510 loss: inf grad: -88.018102426703\n",
      "iteration: 2530 loss: inf grad: -89.43645824737973\n",
      "iteration: 2510 loss: inf grad: -85.67678760882227\n",
      "iteration: 2420 loss: inf grad: -86.94288197985897\n",
      "iteration: 2540 loss: inf grad: -88.74588428176679\n",
      "iteration: 2410 loss: inf grad: -83.94499119218648\n",
      "iteration: 2540 loss: inf grad: -84.64353138162767\n",
      "iteration: 2420 loss: inf grad: -82.52071573474717\n",
      "iteration: 2520 loss: inf grad: -88.12395116516316\n",
      "iteration: 2540 loss: inf grad: -89.55460902239076\n",
      "iteration: 2520 loss: inf grad: -85.80913987677981\n",
      "iteration: 2430 loss: inf grad: -87.07309018795833\n",
      "iteration: 2550 loss: inf grad: -88.85715943759541\n",
      "iteration: 2420 loss: inf grad: -84.1268973534069\n",
      "iteration: 2550 loss: inf grad: -84.84560473761829\n",
      "iteration: 2430 loss: inf grad: -82.65799367400903\n",
      "iteration: 2530 loss: inf grad: -88.26770623849319\n",
      "iteration: 2550 loss: inf grad: -89.67040875049486\n",
      "iteration: 2440 loss: inf grad: -87.20971963761158\n",
      "iteration: 2560 loss: inf grad: -88.95623561144777\n",
      "iteration: 2530 loss: inf grad: -85.9991681810179\n",
      "iteration: 2430 loss: inf grad: -84.2818380784825\n",
      "iteration: 2560 loss: inf grad: -85.0246789255268\n",
      "iteration: 2440 loss: inf grad: -82.84724459353583\n",
      "iteration: 2540 loss: inf grad: -88.4501399036169\n",
      "iteration: 2560 loss: inf grad: -89.80311118640134\n",
      "iteration: 2570 loss: inf grad: -89.04524318425182\n",
      "iteration: 2450 loss: inf grad: -87.3552718991117\n",
      "iteration: 2540 loss: inf grad: -86.19069193156164\n",
      "iteration: 2440 loss: inf grad: -84.42554016215003\n",
      "iteration: 2570 loss: inf grad: -85.14222889395698\n",
      "iteration: 2550 loss: inf grad: -88.60667846910468\n",
      "iteration: 2570 loss: inf grad: -89.93321582951617\n",
      "iteration: 2450 loss: inf grad: -83.05662255534807\n",
      "iteration: 2580 loss: inf grad: -89.15811310278825\n",
      "iteration: 2580 loss: inf grad: -85.21358456524977\n",
      "iteration: 2460 loss: inf grad: -87.50287622028812\n",
      "iteration: 2560 loss: inf grad: -88.74004472898375\n",
      "iteration: 2450 loss: inf grad: -84.56630720695986\n",
      "iteration: 2580 loss: inf grad: -90.04445414412336\n",
      "iteration: 2550 loss: inf grad: -86.36936861257308\n",
      "iteration: 2460 loss: inf grad: -83.26715156501804\n",
      "iteration: 2590 loss: inf grad: -89.3082035288449\n",
      "iteration: 2470 loss: inf grad: -87.67356531315917\n",
      "iteration: 2570 loss: inf grad: -88.85091311363973\n",
      "iteration: 2590 loss: inf grad: -90.17923573921101\n",
      "iteration: 2590 loss: inf grad: -85.33977482064074\n",
      "iteration: 2460 loss: inf grad: -84.71664174771198\n",
      "iteration: 2560 loss: inf grad: -86.50641294696322\n",
      "iteration: 2470 loss: inf grad: -83.44666470043565\n",
      "iteration: 2600 loss: inf grad: -89.50071262011545\n",
      "iteration: 2580 loss: inf grad: -88.92529253610832\n",
      "iteration: 2480 loss: inf grad: -87.82672469872558\n",
      "iteration: 2600 loss: inf grad: -90.31435143259387\n",
      "iteration: 2600 loss: inf grad: -85.55380156188897\n",
      "iteration: 2470 loss: inf grad: -84.85595257078623\n",
      "iteration: 2570 loss: inf grad: -86.60015224889298\n",
      "iteration: 2480 loss: inf grad: -83.56143098836594\n",
      "iteration: 2610 loss: inf grad: -89.68471005471724\n",
      "iteration: 2590 loss: inf grad: -88.9928950849077\n",
      "iteration: 2610 loss: inf grad: -90.4702148205117\n",
      "iteration: 2610 loss: inf grad: -85.69422506349702\n",
      "iteration: 2480 loss: inf grad: -84.97024794734644\n",
      "iteration: 2490 loss: inf grad: -87.93997922524474\n",
      "iteration: 2490 loss: inf grad: -83.64568081812287\n",
      "iteration: 2580 loss: inf grad: -86.71125511547095\n",
      "iteration: 2620 loss: inf grad: -89.87031055801403\n",
      "iteration: 2620 loss: inf grad: -90.64041255618135\n",
      "iteration: 2620 loss: inf grad: -85.87726980135398\n",
      "iteration: 2490 loss: inf grad: -85.12394371207589\n",
      "iteration: 2600 loss: inf grad: -89.07522778481429\n",
      "iteration: 2500 loss: inf grad: -88.06450266600766\n",
      "iteration: 2500 loss: inf grad: -83.72637305385146\n",
      "iteration: 2590 loss: inf grad: -86.82619758138853\n",
      "iteration: 2630 loss: inf grad: -90.01701360004947\n",
      "iteration: 2630 loss: inf grad: -90.76326206648977\n",
      "iteration: 2630 loss: inf grad: -86.04938288048777\n",
      "iteration: 2500 loss: inf grad: -85.30813097667996\n",
      "iteration: 2510 loss: inf grad: -88.19579072149716\n",
      "iteration: 2510 loss: inf grad: -83.82434652367584\n",
      "iteration: 2610 loss: inf grad: -89.20886572093136\n",
      "iteration: 2600 loss: inf grad: -86.92838943525717\n",
      "iteration: 2640 loss: inf grad: -90.11374249352022\n",
      "iteration: 2640 loss: inf grad: -90.89013280385298\n",
      "iteration: 2640 loss: inf grad: -86.1652842917269\n",
      "iteration: 2510 loss: inf grad: -85.4540320263547\n",
      "iteration: 2520 loss: inf grad: -88.3239735687938\n",
      "iteration: 2520 loss: inf grad: -83.95266827580214\n",
      "iteration: 2620 loss: inf grad: -89.38179277232805\n",
      "iteration: 2650 loss: inf grad: -90.21220816230917\n",
      "iteration: 2610 loss: inf grad: -87.06727294150502\n",
      "iteration: 2650 loss: inf grad: -91.0292733896607\n",
      "iteration: 2650 loss: inf grad: -86.28955426273257\n",
      "iteration: 2520 loss: inf grad: -85.58162773433926\n",
      "iteration: 2530 loss: inf grad: -84.09641564353458\n",
      "iteration: 2530 loss: inf grad: -88.48458595955296\n",
      "iteration: 2630 loss: inf grad: -89.52008586551655\n",
      "iteration: 2660 loss: inf grad: -90.27395604352927\n",
      "iteration: 2620 loss: inf grad: -87.21464158855176\n",
      "iteration: 2660 loss: inf grad: -91.13293728552075\n",
      "iteration: 2660 loss: inf grad: -86.45891979658309\n",
      "iteration: 2530 loss: inf grad: -85.75333968294234\n",
      "iteration: 2540 loss: inf grad: -84.20798565984336\n",
      "iteration: 2640 loss: inf grad: -89.65641855864345\n",
      "iteration: 2670 loss: inf grad: -90.3348061903545iteration: 2630 loss: inf grad: -87.35751260799421\n",
      "\n",
      "iteration: 2540 loss: inf grad: -88.63771983436959\n",
      "iteration: 2670 loss: inf grad: -91.2373614037055\n",
      "iteration: 2670 loss: inf grad: -86.58447808024329\n",
      "iteration: 2540 loss: inf grad: -85.9375019621065\n",
      "iteration: 2550 loss: inf grad: -84.29986030162729\n",
      "iteration: 2650 loss: inf grad: -89.84194130043048\n",
      "iteration: 2640 loss: inf grad: -87.48474365931241\n",
      "iteration: 2680 loss: inf grad: -90.40582433895702\n",
      "iteration: 2550 loss: inf grad: -88.84020449602943\n",
      "iteration: 2680 loss: inf grad: -86.68501172339033\n",
      "iteration: 2680 loss: inf grad: -91.3507670008309\n",
      "iteration: 2550 loss: inf grad: -86.08137063105737\n",
      "iteration: 2560 loss: inf grad: -84.38408794760889\n",
      "iteration: 2660 loss: inf grad: -90.02044538953655\n",
      "iteration: 2690 loss: inf grad: -90.47275713515998\n",
      "iteration: 2650 loss: inf grad: -87.60691504532286\n",
      "iteration: 2560 loss: inf grad: -89.01017721856711\n",
      "iteration: 2690 loss: inf grad: -91.4713915078644\n",
      "iteration: 2690 loss: inf grad: -86.79393085563433\n",
      "iteration: 2560 loss: inf grad: -86.231726497266\n",
      "iteration: 2570 loss: inf grad: -84.48282077436308\n",
      "iteration: 2670 loss: inf grad: -90.14027738625555\n",
      "iteration: 2700 loss: inf grad: -90.55571728207528\n",
      "iteration: 2660 loss: inf grad: -87.71315776210298\n",
      "iteration: 2700 loss: inf grad: -86.86492758178923\n",
      "iteration: 2570 loss: inf grad: -89.16247235825969\n",
      "iteration: 2700 loss: inf grad: -91.58042625718329\n",
      "iteration: 2580 loss: inf grad: -84.57447236865917\n",
      "iteration: 2570 loss: inf grad: -86.38458755060844\n",
      "iteration: 2710 loss: inf grad: -90.70255008449918\n",
      "iteration: 2670 loss: inf grad: -87.81254630814354iteration: 2680 loss: inf grad: -90.23373996659333\n",
      "\n",
      "iteration: 2710 loss: inf grad: -86.9570217708611\n",
      "iteration: 2580 loss: inf grad: -89.32709983455402\n",
      "iteration: 2710 loss: inf grad: -91.66984540165785\n",
      "iteration: 2590 loss: inf grad: -84.65697122262628\n",
      "iteration: 2580 loss: inf grad: -86.50465541864881\n",
      "iteration: 2680 loss: inf grad: -87.90659031122001\n",
      "iteration: 2720 loss: inf grad: -90.86090032446029\n",
      "iteration: 2690 loss: inf grad: -90.32626579871147\n",
      "iteration: 2720 loss: inf grad: -87.08894235102748\n",
      "iteration: 2590 loss: inf grad: -89.48909650608968\n",
      "iteration: 2720 loss: inf grad: -91.74664534590462\n",
      "iteration: 2600 loss: inf grad: -84.74233628240943\n",
      "iteration: 2590 loss: inf grad: -86.6038591381238\n",
      "iteration: 2690 loss: inf grad: -87.97231461581882\n",
      "iteration: 2730 loss: inf grad: -91.02015242585611\n",
      "iteration: 2700 loss: inf grad: -90.43019511249852\n",
      "iteration: 2730 loss: inf grad: -87.18615712126217\n",
      "iteration: 2730 loss: inf grad: -91.82874493955403\n",
      "iteration: 2600 loss: inf grad: -89.65626340973438\n",
      "iteration: 2610 loss: inf grad: -84.85051985701702\n",
      "iteration: 2600 loss: inf grad: -86.6849873676857\n",
      "iteration: 2740 loss: inf grad: -91.20571868157649\n",
      "iteration: 2700 loss: inf grad: -88.0212865359182\n",
      "iteration: 2710 loss: inf grad: -90.54396390030502\n",
      "iteration: 2740 loss: inf grad: -87.24973653663548\n",
      "iteration: 2740 loss: inf grad: -91.90984659183769\n",
      "iteration: 2610 loss: inf grad: -89.80119734804882\n",
      "iteration: 2610 loss: inf grad: -86.83645683866592\n",
      "iteration: 2620 loss: inf grad: -84.96916260446228\n",
      "iteration: 2750 loss: inf grad: -91.31586723410251\n",
      "iteration: 2710 loss: inf grad: -88.08993741276075\n",
      "iteration: 2720 loss: inf grad: -90.64121371031507\n",
      "iteration: 2750 loss: inf grad: -87.35057196840313\n",
      "iteration: 2750 loss: inf grad: -91.96972748969046\n",
      "iteration: 2620 loss: inf grad: -89.92862629901015\n",
      "iteration: 2620 loss: inf grad: -87.02328362808254\n",
      "iteration: 2760 loss: inf grad: -91.40208365952884\n",
      "iteration: 2720 loss: inf grad: -88.15850486609735\n",
      "iteration: 2630 loss: inf grad: -85.11084485380188\n",
      "iteration: 2730 loss: inf grad: -90.70782811001921\n",
      "iteration: 2760 loss: inf grad: -87.57785830936709\n",
      "iteration: 2760 loss: inf grad: -92.03377238617048\n",
      "iteration: 2770 loss: inf grad: -91.47600035051735\n",
      "iteration: 2630 loss: inf grad: -90.0962801948182\n",
      "iteration: 2630 loss: inf grad: -87.1631870953563\n",
      "iteration: 2730 loss: inf grad: -88.21741474342504\n",
      "iteration: 2640 loss: inf grad: -85.25737124214167\n",
      "iteration: 2770 loss: inf grad: -87.87992946773898\n",
      "iteration: 2740 loss: inf grad: -90.77754847020609\n",
      "iteration: 2770 loss: inf grad: -92.12330487714573\n",
      "iteration: 2780 loss: inf grad: -91.56154913286676\n",
      "iteration: 2740 loss: inf grad: -88.29724022610624\n",
      "iteration: 2640 loss: inf grad: -87.33219041041833\n",
      "iteration: 2640 loss: inf grad: -90.21663377868941\n",
      "iteration: 2650 loss: inf grad: -85.35437642279723\n",
      "iteration: 2780 loss: inf grad: -88.08855286782656\n",
      "iteration: 2750 loss: inf grad: -90.8562235986449\n",
      "iteration: 2780 loss: inf grad: -92.24852437843131\n",
      "iteration: 2790 loss: inf grad: -91.67410574262655\n",
      "iteration: 2750 loss: inf grad: -88.36439752546937\n",
      "iteration: 2650 loss: inf grad: -87.53809134584469\n",
      "iteration: 2650 loss: inf grad: -90.32735122525413\n",
      "iteration: 2660 loss: inf grad: -85.41181160016791\n",
      "iteration: 2760 loss: inf grad: -90.96001798808163\n",
      "iteration: 2790 loss: inf grad: -88.203197427084\n",
      "iteration: 2800 loss: inf grad: -91.7767980670977\n",
      "iteration: 2790 loss: inf grad: -92.40110661053166\n",
      "iteration: 2760 loss: inf grad: -88.43928591060731\n",
      "iteration: 2660 loss: inf grad: -87.6820402086532\n",
      "iteration: 2660 loss: inf grad: -90.45528979543992\n",
      "iteration: 2670 loss: inf grad: -85.47337721729193\n",
      "iteration: 2770 loss: inf grad: -91.06952792202867\n",
      "iteration: 2800 loss: inf grad: -88.33973545920998\n",
      "iteration: 2810 loss: inf grad: -91.8587554948775\n",
      "iteration: 2800 loss: inf grad: -92.51454008489964\n",
      "iteration: 2770 loss: inf grad: -88.53966939266724\n",
      "iteration: 2670 loss: inf grad: -87.79488777292617\n",
      "iteration: 2670 loss: inf grad: -90.59421232655072\n",
      "iteration: 2780 loss: inf grad: -91.1916853614505\n",
      "iteration: 2810 loss: inf grad: -88.47585141787238\n",
      "iteration: 2680 loss: inf grad: -85.54708711649917\n",
      "iteration: 2820 loss: inf grad: -91.97853518095182\n",
      "iteration: 2810 loss: inf grad: -92.58440008730582\n",
      "iteration: 2780 loss: inf grad: -88.61435360258596\n",
      "iteration: 2680 loss: inf grad: -87.90623953838454\n",
      "iteration: 2680 loss: inf grad: -90.73978413950678\n",
      "iteration: 2790 loss: inf grad: -91.31896281020641\n",
      "iteration: 2690 loss: inf grad: -85.62436511649263\n",
      "iteration: 2820 loss: inf grad: -88.60949024395065\n",
      "iteration: 2830 loss: inf grad: -92.12334616457092\n",
      "iteration: 2820 loss: inf grad: -92.6586113293126\n",
      "iteration: 2790 loss: inf grad: -88.68860929420569\n",
      "iteration: 2690 loss: inf grad: -88.04266950713057\n",
      "iteration: 2690 loss: inf grad: -90.89437955562875\n",
      "iteration: 2800 loss: inf grad: -91.4836324344606\n",
      "iteration: 2700 loss: inf grad: -85.6964958790232\n",
      "iteration: 2830 loss: inf grad: -88.77141592424617\n",
      "iteration: 2840 loss: inf grad: -92.27058473267694\n",
      "iteration: 2830 loss: inf grad: -92.72602587607209\n",
      "iteration: 2800 loss: inf grad: -88.75113598452607\n",
      "iteration: 2700 loss: inf grad: -88.1597591261151\n",
      "iteration: 2700 loss: inf grad: -91.03898499088378\n",
      "iteration: 2810 loss: inf grad: -91.68201289004236\n",
      "iteration: 2710 loss: inf grad: -85.7610604397064\n",
      "iteration: 2840 loss: inf grad: -88.94384943546585\n",
      "iteration: 2850 loss: inf grad: -92.41656911204225\n",
      "iteration: 2840 loss: inf grad: -92.81244159878264\n",
      "iteration: 2810 loss: inf grad: -88.8044560416156\n",
      "iteration: 2710 loss: inf grad: -88.25240437585751\n",
      "iteration: 2820 loss: inf grad: -91.83609640723427\n",
      "iteration: 2850 loss: inf grad: -89.10360038887578\n",
      "iteration: 2860 loss: inf grad: -92.53516239082217\n",
      "iteration: 2710 loss: inf grad: -91.21508896189798\n",
      "iteration: 2850 loss: inf grad: -92.94138035001507\n",
      "iteration: 2720 loss: inf grad: -85.83661251519743\n",
      "iteration: 2820 loss: inf grad: -88.87186793354056\n",
      "iteration: 2720 loss: inf grad: -88.354830061108\n",
      "iteration: 2830 loss: inf grad: -91.99208092379607\n",
      "iteration: 2870 loss: inf grad: -92.63858551679252\n",
      "iteration: 2860 loss: inf grad: -89.2436076655926\n",
      "iteration: 2860 loss: inf grad: -93.06805181418079\n",
      "iteration: 2730 loss: inf grad: -85.95398492067847\n",
      "iteration: 2720 loss: inf grad: -91.34564588830267\n",
      "iteration: 2830 loss: inf grad: -88.92120107335572\n",
      "iteration: 2730 loss: inf grad: -88.48143996344567\n",
      "iteration: 2880 loss: inf grad: -92.7333844608649\n",
      "iteration: 2840 loss: inf grad: -92.10876882397821\n",
      "iteration: 2870 loss: inf grad: -89.36342071238721\n",
      "iteration: 2870 loss: inf grad: -93.22802224752687\n",
      "iteration: 2730 loss: inf grad: -91.48730916289591\n",
      "iteration: 2840 loss: inf grad: -88.97065710621602\n",
      "iteration: 2740 loss: inf grad: -86.1003022694659\n",
      "iteration: 2740 loss: inf grad: -88.64549671819498\n",
      "iteration: 2890 loss: inf grad: -92.81580600240977\n",
      "iteration: 2850 loss: inf grad: -92.17823074114001\n",
      "iteration: 2880 loss: inf grad: -89.50222861798896\n",
      "iteration: 2880 loss: inf grad: -93.35815597896986\n",
      "iteration: 2850 loss: inf grad: -89.05057066553542\n",
      "iteration: 2740 loss: inf grad: -91.6594790794691\n",
      "iteration: 2750 loss: inf grad: -88.82972201114318\n",
      "iteration: 2750 loss: inf grad: -86.18800880784678\n",
      "iteration: 2900 loss: inf grad: -92.90723727401256\n",
      "iteration: 2860 loss: inf grad: -92.27199058528788\n",
      "iteration: 2890 loss: inf grad: -89.63948685658423\n",
      "iteration: 2890 loss: inf grad: -93.45894006358881\n",
      "iteration: 2860 loss: inf grad: -89.16779845216305\n",
      "iteration: 2760 loss: inf grad: -88.95366802157417\n",
      "iteration: 2750 loss: inf grad: -91.78520929945418\n",
      "iteration: 2760 loss: inf grad: -86.26823805155973\n",
      "iteration: 2910 loss: inf grad: -93.01708059510676\n",
      "iteration: 2900 loss: inf grad: -89.78459172267753\n",
      "iteration: 2870 loss: inf grad: -92.3540296989536\n",
      "iteration: 2900 loss: inf grad: -93.58162013010433\n",
      "iteration: 2870 loss: inf grad: -89.30051397833293\n",
      "iteration: 2770 loss: inf grad: -89.07377994565417\n",
      "iteration: 2760 loss: inf grad: -91.88435080807486\n",
      "iteration: 2920 loss: inf grad: -93.14445113321956\n",
      "iteration: 2770 loss: inf grad: -86.3782577194225\n",
      "iteration: 2880 loss: inf grad: -92.42385778100066\n",
      "iteration: 2910 loss: inf grad: -89.90631595675978\n",
      "iteration: 2910 loss: inf grad: -93.66943969208462\n",
      "iteration: 2880 loss: inf grad: -89.46803483082772\n",
      "iteration: 2780 loss: inf grad: -89.23371107787997\n",
      "iteration: 2770 loss: inf grad: -91.97214121354867\n",
      "iteration: 2930 loss: inf grad: -93.27016743685229\n",
      "iteration: 2890 loss: inf grad: -92.50008070324978\n",
      "iteration: 2920 loss: inf grad: -89.99629493321743\n",
      "iteration: 2780 loss: inf grad: -86.52213891338454\n",
      "iteration: 2920 loss: inf grad: -93.74994879207719\n",
      "iteration: 2890 loss: inf grad: -89.60450223856137\n",
      "iteration: 2790 loss: inf grad: -89.36509100931059\n",
      "iteration: 2780 loss: inf grad: -92.06658986297295\n",
      "iteration: 2940 loss: inf grad: -93.38171100052308\n",
      "iteration: 2900 loss: inf grad: -92.60610383580594\n",
      "iteration: 2930 loss: inf grad: -90.12265483308086\n",
      "iteration: 2790 loss: inf grad: -86.71218025033275\n",
      "iteration: 2930 loss: inf grad: -93.83584698254006\n",
      "iteration: 2900 loss: inf grad: -89.72031315556752\n",
      "iteration: 2800 loss: inf grad: -89.4910401216199\n",
      "iteration: 2950 loss: inf grad: -93.44551615939119\n",
      "iteration: 2790 loss: inf grad: -92.20436320814667\n",
      "iteration: 2910 loss: inf grad: -92.69710508223173\n",
      "iteration: 2940 loss: inf grad: -90.23000711647836\n",
      "iteration: 2800 loss: inf grad: -86.92048076762008\n",
      "iteration: 2940 loss: inf grad: -93.94374588316333\n",
      "iteration: 2910 loss: inf grad: -89.82040865951382\n",
      "iteration: 2810 loss: inf grad: -89.62748376220699\n",
      "iteration: 2960 loss: inf grad: -93.48380373399999\n",
      "iteration: 2950 loss: inf grad: -90.35606999335562\n",
      "iteration: 2920 loss: inf grad: -92.79969873837524\n",
      "iteration: 2800 loss: inf grad: -92.37221826438858\n",
      "iteration: 2810 loss: inf grad: -87.09950421303236\n",
      "iteration: 2920 loss: inf grad: -89.93534439868799\n",
      "iteration: 2950 loss: inf grad: -94.06463177934121\n",
      "iteration: 2970 loss: inf grad: -93.51950066566785\n",
      "iteration: 2960 loss: inf grad: -90.45473652210097\n",
      "iteration: 2820 loss: inf grad: -89.76259539221519\n",
      "iteration: 2930 loss: inf grad: -92.91693298081819\n",
      "iteration: 2810 loss: inf grad: -92.52408931931856\n",
      "iteration: 2930 loss: inf grad: -90.05595964519317\n",
      "iteration: 2960 loss: inf grad: -94.17251778998414\n",
      "iteration: 2980 loss: inf grad: -93.56005809150346\n",
      "iteration: 2820 loss: inf grad: -87.2580195837934\n",
      "iteration: 2940 loss: inf grad: -93.00054900780503\n",
      "iteration: 2830 loss: inf grad: -89.89582498934126\n",
      "iteration: 2970 loss: inf grad: -90.53814889815716\n",
      "iteration: 2820 loss: inf grad: -92.6395917270532\n",
      "iteration: 2940 loss: inf grad: -90.15978805142146\n",
      "iteration: 2970 loss: inf grad: -94.29846099316897\n",
      "iteration: 2990 loss: inf grad: -93.60103372652466\n",
      "iteration: 2950 loss: inf grad: -93.04898251904464iteration: 2980 loss: inf grad: -90.6382030399791\n",
      "\n",
      "iteration: 2830 loss: inf grad: -87.38366741377166\n",
      "iteration: 2840 loss: inf grad: -90.00230918396241\n",
      "iteration: 2950 loss: inf grad: -90.2757250920082\n",
      "iteration: 2830 loss: inf grad: -92.69520973765145\n",
      "iteration: 2980 loss: inf grad: -94.4458508822807\n",
      "iteration: 3000 loss: inf grad: -93.65384462628336\n",
      "iteration: 2840 loss: inf grad: -87.50774585303542\n",
      "iteration: 2960 loss: inf grad: -93.09939211675095\n",
      "iteration: 2990 loss: inf grad: -90.75962821595071\n",
      "iteration: 2850 loss: inf grad: -90.09216707861775\n",
      "iteration: 2960 loss: inf grad: -90.41321822437189\n",
      "iteration: 2990 loss: inf grad: -94.59430132481772\n",
      "iteration: 2840 loss: inf grad: -92.72215514517839\n",
      "iteration: 3010 loss: inf grad: -93.71248935348217\n",
      "iteration: 3000 loss: inf grad: -90.87104542483299\n",
      "iteration: 2970 loss: inf grad: -93.17697049602748\n",
      "iteration: 2860 loss: inf grad: -90.18081171387846\n",
      "iteration: 2850 loss: inf grad: -87.65447721678872\n",
      "iteration: 2970 loss: inf grad: -90.52701742863283\n",
      "iteration: 3000 loss: inf grad: -94.74169881527908\n",
      "iteration: 3020 loss: inf grad: -93.78489352086741\n",
      "iteration: 2850 loss: inf grad: -92.74827143428557\n",
      "iteration: 3010 loss: inf grad: -90.96303931063679\n",
      "iteration: 2980 loss: inf grad: -93.26631690461228\n",
      "iteration: 2870 loss: inf grad: -90.29629245452904\n",
      "iteration: 2860 loss: inf grad: -87.75651179265331\n",
      "iteration: 2980 loss: inf grad: -90.61620585279934\n",
      "iteration: 3030 loss: inf grad: -93.89065516555434\n",
      "iteration: 3010 loss: inf grad: -94.8535629600147\n",
      "iteration: 3020 loss: inf grad: -91.06589992817439\n",
      "iteration: 2990 loss: inf grad: -93.33833266197834\n",
      "iteration: 2880 loss: inf grad: -90.45387000480503\n",
      "iteration: 2870 loss: inf grad: -87.82408050237242\n",
      "iteration: 2860 loss: inf grad: -92.78535106400368\n",
      "iteration: 2990 loss: inf grad: -90.71389710736487\n",
      "iteration: 3020 loss: inf grad: -94.95407802815313\n",
      "iteration: 3040 loss: inf grad: -94.00715404577886\n",
      "iteration: 3030 loss: inf grad: -91.20424250483843\n",
      "iteration: 3000 loss: inf grad: -93.40512379288323\n",
      "iteration: 2890 loss: inf grad: -90.5945092144278\n",
      "iteration: 2880 loss: inf grad: -87.89198301817427\n",
      "iteration: 2870 loss: inf grad: -92.83135154444923\n",
      "iteration: 3000 loss: inf grad: -90.85022768983632\n",
      "iteration: 3030 loss: inf grad: -95.05908301033901\n",
      "iteration: 3040 loss: inf grad: -91.36103250708912\n",
      "iteration: 3010 loss: inf grad: -93.49108285513941\n",
      "iteration: 3050 loss: inf grad: -94.09033710558862\n",
      "iteration: 2900 loss: inf grad: -90.70155261755875\n",
      "iteration: 2890 loss: inf grad: -87.97646343802094\n",
      "iteration: 2880 loss: inf grad: -92.88812525273164\n",
      "iteration: 3010 loss: inf grad: -91.00739251652436\n",
      "iteration: 3060 loss: inf grad: -94.21704043462924\n",
      "iteration: 3020 loss: inf grad: -93.56152614768669\n",
      "iteration: 3050 loss: inf grad: -91.51402274654393\n",
      "iteration: 3040 loss: inf grad: -95.13684643828066\n",
      "iteration: 2910 loss: inf grad: -90.82765152673196\n",
      "iteration: 2900 loss: inf grad: -88.06921843497355\n",
      "iteration: 2890 loss: inf grad: -92.9435564099984\n",
      "iteration: 3020 loss: inf grad: -91.1739778522576\n",
      "iteration: 3070 loss: inf grad: -94.39405453997944\n",
      "iteration: 3030 loss: inf grad: -93.61752703467744\n",
      "iteration: 3060 loss: inf grad: -91.67525265061016\n",
      "iteration: 3050 loss: inf grad: -95.20983645681774\n",
      "iteration: 2920 loss: inf grad: -90.9886981962118\n",
      "iteration: 2910 loss: inf grad: -88.15815684585414\n",
      "iteration: 2900 loss: inf grad: -93.00300252061203\n",
      "iteration: 3030 loss: inf grad: -91.3139766019931\n",
      "iteration: 3080 loss: inf grad: -94.5662234676488iteration: 3070 loss: inf grad: -91.83205315496392\n",
      "iteration: 3040 loss: inf grad: -93.6646045571551\n",
      "\n",
      "iteration: 3060 loss: inf grad: -95.28649655787271\n",
      "iteration: 2930 loss: inf grad: -91.13335023536047\n",
      "iteration: 2920 loss: inf grad: -88.29547727579651\n",
      "iteration: 2910 loss: inf grad: -93.0736735165962\n",
      "iteration: 3040 loss: inf grad: -91.41519695821279\n",
      "iteration: 3080 loss: inf grad: -91.97699070153489\n",
      "iteration: 3050 loss: inf grad: -93.73997217694443\n",
      "iteration: 3090 loss: inf grad: -94.67678600761585iteration: 3070 loss: inf grad: -95.37513179038734\n",
      "\n",
      "iteration: 2940 loss: inf grad: -91.30259013412274\n",
      "iteration: 2930 loss: inf grad: -88.46596748090717\n",
      "iteration: 2920 loss: inf grad: -93.15293792230369\n",
      "iteration: 3050 loss: inf grad: -91.5047605649178\n",
      "iteration: 3060 loss: inf grad: -93.83807144279791\n",
      "iteration: 3090 loss: inf grad: -92.07141643184806\n",
      "iteration: 3080 loss: inf grad: -95.47454725160284\n",
      "iteration: 2950 loss: inf grad: -91.45587841369706\n",
      "iteration: 2940 loss: inf grad: -88.63741011781012\n",
      "iteration: 3100 loss: inf grad: -94.74873611817098\n",
      "iteration: 2930 loss: inf grad: -93.26917892380533\n",
      "iteration: 3060 loss: inf grad: -91.5985357530106\n",
      "iteration: 3070 loss: inf grad: -93.93128598121686\n",
      "iteration: 3090 loss: inf grad: -95.54770600272269\n",
      "iteration: 3100 loss: inf grad: -92.15629096554198\n",
      "iteration: 2960 loss: inf grad: -91.55547713379455\n",
      "iteration: 2950 loss: inf grad: -88.74728746738168\n",
      "iteration: 3110 loss: inf grad: -94.83578038686525\n",
      "iteration: 2940 loss: inf grad: -93.40686246905467\n",
      "iteration: 3070 loss: inf grad: -91.68847481589228\n",
      "iteration: 3080 loss: inf grad: -94.03144445060978\n",
      "iteration: 3100 loss: inf grad: -95.57839087837498\n",
      "iteration: 3110 loss: inf grad: -92.27155211485655\n",
      "iteration: 3120 loss: inf grad: -94.90836681878906\n",
      "iteration: 2970 loss: inf grad: -91.64448613117614\n",
      "iteration: 2960 loss: inf grad: -88.83264274329727\n",
      "iteration: 2950 loss: inf grad: -93.5030296994143\n",
      "iteration: 3080 loss: inf grad: -91.79850589361001\n",
      "iteration: 3090 loss: inf grad: -94.12357104811113\n",
      "iteration: 3110 loss: inf grad: -95.61555308769503\n",
      "iteration: 3120 loss: inf grad: -92.39599050983219\n",
      "iteration: 3130 loss: inf grad: -94.98451118943991\n",
      "iteration: 2970 loss: inf grad: -88.9337331449689\n",
      "iteration: 2980 loss: inf grad: -91.73778346347959\n",
      "iteration: 3090 loss: inf grad: -91.90491055654618\n",
      "iteration: 3100 loss: inf grad: -94.19651551109203\n",
      "iteration: 2960 loss: inf grad: -93.59443750781666\n",
      "iteration: 3130 loss: inf grad: -92.50403799270524\n",
      "iteration: 3120 loss: inf grad: -95.69321786778045\n",
      "iteration: 3140 loss: inf grad: -95.12200384941943\n",
      "iteration: 2980 loss: inf grad: -89.05279943360678\n",
      "iteration: 2990 loss: inf grad: -91.79242422109351\n",
      "iteration: 3100 loss: inf grad: -92.02243550902841\n",
      "iteration: 3110 loss: inf grad: -94.30410874638594\n",
      "iteration: 3140 loss: inf grad: -92.59232820197147\n",
      "iteration: 3150 loss: inf grad: -95.28698356875726\n",
      "iteration: 3130 loss: inf grad: -95.82688748752742\n",
      "iteration: 2970 loss: inf grad: -93.68187329266992\n",
      "iteration: 2990 loss: inf grad: -89.19039941987751\n",
      "iteration: 3000 loss: inf grad: -91.82922088972889\n",
      "iteration: 3110 loss: inf grad: -92.10950979972938\n",
      "iteration: 3120 loss: inf grad: -94.43152711692603\n",
      "iteration: 3150 loss: inf grad: -92.69185763380122\n",
      "iteration: 3160 loss: inf grad: -95.37292166954819\n",
      "iteration: 3140 loss: inf grad: -95.97632936645266\n",
      "iteration: 2980 loss: inf grad: -93.75069577836382\n",
      "iteration: 3120 loss: inf grad: -92.21102773944037\n",
      "iteration: 3010 loss: inf grad: -91.86443721801118\n",
      "iteration: 3130 loss: inf grad: -94.51574573096099\n",
      "iteration: 3000 loss: inf grad: -89.38120370262945\n",
      "iteration: 3160 loss: inf grad: -92.82741848706948\n",
      "iteration: 3170 loss: inf grad: -95.4367377691919\n",
      "iteration: 2990 loss: inf grad: -93.82133308802558\n",
      "iteration: 3150 loss: inf grad: -96.10636578524935\n",
      "iteration: 3130 loss: inf grad: -92.29418899104323\n",
      "iteration: 3140 loss: inf grad: -94.55934540552167\n",
      "iteration: 3020 loss: inf grad: -91.90630768472523\n",
      "iteration: 3010 loss: inf grad: -89.5486042856252\n",
      "iteration: 3170 loss: inf grad: -92.9535555864097\n",
      "iteration: 3180 loss: inf grad: -95.53307467875759\n",
      "iteration: 3000 loss: inf grad: -93.90378434107092\n",
      "iteration: 3140 loss: inf grad: -92.34021059452633\n",
      "iteration: 3160 loss: inf grad: -96.22110230466731\n",
      "iteration: 3150 loss: inf grad: -94.60441095502885\n",
      "iteration: 3030 loss: inf grad: -91.96711227212882\n",
      "iteration: 3020 loss: inf grad: -89.6369819565571\n",
      "iteration: 3180 loss: inf grad: -93.05005459526384\n",
      "iteration: 3190 loss: inf grad: -95.70251753122201\n",
      "iteration: 3010 loss: inf grad: -94.01974211845953\n",
      "iteration: 3150 loss: inf grad: -92.36745696900789\n",
      "iteration: 3160 loss: inf grad: -94.67188347987526\n",
      "iteration: 3040 loss: inf grad: -92.01847090115861\n",
      "iteration: 3030 loss: inf grad: -89.70463629546421\n",
      "iteration: 3170 loss: inf grad: -96.31727951549827\n",
      "iteration: 3190 loss: inf grad: -93.1342378395754\n",
      "iteration: 3200 loss: inf grad: -95.82136203456501\n",
      "iteration: 3160 loss: inf grad: -92.4004942451101\n",
      "iteration: 3170 loss: inf grad: -94.79705893799003\n",
      "iteration: 3050 loss: inf grad: -92.06800793611876\n",
      "iteration: 3040 loss: inf grad: -89.78654085641725\n",
      "iteration: 3180 loss: inf grad: -96.39209578802516\n",
      "iteration: 3020 loss: inf grad: -94.14844094212683\n",
      "iteration: 3200 loss: inf grad: -93.23272631829454\n",
      "iteration: 3210 loss: inf grad: -95.90862509293956\n",
      "iteration: 3170 loss: inf grad: -92.45534094656958\n",
      "iteration: 3180 loss: inf grad: -94.96487705777321\n",
      "iteration: 3060 loss: inf grad: -92.10963906439676\n",
      "iteration: 3190 loss: inf grad: -96.48330578054303\n",
      "iteration: 3050 loss: inf grad: -89.92126410994277\n",
      "iteration: 3210 loss: inf grad: -93.3082340862638\n",
      "iteration: 3030 loss: inf grad: -94.23643654099824\n",
      "iteration: 3220 loss: inf grad: -96.00790264824127\n",
      "iteration: 3190 loss: inf grad: -95.08414262840613\n",
      "iteration: 3180 loss: inf grad: -92.52077926950517\n",
      "iteration: 3070 loss: inf grad: -92.14511841230038iteration: 3200 loss: inf grad: -96.59608430986225\n",
      "\n",
      "iteration: 3220 loss: inf grad: -93.35577664221995\n",
      "iteration: 3060 loss: inf grad: -90.07144379059238\n",
      "iteration: 3040 loss: inf grad: -94.31073547272946\n",
      "iteration: 3230 loss: inf grad: -96.06908350507297\n",
      "iteration: 3200 loss: inf grad: -95.19566285548834\n",
      "iteration: 3190 loss: inf grad: -92.58647602501225\n",
      "iteration: 3230 loss: inf grad: -93.40697605032696\n",
      "iteration: 3210 loss: inf grad: -96.71436205824875\n",
      "iteration: 3070 loss: inf grad: -90.20736969529801\n",
      "iteration: 3080 loss: inf grad: -92.1957666705853\n",
      "iteration: 3050 loss: inf grad: -94.41614132941527\n",
      "iteration: 3240 loss: inf grad: -96.12396007281865\n",
      "iteration: 3210 loss: inf grad: -95.27872137654388\n",
      "iteration: 3200 loss: inf grad: -92.64949756691476\n",
      "iteration: 3080 loss: inf grad: -90.30608871145947\n",
      "iteration: 3220 loss: inf grad: -96.82489739159669\n",
      "iteration: 3090 loss: inf grad: -92.2719072707459\n",
      "iteration: 3240 loss: inf grad: -93.48270999688728\n",
      "iteration: 3060 loss: inf grad: -94.56023254399238\n",
      "iteration: 3250 loss: inf grad: -96.17772015016482\n",
      "iteration: 3220 loss: inf grad: -95.35737468309149\n",
      "iteration: 3210 loss: inf grad: -92.7084941846808\n",
      "iteration: 3090 loss: inf grad: -90.38224001530608\n",
      "iteration: 3100 loss: inf grad: -92.38317945891275\n",
      "iteration: 3250 loss: inf grad: -93.58183307415752\n",
      "iteration: 3230 loss: inf grad: -96.89547306109573\n",
      "iteration: 3260 loss: inf grad: -96.26211738720878\n",
      "iteration: 3070 loss: inf grad: -94.71519552500203\n",
      "iteration: 3230 loss: inf grad: -95.45587716791394\n",
      "iteration: 3220 loss: inf grad: -92.80089056096237\n",
      "iteration: 3110 loss: inf grad: -92.4991682205241\n",
      "iteration: 3260 loss: inf grad: -93.67260429417912\n",
      "iteration: 3100 loss: inf grad: -90.4487335707033\n",
      "iteration: 3240 loss: inf grad: -96.97914851526176\n",
      "iteration: 3270 loss: inf grad: -96.36667267279564\n",
      "iteration: 3080 loss: inf grad: -94.89832405507158\n",
      "iteration: 3240 loss: inf grad: -95.58455823355632\n",
      "iteration: 3230 loss: inf grad: -92.93223364428295\n",
      "iteration: 3120 loss: inf grad: -92.58101980715008\n",
      "iteration: 3270 loss: inf grad: -93.72353229773779\n",
      "iteration: 3110 loss: inf grad: -90.54381082193896\n",
      "iteration: 3250 loss: inf grad: -97.0512450127528\n",
      "iteration: 3280 loss: inf grad: -96.46525952629435\n",
      "iteration: 3090 loss: inf grad: -95.00117774103852\n",
      "iteration: 3250 loss: inf grad: -95.70275407825534\n",
      "iteration: 3240 loss: inf grad: -93.07987466824667\n",
      "iteration: 3280 loss: inf grad: -93.76790821900735\n",
      "iteration: 3130 loss: inf grad: -92.64658856871253\n",
      "iteration: 3260 loss: inf grad: -97.12457934935883\n",
      "iteration: 3120 loss: inf grad: -90.66674327607146\n",
      "iteration: 3290 loss: inf grad: -96.53963438558958\n",
      "iteration: 3260 loss: inf grad: -95.8255016987077\n",
      "iteration: 3100 loss: inf grad: -95.06666220263502\n",
      "iteration: 3250 loss: inf grad: -93.18823064629458\n",
      "iteration: 3290 loss: inf grad: -93.82554935285128\n",
      "iteration: 3140 loss: inf grad: -92.73718325209805\n",
      "iteration: 3270 loss: inf grad: -97.19781553009298\n",
      "iteration: 3130 loss: inf grad: -90.77792259786882\n",
      "iteration: 3300 loss: inf grad: -96.58441563682058\n",
      "iteration: 3270 loss: inf grad: -96.01597255193474\n",
      "iteration: 3110 loss: inf grad: -95.15348764151955\n",
      "iteration: 3260 loss: inf grad: -93.26538992416718\n",
      "iteration: 3300 loss: inf grad: -93.93283140424452\n",
      "iteration: 3150 loss: inf grad: -92.83609483586994\n",
      "iteration: 3280 loss: inf grad: -97.28993222709973\n",
      "iteration: 3310 loss: inf grad: -96.63217679722477\n",
      "iteration: 3280 loss: inf grad: -96.198530600147\n",
      "iteration: 3140 loss: inf grad: -90.89394088076364\n",
      "iteration: 3120 loss: inf grad: -95.23731010569335\n",
      "iteration: 3270 loss: inf grad: -93.3413820655903\n",
      "iteration: 3310 loss: inf grad: -94.07431859523172\n",
      "iteration: 3160 loss: inf grad: -92.94033223322828\n",
      "iteration: 3320 loss: inf grad: -96.7136290877063\n",
      "iteration: 3290 loss: inf grad: -97.35857868167415\n",
      "iteration: 3290 loss: inf grad: -96.29594756808983\n",
      "iteration: 3150 loss: inf grad: -90.9801428444419\n",
      "iteration: 3280 loss: inf grad: -93.41244021697203\n",
      "iteration: 3130 loss: inf grad: -95.36800335065172\n",
      "iteration: 3320 loss: inf grad: -94.18695172450106\n",
      "iteration: 3330 loss: inf grad: -96.82959135154776\n",
      "iteration: 3300 loss: inf grad: -96.40310471590135\n",
      "iteration: 3170 loss: inf grad: -93.04373020963044\n",
      "iteration: 3300 loss: inf grad: -97.40686610618002\n",
      "iteration: 3160 loss: inf grad: -91.03167164254401\n",
      "iteration: 3290 loss: inf grad: -93.47385323993504\n",
      "iteration: 3330 loss: inf grad: -94.2936293193813\n",
      "iteration: 3140 loss: inf grad: -95.59854292354288\n",
      "iteration: 3340 loss: inf grad: -96.9550462981021\n",
      "iteration: 3310 loss: inf grad: -96.5265828276814\n",
      "iteration: 3180 loss: inf grad: -93.12293650645015\n",
      "iteration: 3310 loss: inf grad: -97.47039987039089\n",
      "iteration: 3170 loss: inf grad: -91.10813692593482\n",
      "iteration: 3340 loss: inf grad: -94.41542607525398\n",
      "iteration: 3300 loss: inf grad: -93.51974104623893\n",
      "iteration: 3150 loss: inf grad: -95.74838352419891\n",
      "iteration: 3320 loss: inf grad: -96.63900039344651\n",
      "iteration: 3350 loss: inf grad: -97.02898133942148\n",
      "iteration: 3190 loss: inf grad: -93.2107069474858\n",
      "iteration: 3180 loss: inf grad: -91.1932453824939\n",
      "iteration: 3350 loss: inf grad: -94.50085513387435\n",
      "iteration: 3310 loss: inf grad: -93.56298314400681\n",
      "iteration: 3320 loss: inf grad: -97.497091290056\n",
      "iteration: 3160 loss: inf grad: -95.87677831792328\n",
      "iteration: 3360 loss: inf grad: -97.1251863269265\n",
      "iteration: 3330 loss: inf grad: -96.70511234677133\n",
      "iteration: 3200 loss: inf grad: -93.28144786496375\n",
      "iteration: 3190 loss: inf grad: -91.25667626751557\n",
      "iteration: 3360 loss: inf grad: -94.56813261041154\n",
      "iteration: 3320 loss: inf grad: -93.61461460708026\n",
      "iteration: 3330 loss: inf grad: -97.55017251130869\n",
      "iteration: 3170 loss: inf grad: -95.97698998498484\n",
      "iteration: 3370 loss: inf grad: -97.23090555041888\n",
      "iteration: 3340 loss: inf grad: -96.74853040162958\n",
      "iteration: 3210 loss: inf grad: -93.32249005471103\n",
      "iteration: 3370 loss: inf grad: -94.62681075855903\n",
      "iteration: 3200 loss: inf grad: -91.33488966985263\n",
      "iteration: 3330 loss: inf grad: -93.68219066673723\n",
      "iteration: 3340 loss: inf grad: -97.63925454542853\n",
      "iteration: 3180 loss: inf grad: -96.0774777095712\n",
      "iteration: 3380 loss: inf grad: -97.32708742893821\n",
      "iteration: 3350 loss: inf grad: -96.79048426523728\n",
      "iteration: 3220 loss: inf grad: -93.35556481851759\n",
      "iteration: 3210 loss: inf grad: -91.4466608268055\n",
      "iteration: 3340 loss: inf grad: -93.7329196520093\n",
      "iteration: 3380 loss: inf grad: -94.69979136696313\n",
      "iteration: 3350 loss: inf grad: -97.70299186432837\n",
      "iteration: 3190 loss: inf grad: -96.18587254148292\n",
      "iteration: 3390 loss: inf grad: -97.40996165052006\n",
      "iteration: 3360 loss: inf grad: -96.85083213434777\n",
      "iteration: 3230 loss: inf grad: -93.40972474974507\n",
      "iteration: 3350 loss: inf grad: -93.76465626540399\n",
      "iteration: 3220 loss: inf grad: -91.57262421675321\n",
      "iteration: 3390 loss: inf grad: -94.77936730405045\n",
      "iteration: 3200 loss: inf grad: -96.26220274890044\n",
      "iteration: 3400 loss: inf grad: -97.47691034770595\n",
      "iteration: 3370 loss: inf grad: -96.90395081819202\n",
      "iteration: 3360 loss: inf grad: -97.78675921404731\n",
      "iteration: 3240 loss: inf grad: -93.49504139611835\n",
      "iteration: 3230 loss: inf grad: -91.68425519970697iteration: 3360 loss: inf grad: -93.79274541648789\n",
      "\n",
      "iteration: 3400 loss: inf grad: -94.8505360091769\n",
      "iteration: 3210 loss: inf grad: -96.31650228063722\n",
      "iteration: 3410 loss: inf grad: -97.54994711003356\n",
      "iteration: 3380 loss: inf grad: -96.96361367138284\n",
      "iteration: 3370 loss: inf grad: -97.90139880428762\n",
      "iteration: 3250 loss: inf grad: -93.62089848502525\n",
      "iteration: 3370 loss: inf grad: -93.82124725329484\n",
      "iteration: 3240 loss: inf grad: -91.7964958579194\n",
      "iteration: 3410 loss: inf grad: -95.01280753262935\n",
      "iteration: 3220 loss: inf grad: -96.35208436728027\n",
      "iteration: 3420 loss: inf grad: -97.61360013559607\n",
      "iteration: 3390 loss: inf grad: -97.04294541140368\n",
      "iteration: 3380 loss: inf grad: -97.99277727000924\n",
      "iteration: 3260 loss: inf grad: -93.73434269897942\n",
      "iteration: 3380 loss: inf grad: -93.85413068429543\n",
      "iteration: 3250 loss: inf grad: -91.86949913259846\n",
      "iteration: 3420 loss: inf grad: -95.20834913703001\n",
      "iteration: 3230 loss: inf grad: -96.40329110101732\n",
      "iteration: 3430 loss: inf grad: -97.69291117692904\n",
      "iteration: 3400 loss: inf grad: -97.13800485394451\n",
      "iteration: 3390 loss: inf grad: -98.06899073119989\n",
      "iteration: 3270 loss: inf grad: -93.79418257007671\n",
      "iteration: 3390 loss: inf grad: -93.90829966723888\n",
      "iteration: 3260 loss: inf grad: -91.92617276779131\n",
      "iteration: 3440 loss: inf grad: -97.79624878378537\n",
      "iteration: 3240 loss: inf grad: -96.52757395728557\n",
      "iteration: 3430 loss: inf grad: -95.32209043290715\n",
      "iteration: 3410 loss: inf grad: -97.23354560881054\n",
      "iteration: 3400 loss: inf grad: -98.12883290888135\n",
      "iteration: 3280 loss: inf grad: -93.83299374601657\n",
      "iteration: 3400 loss: inf grad: -93.9851404009465\n",
      "iteration: 3450 loss: inf grad: -97.88589800162242\n",
      "iteration: 3270 loss: inf grad: -91.98524905840252\n",
      "iteration: 3250 loss: inf grad: -96.70620043274712\n",
      "iteration: 3420 loss: inf grad: -97.32585843351453\n",
      "iteration: 3410 loss: inf grad: -98.18470736488484\n",
      "iteration: 3440 loss: inf grad: -95.43178436413719\n",
      "iteration: 3290 loss: inf grad: -93.87000488849188\n",
      "iteration: 3410 loss: inf grad: -94.07508019771842\n",
      "iteration: 3280 loss: inf grad: -92.03561198520303\n",
      "iteration: 3460 loss: inf grad: -97.96755524501995\n",
      "iteration: 3260 loss: inf grad: -96.85574365026764\n",
      "iteration: 3430 loss: inf grad: -97.43639073925738\n",
      "iteration: 3420 loss: inf grad: -98.25709346656726\n",
      "iteration: 3450 loss: inf grad: -95.54823995451667\n",
      "iteration: 3300 loss: inf grad: -93.9517147446954\n",
      "iteration: 3420 loss: inf grad: -94.20182553526884\n",
      "iteration: 3270 loss: inf grad: -97.00598100302855\n",
      "iteration: 3470 loss: inf grad: -98.02969371095494\n",
      "iteration: 3290 loss: inf grad: -92.11586238961775\n",
      "iteration: 3440 loss: inf grad: -97.59413384909996\n",
      "iteration: 3430 loss: inf grad: -98.35585669794308\n",
      "iteration: 3460 loss: inf grad: -95.65192819195589\n",
      "iteration: 3310 loss: inf grad: -94.0391610887272\n",
      "iteration: 3430 loss: inf grad: -94.30379931381054\n",
      "iteration: 3280 loss: inf grad: -97.11861379313645\n",
      "iteration: 3480 loss: inf grad: -98.07499206798315\n",
      "iteration: 3450 loss: inf grad: -97.73983826841084\n",
      "iteration: 3300 loss: inf grad: -92.20002507355491\n",
      "iteration: 3440 loss: inf grad: -98.4585458695407\n",
      "iteration: 3320 loss: inf grad: -94.1284141683134\n",
      "iteration: 3440 loss: inf grad: -94.37488021081205\n",
      "iteration: 3470 loss: inf grad: -95.756350376537\n",
      "iteration: 3290 loss: inf grad: -97.20345537898291\n",
      "iteration: 3460 loss: inf grad: -97.82523305534474\n",
      "iteration: 3490 loss: inf grad: -98.15318175430193\n",
      "iteration: 3310 loss: inf grad: -92.2733587468776\n",
      "iteration: 3450 loss: inf grad: -94.45868465254748iteration: 3330 loss: inf grad: -94.21241394508533\n",
      "\n",
      "iteration: 3450 loss: inf grad: -98.56603727648422\n",
      "iteration: 3300 loss: inf grad: -97.28853838391595\n",
      "iteration: 3480 loss: inf grad: -95.85355422138463\n",
      "iteration: 3470 loss: inf grad: -97.89086735736328\n",
      "iteration: 3500 loss: inf grad: -98.2237990209127\n",
      "iteration: 3320 loss: inf grad: -92.35018081312771\n",
      "iteration: 3460 loss: inf grad: -94.55522403889543\n",
      "iteration: 3310 loss: inf grad: -97.36990129013844\n",
      "iteration: 3480 loss: inf grad: -97.96132014083128\n",
      "iteration: 3340 loss: inf grad: -94.3014350370646\n",
      "iteration: 3460 loss: inf grad: -98.6412256971555\n",
      "iteration: 3510 loss: inf grad: -98.27307299044082\n",
      "iteration: 3490 loss: inf grad: -95.91572879215218\n",
      "iteration: 3470 loss: inf grad: -94.6232614595989\n",
      "iteration: 3320 loss: inf grad: -97.4665571523239\n",
      "iteration: 3330 loss: inf grad: -92.39661438761263\n",
      "iteration: 3490 loss: inf grad: -98.04445839726822\n",
      "iteration: 3520 loss: inf grad: -98.31671004825773\n",
      "iteration: 3350 loss: inf grad: -94.4197273054848\n",
      "iteration: 3470 loss: inf grad: -98.70370301648224\n",
      "iteration: 3500 loss: inf grad: -95.96146427792152\n",
      "iteration: 3330 loss: inf grad: -97.55519287614285\n",
      "iteration: 3480 loss: inf grad: -94.66763833722524\n",
      "iteration: 3500 loss: inf grad: -98.14798305663744\n",
      "iteration: 3530 loss: inf grad: -98.39233213311272\n",
      "iteration: 3340 loss: inf grad: -92.43079578465588\n",
      "iteration: 3360 loss: inf grad: -94.53907680853843\n",
      "iteration: 3480 loss: inf grad: -98.76516529599263\n",
      "iteration: 3510 loss: inf grad: -96.01751163109526\n",
      "iteration: 3340 loss: inf grad: -97.6082336593687\n",
      "iteration: 3510 loss: inf grad: -98.26121996273977\n",
      "iteration: 3490 loss: inf grad: -94.70508033264638\n",
      "iteration: 3540 loss: inf grad: -98.50398239257046\n",
      "iteration: 3490 loss: inf grad: -98.84320826762888\n",
      "iteration: 3370 loss: inf grad: -94.66194008018573\n",
      "iteration: 3350 loss: inf grad: -92.47583750309752\n",
      "iteration: 3520 loss: inf grad: -96.101492202981\n",
      "iteration: 3350 loss: inf grad: -97.64953933684394\n",
      "iteration: 3520 loss: inf grad: -98.35554375646761\n",
      "iteration: 3500 loss: inf grad: -94.7396078699029\n",
      "iteration: 3550 loss: inf grad: -98.59068379451969\n",
      "iteration: 3530 loss: inf grad: -96.17086945401743\n",
      "iteration: 3360 loss: inf grad: -92.54762154123148\n",
      "iteration: 3500 loss: inf grad: -98.93357924374507\n",
      "iteration: 3380 loss: inf grad: -94.76696504690264\n",
      "iteration: 3360 loss: inf grad: -97.69693157186502\n",
      "iteration: 3530 loss: inf grad: -98.45463897758671\n",
      "iteration: 3510 loss: inf grad: -94.76751108170387\n",
      "iteration: 3560 loss: inf grad: -98.65173867593522\n",
      "iteration: 3540 loss: inf grad: -96.20736930939903\n",
      "iteration: 3510 loss: inf grad: -99.01223863578483\n",
      "iteration: 3370 loss: inf grad: -92.63065229117761\n",
      "iteration: 3390 loss: inf grad: -94.86052768573973\n",
      "iteration: 3370 loss: inf grad: -97.75248079205153\n",
      "iteration: 3540 loss: inf grad: -98.53227914993124\n",
      "iteration: 3570 loss: inf grad: -98.71427676031142\n",
      "iteration: 3520 loss: inf grad: -94.80555745754974\n",
      "iteration: 3550 loss: inf grad: -96.23689622130175\n",
      "iteration: 3520 loss: inf grad: -99.09957908925058\n",
      "iteration: 3400 loss: inf grad: -94.96212913049625\n",
      "iteration: 3380 loss: inf grad: -92.73690993448045\n",
      "iteration: 3380 loss: inf grad: -97.85232195164332\n",
      "iteration: 3550 loss: inf grad: -98.5645325882367\n",
      "iteration: 3580 loss: inf grad: -98.76279541808643\n",
      "iteration: 3560 loss: inf grad: -96.29160223656487\n",
      "iteration: 3530 loss: inf grad: -94.84954033399589\n",
      "iteration: 3530 loss: inf grad: -99.17205650549067\n",
      "iteration: 3410 loss: inf grad: -95.02605334603085\n",
      "iteration: 3390 loss: inf grad: -92.85319222636465\n",
      "iteration: 3390 loss: inf grad: -97.92053022280709\n",
      "iteration: 3560 loss: inf grad: -98.58648533877434\n",
      "iteration: 3590 loss: inf grad: -98.82897182740948\n",
      "iteration: 3570 loss: inf grad: -96.34912906106241\n",
      "iteration: 3540 loss: inf grad: -99.2116117247148\n",
      "iteration: 3420 loss: inf grad: -95.0659228171946\n",
      "iteration: 3540 loss: inf grad: -94.88292501616687\n",
      "iteration: 3400 loss: inf grad: -92.94688201093365\n",
      "iteration: 3400 loss: inf grad: -97.96209270097374\n",
      "iteration: 3570 loss: inf grad: -98.63251693261668\n",
      "iteration: 3600 loss: inf grad: -98.91667063570966\n",
      "iteration: 3580 loss: inf grad: -96.37194099390251\n",
      "iteration: 3550 loss: inf grad: -99.26314109249279\n",
      "iteration: 3430 loss: inf grad: -95.10797945485808\n",
      "iteration: 3410 loss: inf grad: -97.99635851965843\n",
      "iteration: 3410 loss: inf grad: -93.0071668593438\n",
      "iteration: 3580 loss: inf grad: -98.67819530230902\n",
      "iteration: 3550 loss: inf grad: -94.91548419273568\n",
      "iteration: 3610 loss: inf grad: -99.02883316233905\n",
      "iteration: 3590 loss: inf grad: -96.39338019730172\n",
      "iteration: 3560 loss: inf grad: -99.33284826196066\n",
      "iteration: 3440 loss: inf grad: -95.16685300642942\n",
      "iteration: 3420 loss: inf grad: -98.04497359415143\n",
      "iteration: 3590 loss: inf grad: -98.70028514013826\n",
      "iteration: 3420 loss: inf grad: -93.080443801814\n",
      "iteration: 3620 loss: inf grad: -99.12290687212278\n",
      "iteration: 3560 loss: inf grad: -94.9415667323155\n",
      "iteration: 3600 loss: inf grad: -96.43167853075045\n",
      "iteration: 3570 loss: inf grad: -99.41499939448019\n",
      "iteration: 3450 loss: inf grad: -95.2498579970069\n",
      "iteration: 3430 loss: inf grad: -98.10154964227091\n",
      "iteration: 3600 loss: inf grad: -98.73183542240622\n",
      "iteration: 3630 loss: inf grad: -99.20242553084846\n",
      "iteration: 3430 loss: inf grad: -93.17142530632921\n",
      "iteration: 3570 loss: inf grad: -94.967361045368\n",
      "iteration: 3610 loss: inf grad: -96.47812871546334\n",
      "iteration: 3460 loss: inf grad: -95.31158982551449\n",
      "iteration: 3580 loss: inf grad: -99.45626892039886\n",
      "iteration: 3440 loss: inf grad: -98.14414328598527\n",
      "iteration: 3610 loss: inf grad: -98.77568544736404\n",
      "iteration: 3440 loss: inf grad: -93.2634771491936\n",
      "iteration: 3640 loss: inf grad: -99.27032391219626\n",
      "iteration: 3620 loss: inf grad: -96.5222199341253\n",
      "iteration: 3580 loss: inf grad: -95.00039722613539\n",
      "iteration: 3470 loss: inf grad: -95.34857763303313\n",
      "iteration: 3590 loss: inf grad: -99.49676488655075\n",
      "iteration: 3450 loss: inf grad: -98.17101728335392\n",
      "iteration: 3620 loss: inf grad: -98.80858749950963\n",
      "iteration: 3450 loss: inf grad: -93.35600328172634\n",
      "iteration: 3650 loss: inf grad: -99.33030150472828\n",
      "iteration: 3630 loss: inf grad: -96.56580486386956\n",
      "iteration: 3590 loss: inf grad: -95.04891869795004\n",
      "iteration: 3600 loss: inf grad: -99.57595135079629\n",
      "iteration: 3480 loss: inf grad: -95.3800348444019\n",
      "iteration: 3460 loss: inf grad: -98.20832405379264\n",
      "iteration: 3660 loss: inf grad: -99.4192097048732\n",
      "iteration: 3460 loss: inf grad: -93.4593344842998\n",
      "iteration: 3630 loss: inf grad: -98.83929435196305\n",
      "iteration: 3640 loss: inf grad: -96.61149000056676\n",
      "iteration: 3600 loss: inf grad: -95.08606691345994\n",
      "iteration: 3490 loss: inf grad: -95.41288917344505\n",
      "iteration: 3610 loss: inf grad: -99.68402980609827\n",
      "iteration: 3470 loss: inf grad: -98.29296900081674\n",
      "iteration: 3470 loss: inf grad: -93.55897947092686\n",
      "iteration: 3670 loss: inf grad: -99.48542755435932\n",
      "iteration: 3640 loss: inf grad: -98.89009411773836\n",
      "iteration: 3650 loss: inf grad: -96.67676485845169\n",
      "iteration: 3500 loss: inf grad: -95.444528766373\n",
      "iteration: 3620 loss: inf grad: -99.76060685264274\n",
      "iteration: 3610 loss: inf grad: -95.12544061052262\n",
      "iteration: 3480 loss: inf grad: -98.3736354171021\n",
      "iteration: 3680 loss: inf grad: -99.53324520024283\n",
      "iteration: 3480 loss: inf grad: -93.63002612690607\n",
      "iteration: 3660 loss: inf grad: -96.72714773928418\n",
      "iteration: 3650 loss: inf grad: -98.96721711404919\n",
      "iteration: 3510 loss: inf grad: -95.49747320887639\n",
      "iteration: 3490 loss: inf grad: -98.41430944791006iteration: 3630 loss: inf grad: -99.80436117316114\n",
      "\n",
      "iteration: 3620 loss: inf grad: -95.20610231694698\n",
      "iteration: 3690 loss: inf grad: -99.5774867329236\n",
      "iteration: 3670 loss: inf grad: -96.75800328033876\n",
      "iteration: 3490 loss: inf grad: -93.68179377288459\n",
      "iteration: 3520 loss: inf grad: -95.55557777298081\n",
      "iteration: 3660 loss: inf grad: -99.07622747651402\n",
      "iteration: 3500 loss: inf grad: -98.4334640374876\n",
      "iteration: 3640 loss: inf grad: -99.82630410628046\n",
      "iteration: 3700 loss: inf grad: -99.62560287255037\n",
      "iteration: 3680 loss: inf grad: -96.77023590570943\n",
      "iteration: 3630 loss: inf grad: -95.3302822301251\n",
      "iteration: 3500 loss: inf grad: -93.73821180053594\n",
      "iteration: 3530 loss: inf grad: -95.60183598432883\n",
      "iteration: 3510 loss: inf grad: -98.44646223913219\n",
      "iteration: 3670 loss: inf grad: -99.17754130191472\n",
      "iteration: 3650 loss: inf grad: -99.84764933606158\n",
      "iteration: 3710 loss: inf grad: -99.67823061216163\n",
      "iteration: 3690 loss: inf grad: -96.77704318578972\n",
      "iteration: 3510 loss: inf grad: -93.78908061379488\n",
      "iteration: 3640 loss: inf grad: -95.41874061178959\n",
      "iteration: 3540 loss: inf grad: -95.67477787541836\n",
      "iteration: 3520 loss: inf grad: -98.46409627497229\n",
      "iteration: 3720 loss: inf grad: -99.7420342797429\n",
      "iteration: 3680 loss: inf grad: -99.25360398959552\n",
      "iteration: 3660 loss: inf grad: -99.89277692304576\n",
      "iteration: 3700 loss: inf grad: -96.78726927910783\n",
      "iteration: 3520 loss: inf grad: -93.85271613359669\n",
      "iteration: 3550 loss: inf grad: -95.76863490044536\n",
      "iteration: 3650 loss: inf grad: -95.48088133394802\n",
      "iteration: 3530 loss: inf grad: -98.50792061335234\n",
      "iteration: 3730 loss: inf grad: -99.79701125600488\n",
      "iteration: 3670 loss: inf grad: -99.9324121374394\n",
      "iteration: 3710 loss: inf grad: -96.81620858923674\n",
      "iteration: 3530 loss: inf grad: -93.89692722785387\n",
      "iteration: 3690 loss: inf grad: -99.28724732454549\n",
      "iteration: 3560 loss: inf grad: -95.85890836075862\n",
      "iteration: 3540 loss: inf grad: -98.56260229423168\n",
      "iteration: 3660 loss: inf grad: -95.53918378956665\n",
      "iteration: 3740 loss: inf grad: -99.84741789493046\n",
      "iteration: 3680 loss: inf grad: -99.9656554597197\n",
      "iteration: 3720 loss: inf grad: -96.85325622429973\n",
      "iteration: 3540 loss: inf grad: -93.9157227074181\n",
      "iteration: 3700 loss: inf grad: -99.31050308181622\n",
      "iteration: 3550 loss: inf grad: -98.62932726872047\n",
      "iteration: 3670 loss: inf grad: -95.62460564532145\n",
      "iteration: 3570 loss: inf grad: -95.91886210865323\n",
      "iteration: 3750 loss: inf grad: -99.89914160174757\n",
      "iteration: 3690 loss: inf grad: -99.99038956105642\n",
      "iteration: 3730 loss: inf grad: -96.90261639923499\n",
      "iteration: 3550 loss: inf grad: -93.93580007622688\n",
      "iteration: 3710 loss: inf grad: -99.33771065439981\n",
      "iteration: 3560 loss: inf grad: -98.73375181872677\n",
      "iteration: 3580 loss: inf grad: -95.97362220487548\n",
      "iteration: 3760 loss: inf grad: -99.95884240173602\n",
      "iteration: 3700 loss: inf grad: -100.02707524376768\n",
      "iteration: 3680 loss: inf grad: -95.71731251192887\n",
      "iteration: 3740 loss: inf grad: -96.95786217463686\n",
      "iteration: 3560 loss: inf grad: -93.96780436158815\n",
      "iteration: 3720 loss: inf grad: -99.37256547183213\n",
      "iteration: 3570 loss: inf grad: -98.84805507090613\n",
      "iteration: 3590 loss: inf grad: -96.0308930242717\n",
      "iteration: 3770 loss: inf grad: -99.98939353600457\n",
      "iteration: 3710 loss: inf grad: -100.09473744351831\n",
      "iteration: 3690 loss: inf grad: -95.78714088048787\n",
      "iteration: 3750 loss: inf grad: -97.01157615008705\n",
      "iteration: 3570 loss: inf grad: -93.99670504698656\n",
      "iteration: 3580 loss: inf grad: -98.93542256172617\n",
      "iteration: 3730 loss: inf grad: -99.40401194328683\n",
      "iteration: 3600 loss: inf grad: -96.12711908744238\n",
      "iteration: 3780 loss: inf grad: -100.0029631600569\n",
      "iteration: 3720 loss: inf grad: -100.15153857079898\n",
      "iteration: 3760 loss: inf grad: -97.06277341887252\n",
      "iteration: 3700 loss: inf grad: -95.88136131628214\n",
      "iteration: 3580 loss: inf grad: -94.02093162421278\n",
      "iteration: 3590 loss: inf grad: -99.00421727946134\n",
      "iteration: 3740 loss: inf grad: -99.43920333060795\n",
      "iteration: 3790 loss: inf grad: -100.0151735114641\n",
      "iteration: 3610 loss: inf grad: -96.26800626830467\n",
      "iteration: 3770 loss: inf grad: -97.12896307646089\n",
      "iteration: 3730 loss: inf grad: -100.19996903667922\n",
      "iteration: 3710 loss: inf grad: -95.98445758407885\n",
      "iteration: 3590 loss: inf grad: -94.06023819316513\n",
      "iteration: 3600 loss: inf grad: -99.083038956185\n",
      "iteration: 3750 loss: inf grad: -99.49339054769354\n",
      "iteration: 3800 loss: inf grad: -100.0318423391283\n",
      "iteration: 3620 loss: inf grad: -96.37668082145477\n",
      "iteration: 3780 loss: inf grad: -97.23148596208519\n",
      "iteration: 3720 loss: inf grad: -96.08388657374712\n",
      "iteration: 3740 loss: inf grad: -100.24935839228144\n",
      "iteration: 3600 loss: inf grad: -94.12532945222486\n",
      "iteration: 3610 loss: inf grad: -99.15425302777496\n",
      "iteration: 3810 loss: inf grad: -100.05270588831533\n",
      "iteration: 3760 loss: inf grad: -99.5415708325175\n",
      "iteration: 3630 loss: inf grad: -96.4393274524975\n",
      "iteration: 3790 loss: inf grad: -97.34979884928114\n",
      "iteration: 3730 loss: inf grad: -96.18374956878168\n",
      "iteration: 3750 loss: inf grad: -100.31145753948105\n",
      "iteration: 3620 loss: inf grad: -99.20823009180931\n",
      "iteration: 3610 loss: inf grad: -94.17082167702665\n",
      "iteration: 3820 loss: inf grad: -100.073797481032\n",
      "iteration: 3770 loss: inf grad: -99.60183184376517\n",
      "iteration: 3640 loss: inf grad: -96.48454404742617\n",
      "iteration: 3800 loss: inf grad: -97.45182523463092\n",
      "iteration: 3740 loss: inf grad: -96.26075596158361\n",
      "iteration: 3630 loss: inf grad: -99.26888135617915\n",
      "iteration: 3620 loss: inf grad: -94.21064414766994\n",
      "iteration: 3830 loss: inf grad: -100.10335803496439\n",
      "iteration: 3760 loss: inf grad: -100.39476391329998iteration: 3780 loss: inf grad: -99.64714787052506\n",
      "\n",
      "iteration: 3650 loss: inf grad: -96.52862288502709\n",
      "iteration: 3810 loss: inf grad: -97.55853092583698\n",
      "iteration: 3750 loss: inf grad: -96.3188364144423\n",
      "iteration: 3640 loss: inf grad: -99.35241458921578\n",
      "iteration: 3630 loss: inf grad: -94.27536282381121\n",
      "iteration: 3840 loss: inf grad: -100.18636346060401\n",
      "iteration: 3790 loss: inf grad: -99.67805278331082\n",
      "iteration: 3660 loss: inf grad: -96.56623703057667\n",
      "iteration: 3770 loss: inf grad: -100.4402313718522\n",
      "iteration: 3820 loss: inf grad: -97.67223720935232\n",
      "iteration: 3650 loss: inf grad: -99.44337808340319\n",
      "iteration: 3760 loss: inf grad: -96.38895078105826\n",
      "iteration: 3640 loss: inf grad: -94.37709614632563\n",
      "iteration: 3850 loss: inf grad: -100.32070426523705\n",
      "iteration: 3800 loss: inf grad: -99.70492844264093\n",
      "iteration: 3670 loss: inf grad: -96.63170095217222\n",
      "iteration: 3780 loss: inf grad: -100.47172560307006\n",
      "iteration: 3830 loss: inf grad: -97.77490046895427\n",
      "iteration: 3660 loss: inf grad: -99.57635965685061\n",
      "iteration: 3770 loss: inf grad: -96.51269004350787\n",
      "iteration: 3650 loss: inf grad: -94.47297153931282\n",
      "iteration: 3860 loss: inf grad: -100.42135630504677\n",
      "iteration: 3680 loss: inf grad: -96.70279906772741\n",
      "iteration: 3810 loss: inf grad: -99.7302288577508\n",
      "iteration: 3790 loss: inf grad: -100.49309804443288\n",
      "iteration: 3840 loss: inf grad: -97.8517512158572\n",
      "iteration: 3670 loss: inf grad: -99.72018162131667\n",
      "iteration: 3870 loss: inf grad: -100.48027361407853\n",
      "iteration: 3780 loss: inf grad: -96.61430406220029\n",
      "iteration: 3660 loss: inf grad: -94.52933147861128\n",
      "iteration: 3690 loss: inf grad: -96.74689281601468\n",
      "iteration: 3800 loss: inf grad: -100.52597910240566\n",
      "iteration: 3850 loss: inf grad: -97.910193597305\n",
      "iteration: 3820 loss: inf grad: -99.75675542386827\n",
      "iteration: 3880 loss: inf grad: -100.52799371559398\n",
      "iteration: 3680 loss: inf grad: -99.84275763918502\n",
      "iteration: 3790 loss: inf grad: -96.67293827818008\n",
      "iteration: 3700 loss: inf grad: -96.78721513940084\n",
      "iteration: 3670 loss: inf grad: -94.57027029359074\n",
      "iteration: 3860 loss: inf grad: -97.94998370920192\n",
      "iteration: 3810 loss: inf grad: -100.5698954349458\n",
      "iteration: 3830 loss: inf grad: -99.79020878344325\n",
      "iteration: 3890 loss: inf grad: -100.57759618973694\n",
      "iteration: 3690 loss: inf grad: -99.91724103825273\n",
      "iteration: 3710 loss: inf grad: -96.85036340185972\n",
      "iteration: 3800 loss: inf grad: -96.74392321612495\n",
      "iteration: 3870 loss: inf grad: -97.97332213123613\n",
      "iteration: 3680 loss: inf grad: -94.60688418103493\n",
      "iteration: 3820 loss: inf grad: -100.6208534570793\n",
      "iteration: 3840 loss: inf grad: -99.83148943951883\n",
      "iteration: 3700 loss: inf grad: -99.97183069373938\n",
      "iteration: 3900 loss: inf grad: -100.63203084536559\n",
      "iteration: 3720 loss: inf grad: -96.9716811423433\n",
      "iteration: 3810 loss: inf grad: -96.86221808887188\n",
      "iteration: 3880 loss: inf grad: -98.00728010492728\n",
      "iteration: 3690 loss: inf grad: -94.64418830661576\n",
      "iteration: 3830 loss: inf grad: -100.67294513680359\n",
      "iteration: 3850 loss: inf grad: -99.87626409103785\n",
      "iteration: 3710 loss: inf grad: -100.0321793162613\n",
      "iteration: 3910 loss: inf grad: -100.69786542143437\n",
      "iteration: 3730 loss: inf grad: -97.04976805286941\n",
      "iteration: 3820 loss: inf grad: -96.95770548705897\n",
      "iteration: 3890 loss: inf grad: -98.06869350911803\n",
      "iteration: 3700 loss: inf grad: -94.6712089931863\n",
      "iteration: 3840 loss: inf grad: -100.71127036984228\n",
      "iteration: 3860 loss: inf grad: -99.94127182834355\n",
      "iteration: 3920 loss: inf grad: -100.75014769147926\n",
      "iteration: 3720 loss: inf grad: -100.1063861175144\n",
      "iteration: 3740 loss: inf grad: -97.1093494094959\n",
      "iteration: 3830 loss: inf grad: -97.01754810662833\n",
      "iteration: 3900 loss: inf grad: -98.11679636527052\n",
      "iteration: 3710 loss: inf grad: -94.7050196663614\n",
      "iteration: 3850 loss: inf grad: -100.76249132111474\n",
      "iteration: 3870 loss: inf grad: -100.05454184940501\n",
      "iteration: 3930 loss: inf grad: -100.77931059244712\n",
      "iteration: 3730 loss: inf grad: -100.1793084367404\n",
      "iteration: 3750 loss: inf grad: -97.16911752227224\n",
      "iteration: 3840 loss: inf grad: -97.06715271146774\n",
      "iteration: 3910 loss: inf grad: -98.20382893047989iteration: 3720 loss: inf grad: -94.77567238079573\n",
      "\n",
      "iteration: 3860 loss: inf grad: -100.86226855578799\n",
      "iteration: 3880 loss: inf grad: -100.16532383356504\n",
      "iteration: 3940 loss: inf grad: -100.79354066124796\n",
      "iteration: 3760 loss: inf grad: -97.23938055226822\n",
      "iteration: 3740 loss: inf grad: -100.26385365625629\n",
      "iteration: 3850 loss: inf grad: -97.10255998651867\n",
      "iteration: 3730 loss: inf grad: -94.889426161006\n",
      "iteration: 3870 loss: inf grad: -100.99472816182522\n",
      "iteration: 3920 loss: inf grad: -98.31660786629244\n",
      "iteration: 3890 loss: inf grad: -100.23824428084973\n",
      "iteration: 3950 loss: inf grad: -100.80527580816334\n",
      "iteration: 3770 loss: inf grad: -97.30294230208122\n",
      "iteration: 3750 loss: inf grad: -100.34226178548487\n",
      "iteration: 3860 loss: inf grad: -97.17534788098357\n",
      "iteration: 3740 loss: inf grad: -95.01764114296722\n",
      "iteration: 3880 loss: inf grad: -101.08395746335349\n",
      "iteration: 3930 loss: inf grad: -98.39489060710017\n",
      "iteration: 3900 loss: inf grad: -100.30367944610025\n",
      "iteration: 3960 loss: inf grad: -100.82553451899093\n",
      "iteration: 3780 loss: inf grad: -97.38275344086965\n",
      "iteration: 3760 loss: inf grad: -100.3902664659003iteration: 3870 loss: inf grad: -97.27564207134517\n",
      "\n",
      "iteration: 3750 loss: inf grad: -95.13467045861967\n",
      "iteration: 3890 loss: inf grad: -101.13624541060653\n",
      "iteration: 3940 loss: inf grad: -98.45365447165865\n",
      "iteration: 3910 loss: inf grad: -100.35583772584847\n",
      "iteration: 3790 loss: inf grad: -97.5363704254253\n",
      "iteration: 3970 loss: inf grad: -100.86698289483036\n",
      "iteration: 3880 loss: inf grad: -97.34906439276801\n",
      "iteration: 3770 loss: inf grad: -100.42929850818967\n",
      "iteration: 3760 loss: inf grad: -95.21977267811728\n",
      "iteration: 3950 loss: inf grad: -98.51218875398757\n",
      "iteration: 3900 loss: inf grad: -101.17862618308145\n",
      "iteration: 3800 loss: inf grad: -97.66152327734454\n",
      "iteration: 3920 loss: inf grad: -100.41521266958019\n",
      "iteration: 3890 loss: inf grad: -97.40444007105859\n",
      "iteration: 3780 loss: inf grad: -100.48344995624052\n",
      "iteration: 3980 loss: inf grad: -100.92771063329944\n",
      "iteration: 3770 loss: inf grad: -95.2784261774705\n",
      "iteration: 3960 loss: inf grad: -98.59217294330212\n",
      "iteration: 3910 loss: inf grad: -101.24162395281891\n",
      "iteration: 3810 loss: inf grad: -97.70834204078717\n",
      "iteration: 3930 loss: inf grad: -100.46681745614262\n",
      "iteration: 3900 loss: inf grad: -97.44174047860784\n",
      "iteration: 3790 loss: inf grad: -100.56744146397358\n",
      "iteration: 3780 loss: inf grad: -95.33005060467511\n",
      "iteration: 3990 loss: inf grad: -100.99985538562672\n",
      "iteration: 3970 loss: inf grad: -98.70058881143822\n",
      "iteration: 3920 loss: inf grad: -101.32005421027662\n",
      "iteration: 3820 loss: inf grad: -97.75556673577854\n",
      "iteration: 3940 loss: inf grad: -100.51747347726668\n",
      "iteration: 3910 loss: inf grad: -97.47516598446768\n",
      "iteration: 3800 loss: inf grad: -100.62778025478086\n",
      "iteration: 3790 loss: inf grad: -95.38274343254432\n",
      "iteration: 4000 loss: inf grad: -101.08227029872987\n",
      "iteration: 3980 loss: inf grad: -98.79456506603998\n",
      "iteration: 3830 loss: inf grad: -97.80901641256344\n",
      "iteration: 3930 loss: inf grad: -101.39483413510521\n",
      "iteration: 3920 loss: inf grad: -97.51166777966215\n",
      "iteration: 3950 loss: inf grad: -100.55329458499641\n",
      "iteration: 3810 loss: inf grad: -100.68010111588411\n",
      "iteration: 4010 loss: inf grad: -101.146458198842\n",
      "iteration: 3800 loss: inf grad: -95.43920568212947\n",
      "iteration: 3990 loss: inf grad: -98.88062780736905\n",
      "iteration: 3840 loss: inf grad: -97.83607942943732\n",
      "iteration: 3940 loss: inf grad: -101.48777932963333\n",
      "iteration: 3930 loss: inf grad: -97.5630135494565\n",
      "iteration: 3960 loss: inf grad: -100.58539925388682\n",
      "iteration: 3820 loss: inf grad: -100.74991718234446\n",
      "iteration: 4020 loss: inf grad: -101.22307996304983\n",
      "iteration: 3810 loss: inf grad: -95.4881831962843\n",
      "iteration: 4000 loss: inf grad: -98.96713090497303\n",
      "iteration: 3850 loss: inf grad: -97.85521348793174\n",
      "iteration: 3950 loss: inf grad: -101.57877293469781\n",
      "iteration: 3970 loss: inf grad: -100.6205401125153\n",
      "iteration: 3940 loss: inf grad: -97.62482037914947\n",
      "iteration: 3820 loss: inf grad: -95.53078431506063\n",
      "iteration: 3830 loss: inf grad: -100.87430913230398\n",
      "iteration: 4030 loss: inf grad: -101.31610157040112\n",
      "iteration: 4010 loss: inf grad: -99.03471489756512\n",
      "iteration: 3860 loss: inf grad: -97.89365627258779\n",
      "iteration: 3960 loss: inf grad: -101.66812984266102\n",
      "iteration: 3980 loss: inf grad: -100.66043736257294\n",
      "iteration: 3950 loss: inf grad: -97.67281572608037\n",
      "iteration: 4040 loss: inf grad: -101.36498447159747\n",
      "iteration: 3840 loss: inf grad: -100.98884228853622\n",
      "iteration: 4020 loss: inf grad: -99.08465825457\n",
      "iteration: 3830 loss: inf grad: -95.57334952532733\n",
      "iteration: 3970 loss: inf grad: -101.72521191051577\n",
      "iteration: 3870 loss: inf grad: -97.93574112197246\n",
      "iteration: 3990 loss: inf grad: -100.70177855931777\n",
      "iteration: 3960 loss: inf grad: -97.74791286178956\n",
      "iteration: 4050 loss: inf grad: -101.41906403274878\n",
      "iteration: 4030 loss: inf grad: -99.14476567716488\n",
      "iteration: 3850 loss: inf grad: -101.04162069031888\n",
      "iteration: 3880 loss: inf grad: -97.97119457720086\n",
      "iteration: 3840 loss: inf grad: -95.6325076119951\n",
      "iteration: 3980 loss: inf grad: -101.76363079194495\n",
      "iteration: 4000 loss: inf grad: -100.7256233824822\n",
      "iteration: 3970 loss: inf grad: -97.82025150874985\n",
      "iteration: 4060 loss: inf grad: -101.48678447026535\n",
      "iteration: 4040 loss: inf grad: -99.20790518263473\n",
      "iteration: 3850 loss: inf grad: -95.68161756380675\n",
      "iteration: 3890 loss: inf grad: -98.00598701752415\n",
      "iteration: 3860 loss: inf grad: -101.08411215524012\n",
      "iteration: 3990 loss: inf grad: -101.7985903735976\n",
      "iteration: 4010 loss: inf grad: -100.74606014378438\n",
      "iteration: 3980 loss: inf grad: -97.90098919675472\n",
      "iteration: 4050 loss: inf grad: -99.255944307893\n",
      "iteration: 4070 loss: inf grad: -101.54151054183627\n",
      "iteration: 3900 loss: inf grad: -98.04329973286222\n",
      "iteration: 3870 loss: inf grad: -101.14094929324858\n",
      "iteration: 3860 loss: inf grad: -95.75767939971698\n",
      "iteration: 4000 loss: inf grad: -101.82556925747012\n",
      "iteration: 4020 loss: inf grad: -100.76084323351841\n",
      "iteration: 3990 loss: inf grad: -97.98635019443884\n",
      "iteration: 4060 loss: inf grad: -99.29028706275918\n",
      "iteration: 4080 loss: inf grad: -101.566412397716\n",
      "iteration: 3910 loss: inf grad: -98.11360062009945\n",
      "iteration: 3880 loss: inf grad: -101.20768555301774\n",
      "iteration: 3870 loss: inf grad: -95.87790966157237\n",
      "iteration: 4010 loss: inf grad: -101.85337936885455\n",
      "iteration: 4030 loss: inf grad: -100.7752326633893\n",
      "iteration: 4000 loss: inf grad: -98.06844804141592\n",
      "iteration: 4070 loss: inf grad: -99.32627047699793\n",
      "iteration: 4090 loss: inf grad: -101.5829141566677\n",
      "iteration: 3920 loss: inf grad: -98.22062038085731\n",
      "iteration: 3890 loss: inf grad: -101.31013509551946\n",
      "iteration: 3880 loss: inf grad: -96.03384093021828\n",
      "iteration: 4040 loss: inf grad: -100.81586558974246\n",
      "iteration: 4010 loss: inf grad: -98.1470802027589\n",
      "iteration: 4020 loss: inf grad: -101.90225961002892\n",
      "iteration: 4080 loss: inf grad: -99.36018737229705\n",
      "iteration: 4100 loss: inf grad: -101.62182470304049\n",
      "iteration: 3900 loss: inf grad: -101.37174717634312\n",
      "iteration: 3930 loss: inf grad: -98.34540960437056\n",
      "iteration: 3890 loss: inf grad: -96.19323674510895\n",
      "iteration: 4050 loss: inf grad: -100.92093391410572\n",
      "iteration: 4020 loss: inf grad: -98.23062209958843\n",
      "iteration: 4030 loss: inf grad: -101.96762327193454\n",
      "iteration: 4090 loss: inf grad: -99.40678385924869\n",
      "iteration: 4110 loss: inf grad: -101.68122660188652\n",
      "iteration: 3910 loss: inf grad: -101.38848361645321\n",
      "iteration: 3900 loss: inf grad: -96.33221261700378\n",
      "iteration: 3940 loss: inf grad: -98.4171980938024\n",
      "iteration: 4030 loss: inf grad: -98.32496277242683\n",
      "iteration: 4060 loss: inf grad: -101.00692702081938\n",
      "iteration: 4040 loss: inf grad: -102.00543845450699iteration: 4100 loss: inf grad: -99.49024524655253\n",
      "\n",
      "iteration: 4120 loss: inf grad: -101.7312691594735\n",
      "iteration: 3920 loss: inf grad: -101.40071757888006\n",
      "iteration: 3910 loss: inf grad: -96.45103656556557\n",
      "iteration: 3950 loss: inf grad: -98.49989576723988\n",
      "iteration: 4040 loss: inf grad: -98.44751918266861\n",
      "iteration: 4070 loss: inf grad: -101.04712408240371\n",
      "iteration: 4110 loss: inf grad: -99.59156723487389\n",
      "iteration: 4130 loss: inf grad: -101.79492124206325\n",
      "iteration: 3930 loss: inf grad: -101.41527169442597\n",
      "iteration: 4050 loss: inf grad: -102.04353147685535\n",
      "iteration: 3920 loss: inf grad: -96.58221378716193\n",
      "iteration: 3960 loss: inf grad: -98.58537937009433\n",
      "iteration: 4080 loss: inf grad: -101.07525810126069\n",
      "iteration: 4050 loss: inf grad: -98.5804878564416\n",
      "iteration: 4120 loss: inf grad: -99.67895977725165\n",
      "iteration: 3940 loss: inf grad: -101.43549051103422\n",
      "iteration: 4140 loss: inf grad: -101.87643138675149\n",
      "iteration: 3930 loss: inf grad: -96.7082398314823\n",
      "iteration: 4060 loss: inf grad: -102.08501645326245\n",
      "iteration: 3970 loss: inf grad: -98.66234543201801\n",
      "iteration: 4090 loss: inf grad: -101.11012030987055\n",
      "iteration: 4130 loss: inf grad: -99.75387215182545\n",
      "iteration: 4060 loss: inf grad: -98.69846689866506\n",
      "iteration: 3950 loss: inf grad: -101.4665538999898\n",
      "iteration: 3940 loss: inf grad: -96.79726515562407\n",
      "iteration: 4150 loss: inf grad: -101.9565769415963\n",
      "iteration: 4070 loss: inf grad: -102.1163300914026\n",
      "iteration: 3980 loss: inf grad: -98.73693399002633\n",
      "iteration: 4100 loss: inf grad: -101.15711033345109\n",
      "iteration: 4140 loss: inf grad: -99.82436116016387\n",
      "iteration: 3960 loss: inf grad: -101.50769582702712\n",
      "iteration: 3950 loss: inf grad: -96.84803976531666\n",
      "iteration: 4160 loss: inf grad: -102.0424690608647\n",
      "iteration: 4070 loss: inf grad: -98.77025957465474\n",
      "iteration: 4080 loss: inf grad: -102.14960660406132\n",
      "iteration: 3990 loss: inf grad: -98.78297580813911\n",
      "iteration: 4110 loss: inf grad: -101.2170707568245\n",
      "iteration: 4150 loss: inf grad: -99.90318741983975\n",
      "iteration: 3970 loss: inf grad: -101.58189790764598\n",
      "iteration: 3960 loss: inf grad: -96.88654026590675\n",
      "iteration: 4170 loss: inf grad: -102.10394653814312\n",
      "iteration: 4080 loss: inf grad: -98.82422978917879\n",
      "iteration: 4000 loss: inf grad: -98.82452325314006\n",
      "iteration: 4090 loss: inf grad: -102.17704251345691\n",
      "iteration: 4120 loss: inf grad: -101.29238809376892\n",
      "iteration: 4160 loss: inf grad: -99.95333950504116\n",
      "iteration: 3980 loss: inf grad: -101.6327627006124\n",
      "iteration: 3970 loss: inf grad: -96.92886497690097\n",
      "iteration: 4180 loss: inf grad: -102.13435197600941\n",
      "iteration: 4010 loss: inf grad: -98.85805764553695\n",
      "iteration: 4100 loss: inf grad: -102.19356526581473\n",
      "iteration: 4090 loss: inf grad: -98.88764651177524\n",
      "iteration: 4130 loss: inf grad: -101.33676880375612\n",
      "iteration: 4170 loss: inf grad: -100.00607454428507\n",
      "iteration: 3990 loss: inf grad: -101.68541979358918\n",
      "iteration: 3980 loss: inf grad: -96.99071708557457\n",
      "iteration: 4020 loss: inf grad: -98.90137071605096\n",
      "iteration: 4110 loss: inf grad: -102.21650243913243\n",
      "iteration: 4190 loss: inf grad: -102.16799231643442\n",
      "iteration: 4100 loss: inf grad: -98.95963004599811\n",
      "iteration: 4140 loss: inf grad: -101.40216791143182\n",
      "iteration: 4180 loss: inf grad: -100.08792086898177\n",
      "iteration: 3990 loss: inf grad: -97.05370227310664\n",
      "iteration: 4000 loss: inf grad: -101.7492994627161\n",
      "iteration: 4030 loss: inf grad: -98.95894311287138\n",
      "iteration: 4120 loss: inf grad: -102.25267963583195\n",
      "iteration: 4200 loss: inf grad: -102.19724404035281\n",
      "iteration: 4110 loss: inf grad: -99.00978694207893\n",
      "iteration: 4190 loss: inf grad: -100.17284887733192\n",
      "iteration: 4150 loss: inf grad: -101.50020143629854\n",
      "iteration: 4000 loss: inf grad: -97.11142986627374\n",
      "iteration: 4010 loss: inf grad: -101.79541375975248\n",
      "iteration: 4040 loss: inf grad: -99.02976753820548\n",
      "iteration: 4130 loss: inf grad: -102.29719049567066\n",
      "iteration: 4210 loss: inf grad: -102.21510225830025\n",
      "iteration: 4120 loss: inf grad: -99.07355199494101\n",
      "iteration: 4200 loss: inf grad: -100.23985914232064\n",
      "iteration: 4010 loss: inf grad: -97.17228429536382\n",
      "iteration: 4050 loss: inf grad: -99.12573775178635\n",
      "iteration: 4160 loss: inf grad: -101.58150420160604\n",
      "iteration: 4220 loss: inf grad: -102.23890314165922\n",
      "iteration: 4020 loss: inf grad: -101.83884118581652\n",
      "iteration: 4140 loss: inf grad: -102.36076803430521\n",
      "iteration: 4210 loss: inf grad: -100.27437905495893\n",
      "iteration: 4130 loss: inf grad: -99.16736540348438\n",
      "iteration: 4020 loss: inf grad: -97.2500445513125\n",
      "iteration: 4060 loss: inf grad: -99.24084015759809\n",
      "iteration: 4230 loss: inf grad: -102.27119471047334\n",
      "iteration: 4170 loss: inf grad: -101.65030327048629\n",
      "iteration: 4030 loss: inf grad: -101.88751007288474\n",
      "iteration: 4220 loss: inf grad: -100.29384655847093\n",
      "iteration: 4150 loss: inf grad: -102.41575640661475\n",
      "iteration: 4030 loss: inf grad: -97.32267187870224\n",
      "iteration: 4140 loss: inf grad: -99.24719892932362\n",
      "iteration: 4070 loss: inf grad: -99.34120837889274\n",
      "iteration: 4180 loss: inf grad: -101.69862119897064\n",
      "iteration: 4240 loss: inf grad: -102.31510646246014\n",
      "iteration: 4040 loss: inf grad: -101.95050599822375\n",
      "iteration: 4230 loss: inf grad: -100.33442425395442\n",
      "iteration: 4160 loss: inf grad: -102.47015472511288\n",
      "iteration: 4040 loss: inf grad: -97.41001196024338\n",
      "iteration: 4150 loss: inf grad: -99.32721738493052\n",
      "iteration: 4080 loss: inf grad: -99.39144536769683\n",
      "iteration: 4190 loss: inf grad: -101.73614988873312\n",
      "iteration: 4250 loss: inf grad: -102.38121919843076\n",
      "iteration: 4050 loss: inf grad: -102.02630092056751\n",
      "iteration: 4240 loss: inf grad: -100.3802130872596\n",
      "iteration: 4170 loss: inf grad: -102.50150717528788\n",
      "iteration: 4050 loss: inf grad: -97.51898118791676\n",
      "iteration: 4160 loss: inf grad: -99.39012354802682iteration: 4090 loss: inf grad: -99.43542832371375\n",
      "\n",
      "iteration: 4060 loss: inf grad: -102.11528198771893\n",
      "iteration: 4200 loss: inf grad: -101.79325824767727\n",
      "iteration: 4250 loss: inf grad: -100.39895766891696\n",
      "iteration: 4260 loss: inf grad: -102.47170954853387\n",
      "iteration: 4180 loss: inf grad: -102.53348501350229\n",
      "iteration: 4060 loss: inf grad: -97.57976396292338\n",
      "iteration: 4100 loss: inf grad: -99.49304259059616\n",
      "iteration: 4170 loss: inf grad: -99.4334537014485\n",
      "iteration: 4070 loss: inf grad: -102.2000631908424\n",
      "iteration: 4210 loss: inf grad: -101.83288547886693\n",
      "iteration: 4260 loss: inf grad: -100.41175852642797\n",
      "iteration: 4270 loss: inf grad: -102.52586114828982\n",
      "iteration: 4190 loss: inf grad: -102.55810788562813\n",
      "iteration: 4110 loss: inf grad: -99.5560176509039\n",
      "iteration: 4070 loss: inf grad: -97.62015525240733\n",
      "iteration: 4180 loss: inf grad: -99.47889667123829\n",
      "iteration: 4080 loss: inf grad: -102.24452719517289\n",
      "iteration: 4220 loss: inf grad: -101.86453977256829\n",
      "iteration: 4270 loss: inf grad: -100.43782439865588\n",
      "iteration: 4280 loss: inf grad: -102.54788168530627\n",
      "iteration: 4200 loss: inf grad: -102.57526524017376\n",
      "iteration: 4120 loss: inf grad: -99.61908985962238\n",
      "iteration: 4080 loss: inf grad: -97.6560472140005\n",
      "iteration: 4190 loss: inf grad: -99.54371549565053\n",
      "iteration: 4090 loss: inf grad: -102.29150908015013\n",
      "iteration: 4280 loss: inf grad: -100.50848199447634\n",
      "iteration: 4290 loss: inf grad: -102.56145824509679\n",
      "iteration: 4210 loss: inf grad: -102.60486053773252\n",
      "iteration: 4230 loss: inf grad: -101.90520014888058\n",
      "iteration: 4130 loss: inf grad: -99.71090329658051\n",
      "iteration: 4200 loss: inf grad: -99.61382314210664\n",
      "iteration: 4090 loss: inf grad: -97.68855742032531\n",
      "iteration: 4290 loss: inf grad: -100.59591093826683\n",
      "iteration: 4100 loss: inf grad: -102.3502227817236\n",
      "iteration: 4220 loss: inf grad: -102.64313829146687\n",
      "iteration: 4240 loss: inf grad: -101.94461955294265\n",
      "iteration: 4300 loss: inf grad: -102.57729947011741\n",
      "iteration: 4140 loss: inf grad: -99.78215570643903\n",
      "iteration: 4210 loss: inf grad: -99.69320681424179\n",
      "iteration: 4100 loss: inf grad: -97.72563732352415\n",
      "iteration: 4300 loss: inf grad: -100.67632348169074\n",
      "iteration: 4110 loss: inf grad: -102.41409854171374\n",
      "iteration: 4230 loss: inf grad: -102.67282556291714\n",
      "iteration: 4250 loss: inf grad: -101.9737779357138\n",
      "iteration: 4310 loss: inf grad: -102.59872320716073\n",
      "iteration: 4150 loss: inf grad: -99.84384398046168\n",
      "iteration: 4220 loss: inf grad: -99.76496147988898\n",
      "iteration: 4110 loss: inf grad: -97.75437628770064\n",
      "iteration: 4310 loss: inf grad: -100.73500866448131\n",
      "iteration: 4120 loss: inf grad: -102.49467787063841\n",
      "iteration: 4240 loss: inf grad: -102.70255108911127\n",
      "iteration: 4260 loss: inf grad: -102.00667903150702\n",
      "iteration: 4320 loss: inf grad: -102.62533149442433\n",
      "iteration: 4160 loss: inf grad: -99.92201564430694\n",
      "iteration: 4230 loss: inf grad: -99.80945500134247\n",
      "iteration: 4120 loss: inf grad: -97.78268445026511\n",
      "iteration: 4320 loss: inf grad: -100.77540340529691\n",
      "iteration: 4130 loss: inf grad: -102.56925508202727\n",
      "iteration: 4250 loss: inf grad: -102.71934324397182\n",
      "iteration: 4270 loss: inf grad: -102.05734048102325\n",
      "iteration: 4170 loss: inf grad: -99.99406842604085\n",
      "iteration: 4240 loss: inf grad: -99.8781452093009\n",
      "iteration: 4330 loss: inf grad: -102.65770848573894\n",
      "iteration: 4130 loss: inf grad: -97.8145038807317\n",
      "iteration: 4330 loss: inf grad: -100.81200138880483\n",
      "iteration: 4140 loss: inf grad: -102.60118101863085\n",
      "iteration: 4260 loss: inf grad: -102.73258123002731\n",
      "iteration: 4280 loss: inf grad: -102.08349175669773\n",
      "iteration: 4180 loss: inf grad: -100.06809479081514\n",
      "iteration: 4250 loss: inf grad: -99.95120407029805\n",
      "iteration: 4340 loss: inf grad: -102.70288394170063\n",
      "iteration: 4140 loss: inf grad: -97.83195830504293\n",
      "iteration: 4340 loss: inf grad: -100.86991954026334\n",
      "iteration: 4150 loss: inf grad: -102.61947163265208\n",
      "iteration: 4270 loss: inf grad: -102.76190955705545\n",
      "iteration: 4290 loss: inf grad: -102.09508402038031\n",
      "iteration: 4190 loss: inf grad: -100.12366899245666\n",
      "iteration: 4260 loss: inf grad: -100.0051315630682\n",
      "iteration: 4150 loss: inf grad: -97.86116920707443\n",
      "iteration: 4350 loss: inf grad: -100.93251998491087\n",
      "iteration: 4350 loss: inf grad: -102.76131325893421\n",
      "iteration: 4160 loss: inf grad: -102.6497677673686\n",
      "iteration: 4280 loss: inf grad: -102.8023942646553\n",
      "iteration: 4300 loss: inf grad: -102.10351853019807\n",
      "iteration: 4200 loss: inf grad: -100.18836557037764\n",
      "iteration: 4270 loss: inf grad: -100.06235632918263\n",
      "iteration: 4360 loss: inf grad: -100.9677424886664\n",
      "iteration: 4360 loss: inf grad: -102.79372461967608\n",
      "iteration: 4160 loss: inf grad: -97.91979466788938\n",
      "iteration: 4170 loss: inf grad: -102.72558789565699\n",
      "iteration: 4210 loss: inf grad: -100.25976426467425\n",
      "iteration: 4290 loss: inf grad: -102.82441274966352\n",
      "iteration: 4310 loss: inf grad: -102.11187002465746\n",
      "iteration: 4280 loss: inf grad: -100.12948333272087\n",
      "iteration: 4370 loss: inf grad: -100.9940904074414\n",
      "iteration: 4370 loss: inf grad: -102.80985831582247\n",
      "iteration: 4170 loss: inf grad: -97.99344060424987\n",
      "iteration: 4180 loss: inf grad: -102.83008823420025\n",
      "iteration: 4220 loss: inf grad: -100.3376111499003\n",
      "iteration: 4320 loss: inf grad: -102.12351923476456\n",
      "iteration: 4300 loss: inf grad: -102.84905204093243\n",
      "iteration: 4380 loss: inf grad: -101.03480150150565\n",
      "iteration: 4290 loss: inf grad: -100.15645115264698\n",
      "iteration: 4380 loss: inf grad: -102.82550293744539\n",
      "iteration: 4180 loss: inf grad: -98.10372131039576\n",
      "iteration: 4190 loss: inf grad: -102.89621491432149\n",
      "iteration: 4230 loss: inf grad: -100.38161076921853\n",
      "iteration: 4330 loss: inf grad: -102.1451940898116\n",
      "iteration: 4310 loss: inf grad: -102.87903663627799\n",
      "iteration: 4390 loss: inf grad: -101.08593968201644\n",
      "iteration: 4390 loss: inf grad: -102.84994389550197\n",
      "iteration: 4190 loss: inf grad: -98.18903800545752\n",
      "iteration: 4300 loss: inf grad: -100.18146514690439\n",
      "iteration: 4240 loss: inf grad: -100.44284191959343\n",
      "iteration: 4200 loss: inf grad: -102.93236634712\n",
      "iteration: 4340 loss: inf grad: -102.18285288861013\n",
      "iteration: 4400 loss: inf grad: -101.13382719763594\n",
      "iteration: 4400 loss: inf grad: -102.89903649298142\n",
      "iteration: 4320 loss: inf grad: -102.90160154849625\n",
      "iteration: 4200 loss: inf grad: -98.26626850116818\n",
      "iteration: 4250 loss: inf grad: -100.46841220100602\n",
      "iteration: 4210 loss: inf grad: -102.96837027833675\n",
      "iteration: 4310 loss: inf grad: -100.2237803777729\n",
      "iteration: 4350 loss: inf grad: -102.22562734833153\n",
      "iteration: 4410 loss: inf grad: -101.17740626416571\n",
      "iteration: 4410 loss: inf grad: -102.9620364552535\n",
      "iteration: 4330 loss: inf grad: -102.9208001417374\n",
      "iteration: 4210 loss: inf grad: -98.32827104737291\n",
      "iteration: 4260 loss: inf grad: -100.48577326348783\n",
      "iteration: 4220 loss: inf grad: -102.9994669485746\n",
      "iteration: 4320 loss: inf grad: -100.27891880292688\n",
      "iteration: 4360 loss: inf grad: -102.26399092033283\n",
      "iteration: 4420 loss: inf grad: -101.23211760694912\n",
      "iteration: 4420 loss: inf grad: -103.00890021486947\n",
      "iteration: 4340 loss: inf grad: -102.95280846826296\n",
      "iteration: 4270 loss: inf grad: -100.51664506851932\n",
      "iteration: 4370 loss: inf grad: -102.3011560294853\n",
      "iteration: 4220 loss: inf grad: -98.43222018881444\n",
      "iteration: 4230 loss: inf grad: -103.02045786754545\n",
      "iteration: 4330 loss: inf grad: -100.3466987094098\n",
      "iteration: 4430 loss: inf grad: -101.29617153234449\n",
      "iteration: 4430 loss: inf grad: -103.05952215247615\n",
      "iteration: 4350 loss: inf grad: -103.00374510523048\n",
      "iteration: 4280 loss: inf grad: -100.57104444850465\n",
      "iteration: 4380 loss: inf grad: -102.34986414359396\n",
      "iteration: 4230 loss: inf grad: -98.55179368312297\n",
      "iteration: 4340 loss: inf grad: -100.4226645052988\n",
      "iteration: 4440 loss: inf grad: -101.37695119063083\n",
      "iteration: 4240 loss: inf grad: -103.03920173078738\n",
      "iteration: 4440 loss: inf grad: -103.11351688206827\n",
      "iteration: 4360 loss: inf grad: -103.05418599191393\n",
      "iteration: 4290 loss: inf grad: -100.64221350580748\n",
      "iteration: 4390 loss: inf grad: -102.39880580544774\n",
      "iteration: 4450 loss: inf grad: -101.40720047753845\n",
      "iteration: 4350 loss: inf grad: -100.49926292366925\n",
      "iteration: 4450 loss: inf grad: -103.17201553424823\n",
      "iteration: 4250 loss: inf grad: -103.06320678222815\n",
      "iteration: 4240 loss: inf grad: -98.63133533816318\n",
      "iteration: 4370 loss: inf grad: -103.09077590094475\n",
      "iteration: 4300 loss: inf grad: -100.71526168790211\n",
      "iteration: 4400 loss: inf grad: -102.44934680887062\n",
      "iteration: 4460 loss: inf grad: -101.4439491885784\n",
      "iteration: 4360 loss: inf grad: -100.57839119752722\n",
      "iteration: 4460 loss: inf grad: -103.21543043299302\n",
      "iteration: 4260 loss: inf grad: -103.0932543963844\n",
      "iteration: 4250 loss: inf grad: -98.67110739187262\n",
      "iteration: 4380 loss: inf grad: -103.11230460751\n",
      "iteration: 4310 loss: inf grad: -100.77297999235466\n",
      "iteration: 4410 loss: inf grad: -102.55799908847023\n",
      "iteration: 4470 loss: inf grad: -101.49575345822663\n",
      "iteration: 4470 loss: inf grad: -103.25153923137633\n",
      "iteration: 4370 loss: inf grad: -100.62052294907558\n",
      "iteration: 4260 loss: inf grad: -98.72185124199657\n",
      "iteration: 4390 loss: inf grad: -103.13318030432897\n",
      "iteration: 4270 loss: inf grad: -103.1278579044099\n",
      "iteration: 4320 loss: inf grad: -100.82442269816838\n",
      "iteration: 4420 loss: inf grad: -102.6745585734224\n",
      "iteration: 4480 loss: inf grad: -101.56420690123927\n",
      "iteration: 4480 loss: inf grad: -103.30381490817871\n",
      "iteration: 4380 loss: inf grad: -100.66876781704502\n",
      "iteration: 4400 loss: inf grad: -103.15380993512895\n",
      "iteration: 4270 loss: inf grad: -98.79416097059818\n",
      "iteration: 4280 loss: inf grad: -103.16614005945007\n",
      "iteration: 4330 loss: inf grad: -100.89058513455853\n",
      "iteration: 4430 loss: inf grad: -102.75116221724916\n",
      "iteration: 4490 loss: inf grad: -101.67107998625772\n",
      "iteration: 4490 loss: inf grad: -103.34678233454423\n",
      "iteration: 4390 loss: inf grad: -100.71670315410682\n",
      "iteration: 4340 loss: inf grad: -100.9994281605716\n",
      "iteration: 4410 loss: inf grad: -103.17692973639795\n",
      "iteration: 4280 loss: inf grad: -98.90703089851993\n",
      "iteration: 4290 loss: inf grad: -103.21411058476367\n",
      "iteration: 4500 loss: inf grad: -103.37817303254548\n",
      "iteration: 4440 loss: inf grad: -102.80863442956189\n",
      "iteration: 4500 loss: inf grad: -101.73463382814654\n",
      "iteration: 4400 loss: inf grad: -100.75491737606569\n",
      "iteration: 4350 loss: inf grad: -101.05368610722539\n",
      "iteration: 4420 loss: inf grad: -103.21285657106768\n",
      "iteration: 4290 loss: inf grad: -99.02285457974622\n",
      "iteration: 4300 loss: inf grad: -103.28924234014605\n",
      "iteration: 4510 loss: inf grad: -103.41769597639562\n",
      "iteration: 4450 loss: inf grad: -102.85954107583677\n",
      "iteration: 4510 loss: inf grad: -101.8020539944375\n",
      "iteration: 4410 loss: inf grad: -100.793932453581\n",
      "iteration: 4360 loss: inf grad: -101.1108886743286\n",
      "iteration: 4430 loss: inf grad: -103.26189112369309\n",
      "iteration: 4310 loss: inf grad: -103.37575713023641\n",
      "iteration: 4300 loss: inf grad: -99.11950482819634\n",
      "iteration: 4520 loss: inf grad: -103.45883939896879\n",
      "iteration: 4460 loss: inf grad: -102.92949831666814\n",
      "iteration: 4520 loss: inf grad: -101.90810182577222\n",
      "iteration: 4420 loss: inf grad: -100.82816520507913\n",
      "iteration: 4370 loss: inf grad: -101.17125277428764\n",
      "iteration: 4440 loss: inf grad: -103.32151898763635\n",
      "iteration: 4470 loss: inf grad: -102.99871230380053\n",
      "iteration: 4320 loss: inf grad: -103.43787474191817\n",
      "iteration: 4530 loss: inf grad: -103.49841211196015\n",
      "iteration: 4530 loss: inf grad: -101.99220641541999\n",
      "iteration: 4310 loss: inf grad: -99.24005189716276\n",
      "iteration: 4430 loss: inf grad: -100.87692804564236\n",
      "iteration: 4380 loss: inf grad: -101.23882285302405\n",
      "iteration: 4450 loss: inf grad: -103.37932730753363\n",
      "iteration: 4480 loss: inf grad: -103.0497353241757\n",
      "iteration: 4540 loss: inf grad: -103.54308828247076\n",
      "iteration: 4330 loss: inf grad: -103.4906385240546\n",
      "iteration: 4540 loss: inf grad: -102.0510291846615\n",
      "iteration: 4440 loss: inf grad: -100.93886354042795\n",
      "iteration: 4320 loss: inf grad: -99.32076235201419\n",
      "iteration: 4390 loss: inf grad: -101.31967554922159\n",
      "iteration: 4460 loss: inf grad: -103.4366043440238\n",
      "iteration: 4490 loss: inf grad: -103.0803387108109\n",
      "iteration: 4550 loss: inf grad: -103.58990961903638\n",
      "iteration: 4550 loss: inf grad: -102.13281319430251\n",
      "iteration: 4340 loss: inf grad: -103.54217138753722\n",
      "iteration: 4450 loss: inf grad: -100.99805428440112\n",
      "iteration: 4330 loss: inf grad: -99.3593033876177\n",
      "iteration: 4400 loss: inf grad: -101.37587325301544\n",
      "iteration: 4470 loss: inf grad: -103.50731124050697\n",
      "iteration: 4500 loss: inf grad: -103.10420741358686\n",
      "iteration: 4560 loss: inf grad: -103.6401664944402\n",
      "iteration: 4560 loss: inf grad: -102.19435218887847\n",
      "iteration: 4350 loss: inf grad: -103.57904601270788\n",
      "iteration: 4460 loss: inf grad: -101.05809134059231\n",
      "iteration: 4410 loss: inf grad: -101.41158203549769\n",
      "iteration: 4340 loss: inf grad: -99.39261679849348\n",
      "iteration: 4480 loss: inf grad: -103.56807292900044\n",
      "iteration: 4510 loss: inf grad: -103.14563114327787\n",
      "iteration: 4570 loss: inf grad: -103.69907185982473\n",
      "iteration: 4570 loss: inf grad: -102.2767118252329\n",
      "iteration: 4360 loss: inf grad: -103.62047843952166\n",
      "iteration: 4470 loss: inf grad: -101.1486244452057\n",
      "iteration: 4420 loss: inf grad: -101.44113714301257\n",
      "iteration: 4520 loss: inf grad: -103.19731973848897\n",
      "iteration: 4490 loss: inf grad: -103.60048867117152\n",
      "iteration: 4580 loss: inf grad: -103.77063809325146\n",
      "iteration: 4350 loss: inf grad: -99.44150796012306\n",
      "iteration: 4580 loss: inf grad: -102.37379784600073\n",
      "iteration: 4480 loss: inf grad: -101.24017231581873\n",
      "iteration: 4370 loss: inf grad: -103.66586071866737\n",
      "iteration: 4430 loss: inf grad: -101.48260660499732\n",
      "iteration: 4530 loss: inf grad: -103.22218686224419\n",
      "iteration: 4590 loss: inf grad: -103.82591778696079\n",
      "iteration: 4500 loss: inf grad: -103.61787143571311\n",
      "iteration: 4590 loss: inf grad: -102.43708689875777\n",
      "iteration: 4360 loss: inf grad: -99.49399214929915\n",
      "iteration: 4440 loss: inf grad: -101.53662204685378\n",
      "iteration: 4490 loss: inf grad: -101.34718066969187\n",
      "iteration: 4380 loss: inf grad: -103.71212540114686\n",
      "iteration: 4600 loss: inf grad: -103.86818998522557\n",
      "iteration: 4540 loss: inf grad: -103.24945369286776\n",
      "iteration: 4600 loss: inf grad: -102.51867747039245\n",
      "iteration: 4510 loss: inf grad: -103.63043194727521\n",
      "iteration: 4370 loss: inf grad: -99.53338223222674\n",
      "iteration: 4450 loss: inf grad: -101.58249047560281\n",
      "iteration: 4500 loss: inf grad: -101.4245951109848\n",
      "iteration: 4390 loss: inf grad: -103.75928369273012\n",
      "iteration: 4610 loss: inf grad: -103.89988911656843\n",
      "iteration: 4610 loss: inf grad: -102.56951142016045iteration: 4520 loss: inf grad: -103.64269649086512\n",
      "\n",
      "iteration: 4550 loss: inf grad: -103.28655152418568\n",
      "iteration: 4380 loss: inf grad: -99.59746211332289\n",
      "iteration: 4460 loss: inf grad: -101.62636735250817\n",
      "iteration: 4510 loss: inf grad: -101.46077555518877\n",
      "iteration: 4620 loss: inf grad: -103.92139676077339\n",
      "iteration: 4400 loss: inf grad: -103.80824987176833\n",
      "iteration: 4530 loss: inf grad: -103.65861015148487\n",
      "iteration: 4620 loss: inf grad: -102.5974180322257\n",
      "iteration: 4390 loss: inf grad: -99.6863118598602\n",
      "iteration: 4560 loss: inf grad: -103.30985118388047\n",
      "iteration: 4470 loss: inf grad: -101.67819481487638\n",
      "iteration: 4520 loss: inf grad: -101.48475383138714\n",
      "iteration: 4630 loss: inf grad: -103.93750605506429\n",
      "iteration: 4540 loss: inf grad: -103.68264207000827\n",
      "iteration: 4410 loss: inf grad: -103.84036538332259\n",
      "iteration: 4630 loss: inf grad: -102.62726183615143\n",
      "iteration: 4400 loss: inf grad: -99.77452377152454\n",
      "iteration: 4570 loss: inf grad: -103.33849253041055\n",
      "iteration: 4480 loss: inf grad: -101.72315461339328\n",
      "iteration: 4530 loss: inf grad: -101.517844964289\n",
      "iteration: 4640 loss: inf grad: -103.95473279643527\n",
      "iteration: 4550 loss: inf grad: -103.7191485945979\n",
      "iteration: 4640 loss: inf grad: -102.65985783799997\n",
      "iteration: 4410 loss: inf grad: -99.83875419459477\n",
      "iteration: 4580 loss: inf grad: -103.3862094311294\n",
      "iteration: 4420 loss: inf grad: -103.86899411151867\n",
      "iteration: 4490 loss: inf grad: -101.75856009020059\n",
      "iteration: 4540 loss: inf grad: -101.55619365536126\n",
      "iteration: 4650 loss: inf grad: -103.98561566133004\n",
      "iteration: 4560 loss: inf grad: -103.77951939523099\n",
      "iteration: 4650 loss: inf grad: -102.71244314244827\n",
      "iteration: 4590 loss: inf grad: -103.46512230577909\n",
      "iteration: 4420 loss: inf grad: -99.89158974715758\n",
      "iteration: 4430 loss: inf grad: -103.9008174689987\n",
      "iteration: 4500 loss: inf grad: -101.78820299071208\n",
      "iteration: 4550 loss: inf grad: -101.57419083618274\n",
      "iteration: 4660 loss: inf grad: -104.05860728745853\n",
      "iteration: 4570 loss: inf grad: -103.86974428209378\n",
      "iteration: 4660 loss: inf grad: -102.7831571011621\n",
      "iteration: 4600 loss: inf grad: -103.55918446224541\n",
      "iteration: 4440 loss: inf grad: -103.94396838244569\n",
      "iteration: 4430 loss: inf grad: -99.94568129936005\n",
      "iteration: 4560 loss: inf grad: -101.59745719728762\n",
      "iteration: 4670 loss: inf grad: -104.12450669808993\n",
      "iteration: 4510 loss: inf grad: -101.81404098628363\n",
      "iteration: 4580 loss: inf grad: -103.95189103794792\n",
      "iteration: 4670 loss: inf grad: -102.82547859193146\n",
      "iteration: 4610 loss: inf grad: -103.6361365295879\n",
      "iteration: 4450 loss: inf grad: -103.96404704215554\n",
      "iteration: 4440 loss: inf grad: -99.9852388365801\n",
      "iteration: 4570 loss: inf grad: -101.65389887003707\n",
      "iteration: 4680 loss: inf grad: -104.17594171559227\n",
      "iteration: 4520 loss: inf grad: -101.8293632246222\n",
      "iteration: 4680 loss: inf grad: -102.85500433258417\n",
      "iteration: 4590 loss: inf grad: -104.01047959991662\n",
      "iteration: 4460 loss: inf grad: -103.97663836092181\n",
      "iteration: 4620 loss: inf grad: -103.70444716822092\n",
      "iteration: 4450 loss: inf grad: -100.01230635055771\n",
      "iteration: 4580 loss: inf grad: -101.6880387765267\n",
      "iteration: 4690 loss: inf grad: -104.24637582462395\n",
      "iteration: 4530 loss: inf grad: -101.84089478535131\n",
      "iteration: 4600 loss: inf grad: -104.07056209431974\n",
      "iteration: 4690 loss: inf grad: -102.87698451010763\n",
      "iteration: 4470 loss: inf grad: -103.99974925572127\n",
      "iteration: 4630 loss: inf grad: -103.76400227791723\n",
      "iteration: 4460 loss: inf grad: -100.03680589169736\n",
      "iteration: 4700 loss: inf grad: -104.29409280162304\n",
      "iteration: 4590 loss: inf grad: -101.70856560711027\n",
      "iteration: 4540 loss: inf grad: -101.85639812071295\n",
      "iteration: 4610 loss: inf grad: -104.13733033259075\n",
      "iteration: 4480 loss: inf grad: -104.0381669704791\n",
      "iteration: 4700 loss: inf grad: -102.90189337934859\n",
      "iteration: 4470 loss: inf grad: -100.0638951574068\n",
      "iteration: 4710 loss: inf grad: -104.33313998452962\n",
      "iteration: 4640 loss: inf grad: -103.8081925442001\n",
      "iteration: 4600 loss: inf grad: -101.74338960352101\n",
      "iteration: 4550 loss: inf grad: -101.9033343735683\n",
      "iteration: 4620 loss: inf grad: -104.18119284147369\n",
      "iteration: 4490 loss: inf grad: -104.07321329926184\n",
      "iteration: 4710 loss: inf grad: -102.96654326417828\n",
      "iteration: 4480 loss: inf grad: -100.09407640919108\n",
      "iteration: 4720 loss: inf grad: -104.37277838047578\n",
      "iteration: 4610 loss: inf grad: -101.80755637009656\n",
      "iteration: 4650 loss: inf grad: -103.83309073250652\n",
      "iteration: 4560 loss: inf grad: -101.9402012017587\n",
      "iteration: 4630 loss: inf grad: -104.22081802548118\n",
      "iteration: 4500 loss: inf grad: -104.09833166150432\n",
      "iteration: 4720 loss: inf grad: -103.04262927013524\n",
      "iteration: 4490 loss: inf grad: -100.14575814212891\n",
      "iteration: 4730 loss: inf grad: -104.40593815523304\n",
      "iteration: 4620 loss: inf grad: -101.90138580519866\n",
      "iteration: 4660 loss: inf grad: -103.84950658270351\n",
      "iteration: 4570 loss: inf grad: -101.97057968015125\n",
      "iteration: 4640 loss: inf grad: -104.25609140378589\n",
      "iteration: 4510 loss: inf grad: -104.12297443986978\n",
      "iteration: 4730 loss: inf grad: -103.08466474043071\n",
      "iteration: 4740 loss: inf grad: -104.44191429585162\n",
      "iteration: 4500 loss: inf grad: -100.18863978294915\n",
      "iteration: 4630 loss: inf grad: -102.01011708356903\n",
      "iteration: 4670 loss: inf grad: -103.87024264999661\n",
      "iteration: 4650 loss: inf grad: -104.278713076499\n",
      "iteration: 4580 loss: inf grad: -102.01267848486664iteration: 4520 loss: inf grad: -104.15767505843398\n",
      "\n",
      "iteration: 4740 loss: inf grad: -103.11292164176089\n",
      "iteration: 4750 loss: inf grad: -104.48872297184707\n",
      "iteration: 4510 loss: inf grad: -100.22343434009446\n",
      "iteration: 4640 loss: inf grad: -102.09901521575337\n",
      "iteration: 4680 loss: inf grad: -103.91416681714915\n",
      "iteration: 4660 loss: inf grad: -104.29250174063952\n",
      "iteration: 4530 loss: inf grad: -104.20156832596857\n",
      "iteration: 4750 loss: inf grad: -103.1526090041718\n",
      "iteration: 4760 loss: inf grad: -104.53907295949934\n",
      "iteration: 4590 loss: inf grad: -102.04855944969012\n",
      "iteration: 4650 loss: inf grad: -102.14501851658758\n",
      "iteration: 4520 loss: inf grad: -100.27469127526251\n",
      "iteration: 4690 loss: inf grad: -103.99396313096591\n",
      "iteration: 4540 loss: inf grad: -104.26015270314767\n",
      "iteration: 4670 loss: inf grad: -104.30425320835892\n",
      "iteration: 4770 loss: inf grad: -104.56611944025379\n",
      "iteration: 4760 loss: inf grad: -103.21018187447046\n",
      "iteration: 4600 loss: inf grad: -102.07425414805374\n",
      "iteration: 4660 loss: inf grad: -102.17520098514053\n",
      "iteration: 4530 loss: inf grad: -100.3410298620615\n",
      "iteration: 4700 loss: inf grad: -104.0720778113247\n",
      "iteration: 4550 loss: inf grad: -104.31128047522435\n",
      "iteration: 4680 loss: inf grad: -104.32279780792807\n",
      "iteration: 4770 loss: inf grad: -103.27797471186214\n",
      "iteration: 4780 loss: inf grad: -104.58747748603822\n",
      "iteration: 4610 loss: inf grad: -102.09869885969258\n",
      "iteration: 4710 loss: inf grad: -104.12898027782194\n",
      "iteration: 4540 loss: inf grad: -100.4139741538969\n",
      "iteration: 4670 loss: inf grad: -102.20318528053105\n",
      "iteration: 4560 loss: inf grad: -104.34491080350017\n",
      "iteration: 4780 loss: inf grad: -103.33817521409443\n",
      "iteration: 4690 loss: inf grad: -104.35620246776632\n",
      "iteration: 4790 loss: inf grad: -104.62368272514135\n",
      "iteration: 4620 loss: inf grad: -102.13095868958237\n",
      "iteration: 4720 loss: inf grad: -104.1661870565397\n",
      "iteration: 4550 loss: inf grad: -100.4984127196651\n",
      "iteration: 4680 loss: inf grad: -102.23567088636435\n",
      "iteration: 4570 loss: inf grad: -104.37173463835077\n",
      "iteration: 4790 loss: inf grad: -103.3696005395749\n",
      "iteration: 4800 loss: inf grad: -104.69359062894048\n",
      "iteration: 4700 loss: inf grad: -104.39983456192893\n",
      "iteration: 4730 loss: inf grad: -104.20566290814082\n",
      "iteration: 4630 loss: inf grad: -102.17217126794735\n",
      "iteration: 4560 loss: inf grad: -100.58461528006117\n",
      "iteration: 4690 loss: inf grad: -102.29899397055237\n",
      "iteration: 4580 loss: inf grad: -104.39926089033392\n",
      "iteration: 4800 loss: inf grad: -103.38773716477053\n",
      "iteration: 4810 loss: inf grad: -104.78672440374515\n",
      "iteration: 4710 loss: inf grad: -104.43241884020304\n",
      "iteration: 4740 loss: inf grad: -104.2448839476888\n",
      "iteration: 4570 loss: inf grad: -100.6589716342261\n",
      "iteration: 4700 loss: inf grad: -102.36931128200499\n",
      "iteration: 4590 loss: inf grad: -104.43117633428454\n",
      "iteration: 4640 loss: inf grad: -102.22465147028538\n",
      "iteration: 4810 loss: inf grad: -103.40755470869047\n",
      "iteration: 4720 loss: inf grad: -104.47957068669123\n",
      "iteration: 4750 loss: inf grad: -104.30517354188754\n",
      "iteration: 4820 loss: inf grad: -104.9045552699188\n",
      "iteration: 4580 loss: inf grad: -100.68816774822373\n",
      "iteration: 4710 loss: inf grad: -102.42414946355396\n",
      "iteration: 4600 loss: inf grad: -104.45759841609016iteration: 4650 loss: inf grad: -102.30160967366398\n",
      "\n",
      "iteration: 4820 loss: inf grad: -103.42773633829762\n",
      "iteration: 4730 loss: inf grad: -104.57036693545372\n",
      "iteration: 4760 loss: inf grad: -104.37187589179442\n",
      "iteration: 4590 loss: inf grad: -100.7247765901682\n",
      "iteration: 4830 loss: inf grad: -104.95996516895443\n",
      "iteration: 4720 loss: inf grad: -102.45128939179996\n",
      "iteration: 4830 loss: inf grad: -103.4457883162355\n",
      "iteration: 4660 loss: inf grad: -102.32727153868882\n",
      "iteration: 4610 loss: inf grad: -104.47395235869996\n",
      "iteration: 4770 loss: inf grad: -104.4204940408243\n",
      "iteration: 4740 loss: inf grad: -104.62515473242999\n",
      "iteration: 4840 loss: inf grad: -104.99035178176182\n",
      "iteration: 4600 loss: inf grad: -100.78743080299904\n",
      "iteration: 4730 loss: inf grad: -102.48571596224295\n",
      "iteration: 4840 loss: inf grad: -103.45889422336757\n",
      "iteration: 4620 loss: inf grad: -104.49517007269134\n",
      "iteration: 4670 loss: inf grad: -102.3620440730877\n",
      "iteration: 4780 loss: inf grad: -104.44580480101727\n",
      "iteration: 4750 loss: inf grad: -104.65259834605126\n",
      "iteration: 4850 loss: inf grad: -105.04236093777236\n",
      "iteration: 4740 loss: inf grad: -102.53865671243378\n",
      "iteration: 4850 loss: inf grad: -103.47041400579997\n",
      "iteration: 4630 loss: inf grad: -104.52780414660492\n",
      "iteration: 4610 loss: inf grad: -100.83676570638579\n",
      "iteration: 4680 loss: inf grad: -102.4493429448749\n",
      "iteration: 4790 loss: inf grad: -104.46016231954272\n",
      "iteration: 4760 loss: inf grad: -104.67877330496924\n",
      "iteration: 4860 loss: inf grad: -105.08895591293054\n",
      "iteration: 4860 loss: inf grad: -103.4858921553506\n",
      "iteration: 4750 loss: inf grad: -102.58082130740044\n",
      "iteration: 4640 loss: inf grad: -104.5747940644477\n",
      "iteration: 4690 loss: inf grad: -102.53719284227456\n",
      "iteration: 4620 loss: inf grad: -100.86578130866386\n",
      "iteration: 4770 loss: inf grad: -104.71449506591026\n",
      "iteration: 4870 loss: inf grad: -105.13210815846779\n",
      "iteration: 4800 loss: inf grad: -104.47088158660242\n",
      "iteration: 4870 loss: inf grad: -103.51269026740005\n",
      "iteration: 4760 loss: inf grad: -102.60450880174054\n",
      "iteration: 4650 loss: inf grad: -104.65614097062696\n",
      "iteration: 4700 loss: inf grad: -102.63316617976758\n",
      "iteration: 4630 loss: inf grad: -100.88901206101613\n",
      "iteration: 4780 loss: inf grad: -104.75435419530783\n",
      "iteration: 4880 loss: inf grad: -105.16214426931167\n",
      "iteration: 4810 loss: inf grad: -104.48033343429447\n",
      "iteration: 4880 loss: inf grad: -103.56345941549571\n",
      "iteration: 4660 loss: inf grad: -104.75676981612708\n",
      "iteration: 4770 loss: inf grad: -102.6430041987098\n",
      "iteration: 4710 loss: inf grad: -102.6745625054661\n",
      "iteration: 4640 loss: inf grad: -100.91679010037879\n",
      "iteration: 4790 loss: inf grad: -104.79017807993895\n",
      "iteration: 4890 loss: inf grad: -105.18645583461918\n",
      "iteration: 4820 loss: inf grad: -104.49035691473793\n",
      "iteration: 4670 loss: inf grad: -104.84468171505219\n",
      "iteration: 4780 loss: inf grad: -102.6765668391962\n",
      "iteration: 4890 loss: inf grad: -103.64270927551944\n",
      "iteration: 4720 loss: inf grad: -102.70272595540469\n",
      "iteration: 4650 loss: inf grad: -100.95831945106141\n",
      "iteration: 4800 loss: inf grad: -104.82704581093361\n",
      "iteration: 4900 loss: inf grad: -105.21953851968868\n",
      "iteration: 4830 loss: inf grad: -104.50695602880214\n",
      "iteration: 4680 loss: inf grad: -104.87758573333511\n",
      "iteration: 4790 loss: inf grad: -102.71035878133677\n",
      "iteration: 4730 loss: inf grad: -102.72514642272819\n",
      "iteration: 4660 loss: inf grad: -101.00794080465396\n",
      "iteration: 4910 loss: inf grad: -105.25337006299762\n",
      "iteration: 4900 loss: inf grad: -103.71380527588511\n",
      "iteration: 4810 loss: inf grad: -104.87437930444376\n",
      "iteration: 4840 loss: inf grad: -104.54268827081879\n",
      "iteration: 4690 loss: inf grad: -104.89168827922728\n",
      "iteration: 4800 loss: inf grad: -102.76628358386648\n",
      "iteration: 4740 loss: inf grad: -102.74126654695965\n",
      "iteration: 4670 loss: inf grad: -101.04345143974605\n",
      "iteration: 4920 loss: inf grad: -105.28912856198343\n",
      "iteration: 4820 loss: inf grad: -104.92666710936811\n",
      "iteration: 4850 loss: inf grad: -104.58130258274106\n",
      "iteration: 4910 loss: inf grad: -103.76219274340984\n",
      "iteration: 4700 loss: inf grad: -104.90390226690337\n",
      "iteration: 4810 loss: inf grad: -102.80792013264697\n",
      "iteration: 4750 loss: inf grad: -102.76554494019624\n",
      "iteration: 4680 loss: inf grad: -101.07617790346478\n",
      "iteration: 4930 loss: inf grad: -105.33106261293901\n",
      "iteration: 4830 loss: inf grad: -104.97319453280201\n",
      "iteration: 4920 loss: inf grad: -103.79694303248287\n",
      "iteration: 4710 loss: inf grad: -104.91628086954594\n",
      "iteration: 4860 loss: inf grad: -104.63073763903489\n",
      "iteration: 4820 loss: inf grad: -102.8420016287687\n",
      "iteration: 4760 loss: inf grad: -102.81153161268492\n",
      "iteration: 4940 loss: inf grad: -105.37066439413198\n",
      "iteration: 4690 loss: inf grad: -101.11273010406398\n",
      "iteration: 4840 loss: inf grad: -105.02218454846764\n",
      "iteration: 4720 loss: inf grad: -104.92827327213246\n",
      "iteration: 4930 loss: inf grad: -103.83054216312848\n",
      "iteration: 4870 loss: inf grad: -104.69741942775573\n",
      "iteration: 4830 loss: inf grad: -102.874216162386\n",
      "iteration: 4950 loss: inf grad: -105.39587224155751\n",
      "iteration: 4770 loss: inf grad: -102.84326929664115\n",
      "iteration: 4700 loss: inf grad: -101.1730984866879\n",
      "iteration: 4850 loss: inf grad: -105.04877339689133\n",
      "iteration: 4940 loss: inf grad: -103.86114105428962\n",
      "iteration: 4730 loss: inf grad: -104.94677356080135\n",
      "iteration: 4880 loss: inf grad: -104.75374701197514\n",
      "iteration: 4960 loss: inf grad: -105.42769228091393\n",
      "iteration: 4840 loss: inf grad: -102.91449564721421\n",
      "iteration: 4780 loss: inf grad: -102.86882576226101\n",
      "iteration: 4710 loss: inf grad: -101.2526678288059\n",
      "iteration: 4860 loss: inf grad: -105.07381726081215\n",
      "iteration: 4950 loss: inf grad: -103.89535331589322\n",
      "iteration: 4740 loss: inf grad: -104.9786165928337\n",
      "iteration: 4890 loss: inf grad: -104.80197367866984\n",
      "iteration: 4970 loss: inf grad: -105.45312446924763\n",
      "iteration: 4850 loss: inf grad: -102.9807510223232\n",
      "iteration: 4790 loss: inf grad: -102.90202933579339\n",
      "iteration: 4720 loss: inf grad: -101.32005568189734\n",
      "iteration: 4870 loss: inf grad: -105.11611516686514\n",
      "iteration: 4960 loss: inf grad: -103.93176286529534\n",
      "iteration: 4750 loss: inf grad: -105.01687309982096\n",
      "iteration: 4900 loss: inf grad: -104.83036657948801\n",
      "iteration: 4980 loss: inf grad: -105.4672113631353\n",
      "iteration: 4860 loss: inf grad: -103.03854421135706\n",
      "iteration: 4970 loss: inf grad: -103.96697058930692\n",
      "iteration: 4880 loss: inf grad: -105.1418481936093\n",
      "iteration: 4730 loss: inf grad: -101.36245094186472\n",
      "iteration: 4760 loss: inf grad: -105.04976068746069\n",
      "iteration: 4800 loss: inf grad: -102.93738217115677\n",
      "iteration: 4910 loss: inf grad: -104.85281403443548\n",
      "iteration: 4990 loss: inf grad: -105.48636781935613\n",
      "iteration: 4870 loss: inf grad: -103.08825861891066\n",
      "iteration: 4980 loss: inf grad: -103.99732513359913\n",
      "iteration: 4890 loss: inf grad: -105.15282942722337\n",
      "iteration: 4770 loss: inf grad: -105.07221366940719\n",
      "iteration: 4740 loss: inf grad: -101.38547617411665\n",
      "iteration: 4810 loss: inf grad: -102.96339602394957\n",
      "iteration: 4920 loss: inf grad: -104.9018591137565\n",
      "iteration: 5000 loss: inf grad: -105.53178219525707\n",
      "iteration: 4990 loss: inf grad: -104.0301055403549\n",
      "iteration: 4880 loss: inf grad: -103.14345160344737\n",
      "iteration: 4780 loss: inf grad: -105.09935240508452\n",
      "iteration: 4750 loss: inf grad: -101.40680436360154\n",
      "iteration: 4820 loss: inf grad: -102.98989069170212\n",
      "iteration: 4900 loss: inf grad: -105.16287630884858\n",
      "iteration: 4930 loss: inf grad: -104.97265583701565\n",
      "iteration: 5010 loss: inf grad: -105.6241634758743\n",
      "iteration: 5000 loss: inf grad: -104.07381798361249\n",
      "iteration: 4790 loss: inf grad: -105.16419879755193\n",
      "iteration: 4890 loss: inf grad: -103.18820331684447\n",
      "iteration: 4760 loss: inf grad: -101.44198568332513\n",
      "iteration: 4830 loss: inf grad: -103.03059742543178\n",
      "iteration: 4910 loss: inf grad: -105.18517507930738\n",
      "iteration: 5020 loss: inf grad: -105.69021174269919\n",
      "iteration: 4940 loss: inf grad: -105.02689290033145\n",
      "iteration: 5010 loss: inf grad: -104.09700766373416\n",
      "iteration: 4800 loss: inf grad: -105.23918289127042\n",
      "iteration: 4900 loss: inf grad: -103.22270311649953\n",
      "iteration: 4770 loss: inf grad: -101.47801779221757\n",
      "iteration: 4840 loss: inf grad: -103.07594355290547\n",
      "iteration: 4920 loss: inf grad: -105.24121841623054\n",
      "iteration: 5030 loss: inf grad: -105.73553530808425\n",
      "iteration: 4950 loss: inf grad: -105.06114547952694\n",
      "iteration: 4780 loss: inf grad: -101.50568221057713\n",
      "iteration: 5020 loss: inf grad: -104.10864681204866\n",
      "iteration: 4810 loss: inf grad: -105.26604485449775\n",
      "iteration: 4910 loss: inf grad: -103.24953345610399\n",
      "iteration: 4850 loss: inf grad: -103.10433234330856\n",
      "iteration: 4930 loss: inf grad: -105.29585507312865\n",
      "iteration: 4960 loss: inf grad: -105.08232308362679\n",
      "iteration: 5040 loss: inf grad: -105.7828422305774\n",
      "iteration: 4790 loss: inf grad: -101.5199636318162\n",
      "iteration: 5030 loss: inf grad: -104.12394960859208\n",
      "iteration: 4920 loss: inf grad: -103.27646948009323\n",
      "iteration: 4820 loss: inf grad: -105.27827493445326\n",
      "iteration: 4860 loss: inf grad: -103.12339454457988\n",
      "iteration: 4940 loss: inf grad: -105.32830612252137\n",
      "iteration: 4800 loss: inf grad: -101.54554313957193\n",
      "iteration: 5050 loss: inf grad: -105.81968908752944\n",
      "iteration: 4970 loss: inf grad: -105.10037616766752\n",
      "iteration: 4870 loss: inf grad: -103.1503531889797\n",
      "iteration: 4930 loss: inf grad: -103.3233439061942\n",
      "iteration: 5040 loss: inf grad: -104.16249279219019\n",
      "iteration: 4830 loss: inf grad: -105.29229368019492\n",
      "iteration: 4810 loss: inf grad: -101.60191648918871\n",
      "iteration: 4950 loss: inf grad: -105.35288752474585\n",
      "iteration: 4980 loss: inf grad: -105.1211072453259iteration: 5060 loss: inf grad: -105.84551816949175\n",
      "\n",
      "iteration: 4880 loss: inf grad: -103.2068173059175\n",
      "iteration: 5050 loss: inf grad: -104.22754011636516\n",
      "iteration: 4940 loss: inf grad: -103.38079081267549\n",
      "iteration: 4840 loss: inf grad: -105.31360632140753\n",
      "iteration: 4820 loss: inf grad: -101.64424142288124\n",
      "iteration: 4990 loss: inf grad: -105.14224125738022\n",
      "iteration: 5060 loss: inf grad: -104.26051488422549\n",
      "iteration: 4960 loss: inf grad: -105.37658283570887\n",
      "iteration: 5070 loss: inf grad: -105.87628323734768\n",
      "iteration: 4850 loss: inf grad: -105.34671627980428\n",
      "iteration: 4890 loss: inf grad: -103.28506919830899\n",
      "iteration: 4950 loss: inf grad: -103.42191438474313\n",
      "iteration: 4830 loss: inf grad: -101.69263635014002\n",
      "iteration: 5000 loss: inf grad: -105.16202873091305\n",
      "iteration: 4970 loss: inf grad: -105.4133958787638\n",
      "iteration: 5070 loss: inf grad: -104.28516751619088\n",
      "iteration: 5080 loss: inf grad: -105.91907562895196\n",
      "iteration: 4860 loss: inf grad: -105.37869953150138\n",
      "iteration: 4960 loss: inf grad: -103.45371148852068\n",
      "iteration: 4900 loss: inf grad: -103.34694124750197\n",
      "iteration: 4840 loss: inf grad: -101.72411418628862\n",
      "iteration: 4980 loss: inf grad: -105.44413146857588\n",
      "iteration: 5080 loss: inf grad: -104.31621516442412\n",
      "iteration: 5010 loss: inf grad: -105.1871092495794\n",
      "iteration: 4970 loss: inf grad: -103.47715950887718\n",
      "iteration: 4910 loss: inf grad: -103.40443613037195\n",
      "iteration: 5090 loss: inf grad: -105.96924754101937\n",
      "iteration: 4870 loss: inf grad: -105.40529033069781\n",
      "iteration: 4850 loss: inf grad: -101.73602807706885\n",
      "iteration: 4990 loss: inf grad: -105.4635147822163\n",
      "iteration: 4980 loss: inf grad: -103.49789148649549iteration: 5090 loss: inf grad: -104.34557686951163\n",
      "\n",
      "iteration: 5020 loss: inf grad: -105.23197856912651\n",
      "iteration: 4920 loss: inf grad: -103.46723968405564\n",
      "iteration: 4880 loss: inf grad: -105.4500049478352\n",
      "iteration: 5100 loss: inf grad: -106.03617406437905\n",
      "iteration: 5000 loss: inf grad: -105.48999363018342\n",
      "iteration: 4860 loss: inf grad: -101.75112535850437\n",
      "iteration: 4990 loss: inf grad: -103.52322202845237\n",
      "iteration: 5100 loss: inf grad: -104.36612632813925\n",
      "iteration: 5030 loss: inf grad: -105.30225286868206\n",
      "iteration: 4930 loss: inf grad: -103.5273949264864\n",
      "iteration: 4890 loss: inf grad: -105.48977843158951\n",
      "iteration: 5110 loss: inf grad: -106.09996956271945\n",
      "iteration: 5010 loss: inf grad: -105.51535931771863\n",
      "iteration: 4870 loss: inf grad: -101.81542142229358\n",
      "iteration: 5000 loss: inf grad: -103.54811246182338\n",
      "iteration: 5110 loss: inf grad: -104.38185247296755\n",
      "iteration: 5040 loss: inf grad: -105.37158397753385\n",
      "iteration: 5120 loss: inf grad: -106.1506745510304\n",
      "iteration: 4900 loss: inf grad: -105.52224959920265\n",
      "iteration: 4940 loss: inf grad: -103.58729719824728\n",
      "iteration: 4880 loss: inf grad: -101.87693391231164\n",
      "iteration: 5020 loss: inf grad: -105.53657354016306\n",
      "iteration: 5010 loss: inf grad: -103.5637287449459iteration: 5120 loss: inf grad: -104.40135525164462\n",
      "\n",
      "iteration: 5050 loss: inf grad: -105.4165481647506\n",
      "iteration: 4910 loss: inf grad: -105.54419216049678\n",
      "iteration: 5130 loss: inf grad: -106.2061748385455\n",
      "iteration: 4950 loss: inf grad: -103.64694705803402\n",
      "iteration: 4890 loss: inf grad: -101.90826827516351\n",
      "iteration: 5030 loss: inf grad: -105.57323134188195\n",
      "iteration: 5060 loss: inf grad: -105.46159668634077\n",
      "iteration: 5020 loss: inf grad: -103.57594320171583\n",
      "iteration: 5130 loss: inf grad: -104.45133467867063\n",
      "iteration: 5140 loss: inf grad: -106.26558162803532\n",
      "iteration: 4920 loss: inf grad: -105.56484216733136\n",
      "iteration: 4960 loss: inf grad: -103.69307267008809\n",
      "iteration: 4900 loss: inf grad: -101.92672005175163\n",
      "iteration: 5040 loss: inf grad: -105.62221520306873\n",
      "iteration: 5070 loss: inf grad: -105.5345162701401\n",
      "iteration: 5030 loss: inf grad: -103.5906377494855\n",
      "iteration: 5150 loss: inf grad: -106.30971971431723\n",
      "iteration: 4930 loss: inf grad: -105.59301189214479\n",
      "iteration: 4970 loss: inf grad: -103.73940626076205iteration: 5140 loss: inf grad: -104.48467664574254\n",
      "\n",
      "iteration: 4910 loss: inf grad: -101.95567457738196\n",
      "iteration: 5050 loss: inf grad: -105.65745518613261\n",
      "iteration: 5080 loss: inf grad: -105.5927913594027\n",
      "iteration: 5040 loss: inf grad: -103.61893382544551\n",
      "iteration: 5160 loss: inf grad: -106.34529068055659\n",
      "iteration: 4940 loss: inf grad: -105.63884176909016\n",
      "iteration: 4980 loss: inf grad: -103.79932957331098\n",
      "iteration: 5150 loss: inf grad: -104.52746152970681\n",
      "iteration: 4920 loss: inf grad: -101.98859036825117\n",
      "iteration: 5060 loss: inf grad: -105.67770563510823\n",
      "iteration: 5090 loss: inf grad: -105.63263345353701\n",
      "iteration: 5050 loss: inf grad: -103.65699958006641\n",
      "iteration: 5170 loss: inf grad: -106.384984259948\n",
      "iteration: 4950 loss: inf grad: -105.71612328401096\n",
      "iteration: 4930 loss: inf grad: -102.02397147916447\n",
      "iteration: 4990 loss: inf grad: -103.85258247192687\n",
      "iteration: 5070 loss: inf grad: -105.68939019804745\n",
      "iteration: 5100 loss: inf grad: -105.65300333703203\n",
      "iteration: 5160 loss: inf grad: -104.59240250576826\n",
      "iteration: 5180 loss: inf grad: -106.42286598103253\n",
      "iteration: 5060 loss: inf grad: -103.67704581218607\n",
      "iteration: 4940 loss: inf grad: -102.07077341792699\n",
      "iteration: 4960 loss: inf grad: -105.78096701768388iteration: 5000 loss: inf grad: -103.90059693109609\n",
      "\n",
      "iteration: 5110 loss: inf grad: -105.67267368996028\n",
      "iteration: 5080 loss: inf grad: -105.69886693738425\n",
      "iteration: 5190 loss: inf grad: -106.44367178260086\n",
      "iteration: 5170 loss: inf grad: -104.66831629525987\n",
      "iteration: 5070 loss: inf grad: -103.7093149560661\n",
      "iteration: 4950 loss: inf grad: -102.14210188997319\n",
      "iteration: 5010 loss: inf grad: -103.95031062603249\n",
      "iteration: 4970 loss: inf grad: -105.82126408582212\n",
      "iteration: 5120 loss: inf grad: -105.70018929387876\n",
      "iteration: 5090 loss: inf grad: -105.71837607212616\n",
      "iteration: 5200 loss: inf grad: -106.44939697758255\n",
      "iteration: 5080 loss: inf grad: -103.73903225127488\n",
      "iteration: 5180 loss: inf grad: -104.7806044038005\n",
      "iteration: 4960 loss: inf grad: -102.21434933120696\n",
      "iteration: 5130 loss: inf grad: -105.74436739208565\n",
      "iteration: 5020 loss: inf grad: -104.01474822387468\n",
      "iteration: 4980 loss: inf grad: -105.86097771670185\n",
      "iteration: 5100 loss: inf grad: -105.78058902755481\n",
      "iteration: 5210 loss: inf grad: -106.45153008767525\n",
      "iteration: 5090 loss: inf grad: -103.7889176863915\n",
      "iteration: 5190 loss: inf grad: -104.9094367206225\n",
      "iteration: 4970 loss: inf grad: -102.2628283716446\n",
      "iteration: 5140 loss: inf grad: -105.8156390261679\n",
      "iteration: 5030 loss: inf grad: -104.04505657051669\n",
      "iteration: 4990 loss: inf grad: -105.90172828344288\n",
      "iteration: 5110 loss: inf grad: -105.87012066902132\n",
      "iteration: 5220 loss: inf grad: -106.45318407408706\n",
      "iteration: 5100 loss: inf grad: -103.84162072282803\n",
      "iteration: 4980 loss: inf grad: -102.32080462036716\n",
      "iteration: 5150 loss: inf grad: -105.88551961763164\n",
      "iteration: 5200 loss: inf grad: -105.00155415261644\n",
      "iteration: 5000 loss: inf grad: -105.92369123364529\n",
      "iteration: 5120 loss: inf grad: -105.95156221532218\n",
      "iteration: 5230 loss: inf grad: -106.45496137573747\n",
      "iteration: 5040 loss: inf grad: -104.05950667123233\n",
      "iteration: 5110 loss: inf grad: -103.94686676093283\n",
      "iteration: 5160 loss: inf grad: -105.95595142258112\n",
      "iteration: 5210 loss: inf grad: -105.06423880377278\n",
      "iteration: 4990 loss: inf grad: -102.3632580682561\n",
      "iteration: 5240 loss: inf grad: -106.45703448380803\n",
      "iteration: 5130 loss: inf grad: -106.02208240845349\n",
      "iteration: 5010 loss: inf grad: -105.94816447788816\n",
      "iteration: 5050 loss: inf grad: -104.07434427543313\n",
      "iteration: 5120 loss: inf grad: -104.02396539617818\n",
      "iteration: 5170 loss: inf grad: -106.00490294786341\n",
      "iteration: 5000 loss: inf grad: -102.39535149121264\n",
      "iteration: 5220 loss: inf grad: -105.1195747938929\n",
      "iteration: 5250 loss: inf grad: -106.45948869293142\n",
      "iteration: 5140 loss: inf grad: -106.08035550067252\n",
      "iteration: 5020 loss: inf grad: -105.98432762098165iteration: 5060 loss: inf grad: -104.09791975230475\n",
      "\n",
      "iteration: 5130 loss: inf grad: -104.05421224502632\n",
      "iteration: 5180 loss: inf grad: -106.04253021835655\n",
      "iteration: 5260 loss: inf grad: -106.4624712758271\n",
      "iteration: 5010 loss: inf grad: -102.42668231903862\n",
      "iteration: 5150 loss: inf grad: -106.12154839016178\n",
      "iteration: 5230 loss: inf grad: -105.1700016407492\n",
      "iteration: 5030 loss: inf grad: -106.02192923977795\n",
      "iteration: 5070 loss: inf grad: -104.12722288830653\n",
      "iteration: 5140 loss: inf grad: -104.10778394348023\n",
      "iteration: 5190 loss: inf grad: -106.09456797504228\n",
      "iteration: 5270 loss: inf grad: -106.4667213754918\n",
      "iteration: 5020 loss: inf grad: -102.44026413292826\n",
      "iteration: 5160 loss: inf grad: -106.14367924128437\n",
      "iteration: 5240 loss: inf grad: -105.23207367964\n",
      "iteration: 5040 loss: inf grad: -106.04977113794884\n",
      "iteration: 5080 loss: inf grad: -104.15313469692013\n",
      "iteration: 5150 loss: inf grad: -104.14806804487318\n",
      "iteration: 5200 loss: inf grad: -106.14592140991385\n",
      "iteration: 5280 loss: inf grad: -106.47561680807593\n",
      "iteration: 5170 loss: inf grad: -106.16725491366248\n",
      "iteration: 5250 loss: inf grad: -105.28983517900741\n",
      "iteration: 5030 loss: inf grad: -102.47458663752613\n",
      "iteration: 5050 loss: inf grad: -106.06714517912374\n",
      "iteration: 5210 loss: inf grad: -106.18853345890042\n",
      "iteration: 5090 loss: inf grad: -104.1802816783212\n",
      "iteration: 5160 loss: inf grad: -104.18509872320182\n",
      "iteration: 5290 loss: inf grad: -106.500167111656\n",
      "iteration: 5180 loss: inf grad: -106.2083913802121\n",
      "iteration: 5260 loss: inf grad: -105.33503078490239\n",
      "iteration: 5040 loss: inf grad: -102.50171576931677\n",
      "iteration: 5060 loss: inf grad: -106.07734444159536\n",
      "iteration: 5220 loss: inf grad: -106.23796550308133\n",
      "iteration: 5100 loss: inf grad: -104.21780434420292\n",
      "iteration: 5300 loss: inf grad: -106.55061567380368\n",
      "iteration: 5270 loss: inf grad: -105.38989878288638\n",
      "iteration: 5070 loss: inf grad: -106.08719680953035\n",
      "iteration: 5190 loss: inf grad: -106.26410343425081\n",
      "iteration: 5050 loss: inf grad: -102.53472961619988\n",
      "iteration: 5170 loss: inf grad: -104.21053933103872\n",
      "iteration: 5230 loss: inf grad: -106.29036662901171\n",
      "iteration: 5310 loss: inf grad: -106.59751974231104\n",
      "iteration: 5110 loss: inf grad: -104.2614923876184\n",
      "iteration: 5280 loss: inf grad: -105.46391402342711\n",
      "iteration: 5080 loss: inf grad: -106.1003528509784\n",
      "iteration: 5060 loss: inf grad: -102.56252122128791\n",
      "iteration: 5240 loss: inf grad: -106.32884847632803\n",
      "iteration: 5180 loss: inf grad: -104.2368336562476\n",
      "iteration: 5200 loss: inf grad: -106.30638553485892\n",
      "iteration: 5320 loss: inf grad: -106.62444879776679\n",
      "iteration: 5120 loss: inf grad: -104.2965883454778\n",
      "iteration: 5290 loss: inf grad: -105.50089059098777\n",
      "iteration: 5090 loss: inf grad: -106.11399488287728\n",
      "iteration: 5070 loss: inf grad: -102.57793186040452\n",
      "iteration: 5190 loss: inf grad: -104.27608205511567iteration: 5210 loss: inf grad: -106.33317783434487\n",
      "\n",
      "iteration: 5250 loss: inf grad: -106.36204725628559\n",
      "iteration: 5330 loss: inf grad: -106.64187326025075\n",
      "iteration: 5130 loss: inf grad: -104.32220434340714\n",
      "iteration: 5300 loss: inf grad: -105.55383572917093\n",
      "iteration: 5080 loss: inf grad: -102.58971251744714\n",
      "iteration: 5100 loss: inf grad: -106.12485504599434\n",
      "iteration: 5260 loss: inf grad: -106.38822516067098\n",
      "iteration: 5200 loss: inf grad: -104.30431359096295\n",
      "iteration: 5220 loss: inf grad: -106.35906731509783\n",
      "iteration: 5340 loss: inf grad: -106.6553997740845\n",
      "iteration: 5140 loss: inf grad: -104.33842675610454\n",
      "iteration: 5310 loss: inf grad: -105.60505182146954\n",
      "iteration: 5090 loss: inf grad: -102.60990302853628\n",
      "iteration: 5110 loss: inf grad: -106.13644896493506\n",
      "iteration: 5210 loss: inf grad: -104.32194012822876\n",
      "iteration: 5270 loss: inf grad: -106.41393225833372\n",
      "iteration: 5230 loss: inf grad: -106.39829163627186\n",
      "iteration: 5150 loss: inf grad: -104.35245775633288\n",
      "iteration: 5350 loss: inf grad: -106.66824781296187\n",
      "iteration: 5100 loss: inf grad: -102.66383255790885\n",
      "iteration: 5320 loss: inf grad: -105.63209757348167\n",
      "iteration: 5120 loss: inf grad: -106.15302133425415\n",
      "iteration: 5220 loss: inf grad: -104.34899785005334\n",
      "iteration: 5280 loss: inf grad: -106.44532885027058\n",
      "iteration: 5240 loss: inf grad: -106.44136888637381\n",
      "iteration: 5360 loss: inf grad: -106.68409904514638\n",
      "iteration: 5160 loss: inf grad: -104.37141685854291\n",
      "iteration: 5110 loss: inf grad: -102.74707854154536\n",
      "iteration: 5330 loss: inf grad: -105.6497514148452\n",
      "iteration: 5130 loss: inf grad: -106.1721767699974\n",
      "iteration: 5230 loss: inf grad: -104.39078659413897\n",
      "iteration: 5290 loss: inf grad: -106.46076952866564\n",
      "iteration: 5120 loss: inf grad: -102.82400102554152\n",
      "iteration: 5250 loss: inf grad: -106.48123350199381\n",
      "iteration: 5370 loss: inf grad: -106.70661403672413\n",
      "iteration: 5170 loss: inf grad: -104.39564609393815\n",
      "iteration: 5340 loss: inf grad: -105.66622968171082\n",
      "iteration: 5140 loss: inf grad: -106.18812294380983\n",
      "iteration: 5240 loss: inf grad: -104.42837864352552\n",
      "iteration: 5300 loss: inf grad: -106.47207502601344\n",
      "iteration: 5130 loss: inf grad: -102.89123780539177\n",
      "iteration: 5380 loss: inf grad: -106.73469779128246\n",
      "iteration: 5260 loss: inf grad: -106.53973430507949\n",
      "iteration: 5180 loss: inf grad: -104.42007624671953\n",
      "iteration: 5150 loss: inf grad: -106.20518507503556\n",
      "iteration: 5350 loss: inf grad: -105.6821918201195\n",
      "iteration: 5250 loss: inf grad: -104.45929945645152\n",
      "iteration: 5310 loss: inf grad: -106.48481789573611\n",
      "iteration: 5140 loss: inf grad: -102.93475745134991\n",
      "iteration: 5390 loss: inf grad: -106.75909229608894\n",
      "iteration: 5270 loss: inf grad: -106.5961097930871\n",
      "iteration: 5160 loss: inf grad: -106.22636738608145\n",
      "iteration: 5360 loss: inf grad: -105.69599130618319\n",
      "iteration: 5190 loss: inf grad: -104.44335464411805\n",
      "iteration: 5260 loss: inf grad: -104.49583087357718\n",
      "iteration: 5320 loss: inf grad: -106.50104609147627\n",
      "iteration: 5150 loss: inf grad: -102.98453186111084\n",
      "iteration: 5170 loss: inf grad: -106.24965961623371\n",
      "iteration: 5400 loss: inf grad: -106.77549221981648\n",
      "iteration: 5370 loss: inf grad: -105.71307931524615\n",
      "iteration: 5270 loss: inf grad: -104.55991144141714\n",
      "iteration: 5280 loss: inf grad: -106.63558931694362\n",
      "iteration: 5330 loss: inf grad: -106.51706474479957\n",
      "iteration: 5200 loss: inf grad: -104.4689723149825\n",
      "iteration: 5160 loss: inf grad: -103.03209757020676\n",
      "iteration: 5180 loss: inf grad: -106.26812083990646\n",
      "iteration: 5410 loss: inf grad: -106.79305539929877\n",
      "iteration: 5380 loss: inf grad: -105.74374854212624\n",
      "iteration: 5280 loss: inf grad: -104.60931722626886\n",
      "iteration: 5340 loss: inf grad: -106.53115897940009\n",
      "iteration: 5210 loss: inf grad: -104.5041869996575\n",
      "iteration: 5170 loss: inf grad: -103.06171593837749\n",
      "iteration: 5290 loss: inf grad: -106.69630504064506\n",
      "iteration: 5420 loss: inf grad: -106.82187519110192\n",
      "iteration: 5190 loss: inf grad: -106.28052501000909\n",
      "iteration: 5390 loss: inf grad: -105.78919079573453\n",
      "iteration: 5350 loss: inf grad: -106.54848196859928\n",
      "iteration: 5290 loss: inf grad: -104.67801586904\n",
      "iteration: 5180 loss: inf grad: -103.07846080212414\n",
      "iteration: 5220 loss: inf grad: -104.54716404188544\n",
      "iteration: 5300 loss: inf grad: -106.78488487207119\n",
      "iteration: 5430 loss: inf grad: -106.8620408266896\n",
      "iteration: 5200 loss: inf grad: -106.29334767566954\n",
      "iteration: 5400 loss: inf grad: -105.8363781714409\n",
      "iteration: 5360 loss: inf grad: -106.57231037068519\n",
      "iteration: 5300 loss: inf grad: -104.7592831902264\n",
      "iteration: 5190 loss: inf grad: -103.09768147230224\n",
      "iteration: 5230 loss: inf grad: -104.58401743685657\n",
      "iteration: 5310 loss: inf grad: -106.84592693563451\n",
      "iteration: 5440 loss: inf grad: -106.90140223727019\n",
      "iteration: 5210 loss: inf grad: -106.31403045800948\n",
      "iteration: 5410 loss: inf grad: -105.87436727880664\n",
      "iteration: 5370 loss: inf grad: -106.59926579473543\n",
      "iteration: 5310 loss: inf grad: -104.81642380101081\n",
      "iteration: 5200 loss: inf grad: -103.13332656998183\n",
      "iteration: 5240 loss: inf grad: -104.63898236807809\n",
      "iteration: 5450 loss: inf grad: -106.92567995393225\n",
      "iteration: 5220 loss: inf grad: -106.34035630500287\n",
      "iteration: 5420 loss: inf grad: -105.9115015478796\n",
      "iteration: 5320 loss: inf grad: -106.8800780127408\n",
      "iteration: 5380 loss: inf grad: -106.6291968541558\n",
      "iteration: 5320 loss: inf grad: -104.8544467928873\n",
      "iteration: 5210 loss: inf grad: -103.18550311995696\n",
      "iteration: 5230 loss: inf grad: -106.36966082284877\n",
      "iteration: 5460 loss: inf grad: -106.95553959532225\n",
      "iteration: 5250 loss: inf grad: -104.70053566245159\n",
      "iteration: 5330 loss: inf grad: -106.91208221178329\n",
      "iteration: 5430 loss: inf grad: -105.96766183742929\n",
      "iteration: 5390 loss: inf grad: -106.6721258063238\n",
      "iteration: 5220 loss: inf grad: -103.20999882378405\n",
      "iteration: 5330 loss: inf grad: -104.87584560980034\n",
      "iteration: 5240 loss: inf grad: -106.40158265667989\n",
      "iteration: 5260 loss: inf grad: -104.72760291517761\n",
      "iteration: 5470 loss: inf grad: -106.96504991210958\n",
      "iteration: 5340 loss: inf grad: -106.94088756551815\n",
      "iteration: 5440 loss: inf grad: -106.04024442509137\n",
      "iteration: 5400 loss: inf grad: -106.7283570543899\n",
      "iteration: 5230 loss: inf grad: -103.21979168918008\n",
      "iteration: 5340 loss: inf grad: -104.90378154906858\n",
      "iteration: 5480 loss: inf grad: -106.96717952309292\n",
      "iteration: 5270 loss: inf grad: -104.76059975719818\n",
      "iteration: 5350 loss: inf grad: -106.96618231476701\n",
      "iteration: 5410 loss: inf grad: -106.76567617512248\n",
      "iteration: 5250 loss: inf grad: -106.42728579609228\n",
      "iteration: 5450 loss: inf grad: -106.1236175303176\n",
      "iteration: 5240 loss: inf grad: -103.23152354174819\n",
      "iteration: 5350 loss: inf grad: -104.94212374433728\n",
      "iteration: 5490 loss: inf grad: -106.96958479671494\n",
      "iteration: 5360 loss: inf grad: -106.9929991554037\n",
      "iteration: 5280 loss: inf grad: -104.80915605256075\n",
      "iteration: 5420 loss: inf grad: -106.77988970908918\n",
      "iteration: 5260 loss: inf grad: -106.46446594332349\n",
      "iteration: 5460 loss: inf grad: -106.19696438551712\n",
      "iteration: 5250 loss: inf grad: -103.25615865427963\n",
      "iteration: 5360 loss: inf grad: -104.9768600033085\n",
      "iteration: 5500 loss: inf grad: -106.9743933343527\n",
      "iteration: 5370 loss: inf grad: -107.01079014010071\n",
      "iteration: 5290 loss: inf grad: -104.82858925021226\n",
      "iteration: 5430 loss: inf grad: -106.78449939433466\n",
      "iteration: 5270 loss: inf grad: -106.50565128549006\n",
      "iteration: 5470 loss: inf grad: -106.23545128749689\n",
      "iteration: 5260 loss: inf grad: -103.30847591920673\n",
      "iteration: 5370 loss: inf grad: -104.98943605328336\n",
      "iteration: 5510 loss: inf grad: -106.98907592336195\n",
      "iteration: 5380 loss: inf grad: -107.02389170713892\n",
      "iteration: 5300 loss: inf grad: -104.8390992558362\n",
      "iteration: 5480 loss: inf grad: -106.26758468311428\n",
      "iteration: 5280 loss: inf grad: -106.53381157703323\n",
      "iteration: 5440 loss: inf grad: -106.78831221562747\n",
      "iteration: 5270 loss: inf grad: -103.3656406816683\n",
      "iteration: 5380 loss: inf grad: -105.00787393279944\n",
      "iteration: 5520 loss: inf grad: -107.03127133826483\n",
      "iteration: 5390 loss: inf grad: -107.04687093593847\n",
      "iteration: 5290 loss: inf grad: -106.56415380373798\n",
      "iteration: 5310 loss: inf grad: -104.85097922657064\n",
      "iteration: 5490 loss: inf grad: -106.30409760795732\n",
      "iteration: 5280 loss: inf grad: -103.40056441993856\n",
      "iteration: 5450 loss: inf grad: -106.79738742550515\n",
      "iteration: 5530 loss: inf grad: -107.07672247885799\n",
      "iteration: 5390 loss: inf grad: -105.03382710001239\n",
      "iteration: 5400 loss: inf grad: -107.09248734370067\n",
      "iteration: 5300 loss: inf grad: -106.59351524769494\n",
      "iteration: 5320 loss: inf grad: -104.87054362591101\n",
      "iteration: 5500 loss: inf grad: -106.33505759884329\n",
      "iteration: 5290 loss: inf grad: -103.43838060956807\n",
      "iteration: 5460 loss: inf grad: -106.81291915925135\n",
      "iteration: 5540 loss: inf grad: -107.11489170153556\n",
      "iteration: 5400 loss: inf grad: -105.0514926313776\n",
      "iteration: 5310 loss: inf grad: -106.63034565160645\n",
      "iteration: 5510 loss: inf grad: -106.36955393645198\n",
      "iteration: 5410 loss: inf grad: -107.15460319658807\n",
      "iteration: 5330 loss: inf grad: -104.90971065295055\n",
      "iteration: 5470 loss: inf grad: -106.84128936008108\n",
      "iteration: 5300 loss: inf grad: -103.48815491104553\n",
      "iteration: 5550 loss: inf grad: -107.16038199137824\n",
      "iteration: 5410 loss: inf grad: -105.06862779262309\n",
      "iteration: 5520 loss: inf grad: -106.40814375924464\n",
      "iteration: 5320 loss: inf grad: -106.69669547411746\n",
      "iteration: 5420 loss: inf grad: -107.20356488216264\n",
      "iteration: 5340 loss: inf grad: -104.96986649443026\n",
      "iteration: 5480 loss: inf grad: -106.89402613913902\n",
      "iteration: 5310 loss: inf grad: -103.53557539380161\n",
      "iteration: 5560 loss: inf grad: -107.19497094985516\n",
      "iteration: 5530 loss: inf grad: -106.44641049494906\n",
      "iteration: 5430 loss: inf grad: -107.23023739600663\n",
      "iteration: 5330 loss: inf grad: -106.78151373392572\n",
      "iteration: 5420 loss: inf grad: -105.09542394028722\n",
      "iteration: 5350 loss: inf grad: -105.02939393135219\n",
      "iteration: 5490 loss: inf grad: -106.95309389911316\n",
      "iteration: 5320 loss: inf grad: -103.56851221420845\n",
      "iteration: 5570 loss: inf grad: -107.21400529389442\n",
      "iteration: 5440 loss: inf grad: -107.24679828948373\n",
      "iteration: 5540 loss: inf grad: -106.47512642266693\n",
      "iteration: 5430 loss: inf grad: -105.12538246297314\n",
      "iteration: 5340 loss: inf grad: -106.84187957089523\n",
      "iteration: 5360 loss: inf grad: -105.06824863007921\n",
      "iteration: 5500 loss: inf grad: -106.98469987324532\n",
      "iteration: 5330 loss: inf grad: -103.58206487469141\n",
      "iteration: 5580 loss: inf grad: -107.22803673625081\n",
      "iteration: 5550 loss: inf grad: -106.52039679686092\n",
      "iteration: 5450 loss: inf grad: -107.26010363380485\n",
      "iteration: 5440 loss: inf grad: -105.16350248324336\n",
      "iteration: 5350 loss: inf grad: -106.88474782527837\n",
      "iteration: 5370 loss: inf grad: -105.12104143481642\n",
      "iteration: 5340 loss: inf grad: -103.58836655467967\n",
      "iteration: 5590 loss: inf grad: -107.24146982022283\n",
      "iteration: 5510 loss: inf grad: -107.00803702647332\n",
      "iteration: 5450 loss: inf grad: -105.21884188767832\n",
      "iteration: 5560 loss: inf grad: -106.57346496932072\n",
      "iteration: 5460 loss: inf grad: -107.27110108429122\n",
      "iteration: 5360 loss: inf grad: -106.9277090783294\n",
      "iteration: 5380 loss: inf grad: -105.18169838952323\n",
      "iteration: 5350 loss: inf grad: -103.59398431697394\n",
      "iteration: 5520 loss: inf grad: -107.03667624650018\n",
      "iteration: 5600 loss: inf grad: -107.25532097349185\n",
      "iteration: 5460 loss: inf grad: -105.29232196926908\n",
      "iteration: 5470 loss: inf grad: -107.28131850032824\n",
      "iteration: 5390 loss: inf grad: -105.21861685307695\n",
      "iteration: 5570 loss: inf grad: -106.6059203735391\n",
      "iteration: 5370 loss: inf grad: -106.9807188372927\n",
      "iteration: 5530 loss: inf grad: -107.06368329957513\n",
      "iteration: 5360 loss: inf grad: -103.6001971641779\n",
      "iteration: 5610 loss: inf grad: -107.26988041396052\n",
      "iteration: 5470 loss: inf grad: -105.36121408273388\n",
      "iteration: 5480 loss: inf grad: -107.30205983327934\n",
      "iteration: 5580 loss: inf grad: -106.63886720497027\n",
      "iteration: 5400 loss: inf grad: -105.24706312183724\n",
      "iteration: 5380 loss: inf grad: -107.04626097900545\n",
      "iteration: 5540 loss: inf grad: -107.07853773685785\n",
      "iteration: 5620 loss: inf grad: -107.2867985685715\n",
      "iteration: 5370 loss: inf grad: -103.60627671058901\n",
      "iteration: 5480 loss: inf grad: -105.40901573083781\n",
      "iteration: 5490 loss: inf grad: -107.33847882235015\n",
      "iteration: 5590 loss: inf grad: -106.7019854102214\n",
      "iteration: 5410 loss: inf grad: -105.28876221738138\n",
      "iteration: 5390 loss: inf grad: -107.10278777847205\n",
      "iteration: 5550 loss: inf grad: -107.0859921600304\n",
      "iteration: 5630 loss: inf grad: -107.31489469386337\n",
      "iteration: 5490 loss: inf grad: -105.47104728562007\n",
      "iteration: 5380 loss: inf grad: -103.61833311396936\n",
      "iteration: 5500 loss: inf grad: -107.3753654967623\n",
      "iteration: 5600 loss: inf grad: -106.73298143973469\n",
      "iteration: 5420 loss: inf grad: -105.36074884917471\n",
      "iteration: 5400 loss: inf grad: -107.14064903316572\n",
      "iteration: 5560 loss: inf grad: -107.09354768388496\n",
      "iteration: 5640 loss: inf grad: -107.35875063067193\n",
      "iteration: 5390 loss: inf grad: -103.66987259541789\n",
      "iteration: 5500 loss: inf grad: -105.56240456532873\n",
      "iteration: 5510 loss: inf grad: -107.41833939093004\n",
      "iteration: 5610 loss: inf grad: -106.76620665959348\n",
      "iteration: 5430 loss: inf grad: -105.4366732699475\n",
      "iteration: 5570 loss: inf grad: -107.10949867335822\n",
      "iteration: 5410 loss: inf grad: -107.18621450597726\n",
      "iteration: 5650 loss: inf grad: -107.40101406803005\n",
      "iteration: 5510 loss: inf grad: -105.6261846684866\n",
      "iteration: 5520 loss: inf grad: -107.44491767659787\n",
      "iteration: 5400 loss: inf grad: -103.70366508988008\n",
      "iteration: 5440 loss: inf grad: -105.49369495275207\n",
      "iteration: 5620 loss: inf grad: -106.88196189068884\n",
      "iteration: 5580 loss: inf grad: -107.13805258432058\n",
      "iteration: 5420 loss: inf grad: -107.23530143018998\n",
      "iteration: 5660 loss: inf grad: -107.44499984666173\n",
      "iteration: 5530 loss: inf grad: -107.45167980355852\n",
      "iteration: 5520 loss: inf grad: -105.66939170997031\n",
      "iteration: 5410 loss: inf grad: -103.71227760696038\n",
      "iteration: 5450 loss: inf grad: -105.54192410537594\n",
      "iteration: 5590 loss: inf grad: -107.16241247947389\n",
      "iteration: 5630 loss: inf grad: -106.99247959567941\n",
      "iteration: 5540 loss: inf grad: -107.45616773649317\n",
      "iteration: 5670 loss: inf grad: -107.48762958313375\n",
      "iteration: 5530 loss: inf grad: -105.69919423008943\n",
      "iteration: 5420 loss: inf grad: -103.71596451634606\n",
      "iteration: 5430 loss: inf grad: -107.26197251400288\n",
      "iteration: 5460 loss: inf grad: -105.55061181316646\n",
      "iteration: 5600 loss: inf grad: -107.18622022354705\n",
      "iteration: 5640 loss: inf grad: -107.04829435686045\n",
      "iteration: 5550 loss: inf grad: -107.46357828678696\n",
      "iteration: 5680 loss: inf grad: -107.5194777112435\n",
      "iteration: 5540 loss: inf grad: -105.72095445137315\n",
      "iteration: 5430 loss: inf grad: -103.71977215220792\n",
      "iteration: 5440 loss: inf grad: -107.29168551139631\n",
      "iteration: 5470 loss: inf grad: -105.55513178863683\n",
      "iteration: 5650 loss: inf grad: -107.07814397981969\n",
      "iteration: 5610 loss: inf grad: -107.21613791970002\n",
      "iteration: 5690 loss: inf grad: -107.539606745328\n",
      "iteration: 5560 loss: inf grad: -107.47731699591421\n",
      "iteration: 5440 loss: inf grad: -103.72540334936255\n",
      "iteration: 5450 loss: inf grad: -107.32927070172607\n",
      "iteration: 5550 loss: inf grad: -105.738636160152\n",
      "iteration: 5480 loss: inf grad: -105.55980404634457\n",
      "iteration: 5660 loss: inf grad: -107.09306922557133\n",
      "iteration: 5700 loss: inf grad: -107.55590640308658\n",
      "iteration: 5620 loss: inf grad: -107.24250104276314\n",
      "iteration: 5450 loss: inf grad: -103.74003410536825\n",
      "iteration: 5570 loss: inf grad: -107.49586082310749\n",
      "iteration: 5560 loss: inf grad: -105.75352079449232\n",
      "iteration: 5460 loss: inf grad: -107.35483214467044\n",
      "iteration: 5490 loss: inf grad: -105.56592986660405\n",
      "iteration: 5670 loss: inf grad: -107.10217821341513\n",
      "iteration: 5630 loss: inf grad: -107.25811787506541\n",
      "iteration: 5710 loss: inf grad: -107.57763987374081\n",
      "iteration: 5460 loss: inf grad: -103.77260865016763\n",
      "iteration: 5580 loss: inf grad: -107.511469061735\n",
      "iteration: 5470 loss: inf grad: -107.37598781603177\n",
      "iteration: 5680 loss: inf grad: -107.10996647326667\n",
      "iteration: 5570 loss: inf grad: -105.77215719463652\n",
      "iteration: 5500 loss: inf grad: -105.57568965474282\n",
      "iteration: 5470 loss: inf grad: -103.80793170775044\n",
      "iteration: 5640 loss: inf grad: -107.26570454266906\n",
      "iteration: 5720 loss: inf grad: -107.61787834328985\n",
      "iteration: 5590 loss: inf grad: -107.52552561114632\n",
      "iteration: 5480 loss: inf grad: -107.38839970068506\n",
      "iteration: 5690 loss: inf grad: -107.11918365954958\n",
      "iteration: 5580 loss: inf grad: -105.81094296138895\n",
      "iteration: 5510 loss: inf grad: -105.59051725196153\n",
      "iteration: 5650 loss: inf grad: -107.27175478195738\n",
      "iteration: 5730 loss: inf grad: -107.67660828842361\n",
      "iteration: 5480 loss: inf grad: -103.84524899338751\n",
      "iteration: 5600 loss: inf grad: -107.54132110407357\n",
      "iteration: 5490 loss: inf grad: -107.3973725674001\n",
      "iteration: 5520 loss: inf grad: -105.60793257900434\n",
      "iteration: 5700 loss: inf grad: -107.13660899494941\n",
      "iteration: 5590 loss: inf grad: -105.87982975467457\n",
      "iteration: 5660 loss: inf grad: -107.28029767212347\n",
      "iteration: 5740 loss: inf grad: -107.71225773519217\n",
      "iteration: 5490 loss: inf grad: -103.87863062280984\n",
      "iteration: 5610 loss: inf grad: -107.5630779742149\n",
      "iteration: 5710 loss: inf grad: -107.17119893325545\n",
      "iteration: 5500 loss: inf grad: -107.40595871910119\n",
      "iteration: 5600 loss: inf grad: -105.97681081308443\n",
      "iteration: 5530 loss: inf grad: -105.63478564658885\n",
      "iteration: 5750 loss: inf grad: -107.73448457103356\n",
      "iteration: 5670 loss: inf grad: -107.29994641270443\n",
      "iteration: 5500 loss: inf grad: -103.922335166756\n",
      "iteration: 5720 loss: inf grad: -107.20306879540482\n",
      "iteration: 5510 loss: inf grad: -107.41582116956019\n",
      "iteration: 5620 loss: inf grad: -107.58332983791801\n",
      "iteration: 5610 loss: inf grad: -106.04782604661412\n",
      "iteration: 5540 loss: inf grad: -105.67504095182937\n",
      "iteration: 5760 loss: inf grad: -107.74433361348068\n",
      "iteration: 5680 loss: inf grad: -107.35089275612691\n",
      "iteration: 5730 loss: inf grad: -107.22657405587634\n",
      "iteration: 5510 loss: inf grad: -103.9721998651357\n",
      "iteration: 5520 loss: inf grad: -107.4293034146896\n",
      "iteration: 5620 loss: inf grad: -106.10202269595581\n",
      "iteration: 5630 loss: inf grad: -107.59520235797277\n",
      "iteration: 5550 loss: inf grad: -105.6989346884157\n",
      "iteration: 5770 loss: inf grad: -107.74734888505273\n",
      "iteration: 5690 loss: inf grad: -107.38878454557877\n",
      "iteration: 5740 loss: inf grad: -107.24637249669706\n",
      "iteration: 5630 loss: inf grad: -106.16535876688056\n",
      "iteration: 5530 loss: inf grad: -107.4485007018951\n",
      "iteration: 5640 loss: inf grad: -107.60390225444021\n",
      "iteration: 5520 loss: inf grad: -104.04148424252313\n",
      "iteration: 5780 loss: inf grad: -107.74901901614854\n",
      "iteration: 5560 loss: inf grad: -105.71484906571663\n",
      "iteration: 5750 loss: inf grad: -107.26510530574237\n",
      "iteration: 5700 loss: inf grad: -107.40668630050689\n",
      "iteration: 5640 loss: inf grad: -106.22746559729134\n",
      "iteration: 5650 loss: inf grad: -107.61320472502479\n",
      "iteration: 5540 loss: inf grad: -107.4718406734099\n",
      "iteration: 5790 loss: inf grad: -107.75045313199706\n",
      "iteration: 5530 loss: inf grad: -104.13270822821059\n",
      "iteration: 5760 loss: inf grad: -107.28470347128095\n",
      "iteration: 5570 loss: inf grad: -105.73314348100854\n",
      "iteration: 5650 loss: inf grad: -106.28920455343044\n",
      "iteration: 5660 loss: inf grad: -107.62297335590341\n",
      "iteration: 5710 loss: inf grad: -107.42703399777545\n",
      "iteration: 5550 loss: inf grad: -107.50096206040172\n",
      "iteration: 5540 loss: inf grad: -104.21333226768968\n",
      "iteration: 5800 loss: inf grad: -107.75214466669476\n",
      "iteration: 5770 loss: inf grad: -107.30886262437753\n",
      "iteration: 5580 loss: inf grad: -105.74902684874156\n",
      "iteration: 5670 loss: inf grad: -107.63300687923122\n",
      "iteration: 5660 loss: inf grad: -106.32151529261887\n",
      "iteration: 5720 loss: inf grad: -107.45031728274051\n",
      "iteration: 5780 loss: inf grad: -107.34911805947482\n",
      "iteration: 5560 loss: inf grad: -107.53787557465672\n",
      "iteration: 5810 loss: inf grad: -107.75509177188462\n",
      "iteration: 5550 loss: inf grad: -104.28788387965014\n",
      "iteration: 5590 loss: inf grad: -105.76464957076152\n",
      "iteration: 5680 loss: inf grad: -107.64618691610542\n",
      "iteration: 5670 loss: inf grad: -106.34243380634942\n",
      "iteration: 5790 loss: inf grad: -107.41419206584607\n",
      "iteration: 5730 loss: inf grad: -107.47233101737541\n",
      "iteration: 5570 loss: inf grad: -107.57719245884132\n",
      "iteration: 5820 loss: inf grad: -107.76303054280405\n",
      "iteration: 5560 loss: inf grad: -104.33559283177063\n",
      "iteration: 5600 loss: inf grad: -105.78728561259658\n",
      "iteration: 5690 loss: inf grad: -107.66994137580055\n",
      "iteration: 5680 loss: inf grad: -106.35410310532316\n",
      "iteration: 5800 loss: inf grad: -107.4886183910593\n",
      "iteration: 5740 loss: inf grad: -107.50962780303286\n",
      "iteration: 5580 loss: inf grad: -107.62228807003083\n",
      "iteration: 5570 loss: inf grad: -104.3650192741823\n",
      "iteration: 5830 loss: inf grad: -107.78609604267996\n",
      "iteration: 5700 loss: inf grad: -107.69194001160393\n",
      "iteration: 5610 loss: inf grad: -105.82006802807574\n",
      "iteration: 5690 loss: inf grad: -106.36455088051676\n",
      "iteration: 5810 loss: inf grad: -107.53578785107976\n",
      "iteration: 5750 loss: inf grad: -107.56924069496465\n",
      "iteration: 5590 loss: inf grad: -107.65688656547403\n",
      "iteration: 5580 loss: inf grad: -104.39972461139979\n",
      "iteration: 5840 loss: inf grad: -107.81648743056147\n",
      "iteration: 5710 loss: inf grad: -107.7036526900895\n",
      "iteration: 5620 loss: inf grad: -105.87077369008016\n",
      "iteration: 5700 loss: inf grad: -106.37696702435483\n",
      "iteration: 5820 loss: inf grad: -107.56178176473514\n",
      "iteration: 5850 loss: inf grad: -107.84938282791396\n",
      "iteration: 5760 loss: inf grad: -107.62395820524415\n",
      "iteration: 5600 loss: inf grad: -107.68258347657317\n",
      "iteration: 5630 loss: inf grad: -105.93974494471445\n",
      "iteration: 5590 loss: inf grad: -104.43130253609057\n",
      "iteration: 5710 loss: inf grad: -106.39310242516908\n",
      "iteration: 5720 loss: inf grad: -107.71287213954054\n",
      "iteration: 5830 loss: inf grad: -107.57887121013816\n",
      "iteration: 5860 loss: inf grad: -107.91118060966201\n",
      "iteration: 5610 loss: inf grad: -107.70299316834405\n",
      "iteration: 5770 loss: inf grad: -107.6722371372966\n",
      "iteration: 5640 loss: inf grad: -106.00605475684782\n",
      "iteration: 5600 loss: inf grad: -104.46366938258024\n",
      "iteration: 5730 loss: inf grad: -107.7216378388551\n",
      "iteration: 5840 loss: inf grad: -107.59146062110744\n",
      "iteration: 5720 loss: inf grad: -106.41902462165197\n",
      "iteration: 5870 loss: inf grad: -107.97470278706697\n",
      "iteration: 5620 loss: inf grad: -107.72234573513401\n",
      "iteration: 5780 loss: inf grad: -107.71996326640544\n",
      "iteration: 5650 loss: inf grad: -106.06003426852946\n",
      "iteration: 5610 loss: inf grad: -104.49912351493369\n",
      "iteration: 5740 loss: inf grad: -107.73110719331339\n",
      "iteration: 5850 loss: inf grad: -107.60107196534608\n",
      "iteration: 5880 loss: inf grad: -108.02394975493672\n",
      "iteration: 5730 loss: inf grad: -106.45588989317451\n",
      "iteration: 5630 loss: inf grad: -107.74788666933058\n",
      "iteration: 5660 loss: inf grad: -106.10847964966163\n",
      "iteration: 5790 loss: inf grad: -107.76062099785648\n",
      "iteration: 5750 loss: inf grad: -107.74480828887891\n",
      "iteration: 5620 loss: inf grad: -104.52734680133906\n",
      "iteration: 5860 loss: inf grad: -107.6090913077029\n",
      "iteration: 5890 loss: inf grad: -108.0715318381336\n",
      "iteration: 5640 loss: inf grad: -107.77013950832381\n",
      "iteration: 5740 loss: inf grad: -106.49267350052656\n",
      "iteration: 5800 loss: inf grad: -107.78949269161765\n",
      "iteration: 5760 loss: inf grad: -107.77244446339324\n",
      "iteration: 5630 loss: inf grad: -104.57777581156444\n",
      "iteration: 5670 loss: inf grad: -106.18129198270137\n",
      "iteration: 5870 loss: inf grad: -107.61627491103825\n",
      "iteration: 5900 loss: inf grad: -108.10606811145394\n",
      "iteration: 5750 loss: inf grad: -106.51730795264157\n",
      "iteration: 5650 loss: inf grad: -107.78262730725137\n",
      "iteration: 5770 loss: inf grad: -107.82899143126724\n",
      "iteration: 5640 loss: inf grad: -104.61435916093318\n",
      "iteration: 5680 loss: inf grad: -106.24053837642012\n",
      "iteration: 5810 loss: inf grad: -107.82388850032805\n",
      "iteration: 5880 loss: inf grad: -107.62280162427504\n",
      "iteration: 5910 loss: inf grad: -108.12398688374145\n",
      "iteration: 5760 loss: inf grad: -106.52882274656903\n",
      "iteration: 5650 loss: inf grad: -104.63850558005697\n",
      "iteration: 5660 loss: inf grad: -107.79129863922996\n",
      "iteration: 5690 loss: inf grad: -106.29233427258353\n",
      "iteration: 5780 loss: inf grad: -107.89731513727277\n",
      "iteration: 5820 loss: inf grad: -107.86499413996111\n",
      "iteration: 5890 loss: inf grad: -107.62892877716214\n",
      "iteration: 5770 loss: inf grad: -106.53469288174884\n",
      "iteration: 5920 loss: inf grad: -108.14026065210439\n",
      "iteration: 5670 loss: inf grad: -107.80223469219538\n",
      "iteration: 5660 loss: inf grad: -104.66642737725007\n",
      "iteration: 5790 loss: inf grad: -107.94108343010072\n",
      "iteration: 5700 loss: inf grad: -106.30953037898637\n",
      "iteration: 5900 loss: inf grad: -107.63619892168686\n",
      "iteration: 5830 loss: inf grad: -107.89108679972605\n",
      "iteration: 5930 loss: inf grad: -108.1704946909739\n",
      "iteration: 5680 loss: inf grad: -107.82399271560219\n",
      "iteration: 5670 loss: inf grad: -104.70119872514829\n",
      "iteration: 5780 loss: inf grad: -106.54034313151516\n",
      "iteration: 5800 loss: inf grad: -107.9629730811181\n",
      "iteration: 5710 loss: inf grad: -106.31930184073667\n",
      "iteration: 5910 loss: inf grad: -107.6508626466151\n",
      "iteration: 5840 loss: inf grad: -107.90526010215731\n",
      "iteration: 5690 loss: inf grad: -107.86303077955992\n",
      "iteration: 5680 loss: inf grad: -104.74422237840855\n",
      "iteration: 5940 loss: inf grad: -108.21576331453544\n",
      "iteration: 5790 loss: inf grad: -106.55141156261271\n",
      "iteration: 5810 loss: inf grad: -107.97761419938662iteration: 5720 loss: inf grad: -106.33776728352308\n",
      "\n",
      "iteration: 5920 loss: inf grad: -107.67798259064259\n",
      "iteration: 5850 loss: inf grad: -107.92440914275858\n",
      "iteration: 5700 loss: inf grad: -107.90023345317513\n",
      "iteration: 5690 loss: inf grad: -104.8036013889199\n",
      "iteration: 5800 loss: inf grad: -106.58164843183968\n",
      "iteration: 5950 loss: inf grad: -108.27255890396273\n",
      "iteration: 5730 loss: inf grad: -106.36774537500453\n",
      "iteration: 5820 loss: inf grad: -107.99332966600866\n",
      "iteration: 5930 loss: inf grad: -107.69674597440238\n",
      "iteration: 5860 loss: inf grad: -107.95550585831901\n",
      "iteration: 5700 loss: inf grad: -104.8636033959842\n",
      "iteration: 5710 loss: inf grad: -107.93904985587841\n",
      "iteration: 5810 loss: inf grad: -106.65394303097574\n",
      "iteration: 5740 loss: inf grad: -106.4009408881382\n",
      "iteration: 5960 loss: inf grad: -108.32525882490667\n",
      "iteration: 5830 loss: inf grad: -108.01491827261995\n",
      "iteration: 5940 loss: inf grad: -107.70653898656406\n",
      "iteration: 5710 loss: inf grad: -104.91417703912478\n",
      "iteration: 5720 loss: inf grad: -107.99422059987842\n",
      "iteration: 5870 loss: inf grad: -107.97934240371518\n",
      "iteration: 5820 loss: inf grad: -106.7349180694568\n",
      "iteration: 5970 loss: inf grad: -108.35743799595006\n",
      "iteration: 5750 loss: inf grad: -106.43217526482036\n",
      "iteration: 5840 loss: inf grad: -108.04687180337172\n",
      "iteration: 5950 loss: inf grad: -107.71636412274425\n",
      "iteration: 5720 loss: inf grad: -104.95800166404103\n",
      "iteration: 5730 loss: inf grad: -108.0404885352707\n",
      "iteration: 5830 loss: inf grad: -106.78831464481163\n",
      "iteration: 5880 loss: inf grad: -107.99515290149294\n",
      "iteration: 5980 loss: inf grad: -108.38161633011364\n",
      "iteration: 5960 loss: inf grad: -107.72994614928973\n",
      "iteration: 5730 loss: inf grad: -104.991348397829\n",
      "iteration: 5740 loss: inf grad: -108.07130833457245\n",
      "iteration: 5850 loss: inf grad: -108.08649778198397\n",
      "iteration: 5760 loss: inf grad: -106.48125104551379\n",
      "iteration: 5840 loss: inf grad: -106.81462498225645\n",
      "iteration: 5890 loss: inf grad: -108.00778438145335\n",
      "iteration: 5990 loss: inf grad: -108.40937272775255\n",
      "iteration: 5970 loss: inf grad: -107.74743696126711\n",
      "iteration: 5740 loss: inf grad: -105.0304674151736\n",
      "iteration: 5750 loss: inf grad: -108.10113490104787\n",
      "iteration: 5850 loss: inf grad: -106.8351558177634\n",
      "iteration: 5860 loss: inf grad: -108.12535539106213\n",
      "iteration: 5770 loss: inf grad: -106.51172709833351\n",
      "iteration: 5980 loss: inf grad: -107.76285396929589\n",
      "iteration: 5750 loss: inf grad: -105.0613065486873\n",
      "iteration: 6000 loss: inf grad: -108.44601968701859\n",
      "iteration: 5860 loss: inf grad: -106.85593025177972\n",
      "iteration: 5900 loss: inf grad: -108.02038419406026\n",
      "iteration: 5760 loss: inf grad: -108.1285397705722\n",
      "iteration: 5870 loss: inf grad: -108.17482518824846\n",
      "iteration: 5990 loss: inf grad: -107.77434802692139\n",
      "iteration: 5780 loss: inf grad: -106.53007798543538\n",
      "iteration: 6010 loss: inf grad: -108.47188468767739\n",
      "iteration: 5760 loss: inf grad: -105.08231314954196\n",
      "iteration: 5870 loss: inf grad: -106.87766141302495\n",
      "iteration: 5910 loss: inf grad: -108.03674061892602\n",
      "iteration: 5770 loss: inf grad: -108.14969693316206\n",
      "iteration: 5880 loss: inf grad: -108.24798268762541\n",
      "iteration: 5790 loss: inf grad: -106.55249113561567\n",
      "iteration: 6000 loss: inf grad: -107.78501455723897\n",
      "iteration: 5770 loss: inf grad: -105.10995087246262\n",
      "iteration: 6020 loss: inf grad: -108.48900614178453\n",
      "iteration: 5920 loss: inf grad: -108.05877374798862\n",
      "iteration: 5780 loss: inf grad: -108.16873960586165\n",
      "iteration: 5880 loss: inf grad: -106.88719126760158\n",
      "iteration: 5890 loss: inf grad: -108.3202701466104\n",
      "iteration: 5800 loss: inf grad: -106.57803339511395\n",
      "iteration: 5780 loss: inf grad: -105.14204012922141\n",
      "iteration: 6010 loss: inf grad: -107.79696522541866\n",
      "iteration: 6030 loss: inf grad: -108.50628214762199\n",
      "iteration: 5790 loss: inf grad: -108.19311080738048\n",
      "iteration: 5930 loss: inf grad: -108.08611615572606\n",
      "iteration: 5890 loss: inf grad: -106.89337726294546\n",
      "iteration: 5900 loss: inf grad: -108.35954065371823\n",
      "iteration: 5790 loss: inf grad: -105.17027104828387\n",
      "iteration: 5810 loss: inf grad: -106.59508008215386\n",
      "iteration: 6020 loss: inf grad: -107.81154293726775\n",
      "iteration: 5800 loss: inf grad: -108.23030226951977\n",
      "iteration: 6040 loss: inf grad: -108.52680135655442\n",
      "iteration: 5940 loss: inf grad: -108.11605933935425\n",
      "iteration: 5900 loss: inf grad: -106.90191827786157\n",
      "iteration: 5910 loss: inf grad: -108.3822607891163\n",
      "iteration: 5800 loss: inf grad: -105.21292572714876\n",
      "iteration: 5820 loss: inf grad: -106.6098665822071\n",
      "iteration: 5810 loss: inf grad: -108.27066347007488\n",
      "iteration: 5950 loss: inf grad: -108.13571368757948\n",
      "iteration: 6050 loss: inf grad: -108.55363322064366\n",
      "iteration: 6030 loss: inf grad: -107.83310795651647iteration: 5910 loss: inf grad: -106.91992728533866\n",
      "\n",
      "iteration: 5920 loss: inf grad: -108.398656485873\n",
      "iteration: 5810 loss: inf grad: -105.27618941132249\n",
      "iteration: 5830 loss: inf grad: -106.63088376078895\n",
      "iteration: 5820 loss: inf grad: -108.29921870648084\n",
      "iteration: 5960 loss: inf grad: -108.1445009701914\n",
      "iteration: 5920 loss: inf grad: -106.95645990577322\n",
      "iteration: 6060 loss: inf grad: -108.58758947526468\n",
      "iteration: 5930 loss: inf grad: -108.41091123753742\n",
      "iteration: 5820 loss: inf grad: -105.31737705649259\n",
      "iteration: 6040 loss: inf grad: -107.86244263157907\n",
      "iteration: 5840 loss: inf grad: -106.67636386495282\n",
      "iteration: 5830 loss: inf grad: -108.32179159691896\n",
      "iteration: 5930 loss: inf grad: -106.9989482115495\n",
      "iteration: 6070 loss: inf grad: -108.61793663617756\n",
      "iteration: 5970 loss: inf grad: -108.15131735176854\n",
      "iteration: 5830 loss: inf grad: -105.3479879263617\n",
      "iteration: 5940 loss: inf grad: -108.42339585958482\n",
      "iteration: 6050 loss: inf grad: -107.88581683228176\n",
      "iteration: 5850 loss: inf grad: -106.75101887109153\n",
      "iteration: 5840 loss: inf grad: -108.34628164255955\n",
      "iteration: 6080 loss: inf grad: -108.63975657389115\n",
      "iteration: 5940 loss: inf grad: -107.02835651012875\n",
      "iteration: 5840 loss: inf grad: -105.381408347552\n",
      "iteration: 5950 loss: inf grad: -108.44045756789947\n",
      "iteration: 6060 loss: inf grad: -107.90678018104478\n",
      "iteration: 5980 loss: inf grad: -108.16664461203428\n",
      "iteration: 5860 loss: inf grad: -106.80995258610491\n",
      "iteration: 5850 loss: inf grad: -108.37647673130601\n",
      "iteration: 6090 loss: inf grad: -108.66541682542973\n",
      "iteration: 5950 loss: inf grad: -107.038524625897\n",
      "iteration: 5960 loss: inf grad: -108.46573063092336\n",
      "iteration: 5850 loss: inf grad: -105.42100364740116\n",
      "iteration: 5990 loss: inf grad: -108.20289148377296\n",
      "iteration: 6070 loss: inf grad: -107.92496531347038\n",
      "iteration: 5870 loss: inf grad: -106.84665090469827\n",
      "iteration: 5860 loss: inf grad: -108.40319381370233\n",
      "iteration: 6100 loss: inf grad: -108.70483803246374\n",
      "iteration: 5960 loss: inf grad: -107.05101762007189\n",
      "iteration: 5860 loss: inf grad: -105.45225138498648\n",
      "iteration: 5970 loss: inf grad: -108.51710438698578iteration: 6000 loss: inf grad: -108.23835857597197\n",
      "iteration: 6080 loss: inf grad: -107.95665766666602\n",
      "\n",
      "iteration: 5880 loss: inf grad: -106.865710897542\n",
      "iteration: 5870 loss: inf grad: -108.41556531132284\n",
      "iteration: 6110 loss: inf grad: -108.74791223041267\n",
      "iteration: 5970 loss: inf grad: -107.08259639647164\n",
      "iteration: 5870 loss: inf grad: -105.4719549537889\n",
      "iteration: 6010 loss: inf grad: -108.2791049094048\n",
      "iteration: 6090 loss: inf grad: -107.989176449512\n",
      "iteration: 5980 loss: inf grad: -108.57589416463583\n",
      "iteration: 5890 loss: inf grad: -106.88384344343457\n",
      "iteration: 6120 loss: inf grad: -108.78268935991505\n",
      "iteration: 5980 loss: inf grad: -107.14104317544131\n",
      "iteration: 5880 loss: inf grad: -108.42352032983801\n",
      "iteration: 5880 loss: inf grad: -105.49140110115039\n",
      "iteration: 6020 loss: inf grad: -108.33787301767924\n",
      "iteration: 6100 loss: inf grad: -108.00059175791789\n",
      "iteration: 5990 loss: inf grad: -108.63078998759494\n",
      "iteration: 5900 loss: inf grad: -106.90052457043475\n",
      "iteration: 6130 loss: inf grad: -108.8072514945091\n",
      "iteration: 5990 loss: inf grad: -107.19264571448721\n",
      "iteration: 5890 loss: inf grad: -108.43054410511277\n",
      "iteration: 5890 loss: inf grad: -105.52029184476669\n",
      "iteration: 6030 loss: inf grad: -108.3872636700865\n",
      "iteration: 6110 loss: inf grad: -108.00984336120086\n",
      "iteration: 6000 loss: inf grad: -108.67985725758248\n",
      "iteration: 5910 loss: inf grad: -106.92054781718456\n",
      "iteration: 6140 loss: inf grad: -108.82950601792217\n",
      "iteration: 5900 loss: inf grad: -108.43749680168216\n",
      "iteration: 6000 loss: inf grad: -107.22077112018678\n",
      "iteration: 5900 loss: inf grad: -105.56526734415166\n",
      "iteration: 6040 loss: inf grad: -108.41328213057652\n",
      "iteration: 6120 loss: inf grad: -108.01903673542222\n",
      "iteration: 5920 loss: inf grad: -106.95815313142703\n",
      "iteration: 6150 loss: inf grad: -108.85846120939294\n",
      "iteration: 6010 loss: inf grad: -107.2523488184445\n",
      "iteration: 6010 loss: inf grad: -108.71565552973271\n",
      "iteration: 5910 loss: inf grad: -108.44481135300403\n",
      "iteration: 5910 loss: inf grad: -105.60249257385158\n",
      "iteration: 6050 loss: inf grad: -108.44508735236809\n",
      "iteration: 5930 loss: inf grad: -107.01064106020226\n",
      "iteration: 6130 loss: inf grad: -108.02962267171115iteration: 6160 loss: inf grad: -108.909312612762\n",
      "\n",
      "iteration: 6020 loss: inf grad: -107.33044915350659\n",
      "iteration: 6060 loss: inf grad: -108.46863971789813\n",
      "iteration: 6020 loss: inf grad: -108.75013700257976\n",
      "iteration: 5920 loss: inf grad: -108.45282915902294\n",
      "iteration: 5920 loss: inf grad: -105.6145467566678\n",
      "iteration: 6170 loss: inf grad: -108.9628173847254\n",
      "iteration: 5940 loss: inf grad: -107.05627847338505\n",
      "iteration: 6070 loss: inf grad: -108.48295698773546\n",
      "iteration: 6140 loss: inf grad: -108.04352385381031\n",
      "iteration: 5930 loss: inf grad: -105.6330099738999\n",
      "iteration: 6030 loss: inf grad: -107.39221208504966\n",
      "iteration: 5930 loss: inf grad: -108.46224186823528iteration: 6030 loss: inf grad: -108.78700908525178\n",
      "\n",
      "iteration: 6180 loss: inf grad: -109.00088224612303\n",
      "iteration: 5950 loss: inf grad: -107.0899558491225\n",
      "iteration: 6080 loss: inf grad: -108.50114717673227\n",
      "iteration: 6150 loss: inf grad: -108.06110005837708\n",
      "iteration: 5940 loss: inf grad: -105.68254142367331\n",
      "iteration: 6040 loss: inf grad: -108.81805721490814\n",
      "iteration: 5940 loss: inf grad: -108.47619642543287\n",
      "iteration: 6040 loss: inf grad: -107.4233779780813\n",
      "iteration: 6190 loss: inf grad: -109.04697264508073\n",
      "iteration: 6090 loss: inf grad: -108.52867348849223\n",
      "iteration: 6160 loss: inf grad: -108.0772519545846\n",
      "iteration: 5960 loss: inf grad: -107.11804366371173\n",
      "iteration: 5950 loss: inf grad: -105.72306651947335\n",
      "iteration: 6050 loss: inf grad: -108.8386084618757\n",
      "iteration: 6050 loss: inf grad: -107.44946151465604\n",
      "iteration: 5950 loss: inf grad: -108.5037711172231\n",
      "iteration: 6200 loss: inf grad: -109.0934372590478\n",
      "iteration: 6100 loss: inf grad: -108.5909047048052\n",
      "iteration: 6170 loss: inf grad: -108.08855047663505\n",
      "iteration: 5960 loss: inf grad: -105.75260438025128\n",
      "iteration: 6060 loss: inf grad: -108.85323367153724\n",
      "iteration: 5970 loss: inf grad: -107.17121256133059\n",
      "iteration: 6060 loss: inf grad: -107.46683469595297\n",
      "iteration: 5960 loss: inf grad: -108.53671003150302\n",
      "iteration: 6110 loss: inf grad: -108.67383637403948\n",
      "iteration: 6210 loss: inf grad: -109.11923610845079\n",
      "iteration: 6180 loss: inf grad: -108.09761177055415\n",
      "iteration: 5970 loss: inf grad: -105.7806023914803\n",
      "iteration: 6070 loss: inf grad: -108.87162037439532\n",
      "iteration: 6070 loss: inf grad: -107.4835278874799\n",
      "iteration: 5970 loss: inf grad: -108.55817878584554\n",
      "iteration: 5980 loss: inf grad: -107.20879191721797\n",
      "iteration: 6220 loss: inf grad: -109.14294720583\n",
      "iteration: 6190 loss: inf grad: -108.10969696673988\n",
      "iteration: 5980 loss: inf grad: -105.81009403070374\n",
      "iteration: 6080 loss: inf grad: -108.91156164317482\n",
      "iteration: 6120 loss: inf grad: -108.73253815669204\n",
      "iteration: 6080 loss: inf grad: -107.52001157335076\n",
      "iteration: 5980 loss: inf grad: -108.5751760560687\n",
      "iteration: 5990 loss: inf grad: -107.23994898522358\n",
      "iteration: 5990 loss: inf grad: -105.84135234228197\n",
      "iteration: 6200 loss: inf grad: -108.13481625877517\n",
      "iteration: 6230 loss: inf grad: -109.16796695286517\n",
      "iteration: 6130 loss: inf grad: -108.7678036033245\n",
      "iteration: 6090 loss: inf grad: -108.95882686326559\n",
      "iteration: 6090 loss: inf grad: -107.58737780452697\n",
      "iteration: 5990 loss: inf grad: -108.60481724416215\n",
      "iteration: 6000 loss: inf grad: -107.28001604396655\n",
      "iteration: 6000 loss: inf grad: -105.87332684099351\n",
      "iteration: 6210 loss: inf grad: -108.17070838350242\n",
      "iteration: 6140 loss: inf grad: -108.7891084294313\n",
      "iteration: 6100 loss: inf grad: -108.98640292427949\n",
      "iteration: 6100 loss: inf grad: -107.63197974457618\n",
      "iteration: 6240 loss: inf grad: -109.18737697589658\n",
      "iteration: 6000 loss: inf grad: -108.66942176864058\n",
      "iteration: 6010 loss: inf grad: -107.31591498582397\n",
      "iteration: 6010 loss: inf grad: -105.90634631890546\n",
      "iteration: 6220 loss: inf grad: -108.21963766430014\n",
      "iteration: 6150 loss: inf grad: -108.80467895237348\n",
      "iteration: 6110 loss: inf grad: -109.01647290890483\n",
      "iteration: 6110 loss: inf grad: -107.69341953689279\n",
      "iteration: 6010 loss: inf grad: -108.74441330004665\n",
      "iteration: 6250 loss: inf grad: -109.20045072362116\n",
      "iteration: 6020 loss: inf grad: -107.33252828958874\n",
      "iteration: 6020 loss: inf grad: -105.93499723560754\n",
      "iteration: 6230 loss: inf grad: -108.2502386684213iteration: 6120 loss: inf grad: -109.06374782023079\n",
      "\n",
      "iteration: 6120 loss: inf grad: -107.78222901880287\n",
      "iteration: 6160 loss: inf grad: -108.81744406371638\n",
      "iteration: 6260 loss: inf grad: -109.21211493003456\n",
      "iteration: 6020 loss: inf grad: -108.78844657273669\n",
      "iteration: 6030 loss: inf grad: -107.34916181567235iteration: 6030 loss: inf grad: -105.95685013562402\n",
      "\n",
      "iteration: 6130 loss: inf grad: -107.86507839767185\n",
      "iteration: 6130 loss: inf grad: -109.10480761960129\n",
      "iteration: 6170 loss: inf grad: -108.82853634467227\n",
      "iteration: 6270 loss: inf grad: -109.23182747793697\n",
      "iteration: 6240 loss: inf grad: -108.27129659278317\n",
      "iteration: 6030 loss: inf grad: -108.8170091507532\n",
      "iteration: 6040 loss: inf grad: -105.98938115371462\n",
      "iteration: 6040 loss: inf grad: -107.38770539343955\n",
      "iteration: 6140 loss: inf grad: -107.90117776277302\n",
      "iteration: 6180 loss: inf grad: -108.83932168414461\n",
      "iteration: 6280 loss: inf grad: -109.27439407569746\n",
      "iteration: 6040 loss: inf grad: -108.8453416966141\n",
      "iteration: 6140 loss: inf grad: -109.12787386055751\n",
      "iteration: 6050 loss: inf grad: -106.03001361594528\n",
      "iteration: 6250 loss: inf grad: -108.30324170088753\n",
      "iteration: 6050 loss: inf grad: -107.43632670997079\n",
      "iteration: 6150 loss: inf grad: -107.92215811117306\n",
      "iteration: 6190 loss: inf grad: -108.85120966287337\n",
      "iteration: 6050 loss: inf grad: -108.87543730186144\n",
      "iteration: 6290 loss: inf grad: -109.33904931428319\n",
      "iteration: 6150 loss: inf grad: -109.14062098846362\n",
      "iteration: 6260 loss: inf grad: -108.33417600671746\n",
      "iteration: 6060 loss: inf grad: -106.05480094915696\n",
      "iteration: 6060 loss: inf grad: -107.48458791449099\n",
      "iteration: 6160 loss: inf grad: -107.95105586598646\n",
      "iteration: 6200 loss: inf grad: -108.8660894497007\n",
      "iteration: 6060 loss: inf grad: -108.89968370005371\n",
      "iteration: 6300 loss: inf grad: -109.38822010727823\n",
      "iteration: 6160 loss: inf grad: -109.1486187285581\n",
      "iteration: 6270 loss: inf grad: -108.34929233631497\n",
      "iteration: 6070 loss: inf grad: -106.07792893350748\n",
      "iteration: 6070 loss: inf grad: -107.53134922781214\n",
      "iteration: 6210 loss: inf grad: -108.88681386129265\n",
      "iteration: 6070 loss: inf grad: -108.92138289115026\n",
      "iteration: 6170 loss: inf grad: -107.99389647416507\n",
      "iteration: 6080 loss: inf grad: -106.13078136312879\n",
      "iteration: 6280 loss: inf grad: -108.35637480501809\n",
      "iteration: 6170 loss: inf grad: -109.15574907373014\n",
      "iteration: 6310 loss: inf grad: -109.42695728540171\n",
      "iteration: 6080 loss: inf grad: -107.55642732371187\n",
      "iteration: 6220 loss: inf grad: -108.91021893466493\n",
      "iteration: 6080 loss: inf grad: -108.95575011918194\n",
      "iteration: 6320 loss: inf grad: -109.45429333995122\n",
      "iteration: 6090 loss: inf grad: -106.1935246105021\n",
      "iteration: 6180 loss: inf grad: -109.1662419601331\n",
      "iteration: 6290 loss: inf grad: -108.36130914105448\n",
      "iteration: 6180 loss: inf grad: -108.02846367228076\n",
      "iteration: 6090 loss: inf grad: -107.56643324100463\n",
      "iteration: 6230 loss: inf grad: -108.9280719835408\n",
      "iteration: 6090 loss: inf grad: -108.99746580295894\n",
      "iteration: 6330 loss: inf grad: -109.47234182009115\n",
      "iteration: 6300 loss: inf grad: -108.3675327696532\n",
      "iteration: 6190 loss: inf grad: -108.04883905914991\n",
      "iteration: 6100 loss: inf grad: -106.23913468613884\n",
      "iteration: 6190 loss: inf grad: -109.18965807253892\n",
      "iteration: 6240 loss: inf grad: -108.94206376704915\n",
      "iteration: 6100 loss: inf grad: -109.02429353716337\n",
      "iteration: 6100 loss: inf grad: -107.57185423991649\n",
      "iteration: 6340 loss: inf grad: -109.49561451057443\n",
      "iteration: 6310 loss: inf grad: -108.38205151298382\n",
      "iteration: 6200 loss: inf grad: -109.2285062058809\n",
      "iteration: 6110 loss: inf grad: -106.29189099179406\n",
      "iteration: 6250 loss: inf grad: -108.95431007568706\n",
      "iteration: 6200 loss: inf grad: -108.06779722493268\n",
      "iteration: 6110 loss: inf grad: -107.57580972356689iteration: 6110 loss: inf grad: -109.0507186313892\n",
      "\n",
      "iteration: 6320 loss: inf grad: -108.41884467508427\n",
      "iteration: 6210 loss: inf grad: -109.2567600514744\n",
      "iteration: 6260 loss: inf grad: -108.96454464149906\n",
      "iteration: 6350 loss: inf grad: -109.53383096619353\n",
      "iteration: 6120 loss: inf grad: -106.32443141570727\n",
      "iteration: 6210 loss: inf grad: -108.09444189617827\n",
      "iteration: 6120 loss: inf grad: -109.07291528216788\n",
      "iteration: 6120 loss: inf grad: -107.58042660607754\n",
      "iteration: 6330 loss: inf grad: -108.47063781109442\n",
      "iteration: 6220 loss: inf grad: -109.27175322351496\n",
      "iteration: 6270 loss: inf grad: -108.97271614462994\n",
      "iteration: 6360 loss: inf grad: -109.57843666687103\n",
      "iteration: 6130 loss: inf grad: -106.33565559206\n",
      "iteration: 6220 loss: inf grad: -108.12732833288814\n",
      "iteration: 6130 loss: inf grad: -109.08624949494067\n",
      "iteration: 6130 loss: inf grad: -107.58894541457111\n",
      "iteration: 6340 loss: inf grad: -108.51472172606188\n",
      "iteration: 6280 loss: inf grad: -108.9794251298008\n",
      "iteration: 6230 loss: inf grad: -109.28719239674861\n",
      "iteration: 6370 loss: inf grad: -109.6165144450703\n",
      "iteration: 6230 loss: inf grad: -108.15345803522955\n",
      "iteration: 6140 loss: inf grad: -106.34178980926524\n",
      "iteration: 6140 loss: inf grad: -107.60624931593163\n",
      "iteration: 6140 loss: inf grad: -109.10060825632324\n",
      "iteration: 6350 loss: inf grad: -108.53777350641809\n",
      "iteration: 6290 loss: inf grad: -108.98540188099722\n",
      "iteration: 6240 loss: inf grad: -109.30645690536966\n",
      "iteration: 6240 loss: inf grad: -108.17458108483997\n",
      "iteration: 6150 loss: inf grad: -106.35004246422193\n",
      "iteration: 6380 loss: inf grad: -109.648731910529\n",
      "iteration: 6150 loss: inf grad: -107.6312007863504\n",
      "iteration: 6150 loss: inf grad: -109.11800819646066\n",
      "iteration: 6360 loss: inf grad: -108.54781172317263\n",
      "iteration: 6300 loss: inf grad: -108.99087531089594\n",
      "iteration: 6250 loss: inf grad: -109.32073629905526\n",
      "iteration: 6250 loss: inf grad: -108.20681430568578\n",
      "iteration: 6160 loss: inf grad: -106.37436347352049\n",
      "iteration: 6160 loss: inf grad: -107.65516893710335\n",
      "iteration: 6390 loss: inf grad: -109.6831035228982\n",
      "iteration: 6160 loss: inf grad: -109.1382867070055\n",
      "iteration: 6370 loss: inf grad: -108.55513983631909\n",
      "iteration: 6310 loss: inf grad: -108.99568137839913\n",
      "iteration: 6260 loss: inf grad: -109.33240588243214\n",
      "iteration: 6170 loss: inf grad: -106.43994810519261\n",
      "iteration: 6260 loss: inf grad: -108.26678178399078\n",
      "iteration: 6170 loss: inf grad: -107.67815489973205\n",
      "iteration: 6400 loss: inf grad: -109.71151830300917\n",
      "iteration: 6320 loss: inf grad: -109.00042835824726\n",
      "iteration: 6380 loss: inf grad: -108.56396770313259\n",
      "iteration: 6270 loss: inf grad: -109.35242597546052\n",
      "iteration: 6180 loss: inf grad: -106.48557380520998\n",
      "iteration: 6170 loss: inf grad: -109.16552850832309\n",
      "iteration: 6270 loss: inf grad: -108.31789746552903\n",
      "iteration: 6180 loss: inf grad: -107.71391253870968\n",
      "iteration: 6410 loss: inf grad: -109.73246032410802\n",
      "iteration: 6330 loss: inf grad: -109.00591059760364\n",
      "iteration: 6390 loss: inf grad: -108.58123568147275\n",
      "iteration: 6190 loss: inf grad: -106.51547326755457\n",
      "iteration: 6280 loss: inf grad: -109.3942223035632\n",
      "iteration: 6180 loss: inf grad: -109.19889710885181\n",
      "iteration: 6190 loss: inf grad: -107.74498958560605\n",
      "iteration: 6280 loss: inf grad: -108.35300186642849\n",
      "iteration: 6420 loss: inf grad: -109.74671461598811\n",
      "iteration: 6340 loss: inf grad: -109.01324509954878\n",
      "iteration: 6400 loss: inf grad: -108.61534942349182\n",
      "iteration: 6200 loss: inf grad: -106.5514987464721\n",
      "iteration: 6190 loss: inf grad: -109.23099174343636\n",
      "iteration: 6200 loss: inf grad: -107.787881881382\n",
      "iteration: 6290 loss: inf grad: -109.44447157999774\n",
      "iteration: 6290 loss: inf grad: -108.37961041942643\n",
      "iteration: 6410 loss: inf grad: -108.65119603532027\n",
      "iteration: 6430 loss: inf grad: -109.75668706713232\n",
      "iteration: 6210 loss: inf grad: -106.5866619385717\n",
      "iteration: 6350 loss: inf grad: -109.02550094956085\n",
      "iteration: 6200 loss: inf grad: -109.24871419817006\n",
      "iteration: 6210 loss: inf grad: -107.81503538452995\n",
      "iteration: 6300 loss: inf grad: -109.48672584638578\n",
      "iteration: 6300 loss: inf grad: -108.39906424308901\n",
      "iteration: 6420 loss: inf grad: -108.6765200818628\n",
      "iteration: 6440 loss: inf grad: -109.76361482220398\n",
      "iteration: 6360 loss: inf grad: -109.05327443048695\n",
      "iteration: 6220 loss: inf grad: -106.62250503675858\n",
      "iteration: 6220 loss: inf grad: -107.82797193951889\n",
      "iteration: 6210 loss: inf grad: -109.26969318603\n",
      "iteration: 6310 loss: inf grad: -109.54333492720022\n",
      "iteration: 6430 loss: inf grad: -108.70933246897175\n",
      "iteration: 6310 loss: inf grad: -108.41375691261588\n",
      "iteration: 6450 loss: inf grad: -109.76854005887965\n",
      "iteration: 6370 loss: inf grad: -109.09781401364108\n",
      "iteration: 6230 loss: inf grad: -106.6646765058687\n",
      "iteration: 6220 loss: inf grad: -109.3040076589474\n",
      "iteration: 6230 loss: inf grad: -107.83575830141166\n",
      "iteration: 6320 loss: inf grad: -109.58595699446519\n",
      "iteration: 6440 loss: inf grad: -108.76956336774177\n",
      "iteration: 6460 loss: inf grad: -109.77303855682125\n",
      "iteration: 6240 loss: inf grad: -106.68761022110621\n",
      "iteration: 6380 loss: inf grad: -109.13077311175294\n",
      "iteration: 6230 loss: inf grad: -109.35488696965115\n",
      "iteration: 6320 loss: inf grad: -108.42744543408637\n",
      "iteration: 6240 loss: inf grad: -107.84038208539147\n",
      "iteration: 6470 loss: inf grad: -109.77838345658215\n",
      "iteration: 6330 loss: inf grad: -109.60474033164039\n",
      "iteration: 6450 loss: inf grad: -108.83439548777696\n",
      "iteration: 6250 loss: inf grad: -106.7027265663518\n",
      "iteration: 6390 loss: inf grad: -109.16125047937675\n",
      "iteration: 6240 loss: inf grad: -109.40722966712462\n",
      "iteration: 6250 loss: inf grad: -107.84356916874013\n",
      "iteration: 6330 loss: inf grad: -108.45074289315357\n",
      "iteration: 6480 loss: inf grad: -109.78628543795575\n",
      "iteration: 6340 loss: inf grad: -109.62042549680578\n",
      "iteration: 6260 loss: inf grad: -106.7216958839452\n",
      "iteration: 6400 loss: inf grad: -109.19148237899395\n",
      "iteration: 6460 loss: inf grad: -108.87343022933005\n",
      "iteration: 6250 loss: inf grad: -109.44120198412688\n",
      "iteration: 6260 loss: inf grad: -107.84673363978044\n",
      "iteration: 6490 loss: inf grad: -109.80274440145958\n",
      "iteration: 6270 loss: inf grad: -106.74159558023766\n",
      "iteration: 6340 loss: inf grad: -108.49521728201833\n",
      "iteration: 6350 loss: inf grad: -109.64004623609091\n",
      "iteration: 6410 loss: inf grad: -109.22416293734611\n",
      "iteration: 6470 loss: inf grad: -108.8914502739882\n",
      "iteration: 6260 loss: inf grad: -109.45473224292863\n",
      "iteration: 6500 loss: inf grad: -109.84582626159838\n",
      "iteration: 6280 loss: inf grad: -106.7627323134993\n",
      "iteration: 6270 loss: inf grad: -107.85101205271069\n",
      "iteration: 6420 loss: inf grad: -109.26875552399478\n",
      "iteration: 6350 loss: inf grad: -108.53516324332557\n",
      "iteration: 6270 loss: inf grad: -109.47066304860648\n",
      "iteration: 6480 loss: inf grad: -108.9059786947357\n",
      "iteration: 6360 loss: inf grad: -109.67072868277965\n",
      "iteration: 6510 loss: inf grad: -109.89814374632424\n",
      "iteration: 6290 loss: inf grad: -106.80936545510522\n",
      "iteration: 6430 loss: inf grad: -109.30151502633622\n",
      "iteration: 6280 loss: inf grad: -107.85775341973786\n",
      "iteration: 6360 loss: inf grad: -108.55152085328\n",
      "iteration: 6280 loss: inf grad: -109.50036786723726\n",
      "iteration: 6490 loss: inf grad: -108.91326700750022\n",
      "iteration: 6370 loss: inf grad: -109.73764167102529\n",
      "iteration: 6520 loss: inf grad: -109.92414333964254\n",
      "iteration: 6300 loss: inf grad: -106.88142417883195\n",
      "iteration: 6440 loss: inf grad: -109.32765721006686\n",
      "iteration: 6290 loss: inf grad: -107.86977215657049\n",
      "iteration: 6370 loss: inf grad: -108.56205295178101\n",
      "iteration: 6290 loss: inf grad: -109.53273990587181\n",
      "iteration: 6380 loss: inf grad: -109.82894885758456\n",
      "iteration: 6500 loss: inf grad: -108.9219290156185\n",
      "iteration: 6310 loss: inf grad: -106.94358567428361\n",
      "iteration: 6530 loss: inf grad: -109.93766223063567\n",
      "iteration: 6450 loss: inf grad: -109.35074906637128\n",
      "iteration: 6300 loss: inf grad: -107.89072086583757\n",
      "iteration: 6300 loss: inf grad: -109.5521805407053\n",
      "iteration: 6380 loss: inf grad: -108.57778942757497\n",
      "iteration: 6390 loss: inf grad: -109.88661294483165\n",
      "iteration: 6320 loss: inf grad: -106.97783345320798\n",
      "iteration: 6540 loss: inf grad: -109.95166582246284\n",
      "iteration: 6510 loss: inf grad: -108.94664831796719\n",
      "iteration: 6460 loss: inf grad: -109.37029003614163\n",
      "iteration: 6310 loss: inf grad: -107.91641068394307\n",
      "iteration: 6310 loss: inf grad: -109.56735118745459\n",
      "iteration: 6400 loss: inf grad: -109.91326378736389\n",
      "iteration: 6330 loss: inf grad: -107.00330871779983\n",
      "iteration: 6550 loss: inf grad: -109.97846435214527\n",
      "iteration: 6390 loss: inf grad: -108.60239424364048iteration: 6470 loss: inf grad: -109.39220305910578\n",
      "iteration: 6520 loss: inf grad: -108.9565673690265\n",
      "\n",
      "iteration: 6320 loss: inf grad: -107.94515533950668\n",
      "iteration: 6320 loss: inf grad: -109.5904918041303\n",
      "iteration: 6410 loss: inf grad: -109.92595441389116\n",
      "iteration: 6340 loss: inf grad: -107.03142877179604\n",
      "iteration: 6560 loss: inf grad: -109.99242662561332\n",
      "iteration: 6480 loss: inf grad: -109.42270315330026\n",
      "iteration: 6330 loss: inf grad: -107.98225518218791\n",
      "iteration: 6330 loss: inf grad: -109.62233661684306\n",
      "iteration: 6420 loss: inf grad: -109.9395232497354\n",
      "iteration: 6400 loss: inf grad: -108.62527849437305\n",
      "iteration: 6490 loss: inf grad: -109.46446677519988\n",
      "iteration: 6530 loss: inf grad: -108.96284913515353\n",
      "iteration: 6340 loss: inf grad: -108.02126425551172\n",
      "iteration: 6570 loss: inf grad: -109.99736735194801\n",
      "iteration: 6350 loss: inf grad: -107.07226447685001\n",
      "iteration: 6340 loss: inf grad: -109.66068575822091\n",
      "iteration: 6500 loss: inf grad: -109.51820336911638\n",
      "iteration: 6410 loss: inf grad: -108.63583699913508\n",
      "iteration: 6540 loss: inf grad: -108.97433215136707\n",
      "iteration: 6430 loss: inf grad: -109.96520952092203\n",
      "iteration: 6580 loss: inf grad: -110.00342643804574\n",
      "iteration: 6360 loss: inf grad: -107.1296227845308\n",
      "iteration: 6350 loss: inf grad: -108.05307770044485\n",
      "iteration: 6350 loss: inf grad: -109.70733532866879\n",
      "iteration: 6510 loss: inf grad: -109.55928048245381\n",
      "iteration: 6550 loss: inf grad: -108.99189180437534\n",
      "iteration: 6420 loss: inf grad: -108.64357228671935\n",
      "iteration: 6590 loss: inf grad: -110.01828632495743\n",
      "iteration: 6370 loss: inf grad: -107.20660302842143\n",
      "iteration: 6440 loss: inf grad: -109.99924383672322\n",
      "iteration: 6360 loss: inf grad: -108.07879371791057\n",
      "iteration: 6360 loss: inf grad: -109.75427209006332\n",
      "iteration: 6520 loss: inf grad: -109.57843474016502\n",
      "iteration: 6560 loss: inf grad: -109.00929702353002\n",
      "iteration: 6430 loss: inf grad: -108.66223558337404\n",
      "iteration: 6600 loss: inf grad: -110.05556358350003\n",
      "iteration: 6380 loss: inf grad: -107.27037715550375\n",
      "iteration: 6450 loss: inf grad: -110.03219976365227\n",
      "iteration: 6370 loss: inf grad: -109.79474412806486\n",
      "iteration: 6570 loss: inf grad: -109.02675097812048\n",
      "iteration: 6530 loss: inf grad: -109.59019274193093\n",
      "iteration: 6370 loss: inf grad: -108.09374223700748\n",
      "iteration: 6610 loss: inf grad: -110.09036040243498\n",
      "iteration: 6440 loss: inf grad: -108.69070610360072\n",
      "iteration: 6390 loss: inf grad: -107.31131637557922\n",
      "iteration: 6460 loss: inf grad: -110.07070206449944\n",
      "iteration: 6380 loss: inf grad: -109.82518951861857\n",
      "iteration: 6540 loss: inf grad: -109.60396178521027\n",
      "iteration: 6580 loss: inf grad: -109.05085965293912\n",
      "iteration: 6620 loss: inf grad: -110.10772139045015\n",
      "iteration: 6450 loss: inf grad: -108.72131048690716\n",
      "iteration: 6380 loss: inf grad: -108.10093028321734\n",
      "iteration: 6400 loss: inf grad: -107.3436179254974\n",
      "iteration: 6470 loss: inf grad: -110.10275643341711\n",
      "iteration: 6390 loss: inf grad: -109.84245477033073\n",
      "iteration: 6590 loss: inf grad: -109.08219389097309\n",
      "iteration: 6630 loss: inf grad: -110.11751914201378\n",
      "iteration: 6550 loss: inf grad: -109.62422462481248\n",
      "iteration: 6460 loss: inf grad: -108.76255455609481\n",
      "iteration: 6390 loss: inf grad: -108.10519936701306\n",
      "iteration: 6410 loss: inf grad: -107.37394138531752\n",
      "iteration: 6400 loss: inf grad: -109.85974073994282\n",
      "iteration: 6480 loss: inf grad: -110.13401665765576\n",
      "iteration: 6640 loss: inf grad: -110.12565077631152\n",
      "iteration: 6600 loss: inf grad: -109.12152340213514\n",
      "iteration: 6470 loss: inf grad: -108.79952151381792\n",
      "iteration: 6400 loss: inf grad: -108.10917336347123\n",
      "iteration: 6560 loss: inf grad: -109.64342302088784\n",
      "iteration: 6420 loss: inf grad: -107.40692009868793\n",
      "iteration: 6490 loss: inf grad: -110.1707981361719\n",
      "iteration: 6410 loss: inf grad: -109.88471210667404\n",
      "iteration: 6610 loss: inf grad: -109.17241284924208\n",
      "iteration: 6650 loss: inf grad: -110.13430239512837\n",
      "iteration: 6410 loss: inf grad: -108.1159574472224\n",
      "iteration: 6480 loss: inf grad: -108.82737135888834\n",
      "iteration: 6570 loss: inf grad: -109.65565930346975\n",
      "iteration: 6430 loss: inf grad: -107.44325561133145\n",
      "iteration: 6500 loss: inf grad: -110.20436472194413\n",
      "iteration: 6420 loss: inf grad: -108.13145981344039\n",
      "iteration: 6420 loss: inf grad: -109.90975350142486\n",
      "iteration: 6620 loss: inf grad: -109.1843565601749\n",
      "iteration: 6660 loss: inf grad: -110.14442428721706\n",
      "iteration: 6490 loss: inf grad: -108.84607702219591\n",
      "iteration: 6440 loss: inf grad: -107.4745580087849\n",
      "iteration: 6580 loss: inf grad: -109.66524104622125\n",
      "iteration: 6430 loss: inf grad: -108.16022078731453\n",
      "iteration: 6510 loss: inf grad: -110.23022978260992\n",
      "iteration: 6450 loss: inf grad: -107.4991891214828\n",
      "iteration: 6670 loss: inf grad: -110.15743339053896\n",
      "iteration: 6590 loss: inf grad: -109.6750443744279\n",
      "iteration: 6430 loss: inf grad: -109.92531326633319\n",
      "iteration: 6630 loss: inf grad: -109.19327260664957\n",
      "iteration: 6500 loss: inf grad: -108.86070961717432\n",
      "iteration: 6440 loss: inf grad: -108.19648751557193\n",
      "iteration: 6460 loss: inf grad: -107.51950305576489\n",
      "iteration: 6520 loss: inf grad: -110.25195122289462\n",
      "iteration: 6680 loss: inf grad: -110.17442715790773\n",
      "iteration: 6640 loss: inf grad: -109.20663604255232\n",
      "iteration: 6440 loss: inf grad: -109.93782249637277\n",
      "iteration: 6510 loss: inf grad: -108.87143137469809iteration: 6600 loss: inf grad: -109.68550689319112\n",
      "\n",
      "iteration: 6450 loss: inf grad: -108.24683201848592\n",
      "iteration: 6470 loss: inf grad: -107.53517357059067\n",
      "iteration: 6530 loss: inf grad: -110.27696228955828\n",
      "iteration: 6690 loss: inf grad: -110.19305194912371\n",
      "iteration: 6650 loss: inf grad: -109.22561274984716\n",
      "iteration: 6450 loss: inf grad: -109.97086012963264\n",
      "iteration: 6520 loss: inf grad: -108.87878649090105\n",
      "iteration: 6610 loss: inf grad: -109.69755040873486\n",
      "iteration: 6460 loss: inf grad: -108.307017133429\n",
      "iteration: 6480 loss: inf grad: -107.5706482381377\n",
      "iteration: 6540 loss: inf grad: -110.31299915044065\n",
      "iteration: 6700 loss: inf grad: -110.20926560701908\n",
      "iteration: 6660 loss: inf grad: -109.2451990186953\n",
      "iteration: 6460 loss: inf grad: -110.03544098614142\n",
      "iteration: 6530 loss: inf grad: -108.8852597866358\n",
      "iteration: 6470 loss: inf grad: -108.36263518390336\n",
      "iteration: 6620 loss: inf grad: -109.71449541648953\n",
      "iteration: 6490 loss: inf grad: -107.61716259942932\n",
      "iteration: 6550 loss: inf grad: -110.35343041896428\n",
      "iteration: 6710 loss: inf grad: -110.2230148020322\n",
      "iteration: 6670 loss: inf grad: -109.26126955692993\n",
      "iteration: 6540 loss: inf grad: -108.89402566365708\n",
      "iteration: 6480 loss: inf grad: -108.40618610959467\n",
      "iteration: 6630 loss: inf grad: -109.73821750832013\n",
      "iteration: 6470 loss: inf grad: -110.0711166049587\n",
      "iteration: 6500 loss: inf grad: -107.63608910778191\n",
      "iteration: 6560 loss: inf grad: -110.407536188856\n",
      "iteration: 6720 loss: inf grad: -110.2387732041777\n",
      "iteration: 6680 loss: inf grad: -109.2761196664989\n",
      "iteration: 6550 loss: inf grad: -108.91783001176671\n",
      "iteration: 6490 loss: inf grad: -108.44701008938097\n",
      "iteration: 6480 loss: inf grad: -110.08912601946281\n",
      "iteration: 6640 loss: inf grad: -109.77328915302422\n",
      "iteration: 6510 loss: inf grad: -107.65982057697246\n",
      "iteration: 6570 loss: inf grad: -110.4686892049617\n",
      "iteration: 6730 loss: inf grad: -110.25971071444282\n",
      "iteration: 6690 loss: inf grad: -109.29290320344683\n",
      "iteration: 6500 loss: inf grad: -108.49029892730037\n",
      "iteration: 6560 loss: inf grad: -108.9496098284016\n",
      "iteration: 6490 loss: inf grad: -110.10793605249985\n",
      "iteration: 6520 loss: inf grad: -107.69631224124251\n",
      "iteration: 6650 loss: inf grad: -109.81869062137335\n",
      "iteration: 6580 loss: inf grad: -110.49281406212617\n",
      "iteration: 6700 loss: inf grad: -109.31567284313266\n",
      "iteration: 6570 loss: inf grad: -108.97759992232935\n",
      "iteration: 6500 loss: inf grad: -110.1186999164951\n",
      "iteration: 6510 loss: inf grad: -108.53152928428639\n",
      "iteration: 6740 loss: inf grad: -110.27949896002562iteration: 6530 loss: inf grad: -107.73530212915429\n",
      "\n",
      "iteration: 6660 loss: inf grad: -109.85268511554501\n",
      "iteration: 6590 loss: inf grad: -110.50650134898285\n",
      "iteration: 6580 loss: inf grad: -108.99344635484431\n",
      "iteration: 6710 loss: inf grad: -109.35803799623132\n",
      "iteration: 6520 loss: inf grad: -108.55333976192802\n",
      "iteration: 6540 loss: inf grad: -107.77167234254361\n",
      "iteration: 6750 loss: inf grad: -110.29652446075826\n",
      "iteration: 6510 loss: inf grad: -110.13489563487995\n",
      "iteration: 6600 loss: inf grad: -110.52166936642939\n",
      "iteration: 6670 loss: inf grad: -109.87809386366226\n",
      "iteration: 6590 loss: inf grad: -109.00803222500018\n",
      "iteration: 6720 loss: inf grad: -109.44143688884819\n",
      "iteration: 6530 loss: inf grad: -108.5642562107665\n",
      "iteration: 6550 loss: inf grad: -107.80336461740066\n",
      "iteration: 6760 loss: inf grad: -110.32389169488532\n",
      "iteration: 6520 loss: inf grad: -110.15688918753241\n",
      "iteration: 6610 loss: inf grad: -110.53960119342943\n",
      "iteration: 6680 loss: inf grad: -109.9005453120131\n",
      "iteration: 6600 loss: inf grad: -109.02639013670363\n",
      "iteration: 6560 loss: inf grad: -107.82074838232302\n",
      "iteration: 6730 loss: inf grad: -109.52285078805775\n",
      "iteration: 6540 loss: inf grad: -108.5826154929998\n",
      "iteration: 6770 loss: inf grad: -110.3570914122769\n",
      "iteration: 6530 loss: inf grad: -110.17362257504612\n",
      "iteration: 6620 loss: inf grad: -110.55792362403722\n",
      "iteration: 6690 loss: inf grad: -109.9230607534706\n",
      "iteration: 6610 loss: inf grad: -109.04475618543817\n",
      "iteration: 6570 loss: inf grad: -107.83199512922243\n",
      "iteration: 6740 loss: inf grad: -109.54601439903097\n",
      "iteration: 6780 loss: inf grad: -110.37358051489184\n",
      "iteration: 6550 loss: inf grad: -108.62358013244126\n",
      "iteration: 6540 loss: inf grad: -110.19032415337861\n",
      "iteration: 6700 loss: inf grad: -109.94701322953632\n",
      "iteration: 6630 loss: inf grad: -110.5747186876595\n",
      "iteration: 6620 loss: inf grad: -109.06055548988473\n",
      "iteration: 6580 loss: inf grad: -107.84047558275593\n",
      "iteration: 6560 loss: inf grad: -108.65853529080553\n",
      "iteration: 6790 loss: inf grad: -110.38584746543651\n",
      "iteration: 6750 loss: inf grad: -109.56237789145109\n",
      "iteration: 6550 loss: inf grad: -110.21089778505632\n",
      "iteration: 6640 loss: inf grad: -110.58959400208366\n",
      "iteration: 6710 loss: inf grad: -109.97135543252806\n",
      "iteration: 6630 loss: inf grad: -109.08262413117225\n",
      "iteration: 6590 loss: inf grad: -107.84890470001562\n",
      "iteration: 6570 loss: inf grad: -108.69279307476037\n",
      "iteration: 6760 loss: inf grad: -109.59623941222554\n",
      "iteration: 6800 loss: inf grad: -110.40696445355223\n",
      "iteration: 6650 loss: inf grad: -110.60182322395758\n",
      "iteration: 6560 loss: inf grad: -110.23496555638647\n",
      "iteration: 6640 loss: inf grad: -109.13954490110942\n",
      "iteration: 6720 loss: inf grad: -109.99474293263725\n",
      "iteration: 6580 loss: inf grad: -108.7263418977378\n",
      "iteration: 6600 loss: inf grad: -107.85875381113598\n",
      "iteration: 6770 loss: inf grad: -109.65073671292731\n",
      "iteration: 6810 loss: inf grad: -110.44672114186471\n",
      "iteration: 6660 loss: inf grad: -110.61114160629779\n",
      "iteration: 6570 loss: inf grad: -110.25959875349136\n",
      "iteration: 6730 loss: inf grad: -110.01655214358652\n",
      "iteration: 6590 loss: inf grad: -108.74790155440758\n",
      "iteration: 6780 loss: inf grad: -109.69367231929712\n",
      "iteration: 6650 loss: inf grad: -109.1675652859487\n",
      "iteration: 6610 loss: inf grad: -107.87933769433951\n",
      "iteration: 6820 loss: inf grad: -110.48805896112722\n",
      "iteration: 6670 loss: inf grad: -110.61818879155564\n",
      "iteration: 6740 loss: inf grad: -110.03572820219601\n",
      "iteration: 6580 loss: inf grad: -110.28177890215446\n",
      "iteration: 6600 loss: inf grad: -108.77223732340175\n",
      "iteration: 6660 loss: inf grad: -109.19148316564173\n",
      "iteration: 6790 loss: inf grad: -109.72737980394851\n",
      "iteration: 6830 loss: inf grad: -110.51885023694953\n",
      "iteration: 6620 loss: inf grad: -107.9169311566142\n",
      "iteration: 6680 loss: inf grad: -110.62390982622154\n",
      "iteration: 6610 loss: inf grad: -108.8073390509671\n",
      "iteration: 6590 loss: inf grad: -110.30062500647517\n",
      "iteration: 6670 loss: inf grad: -109.224926994187\n",
      "iteration: 6750 loss: inf grad: -110.05385946169707\n",
      "iteration: 6800 loss: inf grad: -109.76495969000777\n",
      "iteration: 6840 loss: inf grad: -110.53688882079703\n",
      "iteration: 6630 loss: inf grad: -107.94572864942964\n",
      "iteration: 6690 loss: inf grad: -110.62908877010462\n",
      "iteration: 6680 loss: inf grad: -109.25676743058746\n",
      "iteration: 6620 loss: inf grad: -108.84308368593072\n",
      "iteration: 6760 loss: inf grad: -110.07566804629997\n",
      "iteration: 6600 loss: inf grad: -110.32006730545774\n",
      "iteration: 6810 loss: inf grad: -109.78972609493584\n",
      "iteration: 6850 loss: inf grad: -110.54834151976635\n",
      "iteration: 6640 loss: inf grad: -107.98075543804357\n",
      "iteration: 6630 loss: inf grad: -108.86624178207595\n",
      "iteration: 6690 loss: inf grad: -109.29768503578315\n",
      "iteration: 6700 loss: inf grad: -110.63429514882304\n",
      "iteration: 6770 loss: inf grad: -110.10082555874254\n",
      "iteration: 6610 loss: inf grad: -110.34076879983238\n",
      "iteration: 6860 loss: inf grad: -110.55926747150122\n",
      "iteration: 6820 loss: inf grad: -109.80001582843693\n",
      "iteration: 6650 loss: inf grad: -108.00662262659102\n",
      "iteration: 6700 loss: inf grad: -109.3401522498607\n",
      "iteration: 6640 loss: inf grad: -108.88319335702262\n",
      "iteration: 6780 loss: inf grad: -110.12201345586524\n",
      "iteration: 6710 loss: inf grad: -110.64001742792718\n",
      "iteration: 6870 loss: inf grad: -110.57291139901409\n",
      "iteration: 6830 loss: inf grad: -109.82211414830749\n",
      "iteration: 6620 loss: inf grad: -110.3574388513224\n",
      "iteration: 6660 loss: inf grad: -108.02500427584067\n",
      "iteration: 6650 loss: inf grad: -108.90281139328007\n",
      "iteration: 6710 loss: inf grad: -109.36485810970942\n",
      "iteration: 6720 loss: inf grad: -110.646853876096\n",
      "iteration: 6790 loss: inf grad: -110.13863559852047\n",
      "iteration: 6880 loss: inf grad: -110.58641022349231\n",
      "iteration: 6630 loss: inf grad: -110.37026380781252\n",
      "iteration: 6840 loss: inf grad: -109.84846499992841\n",
      "iteration: 6670 loss: inf grad: -108.04966626520826\n",
      "iteration: 6660 loss: inf grad: -108.9265226360007\n",
      "iteration: 6720 loss: inf grad: -109.38014198536794\n",
      "iteration: 6730 loss: inf grad: -110.6556546712666\n",
      "iteration: 6800 loss: inf grad: -110.15378068597425\n",
      "iteration: 6890 loss: inf grad: -110.59623163961419\n",
      "iteration: 6640 loss: inf grad: -110.38153747545087\n",
      "iteration: 6680 loss: inf grad: -108.10353552369762\n",
      "iteration: 6850 loss: inf grad: -109.85754440107424\n",
      "iteration: 6670 loss: inf grad: -108.94669016187513\n",
      "iteration: 6730 loss: inf grad: -109.39079956182987\n",
      "iteration: 6740 loss: inf grad: -110.667179074728\n",
      "iteration: 6810 loss: inf grad: -110.16893597638779\n",
      "iteration: 6900 loss: inf grad: -110.60440821032452\n",
      "iteration: 6650 loss: inf grad: -110.39181828967561\n",
      "iteration: 6680 loss: inf grad: -108.96198123957043\n",
      "iteration: 6690 loss: inf grad: -108.15024230635365\n",
      "iteration: 6860 loss: inf grad: -109.86483984858333\n",
      "iteration: 6750 loss: inf grad: -110.68057767349184\n",
      "iteration: 6910 loss: inf grad: -110.61293965689535\n",
      "iteration: 6740 loss: inf grad: -109.40376264232036\n",
      "iteration: 6820 loss: inf grad: -110.18588185066133\n",
      "iteration: 6690 loss: inf grad: -108.97515440147995\n",
      "iteration: 6700 loss: inf grad: -108.17122713836758\n",
      "iteration: 6660 loss: inf grad: -110.40114331662912\n",
      "iteration: 6870 loss: inf grad: -109.87134858350392\n",
      "iteration: 6920 loss: inf grad: -110.62356066776972\n",
      "iteration: 6750 loss: inf grad: -109.43361532241435\n",
      "iteration: 6830 loss: inf grad: -110.20879835129162\n",
      "iteration: 6700 loss: inf grad: -108.98721055640681\n",
      "iteration: 6760 loss: inf grad: -110.69287491090873\n",
      "iteration: 6710 loss: inf grad: -108.19444318338847\n",
      "iteration: 6880 loss: inf grad: -109.87670976595108\n",
      "iteration: 6670 loss: inf grad: -110.41042842871951\n",
      "iteration: 6760 loss: inf grad: -109.47897537844597\n",
      "iteration: 6930 loss: inf grad: -110.63885034263186\n",
      "iteration: 6840 loss: inf grad: -110.2385970732073\n",
      "iteration: 6710 loss: inf grad: -108.99872141954458\n",
      "iteration: 6770 loss: inf grad: -110.70313394350994\n",
      "iteration: 6720 loss: inf grad: -108.23797150354092\n",
      "iteration: 6890 loss: inf grad: -109.88160088463425\n",
      "iteration: 6680 loss: inf grad: -110.42197748943747\n",
      "iteration: 6940 loss: inf grad: -110.65792062172622\n",
      "iteration: 6770 loss: inf grad: -109.51057545179893\n",
      "iteration: 6850 loss: inf grad: -110.26611548603715\n",
      "iteration: 6720 loss: inf grad: -109.01146698051093\n",
      "iteration: 6780 loss: inf grad: -110.71326651243061\n",
      "iteration: 6690 loss: inf grad: -110.43731951190276iteration: 6730 loss: inf grad: -108.29441892633206\n",
      "\n",
      "iteration: 6950 loss: inf grad: -110.67551433428272\n",
      "iteration: 6780 loss: inf grad: -109.54918053338733\n",
      "iteration: 6860 loss: inf grad: -110.29760859696245\n",
      "iteration: 6900 loss: inf grad: -109.88684532673618\n",
      "iteration: 6730 loss: inf grad: -109.03155496197715\n",
      "iteration: 6790 loss: inf grad: -110.72600292237914\n",
      "iteration: 6740 loss: inf grad: -108.35990509402649\n",
      "iteration: 6960 loss: inf grad: -110.69088008213858\n",
      "iteration: 6700 loss: inf grad: -110.45246827781719\n",
      "iteration: 6870 loss: inf grad: -110.33379697448052\n",
      "iteration: 6910 loss: inf grad: -109.89334755954862\n",
      "iteration: 6790 loss: inf grad: -109.58154242699715\n",
      "iteration: 6800 loss: inf grad: -110.7432659562458\n",
      "iteration: 6740 loss: inf grad: -109.06796215533961\n",
      "iteration: 6970 loss: inf grad: -110.7066380552604\n",
      "iteration: 6880 loss: inf grad: -110.37750958580466\n",
      "iteration: 6710 loss: inf grad: -110.46361669542557\n",
      "iteration: 6750 loss: inf grad: -108.40969461835934\n",
      "iteration: 6800 loss: inf grad: -109.59794616934009\n",
      "iteration: 6750 loss: inf grad: -109.09463069803019\n",
      "iteration: 6810 loss: inf grad: -110.7644064108689\n",
      "iteration: 6920 loss: inf grad: -109.90222180007717\n",
      "iteration: 6980 loss: inf grad: -110.72482729086651\n",
      "iteration: 6890 loss: inf grad: -110.43461520785128\n",
      "iteration: 6720 loss: inf grad: -110.47418264056384\n",
      "iteration: 6760 loss: inf grad: -109.1199379444943\n",
      "iteration: 6810 loss: inf grad: -109.61927757095822\n",
      "iteration: 6820 loss: inf grad: -110.79685448057307\n",
      "iteration: 6760 loss: inf grad: -108.45039662654915\n",
      "iteration: 6930 loss: inf grad: -109.91423856229505\n",
      "iteration: 6990 loss: inf grad: -110.74570541589375\n",
      "iteration: 6900 loss: inf grad: -110.51270721517909\n",
      "iteration: 6730 loss: inf grad: -110.4942824986401iteration: 6770 loss: inf grad: -109.15767333226844\n",
      "\n",
      "iteration: 6820 loss: inf grad: -109.65306808258998\n",
      "iteration: 6830 loss: inf grad: -110.84359103349367\n",
      "iteration: 6770 loss: inf grad: -108.50739367627553\n",
      "iteration: 6940 loss: inf grad: -109.92800686535182\n",
      "iteration: 7000 loss: inf grad: -110.76720529404014\n",
      "iteration: 6910 loss: inf grad: -110.56598020326777\n",
      "iteration: 6780 loss: inf grad: -109.1776924348133\n",
      "iteration: 6740 loss: inf grad: -110.52591685570727\n",
      "iteration: 6840 loss: inf grad: -110.87522374210398\n",
      "iteration: 6830 loss: inf grad: -109.68240522792408\n",
      "iteration: 6780 loss: inf grad: -108.55136781540388\n",
      "iteration: 7010 loss: inf grad: -110.78826426987243\n",
      "iteration: 6950 loss: inf grad: -109.93961605452586\n",
      "iteration: 6920 loss: inf grad: -110.5829313065012\n",
      "iteration: 6790 loss: inf grad: -109.1970858727588\n",
      "iteration: 6750 loss: inf grad: -110.54341133190599\n",
      "iteration: 6850 loss: inf grad: -110.8950133258059\n",
      "iteration: 6840 loss: inf grad: -109.69042379507628\n",
      "iteration: 6790 loss: inf grad: -108.56856471198384\n",
      "iteration: 6960 loss: inf grad: -109.94692679667654\n",
      "iteration: 6930 loss: inf grad: -110.59068742629833\n",
      "iteration: 6800 loss: inf grad: -109.22312751932763\n",
      "iteration: 7020 loss: inf grad: -110.81292925130748\n",
      "iteration: 6860 loss: inf grad: -110.90914115130093\n",
      "iteration: 6760 loss: inf grad: -110.55694682226783\n",
      "iteration: 6800 loss: inf grad: -108.5770365504788\n",
      "iteration: 6850 loss: inf grad: -109.69285586269697\n",
      "iteration: 6940 loss: inf grad: -110.59866917327264\n",
      "iteration: 6970 loss: inf grad: -109.95108225656426\n",
      "iteration: 6810 loss: inf grad: -109.25110349921925\n",
      "iteration: 7030 loss: inf grad: -110.83807986948361\n",
      "iteration: 6870 loss: inf grad: -110.93322042709787\n",
      "iteration: 6770 loss: inf grad: -110.57736489470734\n",
      "iteration: 6810 loss: inf grad: -108.58299403023736\n",
      "iteration: 6950 loss: inf grad: -110.6137888757954\n",
      "iteration: 6980 loss: inf grad: -109.95399788325187\n",
      "iteration: 6860 loss: inf grad: -109.6947214308052\n",
      "iteration: 6820 loss: inf grad: -109.27643928954997\n",
      "iteration: 7040 loss: inf grad: -110.85314458568689\n",
      "iteration: 6880 loss: inf grad: -110.98313971355148\n",
      "iteration: 6780 loss: inf grad: -110.5941849248057\n",
      "iteration: 6820 loss: inf grad: -108.58814920492291\n",
      "iteration: 6960 loss: inf grad: -110.64478442364194\n",
      "iteration: 6990 loss: inf grad: -109.9579226566009\n",
      "iteration: 6870 loss: inf grad: -109.69693610924682\n",
      "iteration: 6830 loss: inf grad: -109.2910766672751\n",
      "iteration: 7050 loss: inf grad: -110.86323882047635\n",
      "iteration: 6890 loss: inf grad: -111.02148420494524\n",
      "iteration: 6790 loss: inf grad: -110.60586632264751\n",
      "iteration: 6830 loss: inf grad: -108.59420350413677\n",
      "iteration: 6970 loss: inf grad: -110.68595210318634\n",
      "iteration: 7000 loss: inf grad: -109.96669420536432\n",
      "iteration: 6840 loss: inf grad: -109.30314146091465\n",
      "iteration: 6880 loss: inf grad: -109.70016565928847\n",
      "iteration: 7060 loss: inf grad: -110.87339349304787\n",
      "iteration: 6900 loss: inf grad: -111.03582215565055\n",
      "iteration: 6800 loss: inf grad: -110.62453424608469\n",
      "iteration: 6840 loss: inf grad: -108.61066652562565\n",
      "iteration: 6980 loss: inf grad: -110.72077451161495\n",
      "iteration: 6850 loss: inf grad: -109.32656149360962\n",
      "iteration: 7010 loss: inf grad: -109.98068782811623\n",
      "iteration: 7070 loss: inf grad: -110.89308929894035\n",
      "iteration: 6890 loss: inf grad: -109.70563714152735\n",
      "iteration: 6910 loss: inf grad: -111.04156500566377\n",
      "iteration: 6810 loss: inf grad: -110.63986739458633\n",
      "iteration: 6850 loss: inf grad: -108.63710679675162\n",
      "iteration: 6860 loss: inf grad: -109.36823835959814\n",
      "iteration: 6990 loss: inf grad: -110.76790291469634\n",
      "iteration: 7020 loss: inf grad: -109.99406398810488\n",
      "iteration: 7080 loss: inf grad: -110.93593139246313\n",
      "iteration: 6900 loss: inf grad: -109.71569009683148\n",
      "iteration: 6920 loss: inf grad: -111.04594964124726\n",
      "iteration: 6820 loss: inf grad: -110.64642889482086\n",
      "iteration: 6860 loss: inf grad: -108.64677489801622\n",
      "iteration: 6870 loss: inf grad: -109.39786687983862\n",
      "iteration: 7000 loss: inf grad: -110.80096393726134\n",
      "iteration: 7090 loss: inf grad: -110.98006416134105\n",
      "iteration: 7030 loss: inf grad: -110.01066950677956\n",
      "iteration: 6910 loss: inf grad: -109.73308922860673\n",
      "iteration: 6930 loss: inf grad: -111.05070191042788\n",
      "iteration: 6870 loss: inf grad: -108.65315693849732\n",
      "iteration: 6830 loss: inf grad: -110.65605298116971\n",
      "iteration: 6880 loss: inf grad: -109.42999455398935\n",
      "iteration: 7010 loss: inf grad: -110.8250868649383\n",
      "iteration: 7100 loss: inf grad: -111.01727527360751\n",
      "iteration: 7040 loss: inf grad: -110.03413951516256\n",
      "iteration: 6920 loss: inf grad: -109.756076690481\n",
      "iteration: 6880 loss: inf grad: -108.66124471596027\n",
      "iteration: 6890 loss: inf grad: -109.45943256273736\n",
      "iteration: 6840 loss: inf grad: -110.6773165354879\n",
      "iteration: 6940 loss: inf grad: -111.05686292921666\n",
      "iteration: 7020 loss: inf grad: -110.85974397277091\n",
      "iteration: 7110 loss: inf grad: -111.03092608759198\n",
      "iteration: 7050 loss: inf grad: -110.06620013381979\n",
      "iteration: 6930 loss: inf grad: -109.77629839199757\n",
      "iteration: 6890 loss: inf grad: -108.6706447280727\n",
      "iteration: 6900 loss: inf grad: -109.48685917376456\n",
      "iteration: 7030 loss: inf grad: -110.89245753192262\n",
      "iteration: 7120 loss: inf grad: -111.03705274058831\n",
      "iteration: 6850 loss: inf grad: -110.69852767888835\n",
      "iteration: 6950 loss: inf grad: -111.06629489387251\n",
      "iteration: 7060 loss: inf grad: -110.09889816605019\n",
      "iteration: 6940 loss: inf grad: -109.79039090349306\n",
      "iteration: 6900 loss: inf grad: -108.67852449170806\n",
      "iteration: 6910 loss: inf grad: -109.5144615629422\n",
      "iteration: 7040 loss: inf grad: -110.91099431676152\n",
      "iteration: 7130 loss: inf grad: -111.04269551719142\n",
      "iteration: 7070 loss: inf grad: -110.13262740462582\n",
      "iteration: 6860 loss: inf grad: -110.70748103442074\n",
      "iteration: 6950 loss: inf grad: -109.80165608624478\n",
      "iteration: 6960 loss: inf grad: -111.08098548259963\n",
      "iteration: 6910 loss: inf grad: -108.68448190520941\n",
      "iteration: 6920 loss: inf grad: -109.53982149556722\n",
      "iteration: 7050 loss: inf grad: -110.9239688157971\n",
      "iteration: 7140 loss: inf grad: -111.04922308224639\n",
      "iteration: 7080 loss: inf grad: -110.16413576779246\n",
      "iteration: 6960 loss: inf grad: -109.81622968467707\n",
      "iteration: 6920 loss: inf grad: -108.6899954999364\n",
      "iteration: 6870 loss: inf grad: -110.71229580389164\n",
      "iteration: 6970 loss: inf grad: -111.09861915205616\n",
      "iteration: 6930 loss: inf grad: -109.55969870763414\n",
      "iteration: 7060 loss: inf grad: -110.93860321436495\n",
      "iteration: 7150 loss: inf grad: -111.05923466373409\n",
      "iteration: 7090 loss: inf grad: -110.19251436752884\n",
      "iteration: 6970 loss: inf grad: -109.8423707756522\n",
      "iteration: 6930 loss: inf grad: -108.6978052782219\n",
      "iteration: 6940 loss: inf grad: -109.5728649541044\n",
      "iteration: 6880 loss: inf grad: -110.72117581779379\n",
      "iteration: 6980 loss: inf grad: -111.11664633880852\n",
      "iteration: 7070 loss: inf grad: -110.95551249701357\n",
      "iteration: 7160 loss: inf grad: -111.0799380209929\n",
      "iteration: 6980 loss: inf grad: -109.88010284501297\n",
      "iteration: 7100 loss: inf grad: -110.23235991217209\n",
      "iteration: 6940 loss: inf grad: -108.7215103350375\n",
      "iteration: 6950 loss: inf grad: -109.59986387000066\n",
      "iteration: 6890 loss: inf grad: -110.74699707742427\n",
      "iteration: 6990 loss: inf grad: -111.14003743448359\n",
      "iteration: 7170 loss: inf grad: -111.10394389398232\n",
      "iteration: 7080 loss: inf grad: -110.98013912999697\n",
      "iteration: 6990 loss: inf grad: -109.91112171114827\n",
      "iteration: 7110 loss: inf grad: -110.27655605260225\n",
      "iteration: 6950 loss: inf grad: -108.75195614017991\n",
      "iteration: 6960 loss: inf grad: -109.64055443167304\n",
      "iteration: 6900 loss: inf grad: -110.80100238202728\n",
      "iteration: 7000 loss: inf grad: -111.16715118336762\n",
      "iteration: 7000 loss: inf grad: -109.93051125538327\n",
      "iteration: 7090 loss: inf grad: -111.00800604790882\n",
      "iteration: 7180 loss: inf grad: -111.11515790313442\n",
      "iteration: 6960 loss: inf grad: -108.77262649251956\n",
      "iteration: 6970 loss: inf grad: -109.65526106791764\n",
      "iteration: 7120 loss: inf grad: -110.31451202086569\n",
      "iteration: 7010 loss: inf grad: -111.18503306859543\n",
      "iteration: 6910 loss: inf grad: -110.85738246655285\n",
      "iteration: 7100 loss: inf grad: -111.01977000406974\n",
      "iteration: 7010 loss: inf grad: -109.95362002704664\n",
      "iteration: 7190 loss: inf grad: -111.12598454764591\n",
      "iteration: 6970 loss: inf grad: -108.79540702747792\n",
      "iteration: 7130 loss: inf grad: -110.3623611703621\n",
      "iteration: 6980 loss: inf grad: -109.66160182935253\n",
      "iteration: 7020 loss: inf grad: -111.19344575572997\n",
      "iteration: 6920 loss: inf grad: -110.8913532473143\n",
      "iteration: 7110 loss: inf grad: -111.02781054615262\n",
      "iteration: 7200 loss: inf grad: -111.14880666150472\n",
      "iteration: 7020 loss: inf grad: -109.9972046583554\n",
      "iteration: 6980 loss: inf grad: -108.8186135150375\n",
      "iteration: 7140 loss: inf grad: -110.40455017544926\n",
      "iteration: 6990 loss: inf grad: -109.66998371370204\n",
      "iteration: 7030 loss: inf grad: -111.19884860089\n",
      "iteration: 6930 loss: inf grad: -110.9097986580087\n",
      "iteration: 7120 loss: inf grad: -111.04022188071525\n",
      "iteration: 7210 loss: inf grad: -111.18357670902938\n",
      "iteration: 7030 loss: inf grad: -110.0424751669508\n",
      "iteration: 7150 loss: inf grad: -110.42653972845625\n",
      "iteration: 6990 loss: inf grad: -108.84331454010731\n",
      "iteration: 7040 loss: inf grad: -111.20487054791164\n",
      "iteration: 6940 loss: inf grad: -110.92307681575319\n",
      "iteration: 7000 loss: inf grad: -109.68838538841962\n",
      "iteration: 7220 loss: inf grad: -111.21795232980375\n",
      "iteration: 7040 loss: inf grad: -110.07431382342077\n",
      "iteration: 7130 loss: inf grad: -111.06530825770662\n",
      "iteration: 7000 loss: inf grad: -108.87182137976231\n",
      "iteration: 7160 loss: inf grad: -110.44047544464357\n",
      "iteration: 7050 loss: inf grad: -111.21246303403797\n",
      "iteration: 6950 loss: inf grad: -110.93526763827349\n",
      "iteration: 7010 loss: inf grad: -109.72066843336742\n",
      "iteration: 7230 loss: inf grad: -111.25149590760745\n",
      "iteration: 7050 loss: inf grad: -110.12785811512262\n",
      "iteration: 7170 loss: inf grad: -110.45408384500575\n",
      "iteration: 7140 loss: inf grad: -111.110274989142\n",
      "iteration: 7010 loss: inf grad: -108.89091910597776\n",
      "iteration: 7060 loss: inf grad: -111.22010607225596\n",
      "iteration: 6960 loss: inf grad: -110.94738323101255\n",
      "iteration: 7020 loss: inf grad: -109.74828381027501\n",
      "iteration: 7060 loss: inf grad: -110.16976800751573\n",
      "iteration: 7240 loss: inf grad: -111.27871293731843\n",
      "iteration: 7020 loss: inf grad: -108.90205411547842\n",
      "iteration: 7180 loss: inf grad: -110.46840843635295\n",
      "iteration: 7150 loss: inf grad: -111.14401696330339iteration: 7070 loss: inf grad: -111.22620455419442\n",
      "\n",
      "iteration: 7030 loss: inf grad: -109.76727969623788\n",
      "iteration: 6970 loss: inf grad: -110.95917922140991\n",
      "iteration: 7070 loss: inf grad: -110.18845984552351\n",
      "iteration: 7160 loss: inf grad: -111.17197203816423\n",
      "iteration: 7250 loss: inf grad: -111.2947366466945\n",
      "iteration: 7030 loss: inf grad: -108.91441321006896\n",
      "iteration: 7080 loss: inf grad: -111.23189921489524\n",
      "iteration: 7190 loss: inf grad: -110.4850150631668\n",
      "iteration: 7040 loss: inf grad: -109.77814264525018\n",
      "iteration: 7090 loss: inf grad: -111.23959817753641\n",
      "iteration: 7040 loss: inf grad: -108.92969600439335\n",
      "iteration: 7080 loss: inf grad: -110.19723310633252\n",
      "iteration: 6980 loss: inf grad: -110.9698185874482\n",
      "iteration: 7200 loss: inf grad: -110.51305887776103\n",
      "iteration: 7170 loss: inf grad: -111.18804477142389\n",
      "iteration: 7260 loss: inf grad: -111.30935999257949\n",
      "iteration: 7050 loss: inf grad: -109.78190077110867\n",
      "iteration: 7100 loss: inf grad: -111.24835014432097\n",
      "iteration: 7050 loss: inf grad: -108.94634993179696\n",
      "iteration: 7090 loss: inf grad: -110.20294949886559\n",
      "iteration: 7210 loss: inf grad: -110.56398286285051\n",
      "iteration: 6990 loss: inf grad: -110.98051177275178\n",
      "iteration: 7180 loss: inf grad: -111.19448895584748iteration: 7270 loss: inf grad: -111.32671638137155\n",
      "\n",
      "iteration: 7060 loss: inf grad: -109.78352576077178\n",
      "iteration: 7110 loss: inf grad: -111.25417037164115\n",
      "iteration: 7060 loss: inf grad: -108.9640026629313\n",
      "iteration: 7100 loss: inf grad: -110.20940362520159\n",
      "iteration: 7220 loss: inf grad: -110.59724286116791\n",
      "iteration: 7000 loss: inf grad: -110.99446425578404\n",
      "iteration: 7280 loss: inf grad: -111.34176872173111\n",
      "iteration: 7190 loss: inf grad: -111.19883175844008\n",
      "iteration: 7070 loss: inf grad: -109.7853303248405\n",
      "iteration: 7070 loss: inf grad: -108.9854214252754\n",
      "iteration: 7120 loss: inf grad: -111.25716634434565\n",
      "iteration: 7110 loss: inf grad: -110.22316859213667\n",
      "iteration: 7010 loss: inf grad: -111.0118072375765\n",
      "iteration: 7230 loss: inf grad: -110.6115490593015\n",
      "iteration: 7290 loss: inf grad: -111.35309224874857\n",
      "iteration: 7200 loss: inf grad: -111.20380899076092\n",
      "iteration: 7080 loss: inf grad: -109.01595339063475\n",
      "iteration: 7080 loss: inf grad: -109.78789731425519\n",
      "iteration: 7130 loss: inf grad: -111.25923578852138\n",
      "iteration: 7120 loss: inf grad: -110.25270259621938\n",
      "iteration: 7020 loss: inf grad: -111.02917289453484\n",
      "iteration: 7240 loss: inf grad: -110.62322609153476\n",
      "iteration: 7300 loss: inf grad: -111.36145676770742\n",
      "iteration: 7210 loss: inf grad: -111.21059738496825\n",
      "iteration: 7090 loss: inf grad: -109.05688086705035\n",
      "iteration: 7090 loss: inf grad: -109.79155898062979\n",
      "iteration: 7250 loss: inf grad: -110.63529042920067\n",
      "iteration: 7030 loss: inf grad: -111.04665489000755\n",
      "iteration: 7130 loss: inf grad: -110.27140533689521iteration: 7140 loss: inf grad: -111.26138223867491\n",
      "\n",
      "iteration: 7310 loss: inf grad: -111.36786839278226\n",
      "iteration: 7220 loss: inf grad: -111.22022797285291\n",
      "iteration: 7100 loss: inf grad: -109.09816725065801\n",
      "iteration: 7100 loss: inf grad: -109.79718177805356\n",
      "iteration: 7260 loss: inf grad: -110.6475953727136\n",
      "iteration: 7040 loss: inf grad: -111.0628487342943\n",
      "iteration: 7150 loss: inf grad: -111.26412679326516\n",
      "iteration: 7140 loss: inf grad: -110.28149759791717\n",
      "iteration: 7230 loss: inf grad: -111.23426032788379\n",
      "iteration: 7110 loss: inf grad: -109.13051561965636\n",
      "iteration: 7320 loss: inf grad: -111.37334622463062\n",
      "iteration: 7110 loss: inf grad: -109.80722236788799\n",
      "iteration: 7270 loss: inf grad: -110.66253582900296\n",
      "iteration: 7050 loss: inf grad: -111.07579910415251\n",
      "iteration: 7160 loss: inf grad: -111.26792454677897\n",
      "iteration: 7240 loss: inf grad: -111.25543497833996\n",
      "iteration: 7120 loss: inf grad: -109.15409099786986\n",
      "iteration: 7150 loss: inf grad: -110.2907918952888\n",
      "iteration: 7120 loss: inf grad: -109.82576776619854\n",
      "iteration: 7330 loss: inf grad: -111.380189524158\n",
      "iteration: 7280 loss: inf grad: -110.68412652737845\n",
      "iteration: 7250 loss: inf grad: -111.28622699571129\n",
      "iteration: 7130 loss: inf grad: -109.17583891347529\n",
      "iteration: 7170 loss: inf grad: -111.27308631145587\n",
      "iteration: 7160 loss: inf grad: -110.30026141235874\n",
      "iteration: 7060 loss: inf grad: -111.08722897768286\n",
      "iteration: 7340 loss: inf grad: -111.39984278239022\n",
      "iteration: 7130 loss: inf grad: -109.8516941261239\n",
      "iteration: 7290 loss: inf grad: -110.71154503189356\n",
      "iteration: 7260 loss: inf grad: -111.32671894149493\n",
      "iteration: 7140 loss: inf grad: -109.2007651080303\n",
      "iteration: 7170 loss: inf grad: -110.31016534185667\n",
      "iteration: 7140 loss: inf grad: -109.88237690241215\n",
      "iteration: 7350 loss: inf grad: -111.42454809756933\n",
      "iteration: 7180 loss: inf grad: -111.27925209837225\n",
      "iteration: 7070 loss: inf grad: -111.09945083129321\n",
      "iteration: 7300 loss: inf grad: -110.74056335585927\n",
      "iteration: 7270 loss: inf grad: -111.3748404598862\n",
      "iteration: 7150 loss: inf grad: -109.23298933718857\n",
      "iteration: 7150 loss: inf grad: -109.91663121364891\n",
      "iteration: 7180 loss: inf grad: -110.32075977044877\n",
      "iteration: 7360 loss: inf grad: -111.43351168911691\n",
      "iteration: 7190 loss: inf grad: -111.2853109316596\n",
      "iteration: 7310 loss: inf grad: -110.77538525540513\n",
      "iteration: 7080 loss: inf grad: -111.11651802660035\n",
      "iteration: 7160 loss: inf grad: -109.25878742852109\n",
      "iteration: 7280 loss: inf grad: -111.41570928211272\n",
      "iteration: 7160 loss: inf grad: -109.94919579606531\n",
      "iteration: 7370 loss: inf grad: -111.44085300057085\n",
      "iteration: 7200 loss: inf grad: -111.29071901673316\n",
      "iteration: 7320 loss: inf grad: -110.80592452055618\n",
      "iteration: 7190 loss: inf grad: -110.33216415569198\n",
      "iteration: 7090 loss: inf grad: -111.13098542716313\n",
      "iteration: 7170 loss: inf grad: -109.27154677902993\n",
      "iteration: 7290 loss: inf grad: -111.43766418938299\n",
      "iteration: 7170 loss: inf grad: -109.98156709741517\n",
      "iteration: 7380 loss: inf grad: -111.45024577024671\n",
      "iteration: 7210 loss: inf grad: -111.29621400747942\n",
      "iteration: 7330 loss: inf grad: -110.8180037288353\n",
      "iteration: 7200 loss: inf grad: -110.34473299126418\n",
      "iteration: 7180 loss: inf grad: -109.27977319757474\n",
      "iteration: 7300 loss: inf grad: -111.45015961802541\n",
      "iteration: 7100 loss: inf grad: -111.1379979781002\n",
      "iteration: 7180 loss: inf grad: -110.00102912955845\n",
      "iteration: 7390 loss: inf grad: -111.46232597854853\n",
      "iteration: 7220 loss: inf grad: -111.30350906826439\n",
      "iteration: 7340 loss: inf grad: -110.82578408714346\n",
      "iteration: 7210 loss: inf grad: -110.36061771848777\n",
      "iteration: 7190 loss: inf grad: -109.28698019554142\n",
      "iteration: 7310 loss: inf grad: -111.45969815127924\n",
      "iteration: 7110 loss: inf grad: -111.14322373514541\n",
      "iteration: 7190 loss: inf grad: -110.01150366521185\n",
      "iteration: 7400 loss: inf grad: -111.4762010476547\n",
      "iteration: 7230 loss: inf grad: -111.31746744815308\n",
      "iteration: 7350 loss: inf grad: -110.84177195377376\n",
      "iteration: 7200 loss: inf grad: -109.29436937911433\n",
      "iteration: 7220 loss: inf grad: -110.38138191333377\n",
      "iteration: 7320 loss: inf grad: -111.46838549977778\n",
      "iteration: 7200 loss: inf grad: -110.02037718270724\n",
      "iteration: 7120 loss: inf grad: -111.15065896549638\n",
      "iteration: 7410 loss: inf grad: -111.48953844819587\n",
      "iteration: 7240 loss: inf grad: -111.3360880182221\n",
      "iteration: 7210 loss: inf grad: -109.30320829526413\n",
      "iteration: 7230 loss: inf grad: -110.40114800190636\n",
      "iteration: 7360 loss: inf grad: -110.85733975987401\n",
      "iteration: 7210 loss: inf grad: -110.03463655093329\n",
      "iteration: 7330 loss: inf grad: -111.47688933204248\n",
      "iteration: 7130 loss: inf grad: -111.17195942527775\n",
      "iteration: 7420 loss: inf grad: -111.50052694459703\n",
      "iteration: 7250 loss: inf grad: -111.3442843612618\n",
      "iteration: 7220 loss: inf grad: -109.31558312889592\n",
      "iteration: 7240 loss: inf grad: -110.4255997624411\n",
      "iteration: 7220 loss: inf grad: -110.0620888559843\n",
      "iteration: 7370 loss: inf grad: -110.86271204022299\n",
      "iteration: 7340 loss: inf grad: -111.48571010472132\n",
      "iteration: 7140 loss: inf grad: -111.2148187911681\n",
      "iteration: 7430 loss: inf grad: -111.50912354007727\n",
      "iteration: 7230 loss: inf grad: -109.33466416958376\n",
      "iteration: 7260 loss: inf grad: -111.34740143402107\n",
      "iteration: 7230 loss: inf grad: -110.10235862024027\n",
      "iteration: 7350 loss: inf grad: -111.4957423421116\n",
      "iteration: 7380 loss: inf grad: -110.864916029181\n",
      "iteration: 7250 loss: inf grad: -110.46157336046167\n",
      "iteration: 7150 loss: inf grad: -111.23956742452941\n",
      "iteration: 7440 loss: inf grad: -111.51623462749689\n",
      "iteration: 7240 loss: inf grad: -109.36416155734724\n",
      "iteration: 7270 loss: inf grad: -111.35011692475427\n",
      "iteration: 7240 loss: inf grad: -110.15023653935525\n",
      "iteration: 7390 loss: inf grad: -110.86689456716277\n",
      "iteration: 7260 loss: inf grad: -110.50934651017214\n",
      "iteration: 7360 loss: inf grad: -111.50891397008279\n",
      "iteration: 7160 loss: inf grad: -111.2564756158829\n",
      "iteration: 7450 loss: inf grad: -111.52321453785925\n",
      "iteration: 7250 loss: inf grad: -109.40273203258234\n",
      "iteration: 7280 loss: inf grad: -111.35368901460693\n",
      "iteration: 7250 loss: inf grad: -110.16846232437284\n",
      "iteration: 7270 loss: inf grad: -110.55877867728005\n",
      "iteration: 7370 loss: inf grad: -111.52733394924557\n",
      "iteration: 7170 loss: inf grad: -111.2746069660688\n",
      "iteration: 7400 loss: inf grad: -110.86855531341418\n",
      "iteration: 7460 loss: inf grad: -111.53267981539064\n",
      "iteration: 7260 loss: inf grad: -109.44941063448253\n",
      "iteration: 7290 loss: inf grad: -111.35901840917421\n",
      "iteration: 7260 loss: inf grad: -110.17893165243314\n",
      "iteration: 7410 loss: inf grad: -110.87008260664221\n",
      "iteration: 7380 loss: inf grad: -111.54611437273245\n",
      "iteration: 7180 loss: inf grad: -111.29403150025463\n",
      "iteration: 7280 loss: inf grad: -110.58643033747116\n",
      "iteration: 7470 loss: inf grad: -111.548790530096\n",
      "iteration: 7270 loss: inf grad: -109.50501173447671\n",
      "iteration: 7300 loss: inf grad: -111.36741308502911\n",
      "iteration: 7270 loss: inf grad: -110.1888025528832\n",
      "iteration: 7420 loss: inf grad: -110.87204510564753\n",
      "iteration: 7390 loss: inf grad: -111.56055737727831\n",
      "iteration: 7190 loss: inf grad: -111.32290667940082\n",
      "iteration: 7290 loss: inf grad: -110.61475006402091\n",
      "iteration: 7480 loss: inf grad: -111.57274996267886\n",
      "iteration: 7310 loss: inf grad: -111.38075097058544\n",
      "iteration: 7280 loss: inf grad: -109.53312721638635\n",
      "iteration: 7280 loss: inf grad: -110.20050015922037\n",
      "iteration: 7200 loss: inf grad: -111.35807570127002\n",
      "iteration: 7430 loss: inf grad: -110.8756439930946\n",
      "iteration: 7400 loss: inf grad: -111.57739306888608\n",
      "iteration: 7300 loss: inf grad: -110.64553127132372\n",
      "iteration: 7490 loss: inf grad: -111.60469854420117\n",
      "iteration: 7320 loss: inf grad: -111.40086987989318\n",
      "iteration: 7290 loss: inf grad: -109.55230024168479\n",
      "iteration: 7290 loss: inf grad: -110.22139047218742\n",
      "iteration: 7440 loss: inf grad: -110.8862008463053\n",
      "iteration: 7210 loss: inf grad: -111.37567982735715\n",
      "iteration: 7500 loss: inf grad: -111.64674360443915\n",
      "iteration: 7310 loss: inf grad: -110.67719035258754\n",
      "iteration: 7410 loss: inf grad: -111.60568257060176\n",
      "iteration: 7330 loss: inf grad: -111.42749338188113\n",
      "iteration: 7300 loss: inf grad: -109.57626674378665\n",
      "iteration: 7450 loss: inf grad: -110.91422092285532\n",
      "iteration: 7300 loss: inf grad: -110.26080942310497\n",
      "iteration: 7220 loss: inf grad: -111.3873914057082\n",
      "iteration: 7510 loss: inf grad: -111.69167032848192\n",
      "iteration: 7320 loss: inf grad: -110.71407918483166\n",
      "iteration: 7340 loss: inf grad: -111.45604488099377\n",
      "iteration: 7420 loss: inf grad: -111.63664871627527\n",
      "iteration: 7310 loss: inf grad: -109.6086090879958\n",
      "iteration: 7460 loss: inf grad: -110.9337427947368\n",
      "iteration: 7310 loss: inf grad: -110.29742997386963\n",
      "iteration: 7230 loss: inf grad: -111.39859326628914\n",
      "iteration: 7520 loss: inf grad: -111.72386892527675\n",
      "iteration: 7330 loss: inf grad: -110.748965573633\n",
      "iteration: 7350 loss: inf grad: -111.47874883985835\n",
      "iteration: 7320 loss: inf grad: -109.63778838415865\n",
      "iteration: 7430 loss: inf grad: -111.6565629223681\n",
      "iteration: 7470 loss: inf grad: -110.9387098561282\n",
      "iteration: 7320 loss: inf grad: -110.32190445702821iteration: 7340 loss: inf grad: -110.76982483808706\n",
      "\n",
      "iteration: 7360 loss: inf grad: -111.49182417244486\n",
      "iteration: 7530 loss: inf grad: -111.74448707948424\n",
      "iteration: 7240 loss: inf grad: -111.41730364771084\n",
      "iteration: 7440 loss: inf grad: -111.66994877276491\n",
      "iteration: 7330 loss: inf grad: -109.65985567785116\n",
      "iteration: 7480 loss: inf grad: -110.94002925342119\n",
      "iteration: 7370 loss: inf grad: -111.49833524198357\n",
      "iteration: 7540 loss: inf grad: -111.75885299449135\n",
      "iteration: 7330 loss: inf grad: -110.3510799701842\n",
      "iteration: 7350 loss: inf grad: -110.7844937421759\n",
      "iteration: 7250 loss: inf grad: -111.44961668537402\n",
      "iteration: 7490 loss: inf grad: -110.94032787790617\n",
      "iteration: 7340 loss: inf grad: -109.68256301511829\n",
      "iteration: 7450 loss: inf grad: -111.68269623868522\n",
      "iteration: 7380 loss: inf grad: -111.50168988119924\n",
      "iteration: 7360 loss: inf grad: -110.7968887302557\n",
      "iteration: 7340 loss: inf grad: -110.38133094658957\n",
      "iteration: 7550 loss: inf grad: -111.76974780389868\n",
      "iteration: 7260 loss: inf grad: -111.47192049195175\n",
      "iteration: 7500 loss: inf grad: -110.94068635147649\n",
      "iteration: 7460 loss: inf grad: -111.7037863309287\n",
      "iteration: 7350 loss: inf grad: -109.70839443466096\n",
      "iteration: 7390 loss: inf grad: -111.5035619617505\n",
      "iteration: 7370 loss: inf grad: -110.81282449021771\n",
      "iteration: 7350 loss: inf grad: -110.4014393267834\n",
      "iteration: 7270 loss: inf grad: -111.49523875566116\n",
      "iteration: 7510 loss: inf grad: -110.942231690487\n",
      "iteration: 7470 loss: inf grad: -111.7297480729689\n",
      "iteration: 7560 loss: inf grad: -111.77777126185609\n",
      "iteration: 7400 loss: inf grad: -111.5046923177475\n",
      "iteration: 7360 loss: inf grad: -109.7281580722699\n",
      "iteration: 7380 loss: inf grad: -110.84387619505875\n",
      "iteration: 7520 loss: inf grad: -110.94761419046665\n",
      "iteration: 7360 loss: inf grad: -110.41301053814212\n",
      "iteration: 7280 loss: inf grad: -111.52040414374468\n",
      "iteration: 7480 loss: inf grad: -111.75438080712038\n",
      "iteration: 7570 loss: inf grad: -111.78333265121336\n",
      "iteration: 7410 loss: inf grad: -111.50550862416152\n",
      "iteration: 7370 loss: inf grad: -109.74128607713118\n",
      "iteration: 7390 loss: inf grad: -110.88159111526684\n",
      "iteration: 7530 loss: inf grad: -110.96713828276904\n",
      "iteration: 7290 loss: inf grad: -111.54157678141168\n",
      "iteration: 7490 loss: inf grad: -111.77852496743878\n",
      "iteration: 7370 loss: inf grad: -110.42131057713287\n",
      "iteration: 7420 loss: inf grad: -111.50659869081126\n",
      "iteration: 7580 loss: inf grad: -111.78716601408888\n",
      "iteration: 7400 loss: inf grad: -110.89909706619376\n",
      "iteration: 7540 loss: inf grad: -110.99191342152486\n",
      "iteration: 7380 loss: inf grad: -109.75219424281676\n",
      "iteration: 7500 loss: inf grad: -111.7949367988101\n",
      "iteration: 7300 loss: inf grad: -111.57014140041764\n",
      "iteration: 7380 loss: inf grad: -110.42988014870096\n",
      "iteration: 7430 loss: inf grad: -111.50955656991161\n",
      "iteration: 7590 loss: inf grad: -111.79004522414955\n",
      "iteration: 7550 loss: inf grad: -111.00331256252419\n",
      "iteration: 7390 loss: inf grad: -109.76326880777127\n",
      "iteration: 7410 loss: inf grad: -110.90772926625755\n",
      "iteration: 7510 loss: inf grad: -111.80394270082424\n",
      "iteration: 7310 loss: inf grad: -111.6107509806489iteration: 7390 loss: inf grad: -110.44049887609819\n",
      "\n",
      "iteration: 7440 loss: inf grad: -111.51779161968082\n",
      "iteration: 7600 loss: inf grad: -111.79263675883803iteration: 7560 loss: inf grad: -111.01599391708876\n",
      "\n",
      "iteration: 7400 loss: inf grad: -109.77480066017293\n",
      "iteration: 7420 loss: inf grad: -110.91545692258303\n",
      "iteration: 7520 loss: inf grad: -111.81008509528252\n",
      "iteration: 7400 loss: inf grad: -110.45197859808147\n",
      "iteration: 7450 loss: inf grad: -111.53170321637334\n",
      "iteration: 7570 loss: inf grad: -111.03546911275433\n",
      "iteration: 7320 loss: inf grad: -111.64119053381938\n",
      "iteration: 7610 loss: inf grad: -111.79555214742891\n",
      "iteration: 7410 loss: inf grad: -109.78558922417804\n",
      "iteration: 7430 loss: inf grad: -110.9265676382582\n",
      "iteration: 7530 loss: inf grad: -111.81755595896087iteration: 7460 loss: inf grad: -111.54592570155796\n",
      "\n",
      "iteration: 7410 loss: inf grad: -110.46287661666683\n",
      "iteration: 7580 loss: inf grad: -111.05248557350254\n",
      "iteration: 7330 loss: inf grad: -111.66767828697468\n",
      "iteration: 7420 loss: inf grad: -109.79536055532773\n",
      "iteration: 7620 loss: inf grad: -111.79952382130926\n",
      "iteration: 7440 loss: inf grad: -110.95254619563732\n",
      "iteration: 7470 loss: inf grad: -111.5674048337803\n",
      "iteration: 7540 loss: inf grad: -111.83263103869392\n",
      "iteration: 7590 loss: inf grad: -111.06146967579076\n",
      "iteration: 7420 loss: inf grad: -110.47294463153038\n",
      "iteration: 7340 loss: inf grad: -111.69944557873882\n",
      "iteration: 7430 loss: inf grad: -109.80527221940608\n",
      "iteration: 7630 loss: inf grad: -111.80588999920451\n",
      "iteration: 7450 loss: inf grad: -110.98366185803377\n",
      "iteration: 7480 loss: inf grad: -111.59029353604029\n",
      "iteration: 7600 loss: inf grad: -111.06728458860873\n",
      "iteration: 7430 loss: inf grad: -110.48085062993904\n",
      "iteration: 7350 loss: inf grad: -111.73293577583037\n",
      "iteration: 7550 loss: inf grad: -111.86046455018433\n",
      "iteration: 7440 loss: inf grad: -109.8169621602504\n",
      "iteration: 7640 loss: inf grad: -111.81841350145517\n",
      "iteration: 7460 loss: inf grad: -111.0038723355282\n",
      "iteration: 7490 loss: inf grad: -111.60083722026934\n",
      "iteration: 7610 loss: inf grad: -111.07357320226384\n",
      "iteration: 7560 loss: inf grad: -111.8866369979407\n",
      "iteration: 7440 loss: inf grad: -110.48664071584855\n",
      "iteration: 7360 loss: inf grad: -111.75609760989478\n",
      "iteration: 7450 loss: inf grad: -109.82946246295657\n",
      "iteration: 7650 loss: inf grad: -111.84625876548547\n",
      "iteration: 7470 loss: inf grad: -111.01538362402869\n",
      "iteration: 7500 loss: inf grad: -111.60796873722722\n",
      "iteration: 7620 loss: inf grad: -111.08397193002479\n",
      "iteration: 7450 loss: inf grad: -110.49219055277459\n",
      "iteration: 7370 loss: inf grad: -111.77421771388835\n",
      "iteration: 7570 loss: inf grad: -111.90736592743362\n",
      "iteration: 7660 loss: inf grad: -111.88537116486613\n",
      "iteration: 7460 loss: inf grad: -109.83903752140915\n",
      "iteration: 7510 loss: inf grad: -111.61622705578353\n",
      "iteration: 7480 loss: inf grad: -111.0226049117469\n",
      "iteration: 7630 loss: inf grad: -111.10151034764777\n",
      "iteration: 7460 loss: inf grad: -110.49970269392699\n",
      "iteration: 7380 loss: inf grad: -111.79216700960258\n",
      "iteration: 7580 loss: inf grad: -111.9430568027355\n",
      "iteration: 7670 loss: inf grad: -111.91853181468477\n",
      "iteration: 7520 loss: inf grad: -111.62480872710178\n",
      "iteration: 7490 loss: inf grad: -111.02957493553575\n",
      "iteration: 7470 loss: inf grad: -109.84515836535758\n",
      "iteration: 7470 loss: inf grad: -110.51087047160559\n",
      "iteration: 7640 loss: inf grad: -111.12001065873011\n",
      "iteration: 7390 loss: inf grad: -111.804561363328\n",
      "iteration: 7680 loss: inf grad: -111.94521889486477\n",
      "iteration: 7590 loss: inf grad: -112.00173766080454\n",
      "iteration: 7500 loss: inf grad: -111.03772287734014\n",
      "iteration: 7530 loss: inf grad: -111.63284657224577\n",
      "iteration: 7480 loss: inf grad: -109.84916866744317\n",
      "iteration: 7480 loss: inf grad: -110.52474328368228\n",
      "iteration: 7400 loss: inf grad: -111.81922056088585\n",
      "iteration: 7690 loss: inf grad: -111.95858195467872\n",
      "iteration: 7600 loss: inf grad: -112.04366081551632\n",
      "iteration: 7650 loss: inf grad: -111.1353742014202\n",
      "iteration: 7510 loss: inf grad: -111.0481403725189\n",
      "iteration: 7490 loss: inf grad: -109.85160481098629\n",
      "iteration: 7540 loss: inf grad: -111.64135922924136\n",
      "iteration: 7490 loss: inf grad: -110.53722650675425\n",
      "iteration: 7410 loss: inf grad: -111.85435878196573\n",
      "iteration: 7700 loss: inf grad: -111.96398831153714\n",
      "iteration: 7610 loss: inf grad: -112.0647402233816\n",
      "iteration: 7660 loss: inf grad: -111.15545289020345\n",
      "iteration: 7520 loss: inf grad: -111.06199055259975\n",
      "iteration: 7500 loss: inf grad: -110.54729821926801\n",
      "iteration: 7550 loss: inf grad: -111.65557693614892\n",
      "iteration: 7500 loss: inf grad: -109.8526835098917\n",
      "iteration: 7420 loss: inf grad: -111.8762055348252\n",
      "iteration: 7710 loss: inf grad: -111.96738652356198\n",
      "iteration: 7620 loss: inf grad: -112.08326829044651\n",
      "iteration: 7530 loss: inf grad: -111.08722278835452\n",
      "iteration: 7670 loss: inf grad: -111.18151422736882\n",
      "iteration: 7510 loss: inf grad: -110.55946176209392\n",
      "iteration: 7510 loss: inf grad: -109.85287634079413\n",
      "iteration: 7430 loss: inf grad: -111.88107856874804\n",
      "iteration: 7720 loss: inf grad: -111.9711638105783\n",
      "iteration: 7630 loss: inf grad: -112.10571663918621\n",
      "iteration: 7560 loss: inf grad: -111.69122846684985\n",
      "iteration: 7540 loss: inf grad: -111.13089881169796\n",
      "iteration: 7520 loss: inf grad: -110.57768826882227\n",
      "iteration: 7680 loss: inf grad: -111.20453740359122\n",
      "iteration: 7520 loss: inf grad: -109.85274970963403\n",
      "iteration: 7440 loss: inf grad: -111.88366745851067\n",
      "iteration: 7730 loss: inf grad: -111.97687728737225\n",
      "iteration: 7640 loss: inf grad: -112.12521581802586\n",
      "iteration: 7550 loss: inf grad: -111.15560990378339\n",
      "iteration: 7570 loss: inf grad: -111.72999031761651\n",
      "iteration: 7530 loss: inf grad: -110.59603295135767\n",
      "iteration: 7690 loss: inf grad: -111.222101526288\n",
      "iteration: 7450 loss: inf grad: -111.88672069724441\n",
      "iteration: 7740 loss: inf grad: -111.98617701716705\n",
      "iteration: 7650 loss: inf grad: -112.1411504596162\n",
      "iteration: 7530 loss: inf grad: -109.85262809347198\n",
      "iteration: 7560 loss: inf grad: -111.17269089695112\n",
      "iteration: 7580 loss: inf grad: -111.76136093237056\n",
      "iteration: 7460 loss: inf grad: -111.89099259272477\n",
      "iteration: 7700 loss: inf grad: -111.23632591055029\n",
      "iteration: 7750 loss: inf grad: -112.00029757911742\n",
      "iteration: 7660 loss: inf grad: -112.16173603789932\n",
      "iteration: 7540 loss: inf grad: -110.60925784063917\n",
      "iteration: 7570 loss: inf grad: -111.19087633720791\n",
      "iteration: 7590 loss: inf grad: -111.7936210532084\n",
      "iteration: 7540 loss: inf grad: -109.85259358630913\n",
      "iteration: 7470 loss: inf grad: -111.89739169036275\n",
      "iteration: 7710 loss: inf grad: -111.25107117611878\n",
      "iteration: 7760 loss: inf grad: -112.01867603401695\n",
      "iteration: 7670 loss: inf grad: -112.19262604895117\n",
      "iteration: 7580 loss: inf grad: -111.20582353431611\n",
      "iteration: 7550 loss: inf grad: -110.61858258755339\n",
      "iteration: 7600 loss: inf grad: -111.8243664193561\n",
      "iteration: 7480 loss: inf grad: -111.90722755273032\n",
      "iteration: 7550 loss: inf grad: -109.8527335553208\n",
      "iteration: 7680 loss: inf grad: -112.23119301287932\n",
      "iteration: 7720 loss: inf grad: -111.2714340913536\n",
      "iteration: 7770 loss: inf grad: -112.03865005126033\n",
      "iteration: 7590 loss: inf grad: -111.21726778870325\n",
      "iteration: 7610 loss: inf grad: -111.85082511937512\n",
      "iteration: 7560 loss: inf grad: -110.62687332229791\n",
      "iteration: 7490 loss: inf grad: -111.92092058590796\n",
      "iteration: 7560 loss: inf grad: -109.85398710076532\n",
      "iteration: 7690 loss: inf grad: -112.28588313599202\n",
      "iteration: 7730 loss: inf grad: -111.29952053224406\n",
      "iteration: 7780 loss: inf grad: -112.06068650488501\n",
      "iteration: 7600 loss: inf grad: -111.2277389342238\n",
      "iteration: 7620 loss: inf grad: -111.87307103728207\n",
      "iteration: 7570 loss: inf grad: -110.63537240064767\n",
      "iteration: 7500 loss: inf grad: -111.93573814455438\n",
      "iteration: 7700 loss: inf grad: -112.36366732163556\n",
      "iteration: 7570 loss: inf grad: -109.86331398302416\n",
      "iteration: 7610 loss: inf grad: -111.23823466000529\n",
      "iteration: 7740 loss: inf grad: -111.3330669741258\n",
      "iteration: 7790 loss: inf grad: -112.09680253716826\n",
      "iteration: 7630 loss: inf grad: -111.89081511281586\n",
      "iteration: 7510 loss: inf grad: -111.94950544526768\n",
      "iteration: 7580 loss: inf grad: -110.64432041728662\n",
      "iteration: 7620 loss: inf grad: -111.24841264551975\n",
      "iteration: 7750 loss: inf grad: -111.37680538762825\n",
      "iteration: 7580 loss: inf grad: -109.89541954878226iteration: 7710 loss: inf grad: -112.41471831989894\n",
      "\n",
      "iteration: 7800 loss: inf grad: -112.14110717681993\n",
      "iteration: 7640 loss: inf grad: -111.90444548678455\n",
      "iteration: 7520 loss: inf grad: -111.964762362535\n",
      "iteration: 7630 loss: inf grad: -111.2575704054394\n",
      "iteration: 7590 loss: inf grad: -110.65324361771096\n",
      "iteration: 7720 loss: inf grad: -112.43795684354275\n",
      "iteration: 7760 loss: inf grad: -111.40706762196196\n",
      "iteration: 7590 loss: inf grad: -109.91542790805568\n",
      "iteration: 7650 loss: inf grad: -111.91530020797389\n",
      "iteration: 7810 loss: inf grad: -112.16619312056527\n",
      "iteration: 7530 loss: inf grad: -111.98469696605662\n",
      "iteration: 7730 loss: inf grad: -112.45837669316032\n",
      "iteration: 7640 loss: inf grad: -111.26543508080547\n",
      "iteration: 7600 loss: inf grad: -109.91946624003171\n",
      "iteration: 7600 loss: inf grad: -110.66073245720705\n",
      "iteration: 7770 loss: inf grad: -111.43493537399573\n",
      "iteration: 7820 loss: inf grad: -112.18104494899322\n",
      "iteration: 7660 loss: inf grad: -111.92456742770815\n",
      "iteration: 7540 loss: inf grad: -112.00691689352871\n",
      "iteration: 7740 loss: inf grad: -112.49011026916924\n",
      "iteration: 7650 loss: inf grad: -111.27238390132297\n",
      "iteration: 7610 loss: inf grad: -109.92377317263276\n",
      "iteration: 7610 loss: inf grad: -110.66706918393405\n",
      "iteration: 7780 loss: inf grad: -111.46356301254835\n",
      "iteration: 7830 loss: inf grad: -112.19726105456999\n",
      "iteration: 7550 loss: inf grad: -112.02668202212385\n",
      "iteration: 7670 loss: inf grad: -111.9330155664938\n",
      "iteration: 7750 loss: inf grad: -112.53080371639042\n",
      "iteration: 7620 loss: inf grad: -109.93552926227676\n",
      "iteration: 7660 loss: inf grad: -111.27918222377936\n",
      "iteration: 7620 loss: inf grad: -110.67650591788643\n",
      "iteration: 7840 loss: inf grad: -112.21589372442077\n",
      "iteration: 7560 loss: inf grad: -112.04379015990114\n",
      "iteration: 7790 loss: inf grad: -111.48901728921345\n",
      "iteration: 7760 loss: inf grad: -112.55881457883991\n",
      "iteration: 7680 loss: inf grad: -111.94103576130016\n",
      "iteration: 7630 loss: inf grad: -109.95346092348427\n",
      "iteration: 7670 loss: inf grad: -111.28667655910407\n",
      "iteration: 7630 loss: inf grad: -110.70110959180187\n",
      "iteration: 7570 loss: inf grad: -112.05954869229764\n",
      "iteration: 7800 loss: inf grad: -111.51549584235468\n",
      "iteration: 7850 loss: inf grad: -112.23032527908501\n",
      "iteration: 7770 loss: inf grad: -112.57606413750119\n",
      "iteration: 7640 loss: inf grad: -109.96278430990054iteration: 7690 loss: inf grad: -111.94870536299422\n",
      "\n",
      "iteration: 7680 loss: inf grad: -111.29556240607667\n",
      "iteration: 7640 loss: inf grad: -110.7236227375325\n",
      "iteration: 7860 loss: inf grad: -112.24365133298019\n",
      "iteration: 7810 loss: inf grad: -111.54556104741695\n",
      "iteration: 7580 loss: inf grad: -112.0750332052273\n",
      "iteration: 7780 loss: inf grad: -112.58871319865688\n",
      "iteration: 7650 loss: inf grad: -109.96568215268493\n",
      "iteration: 7700 loss: inf grad: -111.95591685104256\n",
      "iteration: 7690 loss: inf grad: -111.30610718739365\n",
      "iteration: 7650 loss: inf grad: -110.73103765320838\n",
      "iteration: 7820 loss: inf grad: -111.57670378310843\n",
      "iteration: 7590 loss: inf grad: -112.09215983303193\n",
      "iteration: 7870 loss: inf grad: -112.26223836700885\n",
      "iteration: 7790 loss: inf grad: -112.59963554471429\n",
      "iteration: 7660 loss: inf grad: -109.96813622307442\n",
      "iteration: 7710 loss: inf grad: -111.96257935500316\n",
      "iteration: 7700 loss: inf grad: -111.31802258189728\n",
      "iteration: 7660 loss: inf grad: -110.73530972653631\n",
      "iteration: 7830 loss: inf grad: -111.60417226791321\n",
      "iteration: 7600 loss: inf grad: -112.11445315849635\n",
      "iteration: 7880 loss: inf grad: -112.28867091918154\n",
      "iteration: 7800 loss: inf grad: -112.61136821586105\n",
      "iteration: 7670 loss: inf grad: -109.97306730578035\n",
      "iteration: 7720 loss: inf grad: -111.96893193943853\n",
      "iteration: 7710 loss: inf grad: -111.33123298205032\n",
      "iteration: 7670 loss: inf grad: -110.73925433973463\n",
      "iteration: 7840 loss: inf grad: -111.6261951565416\n",
      "iteration: 7610 loss: inf grad: -112.14714547750762\n",
      "iteration: 7890 loss: inf grad: -112.31644221910983\n",
      "iteration: 7810 loss: inf grad: -112.62693608710492\n",
      "iteration: 7680 loss: inf grad: -109.98059868551516\n",
      "iteration: 7730 loss: inf grad: -111.97641110487547\n",
      "iteration: 7720 loss: inf grad: -111.34798608415761iteration: 7850 loss: inf grad: -111.64487990407838\n",
      "\n",
      "iteration: 7680 loss: inf grad: -110.7455335148706\n",
      "iteration: 7620 loss: inf grad: -112.19884685694906\n",
      "iteration: 7820 loss: inf grad: -112.64913627696754\n",
      "iteration: 7900 loss: inf grad: -112.33194425804899\n",
      "iteration: 7690 loss: inf grad: -109.9872008717455\n",
      "iteration: 7740 loss: inf grad: -111.99051637610444\n",
      "iteration: 7690 loss: inf grad: -110.76881097420797\n",
      "iteration: 7860 loss: inf grad: -111.662658910076\n",
      "iteration: 7630 loss: inf grad: -112.25901281452619\n",
      "iteration: 7730 loss: inf grad: -111.37581103177553iteration: 7830 loss: inf grad: -112.67351815409265\n",
      "\n",
      "iteration: 7910 loss: inf grad: -112.35310657131433\n",
      "iteration: 7700 loss: inf grad: -109.99352692379155\n",
      "iteration: 7750 loss: inf grad: -112.01638922785185\n",
      "iteration: 7870 loss: inf grad: -111.67838927671247\n",
      "iteration: 7700 loss: inf grad: -110.82517230228122\n",
      "iteration: 7840 loss: inf grad: -112.6897520440239\n",
      "iteration: 7640 loss: inf grad: -112.2980258065521\n",
      "iteration: 7920 loss: inf grad: -112.38579695012922\n",
      "iteration: 7740 loss: inf grad: -111.4178973477596\n",
      "iteration: 7710 loss: inf grad: -110.00305650668886\n",
      "iteration: 7760 loss: inf grad: -112.03771774314542\n",
      "iteration: 7880 loss: inf grad: -111.68908194331789\n",
      "iteration: 7710 loss: inf grad: -110.87646937382961\n",
      "iteration: 7850 loss: inf grad: -112.6988779576034\n",
      "iteration: 7650 loss: inf grad: -112.32061088604301\n",
      "iteration: 7930 loss: inf grad: -112.40192664936941\n",
      "iteration: 7720 loss: inf grad: -110.01495363879462\n",
      "iteration: 7750 loss: inf grad: -111.45184464992855\n",
      "iteration: 7770 loss: inf grad: -112.05476679507139\n",
      "iteration: 7890 loss: inf grad: -111.69551771963408\n",
      "iteration: 7720 loss: inf grad: -110.9346829869063\n",
      "iteration: 7860 loss: inf grad: -112.70492328947063\n",
      "iteration: 7660 loss: inf grad: -112.33804161864427\n",
      "iteration: 7730 loss: inf grad: -110.02376736381491\n",
      "iteration: 7760 loss: inf grad: -111.47125529418233\n",
      "iteration: 7780 loss: inf grad: -112.07502669768651\n",
      "iteration: 7940 loss: inf grad: -112.41114250010384\n",
      "iteration: 7900 loss: inf grad: -111.70095528769787\n",
      "iteration: 7870 loss: inf grad: -112.70988631699034\n",
      "iteration: 7730 loss: inf grad: -110.99275792111243\n",
      "iteration: 7670 loss: inf grad: -112.35822591657464\n",
      "iteration: 7790 loss: inf grad: -112.09734798996456\n",
      "iteration: 7740 loss: inf grad: -110.02980479765101\n",
      "iteration: 7770 loss: inf grad: -111.48126585409219\n",
      "iteration: 7950 loss: inf grad: -112.41834549439598\n",
      "iteration: 7910 loss: inf grad: -111.71123619662488\n",
      "iteration: 7880 loss: inf grad: -112.71835490764215\n",
      "iteration: 7740 loss: inf grad: -111.02710293650287\n",
      "iteration: 7750 loss: inf grad: -110.03856684999313\n",
      "iteration: 7800 loss: inf grad: -112.11797744975112\n",
      "iteration: 7680 loss: inf grad: -112.38014802415361\n",
      "iteration: 7960 loss: inf grad: -112.42440597276791\n",
      "iteration: 7920 loss: inf grad: -111.7324547702992\n",
      "iteration: 7780 loss: inf grad: -111.48652618392109\n",
      "iteration: 7750 loss: inf grad: -111.0596078678536\n",
      "iteration: 7890 loss: inf grad: -112.74632669136045\n",
      "iteration: 7810 loss: inf grad: -112.13495389618512\n",
      "iteration: 7760 loss: inf grad: -110.05197473793689\n",
      "iteration: 7690 loss: inf grad: -112.39707450760886\n",
      "iteration: 7970 loss: inf grad: -112.43032705187491\n",
      "iteration: 7930 loss: inf grad: -111.75363838599594\n",
      "iteration: 7760 loss: inf grad: -111.10970878186129\n",
      "iteration: 7790 loss: inf grad: -111.48964619392606\n",
      "iteration: 7900 loss: inf grad: -112.77513464551458\n",
      "iteration: 7820 loss: inf grad: -112.14950461248077\n",
      "iteration: 7700 loss: inf grad: -112.4150810558641\n",
      "iteration: 7980 loss: inf grad: -112.43673140155097\n",
      "iteration: 7770 loss: inf grad: -110.06315487941386iteration: 7940 loss: inf grad: -111.77271776640558\n",
      "\n",
      "iteration: 7800 loss: inf grad: -111.49169644754829\n",
      "iteration: 7770 loss: inf grad: -111.13564727571062\n",
      "iteration: 7910 loss: inf grad: -112.78112706231342\n",
      "iteration: 7830 loss: inf grad: -112.16307360955011\n",
      "iteration: 7710 loss: inf grad: -112.45825910080565\n",
      "iteration: 7780 loss: inf grad: -110.07280479035303\n",
      "iteration: 7990 loss: inf grad: -112.44370750022043\n",
      "iteration: 7950 loss: inf grad: -111.79321400277128\n",
      "iteration: 7810 loss: inf grad: -111.49318135001235\n",
      "iteration: 7920 loss: inf grad: -112.78334830797817\n",
      "iteration: 7780 loss: inf grad: -111.14833156519785\n",
      "iteration: 7840 loss: inf grad: -112.17289884368583\n",
      "iteration: 7790 loss: inf grad: -110.08703854299793\n",
      "iteration: 7720 loss: inf grad: -112.5225136593584iteration: 8000 loss: inf grad: -112.45084180006094\n",
      "iteration: 7820 loss: inf grad: -111.4947131805082\n",
      "\n",
      "iteration: 7960 loss: inf grad: -111.80919300990426\n",
      "iteration: 7930 loss: inf grad: -112.78684947256113\n",
      "iteration: 7790 loss: inf grad: -111.15779199535056\n",
      "iteration: 7850 loss: inf grad: -112.17974010808781\n",
      "iteration: 7800 loss: inf grad: -110.09874925507918\n",
      "iteration: 7970 loss: inf grad: -111.82025390342781\n",
      "iteration: 8010 loss: inf grad: -112.4574336681741\n",
      "iteration: 7830 loss: inf grad: -111.49986959066588\n",
      "iteration: 7940 loss: inf grad: -112.7920395908546\n",
      "iteration: 7730 loss: inf grad: -112.55076529165322\n",
      "iteration: 7860 loss: inf grad: -112.18663275598458\n",
      "iteration: 7800 loss: inf grad: -111.16676277579552\n",
      "iteration: 7810 loss: inf grad: -110.10425016817388\n",
      "iteration: 7950 loss: inf grad: -112.79974602743042\n",
      "iteration: 7980 loss: inf grad: -111.82998360647\n",
      "iteration: 8020 loss: inf grad: -112.46297420849233\n",
      "iteration: 7840 loss: inf grad: -111.52669953216318\n",
      "iteration: 7740 loss: inf grad: -112.56156212055299\n",
      "iteration: 7870 loss: inf grad: -112.19502204055094\n",
      "iteration: 7810 loss: inf grad: -111.17842913693765\n",
      "iteration: 7820 loss: inf grad: -110.1073070714505\n",
      "iteration: 7960 loss: inf grad: -112.81177855406654\n",
      "iteration: 7990 loss: inf grad: -111.84106204515192\n",
      "iteration: 8030 loss: inf grad: -112.46745324373168\n",
      "iteration: 7850 loss: inf grad: -111.5841306885853\n",
      "iteration: 7750 loss: inf grad: -112.57217962024782\n",
      "iteration: 7830 loss: inf grad: -110.10965162757638\n",
      "iteration: 7820 loss: inf grad: -111.19564161189206\n",
      "iteration: 7880 loss: inf grad: -112.20515074963927\n",
      "iteration: 7970 loss: inf grad: -112.82986267191694\n",
      "iteration: 8000 loss: inf grad: -111.85593287146506\n",
      "iteration: 8040 loss: inf grad: -112.47120478082854\n",
      "iteration: 7760 loss: inf grad: -112.58399416216676\n",
      "iteration: 7860 loss: inf grad: -111.61641560643939\n",
      "iteration: 7840 loss: inf grad: -110.1122794580102\n",
      "iteration: 7830 loss: inf grad: -111.21267537535995\n",
      "iteration: 7980 loss: inf grad: -112.85248127245512\n",
      "iteration: 7890 loss: inf grad: -112.21650842195959\n",
      "iteration: 8010 loss: inf grad: -111.87593935967584\n",
      "iteration: 8050 loss: inf grad: -112.47462148519696\n",
      "iteration: 7770 loss: inf grad: -112.59707647144691\n",
      "iteration: 7870 loss: inf grad: -111.62107455184574\n",
      "iteration: 7850 loss: inf grad: -110.11767044153576\n",
      "iteration: 7840 loss: inf grad: -111.22301477740211\n",
      "iteration: 7990 loss: inf grad: -112.87685628507951\n",
      "iteration: 8020 loss: inf grad: -111.89733824138423\n",
      "iteration: 8060 loss: inf grad: -112.47812025622265\n",
      "iteration: 7900 loss: inf grad: -112.22833405903015\n",
      "iteration: 7780 loss: inf grad: -112.61224607108\n",
      "iteration: 7860 loss: inf grad: -110.13751340105235\n",
      "iteration: 7880 loss: inf grad: -111.6245757034349\n",
      "iteration: 8000 loss: inf grad: -112.9042199034856\n",
      "iteration: 7850 loss: inf grad: -111.22971720685295\n",
      "iteration: 8030 loss: inf grad: -111.91311683041414\n",
      "iteration: 8070 loss: inf grad: -112.48311736243248\n",
      "iteration: 7870 loss: inf grad: -110.17709325708526\n",
      "iteration: 7910 loss: inf grad: -112.24030778449047\n",
      "iteration: 7790 loss: inf grad: -112.63164035685766\n",
      "iteration: 8010 loss: inf grad: -112.93133561249427\n",
      "iteration: 7860 loss: inf grad: -111.23529466576224\n",
      "iteration: 7890 loss: inf grad: -111.63196785504617\n",
      "iteration: 8040 loss: inf grad: -111.92286739709468\n",
      "iteration: 8080 loss: inf grad: -112.49829912414734\n",
      "iteration: 7920 loss: inf grad: -112.25311781816154\n",
      "iteration: 7880 loss: inf grad: -110.19677985255832\n",
      "iteration: 8020 loss: inf grad: -112.9534936941096\n",
      "iteration: 7800 loss: inf grad: -112.65579039285932\n",
      "iteration: 7900 loss: inf grad: -111.64463590647708\n",
      "iteration: 7870 loss: inf grad: -111.2410282240701\n",
      "iteration: 8050 loss: inf grad: -111.93335338603322\n",
      "iteration: 7930 loss: inf grad: -112.26904222337402\n",
      "iteration: 7890 loss: inf grad: -110.20726816334493\n",
      "iteration: 8090 loss: inf grad: -112.53539360083462\n",
      "iteration: 8030 loss: inf grad: -112.97252861707824\n",
      "iteration: 7810 loss: inf grad: -112.67859463005948\n",
      "iteration: 7910 loss: inf grad: -111.65713445555448\n",
      "iteration: 8060 loss: inf grad: -111.9562016733758\n",
      "iteration: 7880 loss: inf grad: -111.25014409451308\n",
      "iteration: 7940 loss: inf grad: -112.28977997768834\n",
      "iteration: 8040 loss: inf grad: -112.99308882924883\n",
      "iteration: 7900 loss: inf grad: -110.21816597160529\n",
      "iteration: 8100 loss: inf grad: -112.55552488380664\n",
      "iteration: 7820 loss: inf grad: -112.69453878478451\n",
      "iteration: 7920 loss: inf grad: -111.66396206954039\n",
      "iteration: 8070 loss: inf grad: -111.99283866115903\n",
      "iteration: 7950 loss: inf grad: -112.30962863546804\n",
      "iteration: 7890 loss: inf grad: -111.26598021365888\n",
      "iteration: 8110 loss: inf grad: -112.56264652028241\n",
      "iteration: 7910 loss: inf grad: -110.23085278120607\n",
      "iteration: 7830 loss: inf grad: -112.7054040416232\n",
      "iteration: 8050 loss: inf grad: -113.0216831747496\n",
      "iteration: 7930 loss: inf grad: -111.66756255422032\n",
      "iteration: 8080 loss: inf grad: -112.0178422544796\n",
      "iteration: 7960 loss: inf grad: -112.32734057501952\n",
      "iteration: 7900 loss: inf grad: -111.28611486642586\n",
      "iteration: 7840 loss: inf grad: -112.71463801942548\n",
      "iteration: 8120 loss: inf grad: -112.57225119465345\n",
      "iteration: 7920 loss: inf grad: -110.2466781753793\n",
      "iteration: 8060 loss: inf grad: -113.04891716936362\n",
      "iteration: 7970 loss: inf grad: -112.35409729155705\n",
      "iteration: 7940 loss: inf grad: -111.67288903356756\n",
      "iteration: 8090 loss: inf grad: -112.0361349095572\n",
      "iteration: 7850 loss: inf grad: -112.7265260450267\n",
      "iteration: 7910 loss: inf grad: -111.30417462696332\n",
      "iteration: 8130 loss: inf grad: -112.59638009191283\n",
      "iteration: 7930 loss: inf grad: -110.2678827742075\n",
      "iteration: 7980 loss: inf grad: -112.39103993127449\n",
      "iteration: 8070 loss: inf grad: -113.06835248900872\n",
      "iteration: 7860 loss: inf grad: -112.7520075985081\n",
      "iteration: 7950 loss: inf grad: -111.6875146851777\n",
      "iteration: 8100 loss: inf grad: -112.05566213302998\n",
      "iteration: 7920 loss: inf grad: -111.31870329642922\n",
      "iteration: 8140 loss: inf grad: -112.6308607458351\n",
      "iteration: 7940 loss: inf grad: -110.2946024686559\n",
      "iteration: 8080 loss: inf grad: -113.08375614067596\n",
      "iteration: 7870 loss: inf grad: -112.79669152865411\n",
      "iteration: 7960 loss: inf grad: -111.70948659627933\n",
      "iteration: 7990 loss: inf grad: -112.43495098767411\n",
      "iteration: 7930 loss: inf grad: -111.34222133341811\n",
      "iteration: 8110 loss: inf grad: -112.07267951794208\n",
      "iteration: 7950 loss: inf grad: -110.3266983466005\n",
      "iteration: 8090 loss: inf grad: -113.09658378364138\n",
      "iteration: 8150 loss: inf grad: -112.6558744012229\n",
      "iteration: 7880 loss: inf grad: -112.8263250606581\n",
      "iteration: 7970 loss: inf grad: -111.72452492507966\n",
      "iteration: 7940 loss: inf grad: -111.3839384036462\n",
      "iteration: 8000 loss: inf grad: -112.4807622260451\n",
      "iteration: 7960 loss: inf grad: -110.36486564547775\n",
      "iteration: 8100 loss: inf grad: -113.1133204391542\n",
      "iteration: 8120 loss: inf grad: -112.08377976990094\n",
      "iteration: 8160 loss: inf grad: -112.67509648595828\n",
      "iteration: 7890 loss: inf grad: -112.83869253496091\n",
      "iteration: 7980 loss: inf grad: -111.7378478394027\n",
      "iteration: 7950 loss: inf grad: -111.42130369283115\n",
      "iteration: 8010 loss: inf grad: -112.50656203523832\n",
      "iteration: 7970 loss: inf grad: -110.38829365398041\n",
      "iteration: 8110 loss: inf grad: -113.13622147372459\n",
      "iteration: 8130 loss: inf grad: -112.08995223025498\n",
      "iteration: 7900 loss: inf grad: -112.84748849153776\n",
      "iteration: 8170 loss: inf grad: -112.69055258744689\n",
      "iteration: 7990 loss: inf grad: -111.7542736775782\n",
      "iteration: 7960 loss: inf grad: -111.44685532104602\n",
      "iteration: 7980 loss: inf grad: -110.40057957987995\n",
      "iteration: 8020 loss: inf grad: -112.52222108658357\n",
      "iteration: 8120 loss: inf grad: -113.1563975656464\n",
      "iteration: 8140 loss: inf grad: -112.09346485129441\n",
      "iteration: 7910 loss: inf grad: -112.8561932167726\n",
      "iteration: 7990 loss: inf grad: -110.41038409402226\n",
      "iteration: 8180 loss: inf grad: -112.70181209305716\n",
      "iteration: 8130 loss: inf grad: -113.16634050965405\n",
      "iteration: 8150 loss: inf grad: -112.09589834770046\n",
      "iteration: 8030 loss: inf grad: -112.53435095001699\n",
      "iteration: 8000 loss: inf grad: -111.77559571937265\n",
      "iteration: 7920 loss: inf grad: -112.86572847060722\n",
      "iteration: 7970 loss: inf grad: -111.46263620045484\n",
      "iteration: 8140 loss: inf grad: -113.17146765227066\n",
      "iteration: 8160 loss: inf grad: -112.09814538534113\n",
      "iteration: 8000 loss: inf grad: -110.41965902617721\n",
      "iteration: 8190 loss: inf grad: -112.71418738659658\n",
      "iteration: 8040 loss: inf grad: -112.54461556841191\n",
      "iteration: 7930 loss: inf grad: -112.87680690707904\n",
      "iteration: 8010 loss: inf grad: -111.79675901579334\n",
      "iteration: 7980 loss: inf grad: -111.48286047422488\n",
      "iteration: 8150 loss: inf grad: -113.17599907209998\n",
      "iteration: 8200 loss: inf grad: -112.73904114017\n",
      "iteration: 8170 loss: inf grad: -112.101281535471\n",
      "iteration: 8050 loss: inf grad: -112.55425187761958\n",
      "iteration: 7990 loss: inf grad: -111.52208467938976\n",
      "iteration: 8010 loss: inf grad: -110.43157163992834\n",
      "iteration: 8020 loss: inf grad: -111.81126844131666\n",
      "iteration: 7940 loss: inf grad: -112.89025861193826\n",
      "iteration: 8160 loss: inf grad: -113.1826567729658\n",
      "iteration: 8210 loss: inf grad: -112.7766437622831\n",
      "iteration: 8000 loss: inf grad: -111.55545906065957\n",
      "iteration: 8180 loss: inf grad: -112.10914612553351\n",
      "iteration: 8030 loss: inf grad: -111.8202768286062\n",
      "iteration: 8020 loss: inf grad: -110.45343658092331\n",
      "iteration: 8060 loss: inf grad: -112.56432221120383\n",
      "iteration: 7950 loss: inf grad: -112.90712009776388\n",
      "iteration: 8170 loss: inf grad: -113.19847378042564\n",
      "iteration: 8220 loss: inf grad: -112.81113374854444\n",
      "iteration: 8190 loss: inf grad: -112.13350502366062\n",
      "iteration: 8010 loss: inf grad: -111.57808062231175\n",
      "iteration: 8030 loss: inf grad: -110.485594823132\n",
      "iteration: 8040 loss: inf grad: -111.82873800532185\n",
      "iteration: 8070 loss: inf grad: -112.57626469127044\n",
      "iteration: 7960 loss: inf grad: -112.92837170162687\n",
      "iteration: 8180 loss: inf grad: -113.22447020951779\n",
      "iteration: 8230 loss: inf grad: -112.83777237198056\n",
      "iteration: 8200 loss: inf grad: -112.17332229535177\n",
      "iteration: 8020 loss: inf grad: -111.60143474111479\n",
      "iteration: 8040 loss: inf grad: -110.50641895403226\n",
      "iteration: 8080 loss: inf grad: -112.59259232106814\n",
      "iteration: 8050 loss: inf grad: -111.84179649778989\n",
      "iteration: 7970 loss: inf grad: -112.95332620765075\n",
      "iteration: 8210 loss: inf grad: -112.19378942003134\n",
      "iteration: 8030 loss: inf grad: -111.61674993131638\n",
      "iteration: 8190 loss: inf grad: -113.25358299870662\n",
      "iteration: 8240 loss: inf grad: -112.86213604468335\n",
      "iteration: 8050 loss: inf grad: -110.5174072198236\n",
      "iteration: 8090 loss: inf grad: -112.61600814087417\n",
      "iteration: 8060 loss: inf grad: -111.86234255413453\n",
      "iteration: 8040 loss: inf grad: -111.63283615309979\n",
      "iteration: 8220 loss: inf grad: -112.19931247785723\n",
      "iteration: 7980 loss: inf grad: -112.97857986860095\n",
      "iteration: 8200 loss: inf grad: -113.29862766659097\n",
      "iteration: 8250 loss: inf grad: -112.89322898639745\n",
      "iteration: 8060 loss: inf grad: -110.5305233856231\n",
      "iteration: 8050 loss: inf grad: -111.65582176893321\n",
      "iteration: 8230 loss: inf grad: -112.20227094713803\n",
      "iteration: 8100 loss: inf grad: -112.64543892897377\n",
      "iteration: 8070 loss: inf grad: -111.88330872466199\n",
      "iteration: 7990 loss: inf grad: -113.00311454084408\n",
      "iteration: 8210 loss: inf grad: -113.32318873480224\n",
      "iteration: 8260 loss: inf grad: -112.92936612597873\n",
      "iteration: 8070 loss: inf grad: -110.54925551073242\n",
      "iteration: 8060 loss: inf grad: -111.68611979745765\n",
      "iteration: 8110 loss: inf grad: -112.67603749751507\n",
      "iteration: 8240 loss: inf grad: -112.20800863262843\n",
      "iteration: 8080 loss: inf grad: -111.89649322773857\n",
      "iteration: 8220 loss: inf grad: -113.33494157231476\n",
      "iteration: 8000 loss: inf grad: -113.03064010411774\n",
      "iteration: 8080 loss: inf grad: -110.56871587371812\n",
      "iteration: 8270 loss: inf grad: -112.9648369275298\n",
      "iteration: 8070 loss: inf grad: -111.71566135744419\n",
      "iteration: 8120 loss: inf grad: -112.71402928496906\n",
      "iteration: 8250 loss: inf grad: -112.22383143480322\n",
      "iteration: 8090 loss: inf grad: -110.58376279492882\n",
      "iteration: 8230 loss: inf grad: -113.3467307910258\n",
      "iteration: 8090 loss: inf grad: -111.9038546779378\n",
      "iteration: 8280 loss: inf grad: -112.9917292931326\n",
      "iteration: 8010 loss: inf grad: -113.06049562418707\n",
      "iteration: 8080 loss: inf grad: -111.74057470161502\n",
      "iteration: 8130 loss: inf grad: -112.75583774048025\n",
      "iteration: 8260 loss: inf grad: -112.2527833428626\n",
      "iteration: 8240 loss: inf grad: -113.37118503025364\n",
      "iteration: 8100 loss: inf grad: -110.59630048138521\n",
      "iteration: 8100 loss: inf grad: -111.90894980860794\n",
      "iteration: 8290 loss: inf grad: -113.00379984401894\n",
      "iteration: 8270 loss: inf grad: -112.27845981494796\n",
      "iteration: 8020 loss: inf grad: -113.08800899383039\n",
      "iteration: 8140 loss: inf grad: -112.77961740396997\n",
      "iteration: 8110 loss: inf grad: -110.60926790880033\n",
      "iteration: 8250 loss: inf grad: -113.41653171373326\n",
      "iteration: 8090 loss: inf grad: -111.7636977678971\n",
      "iteration: 8280 loss: inf grad: -112.29848167852722\n",
      "iteration: 8110 loss: inf grad: -111.91350442221413\n",
      "iteration: 8120 loss: inf grad: -110.62294399206635\n",
      "iteration: 8100 loss: inf grad: -111.78635757476029iteration: 8030 loss: inf grad: -113.11003173927325\n",
      "\n",
      "iteration: 8300 loss: inf grad: -113.01078375077253\n",
      "iteration: 8260 loss: inf grad: -113.45015299503731\n",
      "iteration: 8150 loss: inf grad: -112.79644628833711\n",
      "iteration: 8290 loss: inf grad: -112.31377527831478\n",
      "iteration: 8130 loss: inf grad: -110.63523684363277\n",
      "iteration: 8110 loss: inf grad: -111.81086856139913\n",
      "iteration: 8270 loss: inf grad: -113.46041916774092\n",
      "iteration: 8120 loss: inf grad: -111.91818191911565\n",
      "iteration: 8160 loss: inf grad: -112.81583560050555\n",
      "iteration: 8310 loss: inf grad: -113.02053096600926\n",
      "iteration: 8040 loss: inf grad: -113.12782350130098\n",
      "iteration: 8300 loss: inf grad: -112.32062065259905\n",
      "iteration: 8280 loss: inf grad: -113.46540589255684\n",
      "iteration: 8140 loss: inf grad: -110.64423934019881\n",
      "iteration: 8170 loss: inf grad: -112.83968369802966\n",
      "iteration: 8120 loss: inf grad: -111.84708949874101\n",
      "iteration: 8320 loss: inf grad: -113.04223525768357\n",
      "iteration: 8130 loss: inf grad: -111.92320971804557\n",
      "iteration: 8310 loss: inf grad: -112.32312418527536\n",
      "iteration: 8050 loss: inf grad: -113.14763976118655\n",
      "iteration: 8180 loss: inf grad: -112.86002797182712\n",
      "iteration: 8290 loss: inf grad: -113.47208495121913\n",
      "iteration: 8150 loss: inf grad: -110.65012087414556\n",
      "iteration: 8320 loss: inf grad: -112.32456775447857\n",
      "iteration: 8130 loss: inf grad: -111.88373073991367\n",
      "iteration: 8330 loss: inf grad: -113.08056184613378\n",
      "iteration: 8140 loss: inf grad: -111.90174489280732\n",
      "iteration: 8160 loss: inf grad: -110.65442987270748\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3654/2306327686.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_timesteps\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbold_bin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mbold_bin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr_analysis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/brain/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/brain/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/brain/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/brain/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/brain/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fc = 1/n_timesteps * bold_bin.T @ bold_bin\n",
    "results = Parallel(n_jobs=8)(delayed(corr_analysis)(i, fc) for i in np.linspace(0, 4, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "780bbadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6c6f2f3760>]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA5yElEQVR4nO2deXhbZ5X/v0ebtcv7nsRL4rRpnG5p6QKUMpQpawottIVfgUKHKbQwfYCBMgzbdIZthmWghU6hZYeydKHQQIdhWgZKS5O0aZZmqRPHjhOvkiNZkmVt7+8P6TqKI9vX0r26i87nefLUkq51315LXx2d95zvISEEGIZhGONj0XoBDMMwjDKwoDMMw5gEFnSGYRiTwILOMAxjEljQGYZhTIJNqxM3NjaKrq4urU7PMAxjSHbs2DElhGgq9phmgt7V1YXt27drdXqGYRhDQkRDiz3GKReGYRiTwILOMAxjEljQGYZhTAILOsMwjEmQJehEdCURHSCiASK6vcjjryCiMBHtzP/7lPJLZRiGYZZi2SoXIrICuAvAFQBGAGwjokeEEC8sOPRPQojXq7BGhmEYRgZyIvQLAQwIIQ4LIZIA7gewRd1lMQzDMCtFjqB3ADhacHskf99CLiai54not0R0VrEnIqL3EtF2Ito+OTlZwnLLJzaXxo//OoS5dEaT8zMMw6iFHEGnIvctNFF/FsAaIcTZAL4B4OFiTySEuEcIsVkIsbmpqWijk+r88eAkPvHQHnzioT1gL3iGYcyEnE7REQCrCm53AjheeIAQIlLw81Yi+iYRNQohppRZpnJMRecAAL/cMYL1LT783ct7NF4RwyxPIpXBwfEZ7BuNYGAiCrfDhmZ/DVp8TrT4nWjx16DBWwOrpVj8xVQLcgR9G4B1RNQN4BiA6wC8rfAAImoFMC6EEER0IXKRf1DpxSpBMJoEAFx5Vis+99t9WNvsxeVnNGu8KobJIYTAxMwcXhiNYN9oBPtGcyJ+eDKKbP4LpcNmQSqTxcIvmBYCmnw1aPblBL7Z78wLfg1a/E40+3OPNXgcsLDwm5JlBV0IkSaiWwE8BsAK4D4hxF4iujn/+N0ArgHwPiJKA5gFcJ3QaT4jFEui1m3HV649G9d8K44P/vQ5PPj+S7Cuxaf10pgqI5nOYmAimhfuCPaN5QQ8FEvOH9NR68KZbT68dmMrzmzz48w2P1bXu5ERAlPROYxH5jARSWB8Jv/fSALjkTmMTM/iueETCBY8l4TDZsF/3XA+Ll/PgYzZkGXOJYTYCmDrgvvuLvj5TgB3Krs0dQjFk6j3OOB22PDtd27GljufxE0/2I6H338p6jwOrZfHmJxEKoMv/HY//joYwsDEDFKZXNzjsFmwvsWHV53ZPC/cZ7b6EXDbiz6PBYS2gAttAdeS50ums5iMzmE8ksgL/hy++Lv9eGL/BAu6CdHMbVErQtEkGvLC3VHrwn/dcD6uv+dpvP/Hz+IH77kQdqu+m2d3Hj2B2x/YhXvfdQE6apd+MzP6487/HcD3/nIEL1vXiMv6enBmmw8b2vzobvTApsJrz2GzoKPWdcpr5dFdo9h1LKz4uRjt0bd6qUAolkSd+2Qkfv6aOnz+zf146nAQ//Lrhb1S+iIcT+GWHz+L/WMz2HX0hNbLYVbIC8cjuPuPh3D1eZ344Xtegttfcwa2nNOBdS0+VcR8MTZ2BLBvNIJ0JluxczKVoeoEPRhLosF7amrl6vM78fcv78EPnx7CD586os3ClkEIgY8+8DzGIgkAwPFwQuMVMSshncniYw/sQq3bjk++/kxN17KpM4BEKouByaim62CUp6oEXQiB6XwOfSEfvfIMvPKMZnzm1y/gLwO6q7bED54awmN7x3H7lWfAZbfi+IlZrZfErIB7/zyI3cfC+JctG1Hr1navZmNHAACwa4TTLmajqgQ9MptGJitQ76k57TGrhfCf152DnkYP3vfjZ3FkKqbBCouz51gY//boPvzNGc246WXdaKt1YjTMgm4UjkzF8JXfH8SrN7TgNRtbtV4Oeho98Dis2MN5dNNRVYIejOWaiuo9xSsHfE477n3nBbAQcNMPtiOSSFVyeUWZSaRw60+eRYPXgf94y9kgIrQHXDh+glMuRkAIgdsf3AWHzYI7rtoIIu3rvy0WwlkdAexmQTcdVSXoUn1vsQhdYnWDG998+/k4MhXDB3/6HDJZ7crphRD4p4f24Oj0LL5+/bnzZZXtBo/Qsxpe00pz/7ajePpwCJ947Zlo8Tu1Xs48/R0BvHCcN0bNRlUKesMy9eYX9zbgs1vOwhMHJvGF3+6rxNKK8rNtR/Hr54/jQ1f04YKu+vn72wIuTMzMIZk23pvxaCiOMz71O+zUqEpHCIE/HpxEIqW+OdtYOIHPPboPF/c04NoLVi3/CxVkU2cAc+ksXpzgjVEzUZWCXmxTdCFvf8kavPPiNfj2nwbxi+1Hlz1eafaPRfDpR/biZesa8b7Lek95rL3WCSGA8Yjx0i4Hx2eQTGfx7NC0Juc/MD6Dd973DG76/nZVRV0IgX9+eA9S2Sy+cHW/LlIthUgbo7t5Y9RUVJWgB1cg6ADwyddvwEvXNuITD+3B9iMhNZd2CvFkGrf8+Fn4XXZ85a3nnOa7IXUHjhqwdFFa85GgNpvOA/mI9M8DU3jfj3aoZqP86O5R/M++cXz4ivVY0+BR5Rzl0N3ggbfGxnl0k1FVgh6KJeF2WOG0W2Udb7NacOfbzkV7rRM3/2gHRqbjKq8wx6d+tReHp2L4z2vPQZPv9Hx/e77rz4ili2N5QR/UqIpocDJ33k++fgMePzCJD/zkOaQUziNPx5L49K/24uzOAG68tEvR51YKi4VwVrufBd1kVJWgT8eK16AvRa3bge+88wLMpbP4ux/sQGwurdLqcjywYwS/3DGCD7xyHS5Z21j0mPba3ObacQNujEoR+uFJjQR9Kob2gBPveWk3PvOGDfjvF8Zx2892Kro5eMejLyA8m8IXrt5U0Q7QldLfEcALoxHFP9AY7dDvq00FgrHkshuixVjb7MU3rj8XB8Yi+PDPn1etSmNgIopP/moPXtJdj3/4m3WLHud22BBw2TFqwNLFsUjuQ+h4eLYiG5MLOTwVQ3dTLgXyrku78U+vPQOP7hrFP/5ylyIVTU8cmMCDzx7D+1/RizPb/GU/n5r0dwaQTGfx4jhvjJqFqhL0UCxZsqPiK9Y34xOv24Df7R3D1/7noMIry7nw3fqTZ+G0W/Gf15277KCC9lqXIVMuo+EE7FaCEMBwqDIpLAkhBA5PRtHdeDKn/d6X9+LDV/ThoeeO4RMP7S7rwzo6l8YnHtqDtc1e3PLKtUosWVX6pY3RYye0XQijGFUn6CtNuRTy7ku78NbNnfj6/w7gHfc9o+hG6R2/eQH7x2bw5beejdbA8vXK7QGn4fxchBAYCydw7qo6AJXPo0/HU4gk0uhasEn5gb9Zhw+8ci3u33YUn35kb8mjCf/jsQM4Hp7FF6/uR41N3j6NlnTxxqjpqDpBLyXlIkFE+Ner+vGxK8/A3mNhXHP3U7junqfwl4GpsuaT/mbXcfz4r8P4+8t6ZHtUG7H9P5JII57M4KLeBgCVF/TBqVxqoafp9KqTD13Rh/fmDdr+9dF9K/577hgK4ftPHcE7L+7C+Wvql/8FHWCxEDZ2+LH7WGT5gxlDUDWCPpvMYDaVWbJLVA4OmwXve0Uv/vSxy/HJ12/A4ckY3vadv+Kau5/C4wcmViwEQ8EYPv7Abpy7uhYfefV62b/XFnDhRDyFeFLdTVolkSpc+lq8aPQ65itOKsXgVC7F093oPe0xIsLHX3MG3nVJF+798yD+/bEDsv+WiVQGH/3lLrQHXPjHv5X/N9QD/XkrXd4YNQdVI+jL+bisFLfDhve8tBv/99HLccdVGzEWTuDG727DG+98Eo/tHZOVi51LZ3DrT54DEfCN689d0XCNjvnSReOkXaRvFG0BJ7obPRiscC364FQUNguhs674YBAiwqffsAHXX7ga33ziEL7+hwFZz3vX4wM4NBnD597cD0+NsWbGbOzIbYweHJ/ReimMAlSNoMvxcSkFp92KGy5ag8c/8gp88ep+RBIp/P0Pd+C1X/8Tfv388SUrJ7742wPYfSyMf3/L2eisc6/ovG35PLuR0i5ShN4acKGrwaNByiWG1fXuJT84iQj/dtVGXH1eJ776PwfxrScOLfmc+0Yj+NYTh/Dm8zpwWV+T0ktWnU2dtQDAzosyCc+m8Nlf78WukRNaL6UoVSjo6nhRO2wWXHvBavzhQ5fha9eeg3RW4AM/fQ5XfPWPeGDHyGl1zv+9dwz3PTmId13Shb89a+WWqlJzkZFKF0fDCRABzb4adDd5MDkzh6jKdf2FHJ6MnVLhshgWC+FL12zCG85uxxd/tx/3/nmw6HGnDK143Qall1sR1tS74auxsTe6DA5NRvGmu57Ed588gp8+U3k7EDlUnaCXsykqB5vVgqvO7cB/3/ZyfPPt56HGZsWHf/E8Lv/yE/jpM8NIprMYmY7jI794Hv0dAXz8tWeUdJ4WvxNEwDEDlS6OhRNo8tbAbrWgJy+slfKdz2YFjgTlCTqQ88f/ylvPxpVnteKO37yAHz49dNox9z05iF0jYXzmjWcZdsB4bmM0wBH6Mvzx4CSuuutJnJhNobPOhUM6nfZUdYJeqTeexUJ4bX8btn7wpfj2Ozajzu3Axx/cjcv+/XG8+3vbkBXAnW87t+TyNofNgiZvjaFSLqORxHyqqCsvrIcrJOhjkQQSqez8eeVgt1rw9evPxd+c0YxPPrwHP992MiqThlZcsaEFr+tvU2PJFaO/M4B9ozOGdO9UGyEEvvOnw7jxu8+go9aFX91yKS7tbcQhnbpUVo2gB2NJ2K0Ev7Oym1ZEhCs2tOBXt1yK77/7QnTUunBwPIrPv7m/bNOmtlqXoQy6xsKz8zX2Ui14pSJ0KV/fswJBB3IfnHe9/Ty8bF0jPvbgLjz83DEIIfDxB3fDbrHgji36GFpRDhs7AkhmeGN0IXPpXPXSvz66D1dsaMED77sEq+rdWNvsRTCWxHQ+SNQTxtqSL4PpWBJ1bodmbz4iwmV9TXj5ukaciKcU+abQUevE/jHjvAlHwwlc0pvzp3HarWgPOCu2MSp9E+guUoO+HE67FffcsBk3fu8ZfOjnO/HEgQk8dTiIz7+5X1YTmN7ZlO8Y3XMsPG+rW+1Mzszh5h/twI6haXzwlWtx26v65l1Pe5tzr6FDk1Fs9uir56CqInS1NkRXAhEplvZpC7gweiJRVlNTpYjOpTGTSJ8igN1NnoqlXI5MxeCyW9HiK02AXQ4r7n3nBThvdR0e3nkcF/c04DqdDa0olTUNbvicNuziPDqA3Afbljv/jL3Hw7jzbefiQ69ef4qF9domHwDoMo9eNYJebtu/HmkLODGbyiA8q/3s0+WQShbbCgW90YPByWhFPpAGp2LoavSc5i2/Ejw1Nnz3xgvwgVeuxZfferbhUy0SRIR+3hgFAGzdPYq33P0UBIBf3nwJXr+p/bRjOupccNgs8976eoIF3cC0G6i5aL4GvWCuZleDB5FEGtNx9T+QBqdiK86fF8PntOPDr14/f+3NQn9HAPureGM0mxX46u8P4v0/fhZntvnwq1svXTT9ZLUQeho9LOhaUq6Pix4x0qCLk12iJ4VQ8lRRO4+eymQxHIrLLlmsRqp5YzSeTOOWnzyL//zDi7j6vE789L0XoXmZ1FxvsxeHNPL0X4qqEPRUJovwbErxLlGtaTdQt6gUoTf7T/4NpEoXtQX9aCiOTFasqGSx2tjUKVnpVlfaZWQ6jqu/9RQe2zuGf37dmfiPt2ySVUq8tsmLo9NxTTz9l6IqBH06LnWJKuPjohcavTWwW8kQNrqjkQQaPI5Txv+tqnfDaqF5F0S1kD4wOEJfnNX1bvid1dUxuu1ICFvufBIjoTjufdcFuOllPbL3RdY2eyGEdqMUF6MqBF0tHxetsVgIrQGnIVIuY+HEaSV+dqsFq+vdODKl7qCLUmvQqwkiQn9n9WyM/mzbMN727afhd9nx0C2Xyratluhtyjl26i2PXmWCbq4cOnCydFHvjIYTp1S4SHQ1uFUvXRyciqHWbTdse36l2NgRwP6xCObS+kojKIkQAv/26Av42AO7cVFPAx5+/6VY23y6nfJy9DR5QKS/0sWqEvQGr/ne0LnJRUaI0GeLNuF0N3pxZCqmauni4JR8D5dqpr8jgFRG4OCYvkRKSX6xYwTf/tMgbrhoDb77rgsQcJeWhnXareisc3GErgXzPi5uEwp6rQtj4YQiA47VIpHKYDqeOqXCRaK7yYPZVAbjkTnVzs+CLo9NHbUAzLsxemQqhs88shcX9zTgs288C7YVzB8oxtom/VW6VIWgB6OSoJtrUxTI+bmkswJTUfUEsVyK1aBLdDdIJl3qRDrxZBqj4QTnz2Wwqt6FgMtuyqHRqUwW//CznbBbLfjyW88uq8FMorfJi8OTUV0FU1Uh6NPxJGrd9rI/kfWIVLqo543R0SJdohKSt4paG6NHlhg7x5yK1DFqxgj9G394Ec8fPYHPvalfsaawtc1ezKWzODatn/ee+RSuCHrxcVEDKY2hZ9fFsUjuBV8sh97md6LGZlGtdFGqcOlqXNlEqGplY0cAB8ZmTLUxuu1ICHc+PoBrzu/E6zYpZ3UsbabqaWO0KgQ9FE2i3oT5c6Bwtqh+ooSFjM6Pnjtd0C0WUnUcnfRB0VWmVXG1IG2MHjCQi+dSRBIp3Hb/TnTWufGZN56l6HPrsXSxOgTdxBG632WD22HVtZ/LWDiBgMsOt6O4W3N3o5qCHker32m44c1aYbaO0U//ai/GIgl87bpz4FX4NVDncaDB4zBehE5EVxLRASIaIKLblzjuAiLKENE1yi2xfELxpClLFoFc3rMt4NR1+/9iNegSXY0eDIfip81dVYLBqShXuKyAzrrcxqgZGox+tfMYHnruGD74ynU4b3WdKufobfIaK0InIiuAuwC8BsAGANcT0WkTcfPHfRHAY0ovshyEEJg2cYQO5EoX9ZxyKdYlWkhPowepjFDlW8bgVKykoRbVChFhU2fA8BYAR0Nx/PNDe3D+mjrccnmvaufJmXQZSNABXAhgQAhxWAiRBHA/gC1FjvsAgAcATCi4vrKJzKaRzgpT1qBLtAdcuvZzkROhA8qXLk7HkpiOp7hkcYVs7Ajg4PiM7oyn5JLJCnz4589DAPjateeoWt3W2+TBdDyFoE7KhuX8n3YAOFpweyR/3zxE1AHgTQDuXuqJiOi9RLSdiLZPTk6udK0lEYzlLrRZUy4A0FbrxOTMnC4rE5LpLKaic2j1L14qJqVElM6jDwbZlKsUjL4xevcfD+GZIyH8y5azsKpe3eqmk5Uu+mgwkiPoxSrwF1bSfw3Ax4QQSyqKEOIeIcRmIcTmpqYmmUssj5NOi+Yy5iqkPV+6OB7WR5RQyHhk8Rp0iUavA74am+IDowcnpZJFFvSV0N9h3I3RnUdP4Ku/P4g3nN2ON53bsfwvlIneKl3kbPuOACgcntgJ4PiCYzYDuD9vPdkI4LVElBZCPKzEIstB6hI123CLQuYHXYRnsbpBX/XWY5HFSxYliAhdjcrPFx2cisFqIayq09c10TuddS7Uuo23MRqbS+O2+59Di9+Jf71qY0VGBHbUuuCyW3WTR5cj6NsArCOibgDHAFwH4G2FBwghuqWfieh7AH6jBzEHCnxcTCzobbX6HXSxVJdoId2NHjw7PK3ouQeDMazKz39k5CN1jBptY/SO37yAoVAcP/27ixBwVcbmw2Ih9DTpZxzdsq90IUQawK3IVa/sA/BzIcReIrqZiG5We4HlEoxVQYQe0O9s0bHw4l2ihXQ1enDsxKyi+wCDk2zKVSr9BtsY/d2eUdy/7Sjed1kvLuppqOi59VS6KCt0EUJsFUL0CSF6hRD/lr/vbiHEaZugQoh3CSF+qfRCS2U6loTbYT1lUo7ZcDmsqHPbdVm6OBpOwFtjg8+5dMTU0+iBEMBwUBlPFyFE3mWRPVxKob8jgHRWYL8BNkbHwgnc/uBubOoM4LZX9VX8/GubvTh2YhazSe0//Ez/XTQUS5q6ZFGiLeDSpZ/LcjXoEkpXuoxH5jCbynANeon0G6RjNJsV+MgvnsdcKouvXXuOJuk1aWNUD3l00wt6MGbeLtFC2mv1OYpuuRp0iS6FBV2qaeca9NLoqHWhzm3HHp3n0e97chB/HpjCp96wAT1N2nwb05NJl+kF3cw+LoXotVt0LJwo6oO+kIDLjgaPQzFBP+myyIJeCrkZo7XYpeMIfe/xML70uwN49YYWXHfBquV/QSW6Gt2wEHBIB3l0FnST0BZwIZJIIzaX1nop86QzWUzMyIvQAWVNugYnY6ixWdAm48OEKU5/hx8v6nRjNJHK4B/u34latx1fuHpTRUoUF6PGZsXqercumouqQ9CrIIfersPSxcnoHLICaC0yeq4YXQoK+pFgrsJFick01YqeN0Y/v3UfBiai+PJbz9ZFwKaXShdTC/psMoPZVAb1VZBDlwZdHNNR6aLcGnSJ7kYPJmbmEFXgW8ZhniNaNv2dtQCA3SMnNF3HQh7fP4HvPzWE97y0Gy9bV5mO8+VY2+zF4FRM83F0phb0eR8XHXyCq818hK6jPPrYEoMtiiEJcLkWAOlMFsPBOAt6mbQHnKj3OHRV6ZLOZPHRB3bhjFYf/vFv12u9nHl6m71IZrI4GlJnlKJcTC3o07EUAHP7uEi0+J0ggq5cF0uJ0IHyK11GpmeRzgoW9DIhImzUWcfo8RMJTM7M4cZLu3TVW6IXTxdTC7oUodd7KtMGrCV2qwXNvhqdReizcNotstuwpTFx5Ubo0gdCD9egl82mjgBenIjqZmN0KJT7267R2UjBtTqpRTe1oEs+LtUQoQP50kUdbYrmatBdsisQXA4r2gLOsiN0yeSLu0TLZ2NHAJmswL7RiNZLAQAM5TuJ1+jMhC7gtqPRW8MRupqcFHTz59CBnKfLqI42ReXWoBfSrYDr4uBUFH6nDXVu838zUxu9zRgdDsXhsFnQ4tNfOeraZg9H6GoSiiVhsxD8zuoYENwWcOJ4eBZCaLvTLiG3S7SQrkYPjgTLE/QjU3F0N3k1rU02C20BJxo8DuzWSR59KBjD6nq3LstRpdJFLd9/phf0Oo+jat7Y7bUuJFJZTMdTWi8F2azAeESej0shPY0enIinMJ3/dlUKg1MxbvlXCGljVC8R+lAwjtUqTyEqlbXNXkQSaUxFS3/tloupBT0YS1ZFyaKEVLqoBwuAqdgc0lmx4gh9vtKlxCg9kcrg2IlZrnBRkE2d+tgYFUJgOKRfQddDpYupBb1a2v4lpOYiPbgunqxBl9clKjFv0lViG/URniOqONLG6Asab4xORZOIJzO62xCV0INJl6kFfbraBF1HEfpKa9AlVtW5YbVQyZUu0gcBC7pyzG+MapxHHw7ps8JFoi3ghNth5QhdLYJVJuiNnho4rBZdlC6utEtUwmGzYFWdq+SUy8mSRRZ0pWj1O9Ho1b5jdDhfg766Xp9/WyJCb5OXI3Q1SGWyCM+mqkrQLRZCa8Cpi9LF0XACDqulJGO0rkZPySmXwakYmn018NRUR2VTJZA2RrUeGj0UjIMIWFW/sjReJVnb7NXURte0gj4dN/8s0WK0BZy6cFwcC8+iJVBTUnlZd750sZTyr0E25VKFTfkZo1qOWRsOxtHmd6LGpp+W/4X0NnlwPJzQzMbavIJeRT4uhXTUunQxLHo0nECbv7RIqrvRg3gyg4mZuRX/7pGpGLf8q8DGjgCyAppujA6F4lit0/y5hLQxelgjb3TTCrrk41JXBT4uhbTVOjEWSWhu4zlWQg26hBRhr/RNEY6nEIwlOUJXgU06sNIdCsaxRqf5c4n50sVJbTzkTSvoUtt/Q5VF6G0BFzJZgckSolulEEKU1CUqMW+ju8KN0cEge7ioRYu/Bo3eGuw+pk2EHptLYyo6p/sIfU2DB1YL4dAER+iKUm0+LhJSc9ExDUsXp+MpJNPZkiP09oALDptlxaWLg/nB0ByhKw8Rob/Dr9nGqFSyqNemIgmHzYI19W7NShdNL+jVZtDUXis1F2kn6NK5S43QLRZCV4N7xSmXwckYLKT/N71R6e+sxYsTM4gnK7/hp1eXxWL0NmtXumhqQQ+47LBZTfu/WJT5blENN0ZL7RItpKth5SZdh6di6Kxzw2Grrr95pejPb4xqYaUrTQLSew4dyG2MHgnGkM5kK35u077yq83HRcLvtMHjsGqacim1S7SQ7iYPhoPxFW3ucsmiukgdo1pMMBoKxRBw2REwwDfu3iYvUhkxnyaqJKYV9FC0urpEJYgI7bUuTVMuY+EErBZCo7f0DemeRg+SmaxsGwMhBI6woKtKi9+JJl+NJh2jQ8G4IdItwMnSRS3y6KYV9Ol4dQo6ALTVujQ16BoNJ9Diq4G1DM9qaRyd3GEXkzNziCUzXIOuMv0adYzq2WVxIdJrcECDPLppBb3afFwKaQ84NW0uGovMllzhItHdJLkuyntTsIdLZejvCGBgIlrRjdF0Jotj07OGidD9Tjta/DWalC6aUtCFEFXntFhIe60LU9E5zKW1adOWZomWQ5O3Bt4aG44E5eUhB1nQK4K0MfrC8cptjB4/kUA6KwyxISrR2+TlCF0pIrNppLOiagVd2owc0yDtIoTIzRItM0InInQ1umWnXAanYnDYLGgv84OEWZp+DWaMDuVdFlcZJOUC5PLohzUYR2dKQQ9Jxlze6hR0qRZdi7RLJJFGPJkpq8JForvRO98stByHJ2PobvDoctakmWj21aDObcfB8cq1thupBl2it8mLmbl0SX5E5WBOQZd8XEqwbjUDkphqMeiiVB/0YnQ3uHFselZW6mhwKoquRuO84Y0KEaGvxYeD45VLJwyH4nDYLGj1l/+aqhTz04sqXOliSkEPRqvTx0VCy27RcrtEC+lu8iArTjaVLEYmm6v5ZQ+XyrC+1YeDYzMVSycMB+NYVecy1Lev+dLFCufRTSno8z4uVZpycdqtqPc4cFyDHLoSXaISkkAPTi0t6MemZ5HKCPTwhmhFWNfiw8xcumKlsUOhONY0GOtv2+zLbepzhK4AUg69lGk5ZqEt4NQk5TIaToAo94Iul+78m3i5PPphyZSLa9ArwvoWHwDgQAXy6EIIDAdjhqlBlyAi9DZXvtLFnIIeTcJlt8Ll0O9kE7Vpr3Vp4ucyFk6gyVsDuwIeOgG3HfUex7Kui1yyWFn6WnLfnF6sgKAHY0nEkhlDbYhK9DZ5Kt4tak5Br+IadIn2gFOTYdGjkdJ90IvR1eCWJeg+p60qvXu0oNbtQIu/BgfG1BcrI1a4SKxt9mI8MoeZRKpi5zSloAdjyaotWZRoq3VhJpGu6IsJyM0SVaLCRSJXuri8oPc0ekBknE0zo5OrdFE/Qh/O16CvNlBTkYQ0vehQBcfRyRJ0IrqSiA4Q0QAR3V7k8S1EtIuIdhLRdiJ6qfJLlU81+7hInKx0qWzaRYku0UJ6mjwYj8wtOXT38GQMXZxuqSh9LT68ODGDrMqjDoeCcRABnXXGaxjTonRxWUEnIiuAuwC8BsAGANcT0YYFh/0BwNlCiHMAvBvAdxRe54oIRpNVvSEK5FIuQGVr0aNzacwk0opG6JJJ12Le6IlUBsfDs5w/rzDrW3xIpLI4Oq2uRexwMI5WvxNOu/H2w1bXu2GzUEU3RuVE6BcCGBBCHBZCJAHcD2BL4QFCiKg4WZTqAaDphGLOoedSLkBlI/QxBXzQFyIJ9WJpl+FQHELwhmil6WvNV7qMqZt2GTKQy+JC7FYLuho9+orQAXQAOFpweyR/3ykQ0ZuIaD+AR5GL0k+DiN6bT8lsn5ycLGW9yzKbzGA2lanaGnSJFl8NLFTZCH2+Bl3Bjj6p+/PIIoIujanr4aaiirIun05QO49uJB/0YqytsEmXHEEvttN0WgQuhHhICHEGgKsA3FHsiYQQ9wghNgshNjc1Na1ooXKZ93Gp8gjdZrWgxV9ZG92TXaLK5TvdDhta/c5FTbqkyJ3b/iuLp8aGzjoXDqhoARBPpjEVnTNcU1Ehvc25yVupCo2jkyPoIwBWFdzuBHB8sYOFEP8HoJeIGstcW0mEotJw6OoWdCCX+qhk+78UoTf7lbVc6G70LJpyGZyKoslXA59T/6PJzMb6Fp+qtejSCDejplyA3MZoOiswtML5uKUiR9C3AVhHRN1E5ABwHYBHCg8gorWUrxkjovMAOAAElV6sHIJ5Y65qL1sEcnn0SqZcRiMJNHgcim9gdTV6Fk258BxR7ehr9eHQZFS16NPINegSUuniQIWGXSwr6EKINIBbATwGYB+Anwsh9hLRzUR0c/6wqwHsIaKdyFXEXCsqbQScZ97HpUqNuQrpyI+iq9SfQgkf9GL0NHowHU/hRD6dVsjgVGzeIoCpLH0tuWHIi33YlsuwJOgGrEGXOFmLXpk8uk3OQUKIrQC2Lrjv7oKfvwjgi8ourTROCjpH6G0BJ+bSWYRiSTSUMbBZLqPhBDpqlRf0wkqXc1ef/LtGEilMRZPs4aIRfQWeLuvyPyvJUCiGgMuOgNu46TRPjQ1tAWfFLABM1ykaiiVhsxD8TlmfVaZG2pysVOmi0l2iEl2LlC4eYQ8XTelt8sJCUM0bfSho3JLFQtY2eysWoZtS0Os8Dm4DB9Cej5aPVSCPnkhlMB1PKVrhIrG63g0LnS7o0m22zdUGp92KrkYPDqpUiz4cimO1gfPnEr1NXhyq0Dg60wl6MJas+pJFifn2/woIuho16BIOmwWddaebdB2ejIEIpnjTG5W+ZnU8XdKZLI5Nz2KNCSL03mYvYskMxiLqf1M2naBPx5JcspinweOAw2apSMplVIUu0UKKlS4OTsXQWedCjc14beFmoa/VhyPBGBKp5ccEroTjJxJIZ4WhK1wk1s5XuqifdjGdoIdiyarvEpUgIrQFnBVJuYxFcudQI4cOnBT0wq+tuZJF7hDVkvUtPmSF8lUcJ2vQjZ9O623O/T9UwgLAdILOKZdTaQ+4Khqhqyno8WQGk/kp6kKIedtcRjvWt6pjATCUt801Q4Te5K2B32mriAWAqQQ9lckiPJviksUC2mqdFcuhB1x2uB3qVBdJlSySBcBUNInoXBpdJnjDG5k1DR7YraT4sIvhYBwOm0WVPZlKI42jO1SB5iJTCfqJeG6YAwv6SdoDLoxFEkir7CWR80FX780nCbpUqjg/dq6JUy5aYrda0NvkVT5CD8axqs4Fi8Uc1WqVMukylaBzU9HptNe6kBXARD5VoRZqdYlKtNe64LBa5oVcGhzNKRftUWN60VAobmhTroX0NnsxOTOH8Ky6E8RMJeiSjwsL+kna8rXoapt0qR2hWy2ENQ3u+ZTL4akYHFbLfGkmox3rW30YmZ5FdImpUitBCIHhYMwUTUUSaytkAWAqQZci9Ab2cZmnPd/oo6aNbjKdxVR0Dq1+dcW10KRrcDKGNQ1uWE3yldzISN7oSjkvBmNJxJIZcwl6c2VKF00l6NN5Qa/zGNf7QWmkCF1N18XxiLo16BI9jR4MBePIZAW7LOqI9fnpRUqlXczgsriQzrpcypAj9BUQjLEX+kL8Tjt8NTZVSxelDjg1c+hAbmM0mcniaCiOoWCcTbl0wqo6N5x2i2KeLsMmKlmUsFkt6K7AODpTCXoolkTAZYfdaqr/rbJpq3WqGqGr3SUqIZl0PXloCslMlm1zdYLFQopujA4HZ0EEdNaZR9CBXIPRoUl1SxdNpXzcVFSctoALx1XcFB0Lq9slKiFVtDy+fwIAuyzqiXXNPsUGRg+FYmj1OxUflKI1a5u8GArGMJdW1iahEFMJ+nTeaZE5lfZaF0ZV3BQdDSfgrbGpPgauyVcDj8OKPw9MAQCnXHTE+lYvJmbmig4hWSnDJrHNXUhvsxdZcXKPQA1MJeihWJJLFovQHnAiGEsqbqAkoXYNugQRoavRg0QqC2+NDU0VGNrByEMadqFEHj1Xg25CQa+ASZepBJ1TLsVpq1V30IXaNeiFSGmW7kYPe97rCKnS5UCZefR4Mo3JmTlTNRVJ9OS/UbKgy0AIgWmO0IsiDbpQy9NlLJyomOdGoaAz+qHV74Svxlb2sAvJZXGVCVMubocNHbUuVUsXTSPokUQa6axgQS/CfHORChF6OpPFxIw2ETqjH4gIfa2+siP0+Rp0Ewo6kGsw4ghdBuzjsjhSfluNCH0yOoesAFpVGD1XDKnjrreZTbn0Rl+LDy+Oz5Q1am3YhE1FhfQ2eXF4MoZsVp1xdCYSdPZxWQyn3YoGj0OV0sVK1aBL9HcEcM8N5+M1G1srcj5GPutbvJiOpzAZLd0IbigUg99pQ61JmwPXNnsxm8qoVkZsGkEPRtnHZSnaa12q+LmMqTzYYiFEhFef1crNYzpkvtKlDG/04dCsKTdEJXrzG6NqNRiZ5l0xHWcfl6VoCzhVcVysdITO6Jc+BSpdhoMxUw/9VtukyzSCHmSnxSVRL0KfhdNuQcDFH6TVTqO3Bg0eR8mui+lMFiPTs6bdEAVyKeGOWhfiClkNL0SdeWEaEIom4bJb4XKYq11YKdprnYjOpRFJpOBXsKMzV4Pu4ppwBkAu7VJqhD4aTiCdFabdEAVyKcM/f+xy1d4vponQuUt0adryVShKWwBUsgad0T99LV4cHCut0kUqWVxdb94cOgBVgx/zCHqcBX0p2lXyRa9klyijf/pafYglMzhWwutsKG+ba+YcutqYR9A5Ql8SaVSbkuVS2azAeKQyPi6MMVjfUvqwi+FgHA6rhb/xlYFpBD0YZR+XpWj2OWG1kKIpl6nYHNJZwRE6M8+6Mky6hoJxdNa7eKxgGZhG0DlCXxqrhdDiq1E0Qj9Zg86DmpkcAZcdrX5nSZ4uQ6G4qStcKoEpBH02mcFsKsNe6MvQVutSNIfONehMMUrxdBFCYDgYM3VTUSUwhaCH4lINOgv6UrTXuhS10K10lyhjDNa35AyoMivwKwnFkoglM6YcbFFJzCHoUTbmkkN7wInRcEIxY6DRcAIOqwX1JvXdYEqjr8WHuXR23gpXDkMhc5tyVQpTCHowb8zV4GVhWYq2gBPJdHa+q7ZcxsKzaAnUwMKbWEwBkqfLSmaMmt1lsVKYQtDnfVw4UlyS9vnJRcrk0UfDCbT5eUOUOZV1LTm/kpWULkpNRZ11LOjlYApBZ6dFeczXoitUujjGNehMEdwOG1bXu1e0MToUiqHV74TTztYd5WAKQQ/FkrBZCH6XaaxpVEGqRlGi0kUIwV2izKJIwy7kMhyMc4eoAphG0Os8DjaIWoZ6jwM1NosiKZfpeArJdJYjdKYofS25yTzJdFbW8VyDrgyyBJ2IriSiA0Q0QES3F3n87US0K//vL0R0tvJLXZxQLMmVFjIgopyNrgKli9KHAkfoTDHWt/qQzgoMTi0/yCGeTGNyZo43RBVgWUEnIiuAuwC8BsAGANcT0YYFhw0CuEwIsQnAHQDuUXqhS8FdovJpCzgVmS3KXaLMUsxXushIu0jljau5qahs5EToFwIYEEIcFkIkAdwPYEvhAUKIvwghpvM3nwbQqewylyYUS6KeSxZl0RZQZtAFd4kyS9HT5IHVQrLy6PMli5xyKRs5gt4B4GjB7ZH8fYvxHgC/LfYAEb2XiLYT0fbJyUn5q1yGYIyNueTSUevExEwC6Yy83OZijIUTsFoIjV6uLGJOp8ZmRVeDW1Yt+jA3FSmGHEEvttNYtNWQiC5HTtA/VuxxIcQ9QojNQojNTU1N8le5BOlMFuHZFNegy6St1oWsAMZnSp/MDuQi9BZfDTvjMYuyvtUnqxZ9KBiH32lDLb+Hy0aOoI8AWFVwuxPA8YUHEdEmAN8BsEUIEVRmecszHU8B4C5RuUi16I/vnyjrecYis1zhwixJX4sPQ6E4ZpOZJY8bCsXZlEsh5Aj6NgDriKibiBwArgPwSOEBRLQawIMAbhBCHFR+mYsTirGPy0p4SXc9Luiqwz8/vAdf+e8DJfu6SLNEGWYx1rf4IARwaHJpb/ThYIxNuRRiWUEXQqQB3ArgMQD7APxcCLGXiG4mopvzh30KQAOAbxLRTiLartqKFyD5uLCgy8Npt+JHN70Ebzm/E1//3wHc8pNnEU+ubAK5ECI3S5QjdGYJ1snwdElnshiZnuWmIoWQ1VophNgKYOuC++4u+PkmADcpuzR5TMdyKRcWdPnU2Kz40jWbsL7Vh89t3Yfhu+P49js2z6djliOSSCOezHCFC7MkXQ1uOKyWJfPoo+EE0lnBFS4KYfhO0RBH6CVBRLjpZT24950XYCgYxxvvfBLPDU8v/4tgH3RGHjarBb3N3iVr0SVTLo7QlcHwgi5ZwXKVS2lcfkYzHnr/JXA7rLj2nqfx8HPHlv0d7hJl5LK+xYsXl5gvOhTKdZLypqgyGF7QQ7Ek/E4b7FbD/69oxroWHx6+5VKcu6oWt/1sJ770u/1LbpZylygjl3UtPhw7MYuZRKro48OhOBxWC1r9HBwogeFVMBRLooGbW8qm3uPAD9/zElx/4Wp884lDuPlHOxCbK75ZOhpOgAho9vF1Z5ZmfX5j9OAiUfpwMI7Oehf3MyiEKQSd8+fK4LBZ8Lk3bcSn37AB/7NvHFd/6y8YmT59jNhYOIEmbw1/K2KWZX2rJOjF8+hDQXZZVBLDvyNZ0JWFiHDjpd343o0X4tiJWWy580lsPxI65ZjRCPugM/LoqHXB7bAWFXQhBIZDca5BVxDDC3qQrXNV4eV9TXj4lkvhd9nxtm//Fb/cMTL/2FiYu0QZeVgshHXN3qKCHoolEZ1Ls8uighha0IUQmGanRdXobfLiofdfggu66/CRXzyPz2/dh0xWcJcosyL6Wnw4MHZ6Dn0oxC6LSmNoQY8k0khnBTstqkit24Hv3Xgh3nHxGvzX/x3Gjd/bhplEmiN0RjbrW32Yis7N23RIzNvmcg26Yhha0NnHpTLYrRb8y5aNuOOqjXhyYAoA16Az8ulrKb4xKjUVreIIXTEMLui5LtE6FvSKcMNFa/DDd1+Ii3rqcUFXvdbLYQzCooIeiqHV74TTbtViWaZElpeLXgnlfVw45VI5LlnbiEvWNmq9DMZAtPhr4HfaTjPpOhqKc8u/wpgiQueUC8PoFyIqOuyCa9CVx9CCLvm4NHi4Y5Fh9Exfiw8Hx6MQImcpMZvMYGJmjjdEFcbQgh6KJuG0W+BycA6OYfRMX4sP4dkUJvKjD6U5orwhqizGFvR4kqNzhjEAfQuGXQwF2WVRDYwt6Nz2zzCGoK/FC+BkpcswNxWpAgs6wzCq0+CtQaO3Zl7Qh4Jx+Jw21LrtGq/MXBha0INRFnSGMQp9LV4cyNvoDoXiWNPgBhHb5iqJoQV9Os6CzjBGoa/FhxfHZ5DNCgwHY1hTz/lzpTGsoCdSGcSTGRZ0hjEI61t9iCczGA7FMTI9y01FKmBYQT9Zg86CzjBGQKp0+ePBSaSzgjdEVcCwgh6K5odDs6AzjCFYl690+f0L4wDAEboKGFfQ4xyhM4yR8DvtaA848fThIADwpCIVMK6gs48LwxiOvlYf0lkBu5V4SIoKGFbQg1H2cWEYo7E+n0dfVeeG1cIli0pjWEEPxZKwWgg+p6EdgBmmqliXF3TOn6uDYQV9Op5EndsBC3/KM4xhkCJ0rnBRB8MKejCa5A1RhjEY61q8aPbV4HyeeKUKhs1XsI8LwxgPp92KZz7xKq2XYVoMG6GzoDMMw5yKcQWdfVwYhmFOwZCCns5kcSKeYkFnGIYpwJCCPh1PAQAavCzoDMMwEoYU9FDemKvOzYLOMAwjYWhB57JFhmGYkxha0Os55cIwDDOPQQWdjbkYhmEWYkhBD3IOnWEY5jRkCToRXUlEB4hogIhuL/L4GUT0FBHNEdFHlF/mqUzHkvA7bbBbDfl5xDAMowrLtv4TkRXAXQCuADACYBsRPSKEeKHgsBCADwK4So1FLiQYS6LBy7a5DMMwhcgJcS8EMCCEOCyESAK4H8CWwgOEEBNCiG0AUiqs8TS47Z9hGOZ05Ah6B4CjBbdH8vetGCJ6LxFtJ6Ltk5OTpTwFgJygc/6cYRjmVOQIejHDcVHKyYQQ9wghNgshNjc1NZXyFABygs416AzDMKciR9BHAKwquN0J4Lg6y1keIQSm40muQWcYhlmAHEHfBmAdEXUTkQPAdQAeUXdZixNJpJHKCI7QGYZhFrBslYsQIk1EtwJ4DIAVwH1CiL1EdHP+8buJqBXAdgB+AFkiug3ABiFEROkFs48LwzBMcWRNLBJCbAWwdcF9dxf8PIZcKkZ1uO2fYRimOIbrzGFjLoZhmOIYTtDr3Ha8ZmMrWv1OrZfCMAyjKww3JHpzVz0288RwhmGY0zBchM4wDMMUhwWdYRjGJLCgMwzDmAQWdIZhGJPAgs4wDGMSWNAZhmFMAgs6wzCMSWBBZxiGMQkkREnW5uWfmGgSwFCJv94IYErB5SiFXtcF6HdtvK6VwetaGWZc1xohRNGBEpoJejkQ0XYhxGat17EQva4L0O/aeF0rg9e1MqptXZxyYRiGMQks6AzDMCbBqIJ+j9YLWAS9rgvQ79p4XSuD17UyqmpdhsyhMwzDMKdj1AidYRiGWQALOsMwjEnQtaAT0ZVEdICIBojo9iKPExF9Pf/4LiI6TyfregURhYloZ/7fpyq0rvuIaIKI9izyuFbXa7l1Vfx6EdEqInqciPYR0V4i+ocix1T8eslclxbXy0lEzxDR8/l1fbbIMVpcLznr0uT9mD+3lYieI6LfFHlM+eslhNDlPwBWAIcA9ABwAHgewIYFx7wWwG8BEICLAPxVJ+t6BYDfaHDNXg7gPAB7Fnm84tdL5roqfr0AtAE4L/+zD8BBnby+5KxLi+tFALz5n+0A/grgIh1cLznr0uT9mD/3hwD8pNj51bheeo7QLwQwIIQ4LIRIArgfwJYFx2wB8AOR42kAtUTUpoN1aYIQ4v8AhJY4RIvrJWddFUcIMSqEeDb/8wyAfQA6FhxW8eslc10VJ38Novmb9vy/hRUVWlwvOevSBCLqBPA6AN9Z5BDFr5eeBb0DwNGC2yM4/YUt5xgt1gUAF+e/Bv6WiM5SeU1y0eJ6yUWz60VEXQDORS66K0TT67XEugANrlc+fbATwASA3wshdHG9ZKwL0Ob19TUAHwWQXeRxxa+XngWdity38JNXzjFKI+eczyLnt3A2gG8AeFjlNclFi+slB82uFxF5ATwA4DYhRGThw0V+pSLXa5l1aXK9hBAZIcQ5ADoBXEhEGxccosn1krGuil8vIno9gAkhxI6lDityX1nXS8+CPgJgVcHtTgDHSzim4usSQkSkr4FCiK0A7ETUqPK65KDF9VoWra4XEdmRE80fCyEeLHKIJtdruXVp/foSQpwA8ASAKxc8pOnra7F1aXS9LgXwRiI6glxa9pVE9KMFxyh+vfQs6NsArCOibiJyALgOwCMLjnkEwDvyu8UXAQgLIUa1XhcRtRIR5X++ELnrHFR5XXLQ4notixbXK3++ewHsE0J8ZZHDKn695KxLo+vVRES1+Z9dAF4FYP+Cw7S4XsuuS4vrJYT4uBCiUwjRhZxG/K8Q4v8tOEzx62Ur55fVRAiRJqJbATyGXGXJfUKIvUR0c/7xuwFsRW6neABAHMCNOlnXNQDeR0RpALMArhP5bW01IaKfIrej30hEIwA+jdwmkWbXS+a6tLhelwK4AcDufP4VAP4JwOqCdWlxveSsS4vr1Qbg+0RkRU4Qfy6E+I3W70eZ69Lk/VgMta8Xt/4zDMOYBD2nXBiGYZgVwILOMAxjEljQGYZhTAILOsMwjElgQWcYhjEJLOgMwzAmgQWdYRjGJPx/o7qoWRECQiYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 loss: -1143.8887577835264\n",
      "iteration: 10 loss: -1089.3616006768607\n",
      "iteration: 20 loss: -1040.1259801254557\n",
      "iteration: 30 loss: -995.8901156940434\n",
      "iteration: 40 loss: -955.2442266969058\n",
      "iteration: 50 loss: -918.6159374310795\n",
      "iteration: 60 loss: -886.6750608890139\n",
      "iteration: 70 loss: -859.74050046945\n",
      "iteration: 80 loss: -837.8430885356095\n",
      "iteration: 90 loss: -820.971252323633\n",
      "iteration: 100 loss: -809.2235321918542\n",
      "iteration: 110 loss: -802.9051383570652\n",
      "iteration: 120 loss: -802.120856062984\n",
      "iteration: 130 loss: -806.2758234974267\n",
      "iteration: 140 loss: -814.6786100371173\n",
      "iteration: 150 loss: -827.0957809513236\n",
      "iteration: 160 loss: -843.3254045897955\n",
      "iteration: 170 loss: -863.2205846839307\n",
      "iteration: 180 loss: -886.2824494032145\n",
      "iteration: 190 loss: -911.8789365251874\n",
      "iteration: 0 loss: -1146.6466992878509\n",
      "iteration: 10 loss: -1091.7581161724017\n",
      "iteration: 20 loss: -1041.9534429832092\n",
      "iteration: 30 loss: -997.0143001417274\n",
      "iteration: 40 loss: -955.9968040366158\n",
      "iteration: 50 loss: -919.0158235743097\n",
      "iteration: 60 loss: -886.8184987527488\n",
      "iteration: 70 loss: -859.7003693487222\n",
      "iteration: 80 loss: -837.4132456095017\n",
      "iteration: 90 loss: -820.1367393657209\n",
      "iteration: 100 loss: -808.1260944245349\n",
      "iteration: 110 loss: -801.4455195682648\n",
      "iteration: 120 loss: -800.0919350652384\n",
      "iteration: 130 loss: -803.5548430808683\n",
      "iteration: 140 loss: -811.4217828268679\n",
      "iteration: 150 loss: -823.4738286030065\n",
      "iteration: 160 loss: -839.4918482048602\n",
      "iteration: 170 loss: -859.3601944415295\n",
      "iteration: 180 loss: -882.4162016455549\n",
      "iteration: 190 loss: -907.9980281020407\n",
      "iteration: 0 loss: -1163.3607706762741\n",
      "iteration: 10 loss: -1108.332208888519\n",
      "iteration: 20 loss: -1058.4276528640498\n",
      "iteration: 30 loss: -1013.745326392232\n",
      "iteration: 40 loss: -972.7028865984636\n",
      "iteration: 50 loss: -935.4250049669954\n",
      "iteration: 60 loss: -902.7532299241976\n",
      "iteration: 70 loss: -874.8351657272258\n",
      "iteration: 80 loss: -851.6250122804412\n",
      "iteration: 90 loss: -833.3416222708697\n",
      "iteration: 100 loss: -820.2973592432379\n",
      "iteration: 110 loss: -812.6599718418956\n",
      "iteration: 120 loss: -810.3836984237308\n",
      "iteration: 130 loss: -813.0165837984414\n",
      "iteration: 140 loss: -820.1249688719165\n",
      "iteration: 150 loss: -831.56177245495\n",
      "iteration: 160 loss: -847.1810257969173\n",
      "iteration: 170 loss: -866.6247456540389\n",
      "iteration: 180 loss: -889.1948355705449\n",
      "iteration: 190 loss: -914.2664348933694\n",
      "iteration: 0 loss: -1162.487675778482\n",
      "iteration: 10 loss: -1107.7044669549664\n",
      "iteration: 20 loss: -1057.7200875113037\n",
      "iteration: 30 loss: -1012.8222331653179\n",
      "iteration: 40 loss: -971.6538232772165\n",
      "iteration: 50 loss: -934.4481643118744\n",
      "iteration: 60 loss: -901.8437639566127\n",
      "iteration: 70 loss: -874.2161706178672\n",
      "iteration: 80 loss: -851.5885818342813\n",
      "iteration: 90 loss: -834.100723858019\n",
      "iteration: 100 loss: -821.7760735599995\n",
      "iteration: 110 loss: -814.6721989548012\n",
      "iteration: 120 loss: -812.9095167890248\n",
      "iteration: 130 loss: -816.004968418832\n",
      "iteration: 140 loss: -823.5270235211102\n",
      "iteration: 150 loss: -835.2688743228505\n",
      "iteration: 160 loss: -851.0292041385629\n",
      "iteration: 170 loss: -870.4208299070731\n",
      "iteration: 180 loss: -892.9258811559569\n",
      "iteration: 190 loss: -917.9604971043639\n",
      "iteration: 0 loss: -1157.5391892328755\n",
      "iteration: 10 loss: -1102.7979247194996\n",
      "iteration: 20 loss: -1053.0066246772583\n",
      "iteration: 30 loss: -1008.0160185128979\n",
      "iteration: 40 loss: -966.6293400726884\n",
      "iteration: 50 loss: -929.216952404715\n",
      "iteration: 60 loss: -896.3834144835836\n",
      "iteration: 70 loss: -868.4674596109408\n",
      "iteration: 80 loss: -845.6832109491968\n",
      "iteration: 90 loss: -828.0286609265482\n",
      "iteration: 100 loss: -815.5663245415632\n",
      "iteration: 110 loss: -808.4330700692666\n",
      "iteration: 120 loss: -806.7328742568043\n",
      "iteration: 130 loss: -809.8833022545864\n",
      "iteration: 140 loss: -817.1778322988049\n",
      "iteration: 150 loss: -828.4715864644312\n",
      "iteration: 160 loss: -843.8532719092304\n",
      "iteration: 170 loss: -863.0811820296082\n",
      "iteration: 180 loss: -885.4877259279119\n",
      "iteration: 190 loss: -910.496420760089\n",
      "iteration: 0 loss: -1164.278951939975\n",
      "iteration: 10 loss: -1109.1915268186644\n",
      "iteration: 20 loss: -1058.8521741509833\n",
      "iteration: 30 loss: -1013.6732887226071\n",
      "iteration: 40 loss: -972.40456839469\n",
      "iteration: 50 loss: -935.1800853457097\n",
      "iteration: 60 loss: -902.4738480116046\n",
      "iteration: 70 loss: -874.4196502197209\n",
      "iteration: 80 loss: -851.1443442835118\n",
      "iteration: 90 loss: -832.8488775023404\n",
      "iteration: 100 loss: -819.6282843272513\n",
      "iteration: 110 loss: -811.7315557871451\n",
      "iteration: 120 loss: -809.2434162014076\n",
      "iteration: 130 loss: -811.9968613387556\n",
      "iteration: 140 loss: -819.37486608993\n",
      "iteration: 150 loss: -830.9301345669727\n",
      "iteration: 160 loss: -846.5293025710134\n",
      "iteration: 170 loss: -865.8301696044014\n",
      "iteration: 180 loss: -888.2416259502405\n",
      "iteration: 190 loss: -913.1742146511156\n",
      "iteration: 0 loss: -1162.3482242309155\n",
      "iteration: 10 loss: -1107.2140920415727\n",
      "iteration: 20 loss: -1056.9238164754959\n",
      "iteration: 30 loss: -1011.608340125335\n",
      "iteration: 40 loss: -970.2249496446118\n",
      "iteration: 50 loss: -932.9008540585373\n",
      "iteration: 60 loss: -900.0513853499579\n",
      "iteration: 70 loss: -872.1168190278511\n",
      "iteration: 80 loss: -849.1862972905801\n",
      "iteration: 90 loss: -831.3375450626553\n",
      "iteration: 100 loss: -818.6217056876857\n",
      "iteration: 110 loss: -811.2000950963256\n",
      "iteration: 120 loss: -809.1251453939534\n",
      "iteration: 130 loss: -812.0969353245677\n",
      "iteration: 140 loss: -819.7176431362079\n",
      "iteration: 150 loss: -831.5866269042157\n",
      "iteration: 160 loss: -847.4525748893036\n",
      "iteration: 170 loss: -866.8482172978067\n",
      "iteration: 180 loss: -889.2146219666213\n",
      "iteration: 190 loss: -914.0713233812876\n",
      "iteration: 0 loss: -1178.979136861128\n",
      "iteration: 10 loss: -1123.871287120861\n",
      "iteration: 20 loss: -1073.6599001446532\n",
      "iteration: 30 loss: -1028.4590390425212\n",
      "iteration: 40 loss: -987.1130087827221\n",
      "iteration: 50 loss: -949.6175683129089\n",
      "iteration: 60 loss: -916.4862113756609\n",
      "iteration: 70 loss: -888.0074378582314\n",
      "iteration: 80 loss: -864.3167618688673\n",
      "iteration: 90 loss: -845.608139758423\n",
      "iteration: 100 loss: -832.1506867305343\n",
      "iteration: 110 loss: -824.0775616696424\n",
      "iteration: 120 loss: -821.2204560585307\n",
      "iteration: 130 loss: -823.258723863747\n",
      "iteration: 140 loss: -829.7260786934698\n",
      "iteration: 150 loss: -840.3202265419559\n",
      "iteration: 160 loss: -854.9289474822251\n",
      "iteration: 170 loss: -873.4411557534844\n",
      "iteration: 180 loss: -895.2801171350872\n",
      "iteration: 190 loss: -919.747705427888\n",
      "iteration: 0 loss: -1145.8066641682242\n",
      "iteration: 10 loss: -1091.0807185181095\n",
      "iteration: 20 loss: -1041.486501010629\n",
      "iteration: 30 loss: -997.0561485402638\n",
      "iteration: 40 loss: -956.2375707586316\n",
      "iteration: 50 loss: -919.364693752149\n",
      "iteration: 60 loss: -887.0354822193955\n",
      "iteration: 70 loss: -859.5538694676831\n",
      "iteration: 80 loss: -837.0477292173018\n",
      "iteration: 90 loss: -819.5353830468856\n",
      "iteration: 100 loss: -807.1746922187176\n",
      "iteration: 110 loss: -800.2500003453853\n",
      "iteration: 120 loss: -798.726984801731\n",
      "iteration: 130 loss: -802.2516562252757\n",
      "iteration: 140 loss: -810.3315302921175\n",
      "iteration: 150 loss: -822.6747583391294\n",
      "iteration: 160 loss: -839.1271044482253\n",
      "iteration: 170 loss: -859.3398756901647\n",
      "iteration: 180 loss: -882.5250231267568\n",
      "iteration: 190 loss: -908.1046723265076\n",
      "iteration: 0 loss: -1158.7328164096718\n",
      "iteration: 10 loss: -1103.9387983499457\n",
      "iteration: 20 loss: -1054.1644931388325\n",
      "iteration: 30 loss: -1009.4281190086022\n",
      "iteration: 40 loss: -968.3993756309192\n",
      "iteration: 50 loss: -931.2365839415493\n",
      "iteration: 60 loss: -898.663610311895\n",
      "iteration: 70 loss: -871.1414416182622\n",
      "iteration: 80 loss: -848.5388052811492\n",
      "iteration: 90 loss: -830.9140822989746\n",
      "iteration: 100 loss: -818.3170254796632\n",
      "iteration: 110 loss: -810.8494752138404\n",
      "iteration: 120 loss: -808.7179741718603\n",
      "iteration: 130 loss: -811.6835227763169\n",
      "iteration: 140 loss: -819.2060565594617\n",
      "iteration: 150 loss: -830.8882772963815\n",
      "iteration: 160 loss: -846.5772431088848\n",
      "iteration: 170 loss: -865.9742165462088\n",
      "iteration: 180 loss: -888.5795498226802\n",
      "iteration: 190 loss: -913.7528669072336\n",
      "iteration: 0 loss: -1165.200097432167\n",
      "iteration: 10 loss: -1110.7202608520417\n",
      "iteration: 20 loss: -1061.1690017586716\n",
      "iteration: 30 loss: -1016.5247505912703\n",
      "iteration: 40 loss: -975.707018888258\n",
      "iteration: 50 loss: -938.7228966204026\n",
      "iteration: 60 loss: -906.2629742071526\n",
      "iteration: 70 loss: -878.727991590778\n",
      "iteration: 80 loss: -856.3113030955458\n",
      "iteration: 90 loss: -838.8337957354317\n",
      "iteration: 100 loss: -826.359444725485\n",
      "iteration: 110 loss: -819.1024740856715\n",
      "iteration: 120 loss: -817.2925368110855\n",
      "iteration: 130 loss: -820.4368290149688\n",
      "iteration: 140 loss: -827.91916031495\n",
      "iteration: 150 loss: -839.4449202781745\n",
      "iteration: 160 loss: -854.9293858363149\n",
      "iteration: 170 loss: -874.0765533630592\n",
      "iteration: 180 loss: -896.3612823784915\n",
      "iteration: 190 loss: -921.2364119120475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 loss: -1159.8304830411548\n",
      "iteration: 10 loss: -1104.85209669457\n",
      "iteration: 20 loss: -1054.7262175375342\n",
      "iteration: 30 loss: -1009.6921018226961\n",
      "iteration: 40 loss: -968.5495823448146\n",
      "iteration: 50 loss: -931.418351560325\n",
      "iteration: 60 loss: -898.9063624508615\n",
      "iteration: 70 loss: -871.4868297118242\n",
      "iteration: 80 loss: -849.420620517278\n",
      "iteration: 90 loss: -832.6117974565235\n",
      "iteration: 100 loss: -820.8725226335893\n",
      "iteration: 110 loss: -814.3262397684437\n",
      "iteration: 120 loss: -813.1305141790871\n",
      "iteration: 130 loss: -816.6625723054453\n",
      "iteration: 140 loss: -824.4436714794065\n",
      "iteration: 150 loss: -836.3841599962301\n",
      "iteration: 160 loss: -852.3882009243067\n",
      "iteration: 170 loss: -872.040233156782\n",
      "iteration: 180 loss: -894.7851573867899\n",
      "iteration: 190 loss: -920.144562602166\n",
      "iteration: 0 loss: -1169.9497124917793\n",
      "iteration: 10 loss: -1115.002309743749\n",
      "iteration: 20 loss: -1064.7561592227557\n",
      "iteration: 30 loss: -1019.7637165812481\n",
      "iteration: 40 loss: -978.3748606211097\n",
      "iteration: 50 loss: -940.9421054785198\n",
      "iteration: 60 loss: -908.058667262494\n",
      "iteration: 70 loss: -880.1107772284042\n",
      "iteration: 80 loss: -857.2581462873733\n",
      "iteration: 90 loss: -839.3983970910765\n",
      "iteration: 100 loss: -826.6516643742557\n",
      "iteration: 110 loss: -819.1293040810579\n",
      "iteration: 120 loss: -816.8167435927764\n",
      "iteration: 130 loss: -819.4350011160963\n",
      "iteration: 140 loss: -826.5481637101178\n",
      "iteration: 150 loss: -837.7914959362286\n",
      "iteration: 160 loss: -853.0000888289793\n",
      "iteration: 170 loss: -871.9612622219153\n",
      "iteration: 180 loss: -894.1674744034948\n",
      "iteration: 190 loss: -918.9778515811513\n",
      "iteration: 0 loss: -1175.411374594219\n",
      "iteration: 10 loss: -1120.8907417422881\n",
      "iteration: 20 loss: -1071.3952777572142\n",
      "iteration: 30 loss: -1026.7427357211914\n",
      "iteration: 40 loss: -985.6190429081762\n",
      "iteration: 50 loss: -948.336920555697\n",
      "iteration: 60 loss: -915.4783205588429\n",
      "iteration: 70 loss: -887.4275000066691\n",
      "iteration: 80 loss: -864.3012564571628\n",
      "iteration: 90 loss: -846.1526299998027\n",
      "iteration: 100 loss: -832.9831816778027\n",
      "iteration: 110 loss: -824.943815822211\n",
      "iteration: 120 loss: -822.063836636415\n",
      "iteration: 130 loss: -824.2120782002874\n",
      "iteration: 140 loss: -830.9566664551095\n",
      "iteration: 150 loss: -841.9992706575318\n",
      "iteration: 160 loss: -857.0797627258797\n",
      "iteration: 170 loss: -875.9124413211263\n",
      "iteration: 180 loss: -897.9753123024655\n",
      "iteration: 190 loss: -922.6952736726134\n",
      "iteration: 0 loss: -1149.8301855473298\n",
      "iteration: 10 loss: -1094.854106024452\n",
      "iteration: 20 loss: -1045.1221158180088\n",
      "iteration: 30 loss: -1000.5349357431352\n",
      "iteration: 40 loss: -959.5866284208226\n",
      "iteration: 50 loss: -922.4113312126835\n",
      "iteration: 60 loss: -889.6561893816112\n",
      "iteration: 70 loss: -861.9247687076273\n",
      "iteration: 80 loss: -839.3703040331935\n",
      "iteration: 90 loss: -822.1316115524202\n",
      "iteration: 100 loss: -810.1217284244684\n",
      "iteration: 110 loss: -803.3369357859798\n",
      "iteration: 120 loss: -801.9476997136783\n",
      "iteration: 130 loss: -805.4238734320899\n",
      "iteration: 140 loss: -813.1749573718956\n",
      "iteration: 150 loss: -825.0125882942687\n",
      "iteration: 160 loss: -840.9136034985786\n",
      "iteration: 170 loss: -860.5710877978825\n",
      "iteration: 180 loss: -883.2877241113122\n",
      "iteration: 190 loss: -908.5206789165051\n",
      "iteration: 0 loss: -1150.3978885822826\n",
      "iteration: 10 loss: -1095.4407394200264\n",
      "iteration: 20 loss: -1045.3545357903745\n",
      "iteration: 30 loss: -1000.4546077259978\n",
      "iteration: 40 loss: -959.2028734839319\n",
      "iteration: 50 loss: -922.0822456752767\n",
      "iteration: 60 loss: -889.7376473011318\n",
      "iteration: 70 loss: -862.4412049684624\n",
      "iteration: 80 loss: -840.1882943900879\n",
      "iteration: 90 loss: -822.9912631729874\n",
      "iteration: 100 loss: -810.8491145051047\n",
      "iteration: 110 loss: -803.7867706931002\n",
      "iteration: 120 loss: -801.8779900205658\n",
      "iteration: 130 loss: -805.0070430298917\n",
      "iteration: 140 loss: -812.8112215394711\n",
      "iteration: 150 loss: -824.8710705522462\n",
      "iteration: 160 loss: -841.0684813200588\n",
      "iteration: 170 loss: -861.1454814879199\n",
      "iteration: 180 loss: -884.4218688906822\n",
      "iteration: 190 loss: -910.2125152384631\n",
      "iteration: 0 loss: -1155.9599343215514\n",
      "iteration: 10 loss: -1101.030538656161\n",
      "iteration: 20 loss: -1050.7728264695052\n",
      "iteration: 30 loss: -1005.6029291849705\n",
      "iteration: 40 loss: -964.2425430094388\n",
      "iteration: 50 loss: -926.885805482416\n",
      "iteration: 60 loss: -894.1181593288817\n",
      "iteration: 70 loss: -866.2556608723912\n",
      "iteration: 80 loss: -843.1193434010772\n",
      "iteration: 90 loss: -824.8539149879135\n",
      "iteration: 100 loss: -811.799699032984\n",
      "iteration: 110 loss: -804.1783062694021\n",
      "iteration: 120 loss: -802.0536296772596\n",
      "iteration: 130 loss: -804.9828119965873\n",
      "iteration: 140 loss: -812.2557548567469\n",
      "iteration: 150 loss: -823.5842681070083\n",
      "iteration: 160 loss: -839.0300425733021\n",
      "iteration: 170 loss: -858.3424902790895\n",
      "iteration: 180 loss: -880.801715773884\n",
      "iteration: 190 loss: -905.7997803461267\n",
      "iteration: 0 loss: -1164.2563213069086\n",
      "iteration: 10 loss: -1109.7838461309268\n",
      "iteration: 20 loss: -1060.609153538603\n",
      "iteration: 30 loss: -1016.3731918735347\n",
      "iteration: 40 loss: -975.6610952157264\n",
      "iteration: 50 loss: -938.6094069087354\n",
      "iteration: 60 loss: -905.8261967307963\n",
      "iteration: 70 loss: -877.8734978925487\n",
      "iteration: 80 loss: -854.9247813777241\n",
      "iteration: 90 loss: -837.1402855140194\n",
      "iteration: 100 loss: -824.7656341075513\n",
      "iteration: 110 loss: -817.6748929290343\n",
      "iteration: 120 loss: -815.9380479216717\n",
      "iteration: 130 loss: -819.2796139764599\n",
      "iteration: 140 loss: -826.9300735301779\n",
      "iteration: 150 loss: -838.4526860613845\n",
      "iteration: 160 loss: -853.8706485619207\n",
      "iteration: 170 loss: -873.1127738039139\n",
      "iteration: 180 loss: -895.5506058580023\n",
      "iteration: 190 loss: -920.490192296938\n",
      "iteration: 0 loss: -1170.5828596389551\n",
      "iteration: 10 loss: -1115.3765522137355\n",
      "iteration: 20 loss: -1064.6494620821436\n",
      "iteration: 30 loss: -1018.9240791290154\n",
      "iteration: 40 loss: -976.9353507660417\n",
      "iteration: 50 loss: -938.8326816646869\n",
      "iteration: 60 loss: -905.2969434994744\n",
      "iteration: 70 loss: -876.7834442459726\n",
      "iteration: 80 loss: -853.2608398056416\n",
      "iteration: 90 loss: -834.9185215073358\n",
      "iteration: 100 loss: -821.719144504392\n",
      "iteration: 110 loss: -813.6226651107548\n",
      "iteration: 120 loss: -810.9440118038087\n",
      "iteration: 130 loss: -813.5323279459013\n",
      "iteration: 140 loss: -820.721165939957\n",
      "iteration: 150 loss: -831.8956209399173\n",
      "iteration: 160 loss: -847.0106207755657\n",
      "iteration: 170 loss: -865.9953650887509\n",
      "iteration: 180 loss: -888.3486526896352\n",
      "iteration: 190 loss: -913.3719256097129\n",
      "iteration: 0 loss: -1161.653223020965\n",
      "iteration: 10 loss: -1106.4770071529936\n",
      "iteration: 20 loss: -1055.9538728098746\n",
      "iteration: 30 loss: -1010.5803935439492\n",
      "iteration: 40 loss: -968.9926330080091\n",
      "iteration: 50 loss: -931.4413525143857\n",
      "iteration: 60 loss: -898.5092002831825\n",
      "iteration: 70 loss: -870.5978601902688\n",
      "iteration: 80 loss: -847.6478170519046\n",
      "iteration: 90 loss: -829.7593710891078\n",
      "iteration: 100 loss: -817.0346097801845\n",
      "iteration: 110 loss: -809.7525264617947\n",
      "iteration: 120 loss: -807.762413754631\n",
      "iteration: 130 loss: -810.6816583923534\n",
      "iteration: 140 loss: -818.0821342890972\n",
      "iteration: 150 loss: -829.6059410726439\n",
      "iteration: 160 loss: -845.0883809276794\n",
      "iteration: 170 loss: -864.3376299445129\n",
      "iteration: 180 loss: -886.836960220397\n",
      "iteration: 190 loss: -911.9747118962329\n"
     ]
    }
   ],
   "source": [
    "plt.plot(np.linspace(0, 4, 20), results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d8f2e67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35449635313285105"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series, sim_fc = sim.getTimeseries(n_timesteps)\n",
    "c = np.corrcoef(np.triu(fc).flatten(), np.triu(J_hist[-2]).flatten())[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfdbaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = []\n",
    "# for T in np.linspace(1.53, 3.28, 2):\n",
    "beta = 1\n",
    "sim = IsingSimulation(n_rois, beta, coupling_mat = True, J=fc)\n",
    "n_timesteps = 116\n",
    "E1 = M1 = E2 = M2 = 0\n",
    "M = []\n",
    "corr = []\n",
    "for i in range(eqSteps):         # equilibrate\n",
    "    if i%1000 == 0:\n",
    "        print(i)\n",
    "        time_series, sim_fc = sim.getTimeseries(n_timesteps)\n",
    "        c = np.corrcoef(np.triu(fc).flatten(), np.triu(sim_fc).flatten())[0, 1]\n",
    "        corr.append(c)\n",
    "    sim.step()           # Monte Carlo moves\n",
    "    E.append(sim.calcEnergy())\n",
    "    M.append(sim.calcMag())\n",
    "plt.plot(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ded438f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb3799bb9d0>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr = []\n",
    "for i in J_hist:\n",
    "#     corr.append(loss(i, bold_bin))\n",
    "    c = np.corrcoef(np.triu(fc).flatten(), np.triu(i).flatten())[0, 1]\n",
    "    corr.append(c)\n",
    "plt.plot(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06d1fa73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116, 116)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.63941434,  0.91259763, -1.38667675, ...,  0.97732562,\n",
       "        -1.271148  ,  0.80478887],\n",
       "       [ 0.64981318,  0.88304238, -1.42149644, ...,  0.96666799,\n",
       "        -1.29400744,  0.7888905 ],\n",
       "       [-1.31800438, -1.07956325,  0.5649875 , ..., -1.07396037,\n",
       "         0.73111962, -1.24096553],\n",
       "       ...,\n",
       "       [ 0.67193944,  0.92389227, -1.4235992 , ...,  0.91572338,\n",
       "        -1.25441529,  0.77278526],\n",
       "       [-1.31786791, -1.09330758,  0.5459214 , ..., -1.0628717 ,\n",
       "         0.69546654, -1.24872447],\n",
       "       [ 0.65624307,  0.9102043 , -1.43025217, ...,  0.93631531,\n",
       "        -1.26470871,  0.73094612]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J = np.random.uniform(0, 1, size=(n_rois, n_rois))\n",
    "J = (J + J.T)/2 # making it symmetric\n",
    "np.fill_diagonal(J, 1)\n",
    "gradient(J, bold_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77307f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('neuro')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb4c035b7b14a003fd61df7b977ac5b80c21c00f39fe897eeaf3bf1f3b9ed2c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
