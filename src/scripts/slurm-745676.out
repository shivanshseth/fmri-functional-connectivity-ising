==========================================
SLURM_JOB_ID = 745676
SLURM_NODELIST = gnode079
SLURM_JOB_GPUS = 1,2
==========================================
/home/shivansh.seth/miniconda3/envs/brain/lib/python3.8/site-packages/nilearn/input_data/__init__.py:27: FutureWarning: The import path 'nilearn.input_data' is deprecated in version 0.9. Importing from 'nilearn.input_data' will be possible at least until release 0.13.0. Please import from 'nilearn.maskers' instead.
  warnings.warn(message, FutureWarning)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
/home/shivansh.seth/fmri-functional-connectivity-ising/src/scripts/abide_dataset.py:138: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  excluded_subjects = excluded_subjects.append(this_pheno)
iteration: 0 loss: 120.40718699702384 grad: 580.6982922104579
iteration: 10 loss: 1.072807368002197 grad: 0.4368679959807945
iteration: 20 loss: 0.663636834900651 grad: -0.0033847383687140463
iteration: 30 loss: 0.5001150402582756 grad: -0.0846617164008433
iteration: 40 loss: 0.4067435980934368 grad: -0.10718681362668193
iteration: 50 loss: 0.3450199792398714 grad: -0.11256328712303565
iteration: 0 loss: 82.0087364863266 grad: 540.1065297659694
iteration: 10 loss: 1.4097465679685477 grad: 0.2882756383209768
iteration: 20 loss: 0.862637297873665 grad: -0.08226086740302521
iteration: 30 loss: 0.6363953755510672 grad: -0.13204310624756216
iteration: 40 loss: 0.5083633380922534 grad: -0.13789381708042198
iteration: 50 loss: 0.42493475921384577 grad: -0.13338966391017154
iteration: 60 loss: 0.3658795212941186 grad: -0.12627117276286637
iteration: 0 loss: 80.70546884968404 grad: 547.3282484623153
iteration: 10 loss: 1.3340590147183156 grad: -0.022669418638457457
iteration: 20 loss: 0.819271200011909 grad: -0.19290648232294416
iteration: 30 loss: 0.6068284624737068 grad: -0.1952423425622225
iteration: 40 loss: 0.4864933083008509 grad: -0.18058801578044142
iteration: 50 loss: 0.4079204686749191 grad: -0.16492374574403196
iteration: 0 loss: 132.8000167836981 grad: 604.7501541482718
iteration: 10 loss: 1.1617007692628414 grad: 0.30902352816063494
iteration: 20 loss: 0.716360454785238 grad: -0.030844390380155698
iteration: 30 loss: 0.5353341287665733 grad: -0.08605346051271948
iteration: 40 loss: 0.4324417679928791 grad: -0.09873046303954033
iteration: 50 loss: 0.36489655985993025 grad: -0.09995917976650517
iteration: 0 loss: 86.48552231499512 grad: 522.1915225335117
iteration: 10 loss: 1.432795500552526 grad: 0.27631905773726617
iteration: 20 loss: 0.8760962334564201 grad: -0.07756357446524961
iteration: 30 loss: 0.6454940638524765 grad: -0.13065444174575358
iteration: 40 loss: 0.5151447886472934 grad: -0.13902326426741063
iteration: 50 loss: 0.4302952097929268 grad: -0.13595621526130697
iteration: 60 loss: 0.3702839026254878 grad: -0.12960920131202494
iteration: 0 loss: 71.09716095487498 grad: 458.57611595008706
iteration: 10 loss: 1.4152144729896463 grad: 0.5732905245313218
iteration: 20 loss: 0.8661268631525342 grad: 0.010200250783300172
iteration: 30 loss: 0.6391123965816984 grad: -0.09063286118122464
iteration: 40 loss: 0.5104764917577168 grad: -0.11638708406139393
iteration: 50 loss: 0.42657501833962835 grad: -0.1214219189053972
iteration: 60 loss: 0.367152108517654 grad: -0.11947303757187573
iteration: 0 loss: 80.34701170598038 grad: 536.7984213170096
iteration: 10 loss: 1.3972010584660977 grad: 0.5438237683605844
iteration: 20 loss: 0.8623993467857541 grad: 0.05003325301459917
iteration: 30 loss: 0.6379316532771885 grad: -0.05133728560321105
iteration: 40 loss: 0.51028120279625 grad: -0.08299331057905214
iteration: 50 loss: 0.4268879563170975 grad: -0.09341248031441998
iteration: 60 loss: 0.36776126864110253 grad: -0.09576358464705235
iteration: 0 loss: 49.50193690493973 grad: 434.93415008120405
iteration: 10 loss: 1.7520837576960895 grad: 0.8020774583194543
iteration: 20 loss: 1.0343338606340198 grad: 0.11209687307222059
iteration: 30 loss: 0.7449747168959054 grad: -0.028223716491366332
iteration: 40 loss: 0.5852323669922209 grad: -0.0723719377191977
iteration: 50 loss: 0.4831494593216286 grad: -0.0878285768001551
iteration: 60 loss: 0.4120009011100013 grad: -0.0925029156665383
iteration: 0 loss: 139.2226292618023 grad: 567.0466485489158
iteration: 10 loss: 1.2007109394744475 grad: 0.24981767613019487
iteration: 20 loss: 0.7216348333071106 grad: -0.05397714245227645
iteration: 30 loss: 0.5324254626783923 grad: -0.0960650205893876
iteration: 40 loss: 0.42706032209455524 grad: -0.10338736478397773
iteration: 50 loss: 0.3588385876084265 grad: -0.10212540581833061
iteration: 0 loss: 79.0384483765576 grad: 524.3619217845579
iteration: 10 loss: 1.412190026962807 grad: 0.4601735912787421
iteration: 20 loss: 0.8687855731248585 grad: 0.0060451732701668984
iteration: 30 loss: 0.6422052738028221 grad: -0.08336295846738496
iteration: 40 loss: 0.5135848015424099 grad: -0.10851397940631383
iteration: 50 loss: 0.4296101331670302 grad: -0.1146198463665788
iteration: 60 loss: 0.37008484602698777 grad: -0.11385141575174568
iteration: 0 loss: 53.32914269407328 grad: 475.3347369678059
iteration: 10 loss: 1.6157727624437361 grad: 0.26696490695160796
iteration: 20 loss: 0.9745932364515035 grad: -0.13458996076454788
iteration: 30 loss: 0.7092248497981266 grad: -0.18525131553235927
iteration: 40 loss: 0.5605507668344051 grad: -0.1860527010321128
iteration: 50 loss: 0.4646369412059192 grad: -0.17610882981977977
iteration: 60 loss: 0.39734444919750705 grad: -0.1641726493924853
iteration: 0 loss: 85.45701720263497 grad: 567.0492721163537
iteration: 10 loss: 1.3221465452176993 grad: -0.11117792301058128
iteration: 20 loss: 0.8122566235377401 grad: -0.23844770372452828
iteration: 30 loss: 0.601993184346115 grad: -0.23386049629111608
iteration: 40 loss: 0.48272299204323976 grad: -0.21440984376720063
iteration: 50 loss: 0.4047808115131126 grad: -0.1948884609134438
iteration: 0 loss: 85.5045671254914 grad: 474.1732448888653
iteration: 10 loss: 1.4332984928995425 grad: 0.9600032613797362
iteration: 20 loss: 0.8744460482178262 grad: 0.2634622916728188
iteration: 30 loss: 0.6437047019728859 grad: 0.08783442298736058
iteration: 40 loss: 0.5135335248057593 grad: 0.018911838701126088
iteration: 50 loss: 0.42889098189351155 grad: -0.013667431724423498
iteration: 60 loss: 0.3690627067598226 grad: -0.030631500940284415
iteration: 0 loss: 72.34800384364371 grad: 531.9878088489505
iteration: 10 loss: 1.4145940936454777 grad: 0.017852650240273646
iteration: 20 loss: 0.8730941658131169 grad: -0.20921374656196812
iteration: 30 loss: 0.6453813260134418 grad: -0.21924956352410985
iteration: 40 loss: 0.5157484046259758 grad: -0.2046446867529488
iteration: 50 loss: 0.4310329371696339 grad: -0.18732324625546934
iteration: 60 loss: 0.3709757194593211 grad: -0.17134806071147302
iteration: 0 loss: 99.44709953473925 grad: 543.4653010304553
iteration: 10 loss: 1.2813444162600882 grad: 0.3027183318561261
iteration: 20 loss: 0.7937709357937902 grad: -0.023838023821068532
iteration: 30 loss: 0.5906619919449606 grad: -0.08184486741168262
iteration: 40 loss: 0.4750252874007195 grad: -0.09589711369935439
iteration: 50 loss: 0.3992493139728388 grad: -0.09784501295957534
iteration: 0 loss: 112.56130209252181 grad: 619.5786698105583
iteration: 10 loss: 1.1581149189770175 grad: 0.12940250675742918
iteration: 20 loss: 0.7011480822727456 grad: -0.16770023741708573
iteration: 30 loss: 0.5208961328092052 grad: -0.18659021693306693
iteration: 40 loss: 0.419627489087361 grad: -0.177752752699411
iteration: 50 loss: 0.35357253256725846 grad: -0.16492536487042547
iteration: 0 loss: 88.58938485032901 grad: 527.240995882454
iteration: 10 loss: 1.3315688192902102 grad: 0.208345810077586
iteration: 20 loss: 0.8221836954077265 grad: -0.09346309269564074
iteration: 30 loss: 0.6102108326664496 grad: -0.13768452683685517
iteration: 40 loss: 0.4896434055160419 grad: -0.14244484833592436
iteration: 50 loss: 0.41072660593884147 grad: -0.13761717354129094
iteration: 60 loss: 0.35464478494395735 grad: -0.1303435880726853
iteration: 0 loss: 66.44058192956527 grad: 486.3268456164184
iteration: 10 loss: 1.5465563109408074 grad: 0.6476181820687104
iteration: 20 loss: 0.9372367398407898 grad: 0.04631828608249157
iteration: 30 loss: 0.684812330341546 grad: -0.06750957946692888
iteration: 40 loss: 0.5428543469486778 grad: -0.0995508763480867
iteration: 50 loss: 0.45097369000202225 grad: -0.10818119787478173
iteration: 60 loss: 0.38633979695623566 grad: -0.10852773684229498
iteration: 0 loss: 156.4175314844937 grad: 621.542300654753
iteration: 10 loss: 1.111410598993478 grad: 0.5366652489351925
iteration: 20 loss: 0.66507775852882 grad: 0.09223516258326038
iteration: 30 loss: 0.49466838883753633 grad: -0.00010327814164202455
iteration: 40 loss: 0.3995968511731421 grad: -0.03386204883094272
iteration: 50 loss: 0.3376560330929847 grad: -0.04871094648867548
iteration: 0 loss: 73.41255234719046 grad: 493.6910497787229
iteration: 10 loss: 1.5279537310350613 grad: 0.6455509575714298
iteration: 20 loss: 0.9271293144449207 grad: 0.0800834042795204
iteration: 30 loss: 0.6789751929340624 grad: -0.041358465546228664
iteration: 40 loss: 0.5394219731164909 grad: -0.07986763785703227
iteration: 50 loss: 0.4489942396950772 grad: -0.09300434497298807
iteration: 60 loss: 0.38528501352935896 grad: -0.09650025469902025
iteration: 0 loss: 90.03360293289498 grad: 564.3653397500395
iteration: 10 loss: 1.3106014849777652 grad: 0.12912115603718888
iteration: 20 loss: 0.8105267001809889 grad: -0.10618314712864406
iteration: 30 loss: 0.6020920198460854 grad: -0.13620361876707499
iteration: 40 loss: 0.48339729328100395 grad: -0.1361120612083509
iteration: 50 loss: 0.40565057418065076 grad: -0.12950001170110925
iteration: 0 loss: 95.9663372684188 grad: 589.0397662120266
iteration: 10 loss: 1.2600066440432287 grad: 0.07602290472447376
iteration: 20 loss: 0.7691811973768794 grad: -0.19009444498906733
iteration: 30 loss: 0.5700218367274746 grad: -0.2055846811925935
iteration: 40 loss: 0.45760901014951244 grad: -0.19294932803150416
iteration: 50 loss: 0.38427985774118995 grad: -0.17712387473071525
iteration: 0 loss: 96.95136695010379 grad: 468.73469015600074
iteration: 10 loss: 1.344861479186048 grad: 0.730982393041649
iteration: 20 loss: 0.8293643826783327 grad: 0.113773342514668
iteration: 30 loss: 0.6151442976644664 grad: -0.014452988294173911
iteration: 40 loss: 0.49328411756108503 grad: -0.0572250712029234
iteration: 50 loss: 0.4135202782111016 grad: -0.0737112413278793
iteration: 60 loss: 0.3568473293869994 grad: -0.07989672632204622
iteration: 0 loss: 54.362083257713124 grad: 520.3460177160159
iteration: 10 loss: 1.5629895988310059 grad: -0.05601130484342121
iteration: 20 loss: 0.943443115039124 grad: -0.2819715730048322
iteration: 30 loss: 0.6880753493988695 grad: -0.2748506037006266
iteration: 40 loss: 0.5448661463121673 grad: -0.24858069068395072
iteration: 50 loss: 0.4523357963657459 grad: -0.2233171710414928
iteration: 60 loss: 0.3873188077395753 grad: -0.20167204864882493
iteration: 0 loss: 79.20631881898771 grad: 545.436501739572
iteration: 10 loss: 1.3354653385784496 grad: 0.11631380989211612
iteration: 20 loss: 0.8313337875928152 grad: -0.1550523201917441
iteration: 30 loss: 0.6185715345124911 grad: -0.17970023491887838
iteration: 40 loss: 0.49667308220283635 grad: -0.1736347935205515
iteration: 50 loss: 0.41658840544319403 grad: -0.16209051879175862
iteration: 60 loss: 0.3595681683466616 grad: -0.15027263978591066
iteration: 0 loss: 87.14527807220148 grad: 524.8253529567696
iteration: 10 loss: 1.4031372294486408 grad: 0.4631393303244963
iteration: 20 loss: 0.8501669515465189 grad: 0.003049025031269614
iteration: 30 loss: 0.6261206796267619 grad: -0.08375452379869013
iteration: 40 loss: 0.49998715891524625 grad: -0.10768723332795278
iteration: 50 loss: 0.41794038954429363 grad: -0.11332501592707507
iteration: 60 loss: 0.3598995272521444 grad: -0.11242886403752102
iteration: 0 loss: 170.6933304193051 grad: 623.9433566270153
iteration: 10 loss: 1.1336729233717877 grad: 0.7919754870668447
iteration: 20 loss: 0.6701180685411766 grad: 0.1817399017515891
iteration: 30 loss: 0.494357088594618 grad: 0.05284002057544972
iteration: 40 loss: 0.3971427162952585 grad: 0.003273353769432061
iteration: 50 loss: 0.3342655531743565 grad: -0.02026735138776161
iteration: 0 loss: 153.28635367982088 grad: 601.8610901910828
iteration: 10 loss: 1.225018412837182 grad: 0.5116214339789948
iteration: 20 loss: 0.698228697105895 grad: 0.0310130401076685
iteration: 30 loss: 0.5105539522366432 grad: -0.0544517311924589
iteration: 40 loss: 0.4080205160678154 grad: -0.0771474701804083
iteration: 50 loss: 0.3421931392791025 grad: -0.08353431741165543
iteration: 0 loss: 69.18382480668184 grad: 541.7232793086773
iteration: 10 loss: 1.3950829577897408 grad: -0.19534340846943465
iteration: 20 loss: 0.8608120017216524 grad: -0.29192860304906654
iteration: 30 loss: 0.6363430804226069 grad: -0.26831400259129373
iteration: 40 loss: 0.5087252735450999 grad: -0.23873277497208178
iteration: 50 loss: 0.4253926177060004 grad: -0.21318069951329596
iteration: 60 loss: 0.36633735193109634 grad: -0.19209745194059627
iteration: 0 loss: 76.09342782252733 grad: 493.63701795652076
iteration: 10 loss: 1.4613220639424649 grad: 0.3993337076609489
iteration: 20 loss: 0.8901377636453948 grad: -0.047812609220987325
iteration: 30 loss: 0.6540851890713384 grad: -0.11677405553884614
iteration: 40 loss: 0.5208595333555919 grad: -0.1301675512505829
iteration: 50 loss: 0.4342788880337387 grad: -0.1293050686513006
iteration: 60 loss: 0.3731422381115408 grad: -0.12415199650468155
iteration: 0 loss: 186.5003568322898 grad: 661.1601125664328
iteration: 10 loss: 1.0097506742591909 grad: 0.24281187563016107
iteration: 20 loss: 0.607630381771659 grad: -0.10077398038443586
iteration: 30 loss: 0.45512320772625325 grad: -0.1279212300065074
iteration: 40 loss: 0.3696231805895216 grad: -0.12661537486565835
iteration: 0 loss: 80.3322912580857 grad: 570.6794853866097
iteration: 10 loss: 1.4368082340261026 grad: 0.08374069603524467
iteration: 20 loss: 0.8850003998950342 grad: -0.1231343010087156
iteration: 30 loss: 0.6533714859404592 grad: -0.146618218428882
iteration: 40 loss: 0.5218078923096321 grad: -0.14500179167120192
iteration: 50 loss: 0.43596382145692175 grad: -0.1376795692355467
iteration: 60 loss: 0.37516957577050064 grad: -0.12927751699018603
iteration: 0 loss: 82.92002646621458 grad: 536.4026342454213
iteration: 10 loss: 1.4117255009814471 grad: 0.24172348815385994
iteration: 20 loss: 0.8590488942399216 grad: -0.09017683461759045
iteration: 30 loss: 0.6326670884738467 grad: -0.14012353997246696
iteration: 40 loss: 0.5051257257964608 grad: -0.1466492474397919
iteration: 50 loss: 0.4221893246310422 grad: -0.14229605520069577
iteration: 60 loss: 0.3635395331526855 grad: -0.13500529365275798
iteration: 0 loss: 114.6073521510798 grad: 558.399586910445
iteration: 10 loss: 1.2564351271428842 grad: 0.2864617355232699
iteration: 20 loss: 0.7623068119415124 grad: -0.03933377852640686
iteration: 30 loss: 0.5640473513438503 grad: -0.09749018477581264
iteration: 40 loss: 0.45275230511990894 grad: -0.11009027232361904
iteration: 50 loss: 0.38033796863146563 grad: -0.11050627104593017
iteration: 0 loss: 112.00743614602503 grad: 579.118068530857
iteration: 10 loss: 1.2204845283654735 grad: 0.16431041784076572
iteration: 20 loss: 0.7473569698916187 grad: -0.10717183898666614
iteration: 30 loss: 0.5567698131275889 grad: -0.1445871352084422
iteration: 40 loss: 0.44894070488432836 grad: -0.14715726882119073
iteration: 50 loss: 0.37834741253432597 grad: -0.14133507548418006
iteration: 0 loss: 92.21823671499727 grad: 561.2995444238849
iteration: 10 loss: 1.3111607954857722 grad: 0.3175828343972005
iteration: 20 loss: 0.8019748526385229 grad: -0.08626021586198421
iteration: 30 loss: 0.5946231244438835 grad: -0.14642266951719313
iteration: 40 loss: 0.47729144794894557 grad: -0.15491910818660357
iteration: 50 loss: 0.40061052056047297 grad: -0.1506390342573421
iteration: 0 loss: 98.84685610592524 grad: 537.0852022352128
iteration: 10 loss: 1.2676694722272672 grad: 0.4172726765589398
iteration: 20 loss: 0.787908269601923 grad: -0.03128829366416726
iteration: 30 loss: 0.5881799694637845 grad: -0.10503692415196893
iteration: 40 loss: 0.47382029055196734 grad: -0.12223148821625283
iteration: 50 loss: 0.3985566639007216 grad: -0.1240357548672266
iteration: 0 loss: 108.89974227124486 grad: 624.5709718503538
iteration: 10 loss: 1.137525052009716 grad: 0.01206341877754177
iteration: 20 loss: 0.7004467640475592 grad: -0.20029327340255132
iteration: 30 loss: 0.5237189434030204 grad: -0.20133303574542455
iteration: 40 loss: 0.4232861756083201 grad: -0.18631517563901878
iteration: 50 loss: 0.35733616522198147 grad: -0.17044218082264231
iteration: 0 loss: 82.38402863689845 grad: 508.2521357936216
iteration: 10 loss: 1.3384063012175498 grad: 0.5167207437976966
iteration: 20 loss: 0.8258528859584079 grad: 0.017187201585448095
iteration: 30 loss: 0.6126116862924391 grad: -0.07419827587929939
iteration: 40 loss: 0.4912381486069121 grad: -0.10001702143079108
iteration: 50 loss: 0.4117765097258102 grad: -0.10684553102098279
iteration: 60 loss: 0.3553161059227767 grad: -0.10681479418708262
iteration: 0 loss: 73.18391514254266 grad: 520.1097459288635
iteration: 10 loss: 1.444057211904692 grad: 0.30387447917659327
iteration: 20 loss: 0.8836693569868327 grad: -0.09427317573966218
iteration: 30 loss: 0.6506113681299955 grad: -0.1486991904453287
iteration: 40 loss: 0.5187461045514449 grad: -0.1542863936546779
iteration: 50 loss: 0.4329091536402219 grad: -0.14854391764779884
iteration: 60 loss: 0.37222199777757353 grad: -0.14007899523278283
iteration: 0 loss: 133.72709316166367 grad: 547.3218323822259
iteration: 10 loss: 1.1488222520134017 grad: 0.7532279288590953
iteration: 20 loss: 0.7035530742073619 grad: 0.1367749727227771
iteration: 30 loss: 0.5254875077283611 grad: 0.008332802682302501
iteration: 40 loss: 0.42469284393882845 grad: -0.03622446020785271
iteration: 50 loss: 0.3585905099348979 grad: -0.05482457007868008
iteration: 0 loss: 52.19324470505357 grad: 468.6748079370121
iteration: 10 loss: 1.6698558698173491 grad: 0.38205061380125094
iteration: 20 loss: 0.9958646639452954 grad: -0.09724084853634078
iteration: 30 loss: 0.7212336596581239 grad: -0.16241649896276106
iteration: 40 loss: 0.5685376435150663 grad: -0.16901179411823414
iteration: 50 loss: 0.47047833706991776 grad: -0.16230548320236624
iteration: 60 loss: 0.40188936358158983 grad: -0.15250798502910193
iteration: 0 loss: 187.60025318878417 grad: 682.762314689896
iteration: 10 loss: 1.0011573764245854 grad: 0.3142956624325719
iteration: 20 loss: 0.5877157702683359 grad: -0.09426719947118979
iteration: 30 loss: 0.43583535459021566 grad: -0.11100274237579573
iteration: 40 loss: 0.351948256406269 grad: -0.10860478604487177
iteration: 0 loss: 104.71686978399856 grad: 500.17699697673515
iteration: 10 loss: 1.2904428785600843 grad: 0.784194049898094
iteration: 20 loss: 0.789815860641945 grad: 0.1532155472415058
iteration: 30 loss: 0.5859388064781424 grad: 0.01591196071871065
iteration: 40 loss: 0.47046108620948374 grad: -0.03309206657084102
iteration: 50 loss: 0.39494494772737776 grad: -0.053916484619107925
iteration: 0 loss: 82.6876990588003 grad: 606.1334516973534
iteration: 10 loss: 1.2944672155593004 grad: -0.26311915518245244
iteration: 20 loss: 0.8019893674079206 grad: -0.30255022113256025
iteration: 30 loss: 0.5967693673209479 grad: -0.26950493800644176
iteration: 40 loss: 0.4796933133491786 grad: -0.23703190134201718
iteration: 50 loss: 0.40288257617017237 grad: -0.21048558751305074
iteration: 0 loss: 144.66114773606424 grad: 558.1805897897273
iteration: 10 loss: 1.1392795215041671 grad: 0.8990849626682136
iteration: 20 loss: 0.6991829357630195 grad: 0.23347613523816013
iteration: 30 loss: 0.5225526363112128 grad: 0.07810446274984903
iteration: 40 loss: 0.4225857962743579 grad: 0.016526508463479635
iteration: 50 loss: 0.35703280702050094 grad: -0.013202066527168715
iteration: 0 loss: 126.41238303285974 grad: 483.75458099371514
iteration: 10 loss: 1.2722635707243832 grad: 0.9231665453262436
iteration: 20 loss: 0.7764618557389027 grad: 0.1951130056644901
iteration: 30 loss: 0.5765628834046352 grad: 0.04116964061664688
iteration: 40 loss: 0.46365693140267633 grad: -0.014308464084472053
iteration: 50 loss: 0.3898730859958629 grad: -0.038592475307901976
iteration: 0 loss: 74.91393265323805 grad: 528.3934319814376
iteration: 10 loss: 1.42365024377283 grad: 0.27155252981961125
iteration: 20 loss: 0.8723284102428476 grad: -0.11061123641214654
iteration: 30 loss: 0.6436409808161856 grad: -0.16012366321875182
iteration: 40 loss: 0.5140723533243297 grad: -0.16385774828033786
iteration: 50 loss: 0.42958252101710637 grad: -0.15705975076582918
iteration: 60 loss: 0.36975112459533027 grad: -0.1478181958173751
iteration: 0 loss: 119.6011881705101 grad: 589.3285354413038
iteration: 10 loss: 1.2027057804220518 grad: 0.2828512129604236
iteration: 20 loss: 0.7312856766448849 grad: -0.055581797043310996
iteration: 30 loss: 0.5430991927916528 grad: -0.10949590172285639
iteration: 40 loss: 0.43709092407218786 grad: -0.1194916002651739
iteration: 50 loss: 0.36787963313351857 grad: -0.11824242056306694
iteration: 0 loss: 136.65222612359548 grad: 591.9859130965483
iteration: 10 loss: 1.1480190103827475 grad: 0.49130209769148603
iteration: 20 loss: 0.6934600676967503 grad: 0.01819538737418039
iteration: 30 loss: 0.5154457194349479 grad: -0.06278310494796253
iteration: 40 loss: 0.4157325969305351 grad: -0.0859763342860014
iteration: 50 loss: 0.3507358706629847 grad: -0.09267254151258816
iteration: 0 loss: 138.27809223732254 grad: 611.9625778443242
iteration: 10 loss: 1.1744445381020177 grad: 0.15229764806721124
iteration: 20 loss: 0.6939415349069432 grad: -0.11214181496181334
iteration: 30 loss: 0.5102906553098695 grad: -0.13878105556995085
iteration: 40 loss: 0.408853010679195 grad: -0.13680686081286397
iteration: 50 loss: 0.34338875994754664 grad: -0.1293571359491868
iteration: 0 loss: 77.18052327751614 grad: 546.5739914103449
iteration: 10 loss: 1.3529879405576304 grad: 0.18084103005888655
iteration: 20 loss: 0.8327547608507964 grad: -0.14280857992614843
iteration: 30 loss: 0.6170508368193789 grad: -0.17559455883727332
iteration: 40 loss: 0.4943106176409525 grad: -0.17197036992234033
iteration: 50 loss: 0.41399134977423685 grad: -0.16138379971070518
iteration: 60 loss: 0.3569562776089118 grad: -0.15001455650615256
iteration: 0 loss: 115.27460347199262 grad: 626.2430759059037
iteration: 10 loss: 1.1597144623890265 grad: -0.08662030984122862
iteration: 20 loss: 0.7053433302140051 grad: -0.19847075767723113
iteration: 30 loss: 0.5251090820097901 grad: -0.19477668081645333
iteration: 40 loss: 0.4236079577618842 grad: -0.17911604785575008
iteration: 50 loss: 0.35726215415136475 grad: -0.16346091238689026
iteration: 0 loss: 109.25275931765451 grad: 558.3566357052991
iteration: 10 loss: 1.2775473766699756 grad: 0.48235530763576695
iteration: 20 loss: 0.7856642108552644 grad: 0.029673776891075306
iteration: 30 loss: 0.5838825150542768 grad: -0.05859279347432049
iteration: 40 loss: 0.4693465935444527 grad: -0.08540726658926259
iteration: 50 loss: 0.3943612260735873 grad: -0.09372738410767398
iteration: 0 loss: 60.48596207895769 grad: 500.4691990752221
iteration: 10 loss: 1.5634199968249463 grad: 0.09011137179264767
iteration: 20 loss: 0.9548052741798446 grad: -0.1553484953957652
iteration: 30 loss: 0.6992865944552746 grad: -0.17675331149848653
iteration: 40 loss: 0.5549946528603455 grad: -0.1696394801908464
iteration: 50 loss: 0.4614004680903566 grad: -0.15767217410534137
iteration: 60 loss: 0.39546718764540856 grad: -0.14570300530712088
iteration: 0 loss: 83.13294734490661 grad: 542.9909461391799
iteration: 10 loss: 1.3848553373504728 grad: 0.2290170464041916
iteration: 20 loss: 0.8480711891681328 grad: -0.12672800404611645
iteration: 30 loss: 0.626370158036893 grad: -0.16802416381332175
iteration: 40 loss: 0.5008927374538789 grad: -0.1673367847325921
iteration: 50 loss: 0.4190731074970906 grad: -0.1581650492656564
iteration: 60 loss: 0.3611079784456127 grad: -0.14761822180534778
iteration: 0 loss: 49.39123491784982 grad: 524.7208194533384
iteration: 10 loss: 1.6492766520326851 grad: -0.21609418846153952
iteration: 20 loss: 0.9930757435064482 grad: -0.3074964162870695
iteration: 30 loss: 0.7214506130997258 grad: -0.2754366301806977
iteration: 40 loss: 0.5695516310013938 grad: -0.24157557737202345
iteration: 50 loss: 0.4717054414921387 grad: -0.21368259622905955
iteration: 60 loss: 0.4031374892088438 grad: -0.19121741891530464
iteration: 0 loss: 93.84369282298229 grad: 557.3376868194291
iteration: 10 loss: 1.3214323015326128 grad: 0.35700634676173226
iteration: 20 loss: 0.8182505331107511 grad: -0.02381305154068844
iteration: 30 loss: 0.6080815830799277 grad: -0.09055841632448508
iteration: 40 loss: 0.48822451645380416 grad: -0.10755006819251334
iteration: 50 loss: 0.4096614205443323 grad: -0.11034068937681037
iteration: 60 loss: 0.353788576325397 grad: -0.10818822297070245
iteration: 0 loss: 144.68933298666917 grad: 636.2549919797095
iteration: 10 loss: 1.1077665221123012 grad: 0.506872209913242
iteration: 20 loss: 0.6781550757968944 grad: 0.06466013876267025
iteration: 30 loss: 0.5074352585935974 grad: -0.021570135430098634
iteration: 40 loss: 0.4108672330572848 grad: -0.05120864656588804
iteration: 50 loss: 0.3475190798672801 grad: -0.06309960464800402
iteration: 0 loss: 74.47270946970754 grad: 533.7487068368669
iteration: 10 loss: 1.4586164209793588 grad: 0.17131995308269227
iteration: 20 loss: 0.8987549426442043 grad: -0.10617711327564788
iteration: 30 loss: 0.6628647579669136 grad: -0.14135066661322532
iteration: 40 loss: 0.528774499146576 grad: -0.14289207261545403
iteration: 50 loss: 0.44130170266014995 grad: -0.13665404886239574
iteration: 60 loss: 0.37939252180058247 grad: -0.12866840414856012
iteration: 0 loss: 113.04265726319471 grad: 592.7060522239913
iteration: 10 loss: 1.0854617184782414 grad: 0.263025756872504
iteration: 20 loss: 0.6737374222819364 grad: -0.09987247943091522
iteration: 30 loss: 0.5077261156867198 grad: -0.14058227187854372
iteration: 40 loss: 0.41275916381478467 grad: -0.14366475778741897
iteration: 50 loss: 0.3499679751974492 grad: -0.13836837245135059
iteration: 0 loss: 97.7213136702993 grad: 627.8800380851728
iteration: 10 loss: 1.1997605075687467 grad: -0.2414373877124349
iteration: 20 loss: 0.747041792222097 grad: -0.24993443867953333
iteration: 30 loss: 0.5595991395488951 grad: -0.2188947212351881
iteration: 40 loss: 0.45236440407775674 grad: -0.19271501178350087
iteration: 50 loss: 0.3817227627785707 grad: -0.17193513852637277
iteration: 0 loss: 66.19113065716755 grad: 454.6028946236604
iteration: 10 loss: 1.5861494468855115 grad: 0.5250924190973264
iteration: 20 loss: 0.9535576149276246 grad: 0.09152784591645009
iteration: 30 loss: 0.6940834883940128 grad: -0.005042761425108922
iteration: 40 loss: 0.5491552309079204 grad: -0.038484957739724324
iteration: 50 loss: 0.455739120625863 grad: -0.05191614642627642
iteration: 60 loss: 0.3901964937707486 grad: -0.05734079131517354
iteration: 0 loss: 54.045917374915796 grad: 505.38872344834414
iteration: 10 loss: 1.6550311569277172 grad: 0.018753603427587785
iteration: 20 loss: 0.994073719534299 grad: -0.21221659128323223
iteration: 30 loss: 0.7217699975638177 grad: -0.22168635122699382
iteration: 40 loss: 0.5697090480825037 grad: -0.20568720861503453
iteration: 50 loss: 0.4718220767822045 grad: -0.18732295601418777
iteration: 60 loss: 0.40324838020495174 grad: -0.17065764310138432
iteration: 0 loss: 77.8641922597843 grad: 553.2822433365602
iteration: 10 loss: 1.4394418658983505 grad: 0.40491145538182416
iteration: 20 loss: 0.8790275271358868 grad: 0.03898204710545863
iteration: 30 loss: 0.646944382319072 grad: -0.04429582085704792
iteration: 40 loss: 0.515861713535163 grad: -0.07205724081976508
iteration: 50 loss: 0.4305998997679126 grad: -0.08190053922686144
iteration: 60 loss: 0.3703385985866341 grad: -0.08462918164672883
iteration: 0 loss: 85.2686415547899 grad: 538.6599279434465
iteration: 10 loss: 1.3937324087761558 grad: 0.3505726518602417
iteration: 20 loss: 0.858305513673341 grad: -0.030168719370449598
iteration: 30 loss: 0.6345180835137811 grad: -0.09520021140991647
iteration: 40 loss: 0.5074394231895151 grad: -0.11072941448714127
iteration: 50 loss: 0.4244712550041434 grad: -0.11246304233240466
iteration: 60 loss: 0.3656631299371094 grad: -0.10954423241070092
iteration: 0 loss: 83.07061311422727 grad: 479.51175543395544
iteration: 10 loss: 1.4716781688672944 grad: 0.6743190727944814
iteration: 20 loss: 0.908809985176297 grad: 0.05524380552089077
iteration: 30 loss: 0.6709859817538845 grad: -0.06276595096862739
iteration: 40 loss: 0.535521892022354 grad: -0.096915866460028
iteration: 50 loss: 0.44703826373775385 grad: -0.10671122410109579
iteration: 60 loss: 0.38436018191590515 grad: -0.10773329200523195
iteration: 0 loss: 106.97824144339624 grad: 595.9948240548321
iteration: 10 loss: 1.2439281780601628 grad: 0.2403369285474235
iteration: 20 loss: 0.7598052948871389 grad: -0.08809417150854978
iteration: 30 loss: 0.5639213883146525 grad: -0.13275494940442564
iteration: 40 loss: 0.45326163432483474 grad: -0.1372340041336471
iteration: 50 loss: 0.3809848388743624 grad: -0.1325261472991856
iteration: 0 loss: 134.77123029310812 grad: 542.2049798120927
iteration: 10 loss: 1.223997910885125 grad: 0.8193139222668224
iteration: 20 loss: 0.7413262014995119 grad: 0.14983996878762415
iteration: 30 loss: 0.5495620575753822 grad: 0.017103196298493616
iteration: 40 loss: 0.44183191581694037 grad: -0.029319481044507482
iteration: 50 loss: 0.371605750458829 grad: -0.04906501672735974
iteration: 0 loss: 89.3895383146625 grad: 581.0282321598461
iteration: 10 loss: 1.2575856683868716 grad: 0.17962810413404806
iteration: 20 loss: 0.7823911394992716 grad: -0.15397117357935203
iteration: 30 loss: 0.5840046790781344 grad: -0.18772493921912653
iteration: 40 loss: 0.4704782409827947 grad: -0.1834250827943509
iteration: 50 loss: 0.39580004189586115 grad: -0.17194980516253316
iteration: 0 loss: 90.83403557311166 grad: 591.8382612598723
iteration: 10 loss: 1.2918501674858733 grad: 0.048835572065759565
iteration: 20 loss: 0.788943685851419 grad: -0.16257886441913952
iteration: 30 loss: 0.5839627546449099 grad: -0.1769064490951527
iteration: 40 loss: 0.4682681237434074 grad: -0.16783438770639714
iteration: 50 loss: 0.3928468166478441 grad: -0.15548444010391743
iteration: 0 loss: 176.56038048635 grad: 663.9432357825516
iteration: 10 loss: 1.0529150554600413 grad: 0.5267567220026106
iteration: 20 loss: 0.613779630763185 grad: -0.002909833083435169
iteration: 30 loss: 0.45438342604575155 grad: -0.07226722417100827
iteration: 40 loss: 0.36651950954034296 grad: -0.08966634818511302
iteration: 0 loss: 86.24269102393852 grad: 502.72774745542995
iteration: 10 loss: 1.4552310987919967 grad: 0.5668907482417074
iteration: 20 loss: 0.8912845656470215 grad: 0.047253222781835795
iteration: 30 loss: 0.656721182864699 grad: -0.05316384807510688
iteration: 40 loss: 0.5239023433471381 grad: -0.08342888276316185
iteration: 50 loss: 0.43739029334731394 grad: -0.09296123174229406
iteration: 60 loss: 0.37619650906568 grad: -0.09478652390748862
iteration: 0 loss: 103.50171669133256 grad: 588.7992667928163
iteration: 10 loss: 1.2125094913505812 grad: 0.07212008565355119
iteration: 20 loss: 0.7454238067387202 grad: -0.14776835624103837
iteration: 30 loss: 0.5548612379025144 grad: -0.16911657452095352
iteration: 40 loss: 0.44690415780661913 grad: -0.1638355370850253
iteration: 50 loss: 0.37625770827054855 grad: -0.15360415071415498
iteration: 0 loss: 88.45344691531913 grad: 526.9660520433765
iteration: 10 loss: 1.3766580524455672 grad: 0.3624419953074996
iteration: 20 loss: 0.8360008580241562 grad: -0.02671522538306579
iteration: 30 loss: 0.6158756848645598 grad: -0.09629085898615256
iteration: 40 loss: 0.49194031122599635 grad: -0.11327129805014807
iteration: 50 loss: 0.41134909191524377 grad: -0.11546634092240009
iteration: 60 loss: 0.3543504648255707 grad: -0.11267518556524984
iteration: 0 loss: 71.3547388588397 grad: 485.31395773909526
iteration: 10 loss: 1.4478922982749198 grad: 0.5365988224246014
iteration: 20 loss: 0.8854839423684925 grad: 0.059557868152651794
iteration: 30 loss: 0.651820254548759 grad: -0.040091372380696724
iteration: 40 loss: 0.5195859298806185 grad: -0.07150038334212262
iteration: 50 loss: 0.43351342768883944 grad: -0.08213999910354713
iteration: 60 loss: 0.37267127711592446 grad: -0.08490631203473521
iteration: 0 loss: 101.48342703134023 grad: 490.9876204893175
iteration: 10 loss: 1.2620840468228132 grad: 0.7662695846638292
iteration: 20 loss: 0.7743122391565603 grad: 0.18071624266071845
iteration: 30 loss: 0.5752594992572995 grad: 0.043167961326749035
iteration: 40 loss: 0.462524233826364 grad: -0.008543667267721072
iteration: 50 loss: 0.388796623445077 grad: -0.032045666228096466
iteration: 0 loss: 60.258287605028606 grad: 487.0890271743461
iteration: 10 loss: 1.5807294270063144 grad: 0.5383482812267705
iteration: 20 loss: 0.9514766442307931 grad: 0.007933279470804343
iteration: 30 loss: 0.6932765182859426 grad: -0.09031459460530074
iteration: 40 loss: 0.5488201526517184 grad: -0.11538957591909939
iteration: 50 loss: 0.4555961579322188 grad: -0.12010757500730361
iteration: 60 loss: 0.39013269750086527 grad: -0.11797558860789384
iteration: 0 loss: 80.45867499244308 grad: 572.0136379973778
iteration: 10 loss: 1.3633685788587504 grad: 0.11582362369370157
iteration: 20 loss: 0.843743017969907 grad: -0.16164729402198466
iteration: 30 loss: 0.6254774279039504 grad: -0.1886583296558395
iteration: 40 loss: 0.5010294174728849 grad: -0.1821642708461974
iteration: 50 loss: 0.41956426137469616 grad: -0.16970268353177742
iteration: 60 loss: 0.3617174999618245 grad: -0.15699252200411457
iteration: 0 loss: 144.75920497694327 grad: 579.3514756985235
iteration: 10 loss: 1.1379285070777934 grad: 0.4949858052847819
iteration: 20 loss: 0.6965029477432836 grad: 0.03371718657409887
iteration: 30 loss: 0.5205300413940264 grad: -0.04800532890444121
iteration: 40 loss: 0.42109560355906567 grad: -0.07242202192720512
iteration: 50 loss: 0.3559344250243157 grad: -0.080373756534399
iteration: 0 loss: 126.73517378861412 grad: 632.8405980143509
iteration: 10 loss: 1.1160293378547563 grad: 0.12308955552736468
iteration: 20 loss: 0.688175763184931 grad: -0.1389216647012917
iteration: 30 loss: 0.5164251074283212 grad: -0.16104875879577346
iteration: 40 loss: 0.41863547752349073 grad: -0.15660758403437286
iteration: 50 loss: 0.35422941336086794 grad: -0.1470939873761235
iteration: 0 loss: 136.89902242171857 grad: 601.6442360307328
iteration: 10 loss: 1.2049360737598611 grad: 0.4194293810048295
iteration: 20 loss: 0.7305953683486553 grad: 0.00490748456292458
iteration: 30 loss: 0.54206835659451 grad: -0.061656213217809334
iteration: 40 loss: 0.4360930494045037 grad: -0.08064338832774437
iteration: 50 loss: 0.36701157106654136 grad: -0.08597216823524768
iteration: 0 loss: 100.07793821774145 grad: 575.0517762500826
iteration: 10 loss: 1.314047370767019 grad: -0.017258832132943082
iteration: 20 loss: 0.7996533329435558 grad: -0.15005378628980964
iteration: 30 loss: 0.5907774234149408 grad: -0.1585698328046775
iteration: 40 loss: 0.473275532223273 grad: -0.15069185640673932
iteration: 50 loss: 0.39682583383711806 grad: -0.14031225373210965
iteration: 0 loss: 145.0664171699987 grad: 600.820418387818
iteration: 10 loss: 1.1267421912386701 grad: 0.4619374390666039
iteration: 20 loss: 0.690399343388642 grad: 0.02858253521486848
iteration: 30 loss: 0.5159057208408161 grad: -0.048884324630206394
iteration: 40 loss: 0.41715067948544776 grad: -0.07181516735980543
iteration: 50 loss: 0.3524214748238757 grad: -0.07914197447244714
iteration: 0 loss: 127.4941480129778 grad: 604.545298351619
iteration: 10 loss: 1.2097730272804588 grad: 0.2611819726086353
iteration: 20 loss: 0.7315320639922489 grad: -0.026510325723719447
iteration: 30 loss: 0.5420672782206153 grad: -0.07943885996716771
iteration: 40 loss: 0.4357965235530256 grad: -0.09266324020696559
iteration: 50 loss: 0.36660933566773485 grad: -0.09468894087709413
iteration: 0 loss: 147.98990625070454 grad: 520.464340901548
iteration: 10 loss: 1.2126603475088897 grad: 0.9761953772920539
iteration: 20 loss: 0.7405520215293522 grad: 0.31409353828327424
iteration: 30 loss: 0.550344177850837 grad: 0.1418794424616604
iteration: 40 loss: 0.4431344907147418 grad: 0.0687877308819772
iteration: 50 loss: 0.37315477053611673 grad: 0.030898092552281348
iteration: 0 loss: 203.48870365825252 grad: 619.1662449130046
iteration: 10 loss: 1.0937567803632853 grad: 0.892485720833598
iteration: 20 loss: 0.6449322751628973 grad: 0.24582671312706514
iteration: 30 loss: 0.4769292826531455 grad: 0.09372989604088729
iteration: 40 loss: 0.38407737741942005 grad: 0.03284325887614946
iteration: 50 loss: 0.32394904100733996 grad: 0.0028661704531818223
iteration: 0 loss: 150.46426436067333 grad: 551.3741588777392
iteration: 10 loss: 1.1414765413308652 grad: 0.7597631983832509
iteration: 20 loss: 0.6968360302286204 grad: 0.1427676533249902
iteration: 30 loss: 0.5204536429129637 grad: 0.017931872305969883
iteration: 40 loss: 0.42088297976972006 grad: -0.027578489076647453
iteration: 50 loss: 0.35565698552983865 grad: -0.04776889783573976
iteration: 0 loss: 60.12083332468728 grad: 467.0360172487108
iteration: 10 loss: 1.6055928655584715 grad: 0.49416966276817376
iteration: 20 loss: 0.9666734712170431 grad: -0.018893842734550158
iteration: 30 loss: 0.7039662310031748 grad: -0.10643788645945207
iteration: 40 loss: 0.5569481043638999 grad: -0.12582018816727658
iteration: 50 loss: 0.4620875092389476 grad: -0.12725153708526063
iteration: 60 loss: 0.39549751263083716 grad: -0.12307098957843961
iteration: 0 loss: 64.83152295247737 grad: 440.28724859514784
iteration: 10 loss: 1.5873669331379625 grad: 0.7273983727801082
iteration: 20 loss: 0.9538920413759694 grad: 0.11876751128449209
iteration: 30 loss: 0.6945721887557904 grad: -0.01401144946351463
iteration: 40 loss: 0.549630188189605 grad: -0.05833870429971706
iteration: 50 loss: 0.4561473198312094 grad: -0.07508173822957633
iteration: 60 loss: 0.39052937987873354 grad: -0.08106861473500077
iteration: 0 loss: 87.59660505209703 grad: 486.88433727258075
iteration: 10 loss: 1.4360487734075336 grad: 0.7922091871663337
iteration: 20 loss: 0.8810174703441424 grad: 0.18247506102299116
iteration: 30 loss: 0.6497186968865869 grad: 0.03704272201505071
iteration: 40 loss: 0.5186537486742137 grad: -0.017066202972057125
iteration: 50 loss: 0.4332301530848781 grad: -0.040973735503785975
iteration: 60 loss: 0.37277166531872685 grad: -0.052299804691926764
iteration: 0 loss: 94.53260251028864 grad: 566.4127526188516
iteration: 10 loss: 1.3353525138023874 grad: 0.07557447119391283
iteration: 20 loss: 0.8219679341565841 grad: -0.09738422825067486
iteration: 30 loss: 0.6097968738846579 grad: -0.11838726424307014
iteration: 40 loss: 0.48940649125804403 grad: -0.11837392307470396
iteration: 50 loss: 0.4106778516704501 grad: -0.11346836424212416
iteration: 60 loss: 0.35474919123945176 grad: -0.10747218057891889
iteration: 0 loss: 90.60456596206541 grad: 528.6220844978241
iteration: 10 loss: 1.3132621379326421 grad: 0.34113019186392707
iteration: 20 loss: 0.8054677097830294 grad: 0.0031098709686254167
iteration: 30 loss: 0.5962708071076783 grad: -0.06902594953451963
iteration: 40 loss: 0.4778947351411592 grad: -0.09045307736327453
iteration: 50 loss: 0.4006720191545438 grad: -0.09638641716946818
iteration: 0 loss: 71.64339584682239 grad: 511.3001216913896
iteration: 10 loss: 1.4707912195796615 grad: 0.1836274077396714
iteration: 20 loss: 0.893864555055519 grad: -0.10790120733515669
iteration: 30 loss: 0.6560131638153425 grad: -0.1482433219091251
iteration: 40 loss: 0.5220894084567672 grad: -0.15013159325496284
iteration: 50 loss: 0.4351807317206377 grad: -0.14326755001811464
iteration: 60 loss: 0.3738664312638569 grad: -0.13454905073153076
iteration: 0 loss: 65.23176134974516 grad: 495.04091289908723
iteration: 10 loss: 1.5766874445850012 grad: 0.26458818885719687
iteration: 20 loss: 0.9554924039958181 grad: -0.0707102587768974
iteration: 30 loss: 0.6974101126338197 grad: -0.120202513387892
iteration: 40 loss: 0.5525097940922437 grad: -0.1274516107545882
iteration: 50 loss: 0.4588590687776973 grad: -0.1242912702594173
iteration: 60 loss: 0.39304435090318957 grad: -0.11827971675018151
iteration: 0 loss: 121.53743129514292 grad: 631.408246727294
iteration: 10 loss: 1.0817232982739038 grad: 0.0035464603950074725
iteration: 20 loss: 0.6732972736463952 grad: -0.14359194526121757
iteration: 30 loss: 0.5074933570284966 grad: -0.15295164523702226
iteration: 40 loss: 0.41266593261504814 grad: -0.14666259899808576
iteration: 50 loss: 0.3500228047559259 grad: -0.13759182292067507
iteration: 0 loss: 114.28493893971277 grad: 568.299487633037
iteration: 10 loss: 1.239191002261495 grad: 0.5115805320464665
iteration: 20 loss: 0.7625141139563063 grad: 0.036186288724577916
iteration: 30 loss: 0.568009589673569 grad: -0.0535136856594311
iteration: 40 loss: 0.4574771089422608 grad: -0.0797917407388936
iteration: 50 loss: 0.3849959965020224 grad: -0.0877848932245755
iteration: 0 loss: 133.23946339814592 grad: 546.7454063275634
iteration: 10 loss: 1.2290749351733128 grad: 0.6764310013342458
iteration: 20 loss: 0.7529432768559755 grad: 0.12483747849882787
iteration: 30 loss: 0.5601624462029907 grad: 0.007806990201263865
iteration: 40 loss: 0.4511000428364324 grad: -0.03391950376237024
iteration: 50 loss: 0.37973507339641155 grad: -0.051609290752549844
iteration: 0 loss: 88.20741877807946 grad: 543.7837363240426
iteration: 10 loss: 1.3831917244470704 grad: 0.28119717475403194
iteration: 20 loss: 0.8510790591677836 grad: -0.08031146161070116
iteration: 30 loss: 0.6296724180645635 grad: -0.13432852459765982
iteration: 40 loss: 0.5040014578533539 grad: -0.14245239373222723
iteration: 50 loss: 0.42191967627629556 grad: -0.13902821626846312
iteration: 60 loss: 0.3637045524691613 grad: -0.1323701085563777
iteration: 0 loss: 96.62753399752542 grad: 462.23546989846307
iteration: 10 loss: 1.3990592662067187 grad: 1.0084804137916947
iteration: 20 loss: 0.8615786411594606 grad: 0.22760898379720168
iteration: 30 loss: 0.6376384177528962 grad: 0.04842555247656932
iteration: 40 loss: 0.5102830291970432 grad: -0.016642195486868377
iteration: 50 loss: 0.42701537729656347 grad: -0.04482574952733468
iteration: 60 loss: 0.36793415877367475 grad: -0.05792541495125038
iteration: 0 loss: 111.99735809356456 grad: 561.1960019106541
iteration: 10 loss: 1.1874397645703787 grad: 0.22836097301346717
iteration: 20 loss: 0.7378440918217332 grad: -0.08208255990980254
iteration: 30 loss: 0.552546893264083 grad: -0.12637941384586943
iteration: 40 loss: 0.44662174481783784 grad: -0.1318976986957392
iteration: 50 loss: 0.37685496284567227 grad: -0.1280138624478682
iteration: 0 loss: 72.74536561594219 grad: 516.9307473978097
iteration: 10 loss: 1.461150032870426 grad: 0.37973880781434377
iteration: 20 loss: 0.8971781440661123 grad: -0.0011864911246346373
iteration: 30 loss: 0.6611534098938964 grad: -0.07356567509795045
iteration: 40 loss: 0.5273435832163655 grad: -0.09384485935794645
iteration: 50 loss: 0.44016882772087806 grad: -0.09882404662175134
iteration: 60 loss: 0.3785110160567704 grad: -0.09824370639351745
iteration: 0 loss: 61.67479579767211 grad: 543.4910682601146
iteration: 10 loss: 1.4983026955255396 grad: -0.03496081141484157
iteration: 20 loss: 0.9074571461866402 grad: -0.24829217957630118
iteration: 30 loss: 0.6647506718510902 grad: -0.24687141185363945
iteration: 40 loss: 0.5283464260897248 grad: -0.22620880938222904
iteration: 50 loss: 0.43994917584363563 grad: -0.2051965087260377
iteration: 60 loss: 0.3776525251315351 grad: -0.18671716264410845
iteration: 0 loss: 136.20596734051512 grad: 678.91221983517
iteration: 10 loss: 1.0808758416687025 grad: -0.06480462150083258
iteration: 20 loss: 0.6511572087959145 grad: -0.18848981126960945
iteration: 30 loss: 0.48602996799499204 grad: -0.18162987152878768
iteration: 40 loss: 0.39339045435752434 grad: -0.16693339710421806
iteration: 50 loss: 0.33281990704513603 grad: -0.1529451196083832
iteration: 0 loss: 67.04096244796855 grad: 487.8476617060338
iteration: 10 loss: 1.4887819108776883 grad: 0.36730627443137365
iteration: 20 loss: 0.9075605541314541 grad: -0.05103857923866866
iteration: 30 loss: 0.6668877919607687 grad: -0.11737145132596383
iteration: 40 loss: 0.5311618432464953 grad: -0.1302745555913577
iteration: 50 loss: 0.443005879453515 grad: -0.12934544571658724
iteration: 60 loss: 0.3807718775577252 grad: -0.1242248151612709
iteration: 0 loss: 56.883539344178274 grad: 469.0024866915386
iteration: 10 loss: 1.6157371436286831 grad: 0.3472698222743546
iteration: 20 loss: 0.9699896141153824 grad: -0.054070121560807226
iteration: 30 loss: 0.7048635948277396 grad: -0.11728419370837162
iteration: 40 loss: 0.5568314134060611 grad: -0.12882151524960853
iteration: 50 loss: 0.4614977560379636 grad: -0.12718517470074603
iteration: 60 loss: 0.39467641469375603 grad: -0.12169178569391684
iteration: 0 loss: 66.2841940303012 grad: 496.00843636208464
iteration: 10 loss: 1.5494746948765665 grad: 0.48250432444010466
iteration: 20 loss: 0.9374781273276395 grad: 0.05165699410739967
iteration: 30 loss: 0.6849303232062415 grad: -0.04070850810168316
iteration: 40 loss: 0.5432015535949507 grad: -0.07018423814153876
iteration: 50 loss: 0.45154026080730214 grad: -0.08023590497761475
iteration: 60 loss: 0.387068624442608 grad: -0.08284780483090692
iteration: 0 loss: 71.62385807482157 grad: 553.5712674288345
iteration: 10 loss: 1.4439978957542818 grad: -0.23952633023698144
iteration: 20 loss: 0.8839292004024007 grad: -0.32558325463620796
iteration: 30 loss: 0.650785373165077 grad: -0.29282055630903314
iteration: 40 loss: 0.5188613977029903 grad: -0.25775867607910463
iteration: 50 loss: 0.4329897920384941 grad: -0.22867665926911823
iteration: 60 loss: 0.372281450551203 grad: -0.20513179811214494
iteration: 0 loss: 114.4527766867127 grad: 568.9115545116313
iteration: 10 loss: 1.1454035106072213 grad: 0.3622868713912657
iteration: 20 loss: 0.7120315036351134 grad: -0.00047147454375226147
iteration: 30 loss: 0.5349220225929127 grad: -0.06426387725873939
iteration: 40 loss: 0.43358370966937154 grad: -0.08211476612803775
iteration: 50 loss: 0.36670837614705803 grad: -0.08678030817963835
iteration: 0 loss: 100.7117980308624 grad: 611.5846109394926
iteration: 10 loss: 1.2366908088443695 grad: -0.1339721957629867
iteration: 20 loss: 0.7581992911657587 grad: -0.23438965040149268
iteration: 30 loss: 0.5633335286550143 grad: -0.2204263290776066
iteration: 40 loss: 0.45308789206121053 grad: -0.19887905877933307
iteration: 50 loss: 0.3810352084234513 grad: -0.17957008845651962
iteration: 0 loss: 108.22942915869386 grad: 602.5438887655691
iteration: 10 loss: 1.1857269526272747 grad: 0.12327900766529015
iteration: 20 loss: 0.7149382582923532 grad: -0.17882191521187385
iteration: 30 loss: 0.5298660593963095 grad: -0.19566786930357782
iteration: 40 loss: 0.42606519289984135 grad: -0.1850561340960139
iteration: 50 loss: 0.35844726292841433 grad: -0.17087349216656655
iteration: 0 loss: 86.51137166956046 grad: 576.1543631372422
iteration: 10 loss: 1.281189520952301 grad: 0.016312845614978755
iteration: 20 loss: 0.7789573952491495 grad: -0.19952490352182284
iteration: 30 loss: 0.5758099665860176 grad: -0.2054009065019107
iteration: 40 loss: 0.4614239992747157 grad: -0.19134076898321326
iteration: 50 loss: 0.3869341046441729 grad: -0.1756502567656951
iteration: 0 loss: 88.06542909418671 grad: 545.5267962083086
iteration: 10 loss: 1.3225007797155657 grad: 0.3380150734201479
iteration: 20 loss: 0.8127667451706823 grad: -0.0535834016888122
iteration: 30 loss: 0.6023200033016771 grad: -0.11610713388975896
iteration: 40 loss: 0.482908478653702 grad: -0.12851738337416901
iteration: 50 loss: 0.40485501000363333 grad: -0.1277545563241329
iteration: 0 loss: 150.7327317900439 grad: 620.4328043177632
iteration: 10 loss: 1.0655642039471125 grad: 0.18100020002773642
iteration: 20 loss: 0.6371899351600984 grad: -0.11893913089015717
iteration: 30 loss: 0.4746055627038004 grad: -0.14238250034445257
iteration: 40 loss: 0.3838599316939673 grad: -0.13911728212305594
iteration: 50 loss: 0.32471379550398444 grad: -0.13112341003270417
iteration: 0 loss: 118.24432157596203 grad: 574.6803308933901
iteration: 10 loss: 1.2863468403262563 grad: 0.43631045983489924
iteration: 20 loss: 0.7808513294339718 grad: 0.06694641929788074
iteration: 30 loss: 0.5771604685127882 grad: -0.01665250055715628
iteration: 40 loss: 0.46273372582651395 grad: -0.04654450171175593
iteration: 50 loss: 0.3883095688638191 grad: -0.0589443961725609
iteration: 0 loss: 54.33961463996118 grad: 539.195256979325
iteration: 10 loss: 1.5602486564329907 grad: -0.31222409182711464
iteration: 20 loss: 0.943930150001837 grad: -0.3862019238557927
iteration: 30 loss: 0.6882153433044468 grad: -0.3369892795265344
iteration: 40 loss: 0.5446779221019824 grad: -0.29139759605519466
iteration: 50 loss: 0.45194138426474884 grad: -0.25524013566192405
iteration: 60 loss: 0.38680092135357635 grad: -0.22670755429172101
iteration: 0 loss: 95.09044912568466 grad: 540.483755629187
iteration: 10 loss: 1.3187852486757534 grad: 0.33861126347911574
iteration: 20 loss: 0.8212263784145596 grad: -0.03828607033700386
iteration: 30 loss: 0.6122095134523161 grad: -0.1028078938059736
iteration: 40 loss: 0.4923782976869585 grad: -0.11704319257809519
iteration: 50 loss: 0.4135288411813566 grad: -0.1176632644207344
iteration: 60 loss: 0.3572998226632494 grad: -0.11390607595070285
iteration: 0 loss: 80.54912686420072 grad: 530.7173600327516
iteration: 10 loss: 1.3948115503781082 grad: 0.5112214686277692
iteration: 20 loss: 0.8550280389759295 grad: -0.03256302464936943
iteration: 30 loss: 0.6320997032876634 grad: -0.11815027633232134
iteration: 40 loss: 0.5056013422812281 grad: -0.1357060269021427
iteration: 50 loss: 0.42297265215587354 grad: -0.13596662548427413
iteration: 60 loss: 0.3643742696530436 grad: -0.13098561013902382
iteration: 0 loss: 106.60024245467949 grad: 621.8531077615592
iteration: 10 loss: 1.1999899705302928 grad: -0.09076106544398192
iteration: 20 loss: 0.7321700856140193 grad: -0.23138055749094444
iteration: 30 loss: 0.5452939870127177 grad: -0.22185986207372027
iteration: 40 loss: 0.4396595150857055 grad: -0.2016881433867208
iteration: 50 loss: 0.37051920678583306 grad: -0.18277013637338885
iteration: 0 loss: 69.65956384864855 grad: 542.3589224638918
iteration: 10 loss: 1.5129489903641693 grad: 0.021031436068270154
iteration: 20 loss: 0.9206587751542347 grad: -0.22155556297356932
iteration: 30 loss: 0.6746567878835776 grad: -0.22651827221657111
iteration: 40 loss: 0.5359282136176428 grad: -0.20914769315970053
iteration: 50 loss: 0.44592711132719903 grad: -0.19036600865657616
iteration: 60 loss: 0.3824913418044492 grad: -0.17356229871142648
iteration: 0 loss: 100.88780099383195 grad: 515.664993476963
iteration: 10 loss: 1.3272005848549187 grad: 0.45755301567473794
iteration: 20 loss: 0.8146603303450154 grad: 0.03396985498549769
iteration: 30 loss: 0.6040860848670491 grad: -0.05241988513907751
iteration: 40 loss: 0.4847835037729634 grad: -0.0787122924065925
iteration: 50 loss: 0.40681683676955965 grad: -0.08694958167171855
iteration: 0 loss: 57.59689657995994 grad: 457.31879432996254
iteration: 10 loss: 1.6173265765089788 grad: 0.5875101541142026
iteration: 20 loss: 0.96910282044775 grad: -0.03017650356414847
iteration: 30 loss: 0.704200621068594 grad: -0.12795894593684984
iteration: 40 loss: 0.5563715237670046 grad: -0.14762381300770008
iteration: 50 loss: 0.4611607192671892 grad: -0.1475372960459838
iteration: 60 loss: 0.3944124617876448 grad: -0.14158683590383037
iteration: 0 loss: 120.48183329849813 grad: 593.4259060393323
iteration: 10 loss: 1.156530124483652 grad: 0.43543209432015484
iteration: 20 loss: 0.7200363063692831 grad: -0.011157173978231825
iteration: 30 loss: 0.5409460296424103 grad: -0.08650560571106691
iteration: 40 loss: 0.4383181428612178 grad: -0.1063152867678288
iteration: 50 loss: 0.3705390406170078 grad: -0.11062947528662745
iteration: 0 loss: 96.86661880968614 grad: 575.1312201096034
iteration: 10 loss: 1.3076420880218145 grad: 0.10706065528587795
iteration: 20 loss: 0.8047964950123556 grad: -0.13143905708182171
iteration: 30 loss: 0.5973552080915231 grad: -0.1572234138848494
iteration: 40 loss: 0.47953194430763024 grad: -0.15394674288021915
iteration: 50 loss: 0.4024277536996537 grad: -0.14493720357910084
iteration: 0 loss: 95.3856556421651 grad: 566.2100196467456
iteration: 10 loss: 1.3131622740683353 grad: 0.2757591737385601
iteration: 20 loss: 0.8041780337972259 grad: -0.06939338005241705
iteration: 30 loss: 0.5960711996253436 grad: -0.12374096636011062
iteration: 40 loss: 0.47818586416575604 grad: -0.13272877854629897
iteration: 50 loss: 0.401152352815023 grad: -0.13021310207768558
iteration: 0 loss: 184.53500879355835 grad: 589.8205651910639
iteration: 10 loss: 1.1337325656033335 grad: 0.5778193408422764
iteration: 20 loss: 0.6797364441615421 grad: 0.08637703516540247
iteration: 30 loss: 0.5032578999546538 grad: -0.008845415224638904
iteration: 40 loss: 0.4049082157601085 grad: -0.040849521174380454
iteration: 50 loss: 0.3410586128892729 grad: -0.053644999373628086
iteration: 0 loss: 72.03131036469118 grad: 532.5451191771817
iteration: 10 loss: 1.4245330531801879 grad: 0.1525381959057152
iteration: 20 loss: 0.8731333568703121 grad: -0.14476895467635909
iteration: 30 loss: 0.6434416070315737 grad: -0.17846876250456178
iteration: 40 loss: 0.5133863286664564 grad: -0.17599200583330576
iteration: 50 loss: 0.42868222862009797 grad: -0.16600258484393018
iteration: 60 loss: 0.36876825796257023 grad: -0.15487852127965715
iteration: 0 loss: 99.35195660640126 grad: 556.7872015157898
iteration: 10 loss: 1.1910505393236896 grad: 0.1856302049720201
iteration: 20 loss: 0.738892508020647 grad: -0.09185844936782693
iteration: 30 loss: 0.5526500459657406 grad: -0.13489405475318822
iteration: 40 loss: 0.4463698615090834 grad: -0.14064354251794214
iteration: 50 loss: 0.37647831806490023 grad: -0.13680369387633032
iteration: 0 loss: 176.61105268407835 grad: 623.8906714115453
iteration: 10 loss: 1.0917500099846043 grad: 0.6260422361896588
iteration: 20 loss: 0.6535946316786397 grad: 0.08818850023584215
iteration: 30 loss: 0.48549553605582996 grad: -0.013488024825550098
iteration: 40 loss: 0.3918185774756983 grad: -0.04777621686199572
iteration: 50 loss: 0.33087629902969307 grad: -0.06137013712186978
iteration: 0 loss: 96.28265421340141 grad: 526.5468344980984
iteration: 10 loss: 1.3663041018750268 grad: 0.640432223430237
iteration: 20 loss: 0.830227382712854 grad: 0.0878422911422791
iteration: 30 loss: 0.6128234589973132 grad: -0.028835774450366794
iteration: 40 loss: 0.49031004258565436 grad: -0.06722050051495829
iteration: 50 loss: 0.41051981947963545 grad: -0.08135194596528278
iteration: 60 loss: 0.35400179362717277 grad: -0.08604906158347413
iteration: 0 loss: 120.41979137136646 grad: 565.0898729498679
iteration: 10 loss: 1.2565580417366702 grad: 0.2613465371418088
iteration: 20 loss: 0.7673365047874706 grad: -0.017021037791825365
iteration: 30 loss: 0.5691336852069898 grad: -0.06581585879190079
iteration: 40 loss: 0.45738832390361495 grad: -0.07900026334134183
iteration: 50 loss: 0.3844990313995037 grad: -0.08193302520885563
iteration: 0 loss: 81.83826202119116 grad: 537.4813766458826
iteration: 10 loss: 1.4322660819119515 grad: 0.26164701528747025
iteration: 20 loss: 0.8716565682241874 grad: -0.12320171408022323
iteration: 30 loss: 0.6414387868477537 grad: -0.16969842950506753
iteration: 40 loss: 0.5116088648619552 grad: -0.17104171884639094
iteration: 50 loss: 0.4271922639339788 grad: -0.16262856564374023
iteration: 60 loss: 0.36752773704748426 grad: -0.1522879868008084
iteration: 0 loss: 133.6748296880676 grad: 625.4629612798858
iteration: 10 loss: 1.1015810327993345 grad: 0.13046309270913264
iteration: 20 loss: 0.6792732580788652 grad: -0.07001468632372361
iteration: 30 loss: 0.5098575341300436 grad: -0.10837227186853973
iteration: 40 loss: 0.4135729269290261 grad: -0.11538274229468373
iteration: 50 loss: 0.35020351784946385 grad: -0.11369958068710104
iteration: 0 loss: 127.06761725524638 grad: 591.1056052721704
iteration: 10 loss: 1.185749024617177 grad: 0.3314035474803978
iteration: 20 loss: 0.7352694166455133 grad: -0.003119267613761657
iteration: 30 loss: 0.5504497222216908 grad: -0.06536210966570019
iteration: 40 loss: 0.44498878398991853 grad: -0.08437626506066237
iteration: 50 loss: 0.3755934766661746 grad: -0.09010804766545513
iteration: 0 loss: 95.28696787506645 grad: 483.9949329784142
iteration: 10 loss: 1.4289269210984954 grad: 0.6957241763310471
iteration: 20 loss: 0.8743427715453436 grad: 0.1320666064893618
iteration: 30 loss: 0.644463937826417 grad: 0.009934670107978752
iteration: 40 loss: 0.5145086204273949 grad: -0.033176712768777705
iteration: 50 loss: 0.429906218594128 grad: -0.051235953952788835
iteration: 60 loss: 0.3700606688035722 grad: -0.059143889593673366
iteration: 0 loss: 67.23882446685349 grad: 466.1034171469906
iteration: 10 loss: 1.6156672778970873 grad: 0.5119479552910939
iteration: 20 loss: 0.9694522979850511 grad: 0.075113773512682
iteration: 30 loss: 0.7045552898037964 grad: -0.020719718330799067
iteration: 40 loss: 0.5567336760160205 grad: -0.052779637996214795
iteration: 50 loss: 0.4615585709684224 grad: -0.06486289891329952
iteration: 60 loss: 0.39485394115994993 grad: -0.06910789353274308
iteration: 0 loss: 73.22072183651372 grad: 584.8359716850281
iteration: 10 loss: 1.3502122368166634 grad: -0.10678619292097279
iteration: 20 loss: 0.8374061369139864 grad: -0.25038041944577666
iteration: 30 loss: 0.6215441810691118 grad: -0.23809986747352863
iteration: 40 loss: 0.498321817407321 grad: -0.2150082692857806
iteration: 50 loss: 0.4175826741767113 grad: -0.19392045370571304
iteration: 60 loss: 0.3602054332064158 grad: -0.17610045810451808
iteration: 0 loss: 47.10251903347799 grad: 446.84775395489237
iteration: 10 loss: 1.7284330468282034 grad: 0.24775836826555409
iteration: 20 loss: 1.0234865149505739 grad: -0.059908809934323175
iteration: 30 loss: 0.737237691439077 grad: -0.1083408143993779
iteration: 40 loss: 0.5789737882668388 grad: -0.11603474756413111
iteration: 50 loss: 0.47780685810572654 grad: -0.11345546009217843
iteration: 60 loss: 0.40730394151912397 grad: -0.10800991696980605
iteration: 0 loss: 107.88362712385283 grad: 453.2421150723695
iteration: 10 loss: 1.1579913563284043 grad: 0.9378907188560625
iteration: 20 loss: 0.72151307236221 grad: 0.18810058881186065
iteration: 30 loss: 0.5423669715473807 grad: 0.034112856126231446
iteration: 40 loss: 0.4396408014504645 grad: -0.020515492663345368
iteration: 50 loss: 0.3717532385776808 grad: -0.04412193560229674
iteration: 0 loss: 74.39909646266841 grad: 463.9704732981471
iteration: 10 loss: 1.4790397038261125 grad: 0.5290783377633098
iteration: 20 loss: 0.8954726899232397 grad: 0.03380991086312886
iteration: 30 loss: 0.6563253316255875 grad: -0.06043727327794382
iteration: 40 loss: 0.521989061302272 grad: -0.08780674583574104
iteration: 50 loss: 0.43493012150991656 grad: -0.09558677320989316
iteration: 60 loss: 0.3735633813093955 grad: -0.09623848293251373
iteration: 0 loss: 139.00363783915785 grad: 603.7856364100355
iteration: 10 loss: 1.1386180586759194 grad: 0.23047517316666905
iteration: 20 loss: 0.6934657365828488 grad: -0.0706123343410163
iteration: 30 loss: 0.5158512732193479 grad: -0.11494757679031561
iteration: 40 loss: 0.41574002115802694 grad: -0.1209026695621099
iteration: 50 loss: 0.3503540679042668 grad: -0.11775527728807689
iteration: 0 loss: 99.59018722635359 grad: 511.0375424833438
iteration: 10 loss: 1.3602569079114553 grad: 0.8295751022118205
iteration: 20 loss: 0.836263042702409 grad: 0.1299506578735115
iteration: 30 loss: 0.6195412814565564 grad: -0.01615333354211808
iteration: 40 loss: 0.49642797196958616 grad: -0.06508207631324353
iteration: 50 loss: 0.4159120297655053 grad: -0.08374939616306948
iteration: 60 loss: 0.3587425621464552 grad: -0.0905547176858187
iteration: 0 loss: 109.925017703844 grad: 599.3950425658865
iteration: 10 loss: 1.2153800675010147 grad: 0.13206834956902283
iteration: 20 loss: 0.7368834314723823 grad: -0.17530831549695128
iteration: 30 loss: 0.5457195417271081 grad: -0.19350606642617252
iteration: 40 loss: 0.4383482112710805 grad: -0.18376758342198313
iteration: 50 loss: 0.36845565435793437 grad: -0.1702052299868213
iteration: 0 loss: 140.55234434407836 grad: 601.4991670475104
iteration: 10 loss: 1.232372089418277 grad: 0.3062477962953962
iteration: 20 loss: 0.7447944175892189 grad: -0.040247999726761026
iteration: 30 loss: 0.5512505774481311 grad: -0.09496219055372081
iteration: 40 loss: 0.4428118417025954 grad: -0.10667310166938071
iteration: 50 loss: 0.37228494638020493 grad: -0.10705702562345752
iteration: 0 loss: 125.31059270042111 grad: 648.2024583560121
iteration: 10 loss: 1.1502449098501197 grad: 0.1215117629949003
iteration: 20 loss: 0.6963001836150976 grad: -0.11342713160445884
iteration: 30 loss: 0.5175852404217122 grad: -0.13796661195509724
iteration: 40 loss: 0.41736417167596135 grad: -0.13692440549442025
iteration: 50 loss: 0.3520340614516251 grad: -0.13038373953226412
iteration: 0 loss: 66.91823518307274 grad: 505.43686356620844
iteration: 10 loss: 1.5304976948271112 grad: 0.3034016364107985
iteration: 20 loss: 0.9301729594712511 grad: -0.06869849310449642
iteration: 30 loss: 0.6812892077797105 grad: -0.12464114890523426
iteration: 40 loss: 0.541187313312879 grad: -0.13356233528180717
iteration: 50 loss: 0.45037477533242054 grad: -0.13068289308345366
iteration: 60 loss: 0.3863899841624681 grad: -0.12448092382812204
iteration: 0 loss: 74.31316434622637 grad: 512.8630111606278
iteration: 10 loss: 1.496718318118672 grad: 0.3261204724215905
iteration: 20 loss: 0.9099057963381214 grad: -0.04125864116670683
iteration: 30 loss: 0.6670539843962615 grad: -0.1006258133735638
iteration: 40 loss: 0.5303803372627788 grad: -0.11335430540515938
iteration: 50 loss: 0.4417718047094571 grad: -0.11362219418905045
iteration: 60 loss: 0.3793148470594216 grad: -0.10990868973776681
iteration: 0 loss: 78.86639086413135 grad: 579.9298518056052
iteration: 10 loss: 1.3033160243976454 grad: -0.061837994078179276
iteration: 20 loss: 0.8067107994592416 grad: -0.21411060180774708
iteration: 30 loss: 0.5994125632204569 grad: -0.21186719456579683
iteration: 40 loss: 0.48117963728071284 grad: -0.1949679877171029
iteration: 50 loss: 0.40366229454569047 grad: -0.1777608396090197
iteration: 0 loss: 86.10192078470237 grad: 559.7902865422484
iteration: 10 loss: 1.3084078033040318 grad: -0.013039349278092482
iteration: 20 loss: 0.8098831794680088 grad: -0.18669878032005938
iteration: 30 loss: 0.6018326237895466 grad: -0.19463126405129055
iteration: 40 loss: 0.483221671845885 grad: -0.1819682231355473
iteration: 50 loss: 0.40548360837791814 grad: -0.1670287569969845
iteration: 0 loss: 161.36700662343893 grad: 654.8409311165856
iteration: 10 loss: 1.0425672509658066 grad: 0.304751568417544
iteration: 20 loss: 0.6226352017673972 grad: -0.021167619874145754
iteration: 30 loss: 0.462690579338662 grad: -0.07538714951553901
iteration: 40 loss: 0.3735296603511994 grad: -0.08808967598032111
iteration: 50 loss: 0.31550427658806834 grad: -0.08971032273052353
iteration: 0 loss: 66.7468356119915 grad: 546.9095406677043
iteration: 10 loss: 1.507665037716048 grad: 0.14060409776516203
iteration: 20 loss: 0.917356076053693 grad: -0.19737234680914081
iteration: 30 loss: 0.673020462225147 grad: -0.21747756947548497
iteration: 40 loss: 0.53525869110027 grad: -0.20503779940301228
iteration: 50 loss: 0.44583078295917483 grad: -0.18841539672696017
iteration: 60 loss: 0.3827466480290902 grad: -0.17272244227607145
iteration: 0 loss: 121.43636949093235 grad: 625.9876708589303
iteration: 10 loss: 1.1106793497316507 grad: 0.17013737828803382
iteration: 20 loss: 0.6779872462775529 grad: -0.11912059141285856
iteration: 30 loss: 0.5061022286397268 grad: -0.148709944729918
iteration: 40 loss: 0.4088911907002429 grad: -0.1468224307573322
iteration: 50 loss: 0.3451803404176727 grad: -0.1388000182195651
iteration: 0 loss: 91.38084781354291 grad: 541.8312393879862
iteration: 10 loss: 1.3763250690737798 grad: 0.30807551954297846
iteration: 20 loss: 0.8437636267037967 grad: -0.0013375623055480214
iteration: 30 loss: 0.6241426746223211 grad: -0.06295601209141273
iteration: 40 loss: 0.4999234760920114 grad: -0.08192704073736465
iteration: 50 loss: 0.41889835733970604 grad: -0.08752385697177818
iteration: 60 loss: 0.3614516508633559 grad: -0.087952953085205
iteration: 0 loss: 48.39331060947986 grad: 510.5760357238429
iteration: 10 loss: 1.6893028325867734 grad: -0.08155562520224523
iteration: 20 loss: 1.0148077691038784 grad: -0.24293772121747487
iteration: 30 loss: 0.7356207792611051 grad: -0.2351526012136805
iteration: 40 loss: 0.5797816058189214 grad: -0.21308673456732566
iteration: 50 loss: 0.4795773474338189 grad: -0.19199121563193824
iteration: 60 loss: 0.4094643725691805 grad: -0.17387975168534137
iteration: 0 loss: 137.17405268328977 grad: 612.90279863458
iteration: 10 loss: 1.1982939802001882 grad: 0.41625524872825503
iteration: 20 loss: 0.7248674423356285 grad: 0.031552731254938196
iteration: 30 loss: 0.5376758297348219 grad: -0.03846494105936591
iteration: 40 loss: 0.43270052432068373 grad: -0.0603438798379268
iteration: 50 loss: 0.3643240807283622 grad: -0.06804958732175273
iteration: 0 loss: 77.15618846349584 grad: 507.801940712629
iteration: 10 loss: 1.460007413719888 grad: 0.4354063364406967
iteration: 20 loss: 0.8853217717308077 grad: -0.03570129612364479
iteration: 30 loss: 0.6500214926996208 grad: -0.11243617134708181
iteration: 40 loss: 0.5175144937724823 grad: -0.12891950994434692
iteration: 50 loss: 0.43146249414142157 grad: -0.12948581171522597
iteration: 60 loss: 0.3707140185392332 grad: -0.1250567382953576
iteration: 0 loss: 143.5743280642183 grad: 623.6873333588093
iteration: 10 loss: 1.096558093270687 grad: 0.4410884434350958
iteration: 20 loss: 0.6574497507625149 grad: 0.006729138286840971
iteration: 30 loss: 0.48844139875952597 grad: -0.05817154351098003
iteration: 40 loss: 0.3940756357629983 grad: -0.0761359358326571
iteration: 50 loss: 0.33262851010848943 grad: -0.08124624315192029
iteration: 0 loss: 70.92658194394083 grad: 479.63025727093657
iteration: 10 loss: 1.4934827171138998 grad: 0.5374304881424026
iteration: 20 loss: 0.9071838706667305 grad: -0.004839575820864443
iteration: 30 loss: 0.6654311758304009 grad: -0.09785699373274022
iteration: 40 loss: 0.5291883491713221 grad: -0.1201867601716613
iteration: 50 loss: 0.44075518470278685 grad: -0.12350143250483948
iteration: 60 loss: 0.3783771771365785 grad: -0.12060228531326031
iteration: 0 loss: 128.1259484010874 grad: 593.0650920495166
iteration: 10 loss: 1.164508754982638 grad: 0.15458094107549317
iteration: 20 loss: 0.7191345001371949 grad: -0.06175302791313615
iteration: 30 loss: 0.5371118047962888 grad: -0.10048064343130599
iteration: 40 loss: 0.4336958105184088 grad: -0.10773765417962933
iteration: 50 loss: 0.3658542350820591 grad: -0.10626595987023438
iteration: 0 loss: 68.9860752247402 grad: 476.0870717969697
iteration: 10 loss: 1.5517580034783018 grad: 0.6560662427194587
iteration: 20 loss: 0.9436854639745682 grad: 0.06470585118329505
iteration: 30 loss: 0.691246735010943 grad: -0.05355050865193524
iteration: 40 loss: 0.5489443510185249 grad: -0.08888701528602
iteration: 50 loss: 0.45664728894115053 grad: -0.0997460851035813
iteration: 60 loss: 0.3916059054377415 grad: -0.10164159791876226
iteration: 0 loss: 92.9474575064339 grad: 588.3581061887638
iteration: 10 loss: 1.2982859723600664 grad: -0.027647064151886128
iteration: 20 loss: 0.8030730054443583 grad: -0.20781344397853752
iteration: 30 loss: 0.597153556849662 grad: -0.20669116275073618
iteration: 40 loss: 0.47997084893491276 grad: -0.18987213584451718
iteration: 50 loss: 0.40320119153181044 grad: -0.17288597586558394
iteration: 0 loss: 72.53187289928213 grad: 518.8822456379896
iteration: 10 loss: 1.4146352755123686 grad: 0.08246219774032855
iteration: 20 loss: 0.8681491959564895 grad: -0.17960586623427968
iteration: 30 loss: 0.6410097108860513 grad: -0.20004175381517286
iteration: 40 loss: 0.5122176745670995 grad: -0.19100162776617616
iteration: 50 loss: 0.42820678173037857 grad: -0.17723039233456392
iteration: 60 loss: 0.36870286599737895 grad: -0.163687461195749
iteration: 0 loss: 70.9148953724548 grad: 546.2482446459519
iteration: 10 loss: 1.3867919277223668 grad: -0.1909134775091324
iteration: 20 loss: 0.8502913325754972 grad: -0.27702852632393393
iteration: 30 loss: 0.627636560678101 grad: -0.2564435623036988
iteration: 40 loss: 0.501576295812232 grad: -0.22878113478256612
iteration: 50 loss: 0.4194235858532755 grad: -0.20458062478787695
iteration: 60 loss: 0.3612646889431 grad: -0.1845105804092483
iteration: 0 loss: 114.63166653033505 grad: 542.4909782529294
iteration: 10 loss: 1.2639735118493098 grad: 0.6806541795196577
iteration: 20 loss: 0.7750202557187797 grad: 0.11469012149347699
iteration: 30 loss: 0.5751934475635029 grad: -0.00843153697073342
iteration: 40 loss: 0.4621022186051546 grad: -0.05147114916409305
iteration: 50 loss: 0.3882126975324189 grad: -0.06894221803759783
iteration: 0 loss: 55.86446265231193 grad: 458.6056385276689
iteration: 10 loss: 1.6331003321169688 grad: 0.4987767912497193
iteration: 20 loss: 0.9803603027505481 grad: 0.033018750786966665
iteration: 30 loss: 0.7116395479586738 grad: -0.05887575541418977
iteration: 40 loss: 0.5616944596037104 grad: -0.08526328994027935
iteration: 50 loss: 0.46522034759469866 grad: -0.0926090648715104
iteration: 60 loss: 0.3976588031573556 grad: -0.09310873125274843
iteration: 0 loss: 128.38748993788195 grad: 562.7384869042417
iteration: 10 loss: 1.2139843197115956 grad: 0.40524755307804416
iteration: 20 loss: 0.7444298441489743 grad: 0.02059461708402263
iteration: 30 loss: 0.5541082164289203 grad: -0.05400152925644486
iteration: 40 loss: 0.4464338661054254 grad: -0.07627636331114732
iteration: 50 loss: 0.3759805114199976 grad: -0.08308121268512471
iteration: 0 loss: 110.98291027462784 grad: 571.6318085128775
iteration: 10 loss: 1.22177379960348 grad: 0.1827930225320783
iteration: 20 loss: 0.7435693426269526 grad: -0.09883450834538607
iteration: 30 loss: 0.5518251010389577 grad: -0.1335029000835123
iteration: 40 loss: 0.44378558646985766 grad: -0.13618645492671655
iteration: 50 loss: 0.3732740172237589 grad: -0.13117783151734025
iteration: 0 loss: 9308.627718818589 grad: 2793.569288471292
iteration: 10 loss: 0.0008184892173052173 grad: -0.30070480758933676
iteration: 0 loss: 7472.586560959514 grad: 2620.512150193568
iteration: 0 loss: 8285.397040913851 grad: 2653.6916150845545
iteration: 10 loss: 0.000714208343362605 grad: 0.03841129769838683
iteration: 0 loss: 10329.330114812738 grad: 2906.723666630788
iteration: 10 loss: 0.0006947549938393587 grad: -0.06309580410800983
iteration: 0 loss: 6403.014915673074 grad: 2530.2014434969888
iteration: 0 loss: 4266.510832366391 grad: 2254.815993863739
iteration: 0 loss: 6838.17567621494 grad: 2608.948070139344
iteration: 0 loss: 3663.5486364095905 grad: 2135.004289024529
iteration: 0 loss: 10121.022348919088 grad: 2732.5543824469682
iteration: 10 loss: 0.0006064812292937528 grad: -0.1770872641786842
iteration: 0 loss: 6660.819218849618 grad: 2544.430876806458
iteration: 0 loss: 4168.904831760273 grad: 2326.81543350803
iteration: 0 loss: 8500.63406116519 grad: 2732.6842993416476
iteration: 10 loss: 0.003141109526040964 grad: 0.02966557721359163
iteration: 0 loss: 5795.895677731908 grad: 2317.153599345458
iteration: 0 loss: 6263.466392731511 grad: 2584.648831861642
iteration: 0 loss: 8181.973901092729 grad: 2624.9661391070103
iteration: 0 loss: 11912.114257208525 grad: 2973.2110846222795
iteration: 10 loss: 0.01388800684468482 grad: -0.096122505374654
iteration: 0 loss: 7041.9990241670275 grad: 2551.548771091316
iteration: 0 loss: 4996.222382146148 grad: 2380.724805493628
iteration: 0 loss: 11977.963415525735 grad: 2978.248365203971
iteration: 10 loss: 0.013915656544585627 grad: -0.2111442702542416
iteration: 0 loss: 5884.36720486309 grad: 2410.5027849069743
iteration: 0 loss: 8513.719664300339 grad: 2732.2666570193824
iteration: 0 loss: 10269.324099457894 grad: 2837.735657578299
iteration: 10 loss: 0.03592678122434088 grad: 2.5797671185492965
iteration: 0 loss: 5300.968006971762 grad: 2289.949698790967
iteration: 0 loss: 6419.073862318669 grad: 2520.3024320368213
iteration: 10 loss: 0.0005956563929265195 grad: 0.46618577779731074
iteration: 0 loss: 6596.019084376297 grad: 2647.052305372416
iteration: 0 loss: 5668.316299254229 grad: 2549.6194651359647
iteration: 0 loss: 12863.23196078664 grad: 2993.304839304624
iteration: 10 loss: 0.021426675367613578 grad: 0.6448171668219559
iteration: 0 loss: 12503.082746915841 grad: 2894.9988142768334
iteration: 0 loss: 7299.537889890189 grad: 2607.524099343334
iteration: 0 loss: 5433.107463012111 grad: 2402.3064039199376
iteration: 10 loss: 0.0005460697099227797 grad: -0.1488865975805642
iteration: 0 loss: inf grad: 3166.2883336532077
iteration: 0 loss: 7259.032264983141 grad: 2752.549953367583
iteration: 0 loss: 6962.80698486435 grad: 2600.187698834602
iteration: 0 loss: 9721.14054143938 grad: 2698.502792506436
iteration: 10 loss: 0.0006704857673453675 grad: -0.36691699821715645
iteration: 0 loss: 9482.31410607383 grad: 2784.420216059395
iteration: 0 loss: 7977.46582383913 grad: 2703.751290112119
iteration: 10 loss: 0.0008129217206839133 grad: 0.7896285847339574
iteration: 0 loss: 7513.250945092872 grad: 2594.692427430744
iteration: 10 loss: 0.008898410268805244 grad: 0.41673136148600926
iteration: 0 loss: 12019.673578833206 grad: 2999.078661705754
iteration: 10 loss: 0.014870954817690124 grad: 0.5760682377767614
iteration: 0 loss: 5672.959612815179 grad: 2473.369058266294
iteration: 10 loss: 0.0007107789913954383 grad: 0.018069814001507017
iteration: 0 loss: 6252.834529175275 grad: 2516.7245464817024
iteration: 0 loss: 7898.302306655965 grad: 2637.798007018906
iteration: 10 loss: 0.0005880217901325192 grad: -0.005840807270884873
iteration: 0 loss: 4040.2184498684655 grad: 2290.2786494761913
iteration: 0 loss: 16720.417033372327 grad: 3261.906494538559
iteration: 0 loss: 6580.523890690299 grad: 2425.3429517232275
iteration: 10 loss: 0.000723805816960521 grad: 0.010360419984006664
iteration: 0 loss: 9102.870322845793 grad: 2918.2090775512233
iteration: 0 loss: 9326.325834744492 grad: 2698.571743011992
iteration: 0 loss: 7040.649958898189 grad: 2369.697596430802
iteration: 0 loss: 5849.630497663656 grad: 2562.073527965887
iteration: 0 loss: 10262.36281762235 grad: 2830.92772680526
iteration: 10 loss: 0.0006922945851163769 grad: -0.5979089578280811
iteration: 0 loss: 10703.189228969375 grad: 2844.4118613886594
iteration: 10 loss: 0.23959830933811396 grad: 1.0214275886426847
iteration: 0 loss: 12490.997776589233 grad: 2937.1047879822895
iteration: 0 loss: 6695.039371410806 grad: 2642.2805125535197
iteration: 0 loss: 11342.189434244632 grad: 3010.9267379173125
iteration: 10 loss: 0.0006256282821678641 grad: 0.45563849604605783
iteration: 0 loss: 8220.765740329107 grad: 2694.3961692411494
iteration: 0 loss: 5584.982463532826 grad: 2436.3317590121587
iteration: 0 loss: 7468.098147146549 grad: 2624.3510883035106
iteration: 10 loss: 0.0008326095915187827 grad: 0.016803807696545336
iteration: 0 loss: 5629.035848893169 grad: 2542.95987885412
iteration: 0 loss: 8155.341965957812 grad: 2689.87448455372
iteration: 10 loss: 0.0007160720613848588 grad: 0.05891937205627201
iteration: 0 loss: 12068.73602029574 grad: 3054.3563657578447
iteration: 10 loss: 0.0007414250867441297 grad: -0.009750837378600618
iteration: 0 loss: 6575.091553059867 grad: 2587.077823508474
iteration: 0 loss: 10644.877324600775 grad: 2858.008106242099
iteration: 10 loss: 0.015333288960391656 grad: 1.52189734783046
iteration: 0 loss: 10614.988456209545 grad: 3016.2606384121755
iteration: 10 loss: 0.0009199059938757935 grad: 0.007238127317837044
iteration: 0 loss: 5457.619617672852 grad: 2237.2879981343362
iteration: 0 loss: 5842.797078677418 grad: 2459.500972445011
iteration: 10 loss: 0.0005530920924767005 grad: 0.00023073477249429142
iteration: 0 loss: 6890.6529638510465 grad: 2663.1153990255125
iteration: 0 loss: 7153.747026277394 grad: 2602.6863986565704
iteration: 0 loss: 5372.792098060294 grad: 2345.031087195791
iteration: 0 loss: 9771.193882821819 grad: 2870.3207633776874
iteration: 10 loss: 0.0007499363643794575 grad: -0.009964017077089351
iteration: 0 loss: 7882.704598906234 grad: 2629.1868203035815
iteration: 10 loss: 0.000805619122481651 grad: 0.007141295815560252
iteration: 0 loss: 7895.239976485749 grad: 2798.1978927433684
iteration: 0 loss: 9569.05429992448 grad: 2850.512797161589
iteration: 10 loss: 0.0006078954995080659 grad: -0.0001857501551617549
iteration: 0 loss: 14335.001400836596 grad: 3173.3149924966924
iteration: 10 loss: 0.17834219882074234 grad: 4.639171742160331
iteration: 0 loss: 6017.751459937638 grad: 2451.789054825252
iteration: 0 loss: 10039.669994097932 grad: 2832.303168351379
iteration: 10 loss: 0.000716201042946936 grad: 0.06868705989598906
iteration: 0 loss: 6259.844368080212 grad: 2549.1212377248453
iteration: 0 loss: 5001.989295925456 grad: 2364.646723940723
iteration: 0 loss: 6437.417838911127 grad: 2389.5663108534404
iteration: 0 loss: 5240.044284923457 grad: 2371.4279936825005
iteration: 0 loss: 7616.8525015673795 grad: 2754.638842651594
iteration: 0 loss: 10557.436153727256 grad: 2795.012037972863
iteration: 10 loss: 0.0009328409743664616 grad: 0.01256047681383136
iteration: 0 loss: 12043.98408619081 grad: 3030.885294435717
iteration: 10 loss: 0.0007468428759073669 grad: 0.9597843895812272
iteration: 0 loss: 11107.560814121778 grad: 2890.2946834659515
iteration: 10 loss: 0.013676279311766848 grad: 4.275577545579326
iteration: 0 loss: 9034.444426914934 grad: 2780.584855495942
iteration: 0 loss: 11388.626456604468 grad: 2895.661538695154
iteration: 10 loss: 0.0008836404489226301 grad: 1.0287429978979492
iteration: 0 loss: 11315.712463332677 grad: 2902.121421730663
iteration: 10 loss: 0.0709220251698174 grad: 2.409154870313599
iteration: 0 loss: 8538.779300942238 grad: 2521.4293665911973
iteration: 10 loss: 0.026691579720301725 grad: 0.1870814281434375
iteration: 0 loss: 13359.515140121699 grad: 2976.584264246307
iteration: 10 loss: 0.06452803326696581 grad: 1.7199834992637923
iteration: 0 loss: 9206.948118661256 grad: 2668.525598270714
iteration: 10 loss: 0.0007959505561633374 grad: 0.6239906658036751
iteration: 0 loss: 4995.672384225326 grad: 2285.6990194144255
iteration: 0 loss: 4366.937932266819 grad: 2170.7460531445504
iteration: 0 loss: 5995.560450822001 grad: 2380.8885572529884
iteration: 0 loss: 7942.869088534148 grad: 2732.858914518617
iteration: 0 loss: 7645.067312699102 grad: 2564.127083741195
iteration: 10 loss: 0.0005319501281271434 grad: 0.5975440035644829
iteration: 0 loss: 6216.809052544008 grad: 2485.348093063134
iteration: 0 loss: 5385.843134866595 grad: 2406.942793568021
iteration: 0 loss: 10941.94253398605 grad: 3019.8607468804225
iteration: 10 loss: 0.000755468539533798 grad: 1.157295368286645
iteration: 0 loss: 8846.073059498462 grad: 2738.6595940814123
iteration: 10 loss: 0.002312666703470644 grad: 0.6354529584073701
iteration: 0 loss: 8120.736713419352 grad: 2645.7604228744044
iteration: 10 loss: 0.0007758264109725133 grad: 0.2544514419344059
iteration: 0 loss: 7410.622827344341 grad: 2622.370364526799
iteration: 0 loss: 5313.089291065079 grad: 2262.847760034871
iteration: 0 loss: 8563.137178773371 grad: 2709.342224245983
iteration: 0 loss: 6488.223466257879 grad: 2506.1461279418118
iteration: 0 loss: 6656.650072867114 grad: 2636.536658744227
iteration: 10 loss: 0.0009430577735077928 grad: -0.021371031041207442
iteration: 0 loss: 14678.370877024978 grad: 3243.582970534898
iteration: 0 loss: 4951.6714774824395 grad: 2377.815624987762
iteration: 0 loss: 4834.862479739746 grad: 2289.8161796075547
iteration: 0 loss: 5796.422926152281 grad: 2409.5026610896684
iteration: 0 loss: 7295.924573902847 grad: 2685.097909964007
iteration: 0 loss: 8763.280142622158 grad: 2753.853080699708
iteration: 10 loss: 0.0006169334678402679 grad: -0.033548341804249315
iteration: 0 loss: 10016.25119298692 grad: 2938.8514493107464
iteration: 10 loss: 0.0009010136987357823 grad: 2.3358447810013048
iteration: 0 loss: 12407.833076547377 grad: 2887.1643912137915
iteration: 0 loss: 9810.090919982857 grad: 2777.1634199895334
iteration: 10 loss: 0.06854635263814336 grad: 0.931797430988209
iteration: 0 loss: 7594.664331303034 grad: 2638.0123789922486
iteration: 0 loss: 12540.909027766085 grad: 2974.3002708606264
iteration: 10 loss: 0.021966808261243965 grad: 2.623531812930619
iteration: 0 loss: 9471.711313877913 grad: 2777.30132492511
iteration: 0 loss: 6168.270372801288 grad: 2607.9380690162925
iteration: 10 loss: 0.0008184531351699578 grad: 0.0013430038760597093
iteration: 0 loss: 7125.985887343503 grad: 2618.4032582864265
iteration: 0 loss: 6492.783929512402 grad: 2564.578941791935
iteration: 10 loss: 0.0007022215167208659 grad: 0.00848690607895332
iteration: 0 loss: 11326.882270023414 grad: 2978.6378760940133
iteration: 10 loss: 0.09500825046879155 grad: 4.336613732498729
iteration: 0 loss: 5960.727402043153 grad: 2628.372682105467
iteration: 10 loss: 0.0007748789423865012 grad: 0.00047423600719137234
iteration: 0 loss: 7209.670142005568 grad: 2503.521782865868
iteration: 0 loss: 4091.5821469858183 grad: 2237.9058524443963
iteration: 0 loss: 8482.974403024251 grad: 2855.8091498452695
iteration: 10 loss: 0.000699068801953796 grad: 0.01852279672589477
iteration: 0 loss: 8244.392833263451 grad: 2777.976149773983
iteration: 10 loss: 0.0008878378999229012 grad: 0.011635652066698952
iteration: 0 loss: 8208.177867289029 grad: 2739.7796907849524
iteration: 10 loss: 0.01061727990137032 grad: -0.6861726280045343
iteration: 0 loss: 11588.83475601909 grad: 2844.4921225427015
iteration: 10 loss: 0.23600590724593282 grad: 1.8561986380223239
iteration: 0 loss: 6703.094623488945 grad: 2576.342708663741
iteration: 0 loss: 7750.2344375625835 grad: 2686.7968933002207
iteration: 0 loss: 11834.196061270775 grad: 2989.865659047224
iteration: 10 loss: 0.0009177262863059613 grad: 0.010331246227633414
iteration: 0 loss: 7499.051642579459 grad: 2551.327538590897
iteration: 0 loss: 10143.165549898285 grad: 2733.372985069356
iteration: 10 loss: 0.0006751743547449058 grad: 0.005508565277903162
iteration: 0 loss: 6519.081922758704 grad: 2607.1211115678616
iteration: 0 loss: 11099.433998124952 grad: 3004.589711793865
iteration: 10 loss: 0.0007418326356194236 grad: -0.00307025744443772
iteration: 0 loss: 9496.005695086553 grad: 2846.7999517748885
iteration: 0 loss: 6144.633270139954 grad: 2365.2805451711606
iteration: 0 loss: 4825.465348270805 grad: 2288.11613402767
iteration: 0 loss: 7882.942281331169 grad: 2823.962698445771
iteration: 0 loss: 4044.2616277436773 grad: 2196.4550318746365
iteration: 0 loss: 5358.316856443626 grad: 2217.168688717347
iteration: 0 loss: 4756.01756871103 grad: 2275.018396104352
iteration: 0 loss: 11340.750981822874 grad: 2898.1752467500023
iteration: 10 loss: 0.0006932552840391344 grad: 0.8904148794049249
iteration: 0 loss: 6861.6344928129665 grad: 2484.532339915173
iteration: 0 loss: 12014.15773216028 grad: 2889.5290460285305
iteration: 10 loss: 0.9690256789776455 grad: 2.037845614900506
iteration: 0 loss: 11249.180625719628 grad: 2894.074387189112
iteration: 10 loss: 0.0006285494680262425 grad: 0.5265687769988799
iteration: 0 loss: 13439.048023427791 grad: 3100.1534146769436
iteration: 10 loss: 0.12723523289473218 grad: 3.5006326961705017
iteration: 0 loss: 6081.600482039093 grad: 2454.7662742983284
iteration: 0 loss: 6164.972669325024 grad: 2486.4552305524785
iteration: 0 loss: 8045.409124484238 grad: 2796.420032048508
iteration: 10 loss: 0.0007356864344646139 grad: 0.00576525834849362
iteration: 0 loss: 8201.972017598024 grad: 2705.5350663416666
iteration: 0 loss: 15329.366374412326 grad: 3132.4072749388724
iteration: 0 loss: 6855.289987842503 grad: 2650.2940194006223
iteration: 0 loss: 11869.704585936828 grad: 3000.5054779934853
iteration: 10 loss: 0.002898415407716212 grad: 0.9358553978000572
iteration: 0 loss: 7785.595819777029 grad: 2634.7916560022095
iteration: 0 loss: 5289.624749921487 grad: 2478.3046897821623
iteration: 0 loss: 11086.728958565198 grad: 2944.4025484120157
iteration: 10 loss: 0.0013857756593179974 grad: 1.476555680482832
iteration: 0 loss: 5560.585596954647 grad: 2468.838533536108
iteration: 0 loss: 13457.780836133734 grad: 2994.4476454461123
iteration: 10 loss: 1.00435382324345 grad: 6.726663952088895
iteration: 0 loss: 4940.40735065534 grad: 2343.9520027087246
iteration: 0 loss: 10469.057042137863 grad: 2849.191354682852
iteration: 10 loss: 0.058677096324655315 grad: 0.6208965929728099
iteration: 0 loss: 5364.696166785879 grad: 2324.022248846729
iteration: 0 loss: 8137.939564136549 grad: 2833.579511120699
iteration: 10 loss: 0.0010881169013869526 grad: 0.18313084258351503
iteration: 0 loss: 5919.851812355556 grad: 2528.3290758564553
iteration: 0 loss: 8033.966283268025 grad: 2633.976212770729
iteration: 10 loss: 0.0010738112945215437 grad: -0.23392176549581642
iteration: 0 loss: 7647.952015218759 grad: 2628.042414863452
iteration: 0 loss: 4385.810388134369 grad: 2240.8841692824535
iteration: 0 loss: 9677.94738846089 grad: 2719.5643225379827
iteration: 10 loss: 0.0008773824004244736 grad: 0.4709154743691014
iteration: 0 loss: 9380.601129446704 grad: 2753.0532956680404
iteration: 10 loss: 0.0007275870091027834 grad: 0.6680813546247382
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:57: RuntimeWarning: invalid value encountered in double_scalars
  diff = np.absolute(f_history[-1]-f_history[-2])
iteration: 0 loss: 2155.0657255264377 grad: 1520.3206349613065
iteration: 0 loss: 1714.8656292715664 grad: 1425.8902169278902
iteration: 0 loss: 1861.5302234937883 grad: 1442.0309849497344
iteration: 0 loss: 2431.1929943196506 grad: 1582.2853110401884
iteration: 10 loss: 0.014022555048789152 grad: 0.05395776942284774
iteration: 0 loss: 1466.1926015582808 grad: 1375.756781079379
iteration: 0 loss: 977.6388771578294 grad: 1225.3456818698698
iteration: 0 loss: 1579.1794774788818 grad: 1416.0227322078954
iteration: 0 loss: 807.92159273738 grad: 1161.2683556653215
iteration: 0 loss: 2405.7579556752758 grad: 1486.8107109502625
iteration: 0 loss: 1537.1540409751515 grad: 1382.495158503169
iteration: 0 loss: 918.766149130544 grad: 1263.4860745208757
iteration: 0 loss: 1913.039824246345 grad: 1486.5204115353774
iteration: 10 loss: 0.015600952453413893 grad: 0.06685235932456421
iteration: 0 loss: 1347.5158947199998 grad: 1259.3400029212125
iteration: 0 loss: 1411.8391291647185 grad: 1405.028178407821
iteration: 0 loss: 1896.9544240894413 grad: 1427.4036066357226
iteration: 0 loss: 2731.4819412200773 grad: 1617.7405102440466
iteration: 10 loss: 0.01638597788670185 grad: 0.058941288204632514
iteration: 0 loss: 1623.3220543071234 grad: 1387.285275121012
iteration: 0 loss: 1147.556757424617 grad: 1294.160040081034
iteration: 0 loss: 2849.447041347269 grad: 1622.5467586479494
iteration: 10 loss: 0.03140436680405401 grad: 0.17194308815465997
iteration: 0 loss: 1338.0435628366652 grad: 1309.6972611334788
iteration: 0 loss: 1945.4990539552373 grad: 1485.051466922111
iteration: 0 loss: 2349.3531683395604 grad: 1545.9486918169018
iteration: 10 loss: 0.03770288279602854 grad: -0.6787712613317702
iteration: 0 loss: 1256.297996365906 grad: 1244.3544915219659
iteration: 0 loss: 1392.7027791862713 grad: 1372.2683691264929
iteration: 0 loss: 1496.18125395846 grad: 1437.849773548583
iteration: 0 loss: 1310.1198055108346 grad: 1386.999986843994
iteration: 0 loss: 3086.8720136420375 grad: 1630.3824815221392
iteration: 10 loss: 0.01755330096387876 grad: 0.1880080747764488
iteration: 0 loss: 2944.1504978006183 grad: 1578.3241809349463
iteration: 10 loss: 0.23499205428643935 grad: 2.0589203161455654
iteration: 0 loss: 1609.1967551400544 grad: 1418.147054460543
iteration: 0 loss: 1253.1883155030162 grad: 1305.8438430207284
iteration: 0 loss: 3508.1418157475146 grad: 1721.955068085111
iteration: 10 loss: 0.02410389428273034 grad: 0.2680520615388839
iteration: 0 loss: 1660.0130753592239 grad: 1499.1071635270616
iteration: 0 loss: 1597.7591041758633 grad: 1413.9081486514533
iteration: 0 loss: 2246.9499463330812 grad: 1468.4367705616946
iteration: 10 loss: 0.01549517630734375 grad: 0.11816498361692265
iteration: 0 loss: 2212.4053340809182 grad: 1516.1123773396903
iteration: 10 loss: 0.014757643254018727 grad: 0.0683849854007463
iteration: 0 loss: 1829.5223619520614 grad: 1472.786483838661
iteration: 10 loss: 0.01723954852456121 grad: 0.3709701198885489
iteration: 0 loss: 1743.5723607978432 grad: 1411.8938951414034
iteration: 0 loss: 2746.692697353737 grad: 1631.8049831936505
iteration: 10 loss: 0.0159859532418936 grad: 0.4546196104743283
iteration: 0 loss: 1310.416898427996 grad: 1343.833480573035
iteration: 0 loss: 1399.1553773752266 grad: 1369.340351480951
iteration: 0 loss: 1917.577569891953 grad: 1434.9255532317106
iteration: 0 loss: 891.1718670014544 grad: 1244.3095248891536
iteration: 0 loss: 3952.702273688845 grad: 1776.4549534143239
iteration: 10 loss: 0.16217624613636872 grad: 0.3967048872096246
iteration: 0 loss: 1571.8580335992292 grad: 1319.5461919308586
iteration: 0 loss: 2056.1962748438905 grad: 1588.1495713783925
iteration: 0 loss: 2271.6002801344544 grad: 1467.8499639275963
iteration: 0 loss: 1731.2072161954522 grad: 1286.3612304781288
iteration: 0 loss: 1330.6571555369244 grad: 1392.0509576986842
iteration: 0 loss: 2381.4567670796223 grad: 1541.0084281967468
iteration: 10 loss: 0.015752629640205254 grad: 0.29977335865158916
iteration: 0 loss: 2536.7278272854646 grad: 1548.8852127239848
iteration: 10 loss: 0.025348094561856917 grad: 0.03593596320094668
iteration: 0 loss: 2888.139190558556 grad: 1597.7955274719875
iteration: 10 loss: 0.16112469616332833 grad: 1.0289397662324509
iteration: 0 loss: 1500.3865983927162 grad: 1436.7613589383254
iteration: 0 loss: 2615.120092474828 grad: 1637.6512607165685
iteration: 10 loss: 0.01549367792838232 grad: 0.0718329982885213
iteration: 0 loss: 1940.4937968935728 grad: 1464.947136766019
iteration: 0 loss: 1242.2197122240962 grad: 1322.2103678982398
iteration: 0 loss: 1689.0203907991997 grad: 1426.3413763164722
iteration: 0 loss: 1217.3134501219197 grad: 1382.0989797840764
iteration: 0 loss: 1885.1194569775114 grad: 1463.5203365649486
iteration: 0 loss: 2869.3301868587237 grad: 1663.1465102621617
iteration: 10 loss: 0.015193631035799626 grad: 0.06319672464198521
iteration: 0 loss: 1495.2668567966327 grad: 1405.6340968775914
iteration: 0 loss: 2461.9203574314206 grad: 1555.0988012334797
iteration: 10 loss: 0.03404150660340251 grad: 0.7881857371231701
iteration: 0 loss: 2442.2555704044285 grad: 1640.4928852008175
iteration: 10 loss: 0.014925901555794884 grad: 0.039649616434034696
iteration: 0 loss: 1238.2977386892271 grad: 1212.9460154028366
iteration: 0 loss: 1267.1639343275356 grad: 1336.6261849051145
iteration: 0 loss: 1589.5329357528165 grad: 1449.6408056438243
iteration: 0 loss: 1647.150019009183 grad: 1417.4556888057043
iteration: 0 loss: 1258.5893146406797 grad: 1274.359428624372
iteration: 0 loss: 2256.390891884071 grad: 1561.011856951493
iteration: 10 loss: 0.015568799781745342 grad: 0.1480271456072889
iteration: 0 loss: 1900.2638920989514 grad: 1430.1550513328384
iteration: 10 loss: 0.01624099049579606 grad: 0.08732757558332671
iteration: 0 loss: 1806.0374434028736 grad: 1522.7844212314694
iteration: 10 loss: 0.01493555997612103 grad: 0.0695893229860858
iteration: 0 loss: 2172.2920820372465 grad: 1551.5303786592467
iteration: 10 loss: 0.013788574227047238 grad: 0.06452875400176669
iteration: 0 loss: 3392.737959897687 grad: 1728.6099351176167
iteration: 10 loss: 0.06898268799704965 grad: -1.2090803074526708
iteration: 0 loss: 1404.0378008791622 grad: 1332.0492312617089
iteration: 0 loss: 2304.150544037828 grad: 1541.1103394007646
iteration: 10 loss: 0.015149586309483064 grad: 0.022404386989788003
iteration: 0 loss: 1464.8225822150177 grad: 1387.2187954692463
iteration: 0 loss: 1138.9886127286138 grad: 1286.4660046471295
iteration: 0 loss: 1506.2559714329839 grad: 1299.677695689365
iteration: 0 loss: 1174.879363366736 grad: 1290.0801785029662
iteration: 0 loss: 1735.172738565384 grad: 1499.43240925587
iteration: 0 loss: 2526.3813512632723 grad: 1518.7252310710178
iteration: 10 loss: 0.01601647590459007 grad: -0.41396004371089223
iteration: 0 loss: 2802.544018226696 grad: 1650.1202107802478
iteration: 10 loss: 0.013317522761512506 grad: 0.0342742233126736
iteration: 0 loss: 2613.9575823479595 grad: 1573.941782710448
iteration: 10 loss: 0.04005117438910846 grad: -0.6399152483492895
iteration: 0 loss: 2060.2453866103183 grad: 1512.6810782732537
iteration: 0 loss: 2693.842184421608 grad: 1574.9475837021614
iteration: 10 loss: 0.017856993031852075 grad: 0.3993117693309851
iteration: 0 loss: 2644.136581360842 grad: 1579.9060496869538
iteration: 10 loss: 0.038360098568690854 grad: 0.36695221599334804
iteration: 0 loss: 2087.9268118677733 grad: 1370.1964545745982
iteration: 0 loss: 3263.2949048400665 grad: 1620.770756809508
iteration: 10 loss: 0.015659685804910787 grad: 0.33769382011659044
iteration: 0 loss: 2237.5503646638485 grad: 1449.8649772747938
iteration: 10 loss: 0.014136640794194218 grad: -0.012140920215835467
iteration: 0 loss: 1107.2986876510122 grad: 1240.6188824700355
iteration: 0 loss: 1002.5578693271934 grad: 1179.088960196103
iteration: 0 loss: 1408.14975230785 grad: 1293.3599761568144
iteration: 0 loss: 1851.5017283700968 grad: 1486.336943551889
iteration: 0 loss: 1735.8427906868405 grad: 1392.7315098935292
iteration: 0 loss: 1393.2743800361636 grad: 1349.3580035585915
iteration: 0 loss: 1210.603110983683 grad: 1307.6787049043503
iteration: 0 loss: 2549.2681250302403 grad: 1643.7761709701208
iteration: 10 loss: 0.014557570931422313 grad: 0.05340586732425639
iteration: 0 loss: 2083.4712939782094 grad: 1490.9141725556756
iteration: 10 loss: 0.016237490669564893 grad: 0.32391497615371534
iteration: 0 loss: 1971.0131287624272 grad: 1438.8339670246587
iteration: 0 loss: 1691.1164183483183 grad: 1425.958263226688
iteration: 0 loss: 1272.8965091832622 grad: 1227.9392506360841
iteration: 0 loss: 2037.280374389285 grad: 1474.2696855739
iteration: 0 loss: 1483.905539823358 grad: 1361.2947348537243
iteration: 0 loss: 1482.7338390549094 grad: 1433.847278379179
iteration: 0 loss: 3412.4579832907766 grad: 1767.658219279057
iteration: 10 loss: 0.05362884302237283 grad: 0.6218081827477839
iteration: 0 loss: 1102.8440078285314 grad: 1292.195288109589
iteration: 0 loss: 1069.412052365764 grad: 1243.5469631140857
iteration: 0 loss: 1298.914736596629 grad: 1308.138898872746
iteration: 0 loss: 1630.8236804341425 grad: 1460.957504056858
iteration: 0 loss: 2057.1041353359933 grad: 1498.2734793898762
iteration: 0 loss: 2296.557692554689 grad: 1598.8885820183737
iteration: 10 loss: 0.015231727697193326 grad: 0.09577879771974868
iteration: 0 loss: 2802.913517342589 grad: 1570.3412572921102
iteration: 10 loss: 0.02687445254758297 grad: 1.0102273537215563
iteration: 0 loss: 2191.2125230672386 grad: 1510.4562298449582
iteration: 10 loss: 0.03135809620024785 grad: 1.0337768858666099
iteration: 0 loss: 1727.8306561567158 grad: 1435.5194974528904
iteration: 0 loss: 2935.201351943264 grad: 1620.7122881111627
iteration: 10 loss: 0.020271930876664224 grad: -0.9130743259650201
iteration: 0 loss: 2224.9745824853644 grad: 1510.3010617814161
iteration: 0 loss: 1334.4470289628573 grad: 1418.6565376778299
iteration: 10 loss: 0.01463993132413386 grad: 0.14397068876902613
iteration: 0 loss: 1660.1257013791023 grad: 1422.5339844616065
iteration: 0 loss: 1482.6473772066122 grad: 1396.4309379279234
iteration: 0 loss: 2582.3303815797813 grad: 1622.0667936835152
iteration: 10 loss: 0.0327871384513194 grad: -0.21598903066097563
iteration: 0 loss: 1334.219708226584 grad: 1428.1781245064392
iteration: 10 loss: 0.01436295791328038 grad: 0.1059710607872171
iteration: 0 loss: 1690.233719003566 grad: 1361.4813658421785
iteration: 0 loss: 899.8814028585313 grad: 1214.6042884651438
iteration: 0 loss: 2007.7209002477116 grad: 1554.2168449494318
iteration: 0 loss: 1894.9194444594277 grad: 1511.4616777201675
iteration: 0 loss: 1887.5310038754542 grad: 1489.0441513035462
iteration: 10 loss: 0.015072391541748815 grad: 0.014693636475231996
iteration: 0 loss: 2818.8181512361966 grad: 1548.9976789919288
iteration: 10 loss: 0.016636852793453727 grad: 0.6105357343384398
iteration: 0 loss: 1509.775156626202 grad: 1401.122130210797
iteration: 0 loss: 1772.0786072130147 grad: 1460.823390772844
iteration: 10 loss: 0.013262698037248232 grad: 0.05278407846443219
iteration: 0 loss: 2850.618728344467 grad: 1627.7175517595733
iteration: 10 loss: 0.015467899969239212 grad: -0.056422944508480705
iteration: 0 loss: 1726.2090092927874 grad: 1386.9702497312205
iteration: 0 loss: 2382.9794429768135 grad: 1484.9171924328105
iteration: 10 loss: 0.014255285672499502 grad: 0.06383348666541072
iteration: 0 loss: 1483.1044914228003 grad: 1417.6642162463154
iteration: 0 loss: 2599.9853119851464 grad: 1635.2678543081634
iteration: 0 loss: 2255.8513008170867 grad: 1549.7168193860402
iteration: 10 loss: 0.012863940942307554 grad: 0.09767291758761298
iteration: 0 loss: 1448.784094445572 grad: 1283.9298913901757
iteration: 0 loss: 1108.2579423370837 grad: 1243.5770176798978
iteration: 0 loss: 1769.3901709673532 grad: 1535.8382265418895
iteration: 0 loss: 875.9895708643985 grad: 1191.1954135697783
iteration: 0 loss: 1295.2816123820194 grad: 1206.2909255263369
iteration: 0 loss: 1094.0970763372295 grad: 1237.631821142878
iteration: 0 loss: 2664.8470165229573 grad: 1577.7549799790145
iteration: 0 loss: 1614.1361880913903 grad: 1349.5611558634855
iteration: 10 loss: 0.01483873147861397 grad: 0.05719639641034846
iteration: 0 loss: 2726.3065852489444 grad: 1571.2619815981582
iteration: 10 loss: 0.14356703422659353 grad: 0.46070893444505273
iteration: 0 loss: 2631.2815129232367 grad: 1572.1450117179086
iteration: 10 loss: 0.015026544421826575 grad: 0.12507660500876222
iteration: 0 loss: 3118.14953055556 grad: 1688.4231918484782
iteration: 10 loss: 0.02598096688135146 grad: 0.5115400190758421
iteration: 0 loss: 1373.9658124404916 grad: 1333.0278820003434
iteration: 0 loss: 1412.209611712032 grad: 1351.485187223327
iteration: 0 loss: 1824.5667691931303 grad: 1521.1517764936027
iteration: 0 loss: 1873.8464720568822 grad: 1469.3739677747953
iteration: 0 loss: 3592.59929278574 grad: 1705.541983632082
iteration: 10 loss: 0.024294497361221478 grad: 1.1382582745269993
iteration: 0 loss: 1528.1432962776505 grad: 1441.1740610301183
iteration: 0 loss: 2736.6129702418893 grad: 1632.2129261928765
iteration: 10 loss: 0.019208373344471594 grad: 0.6498432321985426
iteration: 0 loss: 1797.7457982240912 grad: 1430.440539762798
iteration: 0 loss: 1154.6768168929402 grad: 1349.569660634566
iteration: 0 loss: 2620.5098943204107 grad: 1601.494901222415
iteration: 10 loss: 0.016141729709157848 grad: 0.14768942386067002
iteration: 0 loss: 1266.948739376547 grad: 1339.8077672317115
iteration: 0 loss: 3124.727771931562 grad: 1628.301422186996
iteration: 10 loss: 0.2562605283713889 grad: 2.3389847850718373
iteration: 0 loss: 1115.5210660758487 grad: 1273.778597371475
iteration: 0 loss: 2461.7348501939896 grad: 1551.1640603000076
iteration: 0 loss: 1244.073591222163 grad: 1263.2760671934125
iteration: 0 loss: 1864.3944128709518 grad: 1541.460560235505
iteration: 0 loss: 1343.8798286271358 grad: 1373.937660668656
iteration: 0 loss: 1784.854866113761 grad: 1431.499927899531
iteration: 10 loss: 0.015272393572641622 grad: 0.03920215569218262
iteration: 0 loss: 1820.962317551455 grad: 1430.2054762119017
iteration: 0 loss: 977.5013454328969 grad: 1217.5645856059966
iteration: 0 loss: 2283.940326604288 grad: 1480.0807408667556
iteration: 10 loss: 0.014230855280237103 grad: 0.0813365035394383
iteration: 0 loss: 2180.1591036896843 grad: 1498.6281556999127
iteration: 10 loss: 0.016199577726762403 grad: 0.027827710530484977
iteration: 0 loss: 3560.344485397886 grad: 1861.1407986421973
iteration: 10 loss: 0.005665082366745496 grad: 0.08388806628793175
iteration: 0 loss: 2864.168978574884 grad: 1745.9827649296194
iteration: 0 loss: 3148.3034740336534 grad: 1765.974175284041
iteration: 0 loss: 4010.8617927420883 grad: 1935.5994486278828
iteration: 0 loss: 2440.677853196904 grad: 1684.4944720891867
iteration: 0 loss: 1649.787685877381 grad: 1501.072470544942
iteration: 10 loss: 0.00583423560469369 grad: -0.02042923052381749
iteration: 0 loss: 2642.884621978726 grad: 1736.98678803992
iteration: 0 loss: 1384.9284550660384 grad: 1420.7371611346844
iteration: 0 loss: 3986.7395155235686 grad: 1820.9750289086987
iteration: 0 loss: 2560.282245965828 grad: 1692.7151522551353
iteration: 0 loss: 1564.6461164326815 grad: 1547.7857744881785
iteration: 0 loss: 3241.2715384733606 grad: 1822.4138067278907
iteration: 10 loss: 0.005614257901139685 grad: 0.09666336580925494
iteration: 0 loss: 2235.3210470183017 grad: 1540.0985386488878
iteration: 0 loss: 2370.5868869718406 grad: 1720.9372326279636
iteration: 0 loss: 3176.3747417776863 grad: 1747.6679918292612
iteration: 0 loss: 4569.106686992623 grad: 1978.4592016039605
iteration: 10 loss: 0.007866173660493752 grad: 0.5701446605132233
iteration: 0 loss: 2716.1172161611107 grad: 1700.5754247931834
iteration: 0 loss: 1892.9555722379898 grad: 1581.856047503827
iteration: 0 loss: 4679.361069680464 grad: 1985.0279853680117
iteration: 10 loss: 0.00753636128898837 grad: 0.28005501264469906
iteration: 0 loss: 2250.6517275577844 grad: 1606.254313781274
iteration: 0 loss: 3254.9573687004845 grad: 1818.7524414782024
iteration: 0 loss: 3925.3815576770007 grad: 1891.4201913815655
iteration: 10 loss: 0.006576039758246307 grad: 0.015854808844467785
iteration: 0 loss: 2050.6764604540695 grad: 1524.5539596577207
iteration: 0 loss: 2400.4889045627056 grad: 1681.5026266686891
iteration: 0 loss: 2524.382289575289 grad: 1760.5640879025177
iteration: 0 loss: 2183.0475203345613 grad: 1699.9262529246462
iteration: 0 loss: 5039.70546760443 grad: 1993.9679470405022
iteration: 10 loss: 0.011502960002293217 grad: -0.3925846687745238
iteration: 0 loss: 4865.560816023436 grad: 1930.747297363007
iteration: 10 loss: 8.60509919838329 grad: -15.94978943189147
iteration: 0 loss: 2726.7541062285636 grad: 1736.445150868034
iteration: 0 loss: 2089.804361021728 grad: 1599.96156287765
iteration: 0 loss: 5774.29774621436 grad: 2108.0240606217085
iteration: 10 loss: 0.04868058214858386 grad: 1.0614410834280474
iteration: 0 loss: 2779.7306495240573 grad: 1835.8047433140412
iteration: 0 loss: 2668.1284062086784 grad: 1732.805938387458
iteration: 0 loss: 3759.275510709257 grad: 1798.213972170872
iteration: 10 loss: 0.005259707576029045 grad: 0.5342758318454779
iteration: 0 loss: 3685.927928102936 grad: 1856.3136857130162
iteration: 10 loss: 0.0052057927453213115 grad: -0.27124236981748284
iteration: 0 loss: 3077.949952872671 grad: 1804.6772269180037
iteration: 10 loss: 0.028496814691937867 grad: 0.7964175358362044
iteration: 0 loss: 2889.6960152160286 grad: 1729.1410678669508
iteration: 0 loss: 4603.557246598518 grad: 1996.7928277943886
iteration: 10 loss: 0.030246909711091906 grad: 1.2700585724029767
iteration: 0 loss: 2166.3907871460196 grad: 1645.9751207075933
iteration: 0 loss: 2373.5095852003183 grad: 1678.097026517697
iteration: 0 loss: 3094.7559037760907 grad: 1758.1474122405375
iteration: 0 loss: 1508.989807562915 grad: 1525.7189539418582
iteration: 0 loss: 6525.645873931423 grad: 2173.887966123782
iteration: 10 loss: 0.40597950447144365 grad: 2.2311687722605424
iteration: 0 loss: 2581.9200445140964 grad: 1618.5865096730677
iteration: 0 loss: 3459.966602059535 grad: 1943.4968313363074
iteration: 0 loss: 3700.701610690074 grad: 1797.4266461579425
iteration: 0 loss: 2827.206751393859 grad: 1577.7469510739115
iteration: 10 loss: 0.004725134088816544 grad: 0.022485582508532893
iteration: 0 loss: 2231.462813520943 grad: 1706.2380860985377
iteration: 0 loss: 3963.925006484946 grad: 1885.1302198724366
iteration: 10 loss: 0.006018805746200748 grad: 0.41738616831118447
iteration: 0 loss: 4152.434553837618 grad: 1895.0474255194183
iteration: 10 loss: 0.01969390292188936 grad: 0.00795272973130965
iteration: 0 loss: 4821.781443851002 grad: 1955.9017358056428
iteration: 10 loss: 0.27397703340896196 grad: 2.0977630116270785
iteration: 0 loss: 2546.675331505234 grad: 1758.2253836310335
iteration: 0 loss: 4364.048147756837 grad: 2005.9387316476666
iteration: 10 loss: 0.0057134588946196755 grad: 0.02743524945843871
iteration: 0 loss: 3214.4744399888264 grad: 1794.0183980582408
iteration: 0 loss: 2107.924852932957 grad: 1622.2942584086159
iteration: 0 loss: 2836.488012671234 grad: 1746.754786132402
iteration: 0 loss: 2092.032463794814 grad: 1695.4337312257885
iteration: 0 loss: 3163.760624193671 grad: 1793.0280102757904
iteration: 0 loss: 4732.971791105162 grad: 2035.2282771971998
iteration: 10 loss: 0.005153022026537325 grad: -0.007566934300011818
iteration: 0 loss: 2519.674885050879 grad: 1723.1181261476956
iteration: 0 loss: 4107.269191423817 grad: 1904.8364770478192
iteration: 10 loss: 0.03309632982167055 grad: 0.8625065642133554
iteration: 0 loss: 4073.6321364656615 grad: 2007.0296564890411
iteration: 10 loss: 0.005367345062206203 grad: -0.04032775305945164
iteration: 0 loss: 2118.2791373687264 grad: 1490.835323982637
iteration: 0 loss: 2178.231510885937 grad: 1638.1719041518463
iteration: 0 loss: 2668.2335897173875 grad: 1774.2240376871557
iteration: 0 loss: 2749.0722933414127 grad: 1736.0720649535915
iteration: 0 loss: 2087.5641206142577 grad: 1562.552602033326
iteration: 0 loss: 3765.6587903263294 grad: 1911.9491582137655
iteration: 10 loss: 0.005680062346403825 grad: 0.0719606374087167
iteration: 0 loss: 3117.8232476596527 grad: 1751.2247110779113
iteration: 10 loss: 0.0051545490833549675 grad: 0.01566775341259126
iteration: 0 loss: 3038.9283317864715 grad: 1862.4951983939131
iteration: 0 loss: 3636.799174930985 grad: 1899.5557621564742
iteration: 0 loss: 5588.524141501508 grad: 2114.312062674608
iteration: 10 loss: 0.08155757420569319 grad: 1.7735489939002307
iteration: 0 loss: 2338.5518479428447 grad: 1631.2167904105406
iteration: 0 loss: 3856.2807420581667 grad: 1886.4729140910483
iteration: 0 loss: 2429.6089721697817 grad: 1696.876930885636
iteration: 0 loss: 1932.8361877527868 grad: 1576.2600227289213
iteration: 0 loss: 2477.545701873789 grad: 1590.5415074096752
iteration: 0 loss: 1977.6662473421156 grad: 1578.9305926736238
iteration: 0 loss: 2903.9466634773594 grad: 1836.661419218608
iteration: 0 loss: 4155.408597061987 grad: 1861.7661664615835
iteration: 10 loss: 0.007488721779902169 grad: 0.908603334643134
iteration: 0 loss: 4655.076375180355 grad: 2017.2938093795156
iteration: 10 loss: 0.005734708328831898 grad: 0.9950217304335414
iteration: 0 loss: 4307.738002301376 grad: 1927.2181309070018
iteration: 10 loss: 0.06723371505094375 grad: -0.031023576636983963
iteration: 0 loss: 3465.0635328007456 grad: 1851.7242684216326
iteration: 0 loss: 4443.270827003082 grad: 1926.313224891018
iteration: 10 loss: 0.005337590437689373 grad: 0.036971271958512214
iteration: 0 loss: 4386.5577548958945 grad: 1931.4439843532311
iteration: 10 loss: 0.04515443741878367 grad: 0.5645487449097202
iteration: 0 loss: 3362.336659054847 grad: 1680.575255504944
iteration: 0 loss: 5324.505629209407 grad: 1985.073482608713
iteration: 10 loss: 0.005960994182466741 grad: 0.8410829821480833
iteration: 0 loss: 3628.6155416410966 grad: 1777.1173384577748
iteration: 0 loss: 1871.5015296201943 grad: 1517.8451897735533
iteration: 0 loss: 1679.6471705467259 grad: 1445.4689004659629
iteration: 0 loss: 2308.9704925351652 grad: 1583.8522553329426
iteration: 0 loss: 3062.605412048695 grad: 1819.9475515317927
iteration: 0 loss: 2900.675572005551 grad: 1708.3864499477636
iteration: 10 loss: 0.004466763965452133 grad: 0.07050857262207968
iteration: 0 loss: 2354.683450608036 grad: 1653.8445570520714
iteration: 0 loss: 2021.6941802588408 grad: 1601.44838323167
iteration: 0 loss: 4225.068837650798 grad: 2011.4521774218701
iteration: 10 loss: 0.0056362404251227745 grad: 0.02155499595119342
iteration: 0 loss: 3446.5010364784157 grad: 1823.932875399279
iteration: 10 loss: 0.007662704754858913 grad: -0.2481009068895925
iteration: 0 loss: 3193.692210934334 grad: 1760.9519045277514
iteration: 10 loss: 0.0053804724223234434 grad: 0.03791881884405285
iteration: 0 loss: 2803.309250257407 grad: 1745.2531166805288
iteration: 10 loss: 0.005423508309997322 grad: 0.05725355038722489
iteration: 0 loss: 2081.1235133510763 grad: 1505.0309251093327
iteration: 0 loss: 3351.1339614444573 grad: 1801.8183768194801
iteration: 0 loss: 2486.787163100401 grad: 1666.0864669693692
iteration: 0 loss: 2509.2200309836826 grad: 1752.8335886962075
iteration: 0 loss: 5676.295777123574 grad: 2163.1359438221702
iteration: 10 loss: 0.027108653961807828 grad: -0.6780640567115557
iteration: 0 loss: 1873.6345569720272 grad: 1582.2157442165562
iteration: 0 loss: 1823.045297077676 grad: 1521.9602689011467
iteration: 0 loss: 2173.1426620393327 grad: 1603.2961876578024
iteration: 0 loss: 2775.8156788206206 grad: 1790.3948532776715
iteration: 0 loss: 3396.423905393634 grad: 1833.5410444269553
iteration: 10 loss: 0.004684191726978911 grad: -0.01264157803430246
iteration: 0 loss: 3843.3756615335024 grad: 1958.221091553492
iteration: 10 loss: 0.0056332040416732935 grad: 0.021368571029566876
iteration: 0 loss: 4719.725285132364 grad: 1923.0852939222073
iteration: 10 loss: 0.11230236450898122 grad: 0.6734734365035729
iteration: 0 loss: 3713.9080562489285 grad: 1848.8460394255653
iteration: 10 loss: 0.047611715058420406 grad: 0.31750536719545075
iteration: 0 loss: 2923.647669498225 grad: 1757.6369211084223
iteration: 0 loss: 4833.446880635723 grad: 1981.3376348712998
iteration: 10 loss: 0.006882443204548591 grad: 0.06790338683687719
iteration: 0 loss: 3683.357069667422 grad: 1850.5339692529535
iteration: 0 loss: 2283.1311242834854 grad: 1735.713547399634
iteration: 0 loss: 2757.4394385971614 grad: 1742.0470094286566
iteration: 0 loss: 2469.7737940155926 grad: 1710.5927631980078
iteration: 0 loss: 4345.724953138528 grad: 1985.3115450497535
iteration: 10 loss: 0.009968763271568936 grad: 0.0981133014238221
iteration: 0 loss: 2246.9414974039905 grad: 1748.680792759007
iteration: 0 loss: 2802.503848618492 grad: 1666.7832290104548
iteration: 0 loss: 1519.29334526752 grad: 1489.7042859378244
iteration: 0 loss: 3292.9075361296286 grad: 1901.9973585531802
iteration: 0 loss: 3172.0405261480732 grad: 1850.2383044106032
iteration: 0 loss: 3149.340216441877 grad: 1822.9327756782452
iteration: 10 loss: 0.005467684804567729 grad: -0.17632786794247657
iteration: 0 loss: 4584.708158951948 grad: 1895.6578616244592
iteration: 10 loss: 0.19529437157854607 grad: 1.2373777855060368
iteration: 0 loss: 2518.334649097556 grad: 1714.4215802572503
iteration: 0 loss: 2977.145851082969 grad: 1788.4044825353176
iteration: 0 loss: 4662.933706834094 grad: 1990.6634548660268
iteration: 10 loss: 0.005745691437782211 grad: -0.12346464504957133
iteration: 0 loss: 2887.3140096721017 grad: 1697.1668728590344
iteration: 10 loss: 0.005444248556159437 grad: 0.03011041898587634
iteration: 0 loss: 3933.2110478848977 grad: 1820.5590903282641
iteration: 10 loss: 0.004989387329260353 grad: 0.6388246024396251
iteration: 0 loss: 2515.28105708681 grad: 1738.0850859193633
iteration: 0 loss: 4296.358315157829 grad: 2000.8361413251
iteration: 0 loss: 3709.4840423353626 grad: 1896.8505461767543
iteration: 0 loss: 2405.806993959059 grad: 1576.3342167765682
iteration: 0 loss: 1851.2884091373946 grad: 1525.896956138866
iteration: 0 loss: 2989.1921553190277 grad: 1879.4595986359702
iteration: 0 loss: 1501.7675119993417 grad: 1462.388554981957
iteration: 0 loss: 2104.889259879934 grad: 1477.7721108621224
iteration: 0 loss: 1854.1744006776594 grad: 1516.6292071421435
iteration: 0 loss: 4414.700627914692 grad: 1931.7217043274436
iteration: 10 loss: 0.005420595463709271 grad: 0.02432926566140672
iteration: 0 loss: 2652.2081320062152 grad: 1651.4574323967809
iteration: 0 loss: 4587.003933023277 grad: 1924.7043376390327
iteration: 10 loss: 0.2571214777483626 grad: 0.5327262028739964
iteration: 0 loss: 4348.638318363292 grad: 1925.2308895053538
iteration: 0 loss: 5186.870202206337 grad: 2065.2211643332594
iteration: 10 loss: 0.06683469838621518 grad: -0.6624843347489865
iteration: 0 loss: 2335.3224829246496 grad: 1636.4680993750756
iteration: 0 loss: 2354.5203519063475 grad: 1656.8559866965818
iteration: 0 loss: 3071.9218817932638 grad: 1862.5959980940593
iteration: 10 loss: 0.005462478448118252 grad: 0.015032286879087125
iteration: 0 loss: 3140.3414393608878 grad: 1801.5411817596223
iteration: 0 loss: 5949.362529775456 grad: 2086.6920606178937
iteration: 10 loss: 0.11698178433553866 grad: 1.1419415653086333
iteration: 0 loss: 2594.3402591416393 grad: 1765.7996837902838
iteration: 0 loss: 4574.706460836925 grad: 1998.6789118117144
iteration: 10 loss: 0.009107484270564535 grad: -0.49508878696075576
iteration: 0 loss: 3002.874463005899 grad: 1753.6413530961433
iteration: 0 loss: 1983.7228563013414 grad: 1651.3473656006915
iteration: 0 loss: 4345.15493536527 grad: 1961.2045269908062
iteration: 10 loss: 0.005544240254411389 grad: -0.14255468626734333
iteration: 0 loss: 2143.8299392108174 grad: 1641.5800210452055
iteration: 0 loss: 5191.194526029243 grad: 1994.615385350181
iteration: 10 loss: 0.5139190682053926 grad: 2.5631174349218586
iteration: 0 loss: 1877.2114952472475 grad: 1560.2327804321615
iteration: 0 loss: 4080.8931756089924 grad: 1898.569313319923
iteration: 0 loss: 2069.6334135839943 grad: 1547.6351501425875
iteration: 0 loss: 3128.321809330893 grad: 1889.586664916771
iteration: 10 loss: 0.0053470813568723815 grad: 0.4488894932916271
iteration: 0 loss: 2250.4389566016544 grad: 1682.3203952650142
iteration: 0 loss: 3038.5852341339896 grad: 1752.2614106910366
iteration: 10 loss: 0.006265959116386843 grad: 0.010784132797547218
iteration: 0 loss: 2970.0485933606465 grad: 1748.890796634078
iteration: 0 loss: 1668.0410449516685 grad: 1492.9943227874155
iteration: 0 loss: 3747.307970905963 grad: 1810.6873030074084
iteration: 10 loss: 0.006258388636607296 grad: 0.3627370519076622
iteration: 0 loss: 3632.9757680945377 grad: 1834.5887863632838
iteration: 10 loss: 0.005778320851360067 grad: 0.2344016876477047
iteration: 0 loss: inf grad: 3041.890613613292
iteration: 10 loss: 0.0005323052993679249 grad: -0.4132641574027648
iteration: 0 loss: 9140.152414717177 grad: 2853.5961699839386
iteration: 0 loss: 10107.274840658612 grad: 2892.0534328156546
iteration: 10 loss: 0.000408715245694938 grad: -0.15525293611551075
iteration: 0 loss: inf grad: 3169.0142170422623
iteration: 0 loss: 7795.390342667501 grad: 2758.3099247153873
iteration: 10 loss: 0.0005362377427941697 grad: 0.0056512010804358765
iteration: 0 loss: 5275.897120737064 grad: 2460.453307547937
iteration: 0 loss: 8385.31737069882 grad: 2844.4803410055197
iteration: 0 loss: 4542.73996384447 grad: 2331.612239063329
iteration: 0 loss: inf grad: 2979.4868906012707
iteration: 0 loss: 8171.362211470944 grad: 2773.852865664574
iteration: 0 loss: 5185.8855569735815 grad: 2537.4913696214103
iteration: 0 loss: 10415.128613105322 grad: 2980.107593131042
iteration: 10 loss: 0.004547153676288541 grad: 0.6060878535662468
iteration: 0 loss: 7053.934225111672 grad: 2524.790370737538
iteration: 0 loss: 7739.338141824569 grad: 2818.8959759759214
iteration: 0 loss: inf grad: 2862.8711364932606
iteration: 0 loss: 14586.169412083416 grad: 3242.7197284034933
iteration: 0 loss: 8728.614073274495 grad: 2785.5970568735197
iteration: 0 loss: 6129.956804919464 grad: 2590.874340688996
iteration: 0 loss: 14597.010113830731 grad: 3250.2455589248657
iteration: 10 loss: 0.004361801166934046 grad: 2.1280616542540054
iteration: 0 loss: 7235.955617817624 grad: 2628.402038415562
iteration: 0 loss: 10374.997911831086 grad: 2979.0240079131645
iteration: 0 loss: 12600.865919577976 grad: 3096.611192343969
iteration: 10 loss: 0.0353380382981744 grad: -1.8600364133015332
iteration: 0 loss: 6419.479794089488 grad: 2496.3052119984177
iteration: 0 loss: 7925.952381942852 grad: 2752.858729859769
iteration: 10 loss: 0.00038403756281530315 grad: 0.009252312537753457
iteration: 0 loss: 8126.0311392219455 grad: 2883.4399343940095
iteration: 0 loss: 6893.588765952983 grad: 2778.5926297666465
iteration: 0 loss: inf grad: 3264.1879755811533
iteration: 0 loss: inf grad: 3160.426014163537
iteration: 0 loss: 8964.23999516879 grad: 2844.195188577113
iteration: 10 loss: 0.000542583917690949 grad: 0.6640766138142412
iteration: 0 loss: 6722.225552961275 grad: 2621.88029358235
iteration: 0 loss: inf grad: 3451.1766642906505
iteration: 0 loss: 8889.725066595396 grad: 3008.140088320338
iteration: 0 loss: 8551.007892947779 grad: 2836.9001004209713
iteration: 10 loss: 0.0005499729119368236 grad: 0.006350523915343581
iteration: 0 loss: inf grad: 2942.7600346250683
iteration: 0 loss: inf grad: 3038.7128669753156
iteration: 0 loss: 9790.326428365212 grad: 2952.757260937022
iteration: 10 loss: 0.0005431640205312182 grad: 0.006173948885056782
iteration: 0 loss: 9250.494750993548 grad: 2827.825966926508
iteration: 10 loss: 0.0005255760228134353 grad: 0.06651064002508858
iteration: 0 loss: inf grad: 3271.721875456712
iteration: 0 loss: 6902.517314699667 grad: 2695.749693162339
iteration: 0 loss: 7712.107571993203 grad: 2744.913444477387
iteration: 0 loss: 9674.715065524057 grad: 2879.475471814523
iteration: 10 loss: 0.00048592356192371386 grad: 0.009414037416794087
iteration: 0 loss: 4947.046626385424 grad: 2495.2226720291746
iteration: 0 loss: inf grad: 3558.633901742626
iteration: 0 loss: inf grad: 2648.278419366404
iteration: 10 loss: 0.0005368290188595314 grad: 0.0014062839231683583
iteration: 0 loss: 11180.48024954145 grad: 3183.6513608927926
iteration: 0 loss: inf grad: 2941.9673946784196
iteration: 10 loss: 0.000487894253720614 grad: -0.006511970021664889
iteration: 0 loss: 8702.88176665824 grad: 2584.806513724474
iteration: 0 loss: 7157.436691046835 grad: 2793.977044277435
iteration: 0 loss: inf grad: 3083.3801243870307
iteration: 0 loss: 13026.246328299325 grad: 3101.9887279430886
iteration: 10 loss: 0.06212801596583714 grad: 0.4452629588285983
iteration: 0 loss: inf grad: 3199.171994723656
iteration: 0 loss: 8146.381331277126 grad: 2875.92816412346
iteration: 0 loss: 13826.607461645182 grad: 3283.042065955654
iteration: 10 loss: 0.0229679526951672 grad: 1.0905394858616935
iteration: 0 loss: 10097.625122781743 grad: 2936.11772599081
iteration: 0 loss: 6910.5519079586 grad: 2657.713131170787
iteration: 0 loss: 9159.622739742177 grad: 2861.3971457112093
iteration: 10 loss: 0.000471363590225916 grad: -0.10305216825500227
iteration: 0 loss: 6881.610643636143 grad: 2772.312939692141
iteration: 0 loss: 9983.239613079406 grad: 2933.8871796725775
iteration: 10 loss: 0.00038193550161932677 grad: 0.0028511427797879873
iteration: 0 loss: inf grad: 3331.137693251407
iteration: 0 loss: 8029.518600830866 grad: 2821.892862607313
iteration: 0 loss: 13042.310369538456 grad: 3120.053885524222
iteration: 0 loss: 12955.459097790766 grad: 3289.5247558654455
iteration: 10 loss: 0.0004941709868779236 grad: 0.005554689341535549
iteration: 0 loss: 6788.926050443165 grad: 2443.586227305916
iteration: 0 loss: 7185.849730571427 grad: 2683.802861236245
iteration: 10 loss: 0.00046714733285956424 grad: 0.8948310323736146
iteration: 0 loss: 8510.291614569824 grad: 2908.4386310151954
iteration: 0 loss: 8767.263867680736 grad: 2841.467433813286
iteration: 0 loss: 6563.61478514611 grad: 2560.1239373743883
iteration: 0 loss: 11901.882938063914 grad: 3128.15238130978
iteration: 10 loss: 0.0005106363784183155 grad: 0.01722951584448644
iteration: 0 loss: 9621.293230004061 grad: 2867.0832940277096
iteration: 10 loss: 0.0005130541341548616 grad: -0.0035019107898540615
iteration: 0 loss: 9696.715941373892 grad: 3048.6309175544093
iteration: 10 loss: 0.0005715593394019048 grad: 0.2504498324354668
iteration: 0 loss: 11667.006660181667 grad: 3107.962907435316
iteration: 10 loss: 0.00041763454051264984 grad: 0.7673311707072543
iteration: 0 loss: inf grad: 3460.1858667239903
iteration: 0 loss: 7293.008072207777 grad: 2669.871982622643
iteration: 0 loss: 12310.884052876738 grad: 3089.5958513219357
iteration: 10 loss: 0.0005715473724359816 grad: 0.025716776709440538
iteration: 0 loss: 7754.016979562345 grad: 2779.7158527480196
iteration: 10 loss: 0.0005206626533022659 grad: 0.010698859400275294
iteration: 0 loss: 6149.77173281579 grad: 2581.9314132032123
iteration: 0 loss: 7731.902173221219 grad: 2605.403309868118
iteration: 0 loss: 6435.61347727123 grad: 2588.089162303473
iteration: 0 loss: 9326.152379356587 grad: 3006.7402139393384
iteration: 0 loss: inf grad: 3045.809322761172
iteration: 10 loss: 0.0010586231444242665 grad: -1.2566530906727311
iteration: 0 loss: 14739.237470461749 grad: 3306.2864695756416
iteration: 10 loss: 0.00040808968796310097 grad: 0.2878862361322537
iteration: 0 loss: inf grad: 3153.0860197331863
iteration: 0 loss: inf grad: 3032.3826596191925
iteration: 0 loss: 13927.472026900236 grad: 3156.089246002576
iteration: 10 loss: 0.000812896641119468 grad: 1.6445628203475633
iteration: 0 loss: inf grad: 3160.7680234821146
iteration: 0 loss: 10399.269635265722 grad: 2751.240905262306
iteration: 10 loss: 0.1297465913045347 grad: 0.641577184389516
iteration: 0 loss: inf grad: 3246.30734091472
iteration: 0 loss: 11239.686446706253 grad: 2909.4757554026164
iteration: 10 loss: 0.0006334928224201907 grad: 0.00786813604495327
iteration: 0 loss: 6139.08915532188 grad: 2493.4543511120264
iteration: 0 loss: 5274.507748041715 grad: 2361.9684288103795
iteration: 0 loss: 7353.583762234976 grad: 2595.6286415880286
iteration: 0 loss: 9728.198892798426 grad: 2981.5641697538995
iteration: 0 loss: inf grad: 2796.5464299209307
iteration: 0 loss: 7669.338326577907 grad: 2708.0635023639998
iteration: 0 loss: 6601.013292936555 grad: 2625.27318310633
iteration: 0 loss: 13325.86535546803 grad: 3291.53929593035
iteration: 10 loss: 0.0003834418656135147 grad: 0.031089444613649507
iteration: 0 loss: 10729.065429382748 grad: 2985.332020934393
iteration: 10 loss: 0.03083617854810489 grad: -1.8882918610295918
iteration: 0 loss: 9905.817705440148 grad: 2886.746089239732
iteration: 10 loss: 0.0005294056741563095 grad: -0.0014385509051805887
iteration: 0 loss: 8996.94468980225 grad: 2859.1690436429544
iteration: 0 loss: 6489.848786204036 grad: 2464.593402736562
iteration: 0 loss: 10603.868080298462 grad: 2954.576529661431
iteration: 0 loss: 7931.013065987759 grad: 2730.613955719361
iteration: 0 loss: 8201.293275378202 grad: 2872.8601226816136
iteration: 10 loss: 0.00048050437577661467 grad: 0.026638936971766704
iteration: 0 loss: 17921.74317340559 grad: 3540.5055030086733
iteration: 0 loss: 6049.09871338778 grad: 2592.9186592134683
iteration: 10 loss: 0.00041306025311562485 grad: 0.00705363300090206
iteration: 0 loss: 5975.28397920077 grad: 2497.0943533443196
iteration: 0 loss: 7038.629820154118 grad: 2627.5502393643774
iteration: 0 loss: 8922.501195345509 grad: 2926.5646809749387
iteration: 0 loss: 10707.56266463278 grad: 3002.475375034333
iteration: 10 loss: 0.00046531148043207145 grad: 0.01801582847347292
iteration: 0 loss: 12254.3535192505 grad: 3206.647834165244
iteration: 10 loss: 0.000633055952758613 grad: 0.682945855974637
iteration: 0 loss: inf grad: 3147.083795173834
iteration: 0 loss: inf grad: 3028.5154310780977
iteration: 0 loss: 9268.771744261028 grad: 2879.0180173082567
iteration: 0 loss: 15366.366933439178 grad: 3247.2043977986978
iteration: 0 loss: inf grad: 3029.7775727741828
iteration: 10 loss: 0.0004970007205635986 grad: 0.0005142352808074251
iteration: 0 loss: 7611.343938202318 grad: 2843.757759774194
iteration: 10 loss: 0.0004067248215009882 grad: -0.058198228198488494
iteration: 0 loss: 8727.486178378766 grad: 2854.2895657310446
iteration: 0 loss: 7921.150293009335 grad: 2800.6334389291587
iteration: 0 loss: 13849.07509934422 grad: 3250.4714726384514
iteration: 10 loss: 0.08532485764060932 grad: 4.576665458525149
iteration: 0 loss: 7293.262402081606 grad: 2864.220114296904
iteration: 0 loss: 8923.746536029417 grad: 2732.3969353297543
iteration: 0 loss: 4961.85594079839 grad: 2440.5580689326807
iteration: 0 loss: 10336.103221742687 grad: 3110.459620514302
iteration: 0 loss: 10060.20625404683 grad: 3026.801589293287
iteration: 10 loss: 0.0004745322671210901 grad: -0.007747756726133355
iteration: 0 loss: 10010.527132813002 grad: 2988.0551240446684
iteration: 10 loss: 0.0005849438020959496 grad: 0.01043885849223435
iteration: 0 loss: inf grad: 3105.2738445037257
iteration: 0 loss: 8211.02241209799 grad: 2809.947992513365
iteration: 0 loss: inf grad: 2929.1642523581286
iteration: 10 loss: 0.0004574457284020768 grad: 0.011588666149958463
iteration: 0 loss: 14379.29463986474 grad: 3259.2094123604243
iteration: 10 loss: 0.0007531034987716174 grad: 1.9106656147845698
iteration: 0 loss: 9142.860937965912 grad: 2780.9347304108883
iteration: 0 loss: 12373.74923615007 grad: 2977.816010828018
iteration: 10 loss: 0.000702848864338276 grad: 1.6756263817997485
iteration: 0 loss: 7988.873949128793 grad: 2842.451155968933
iteration: 0 loss: 13487.559207769167 grad: 3274.8820679426744
iteration: 10 loss: 0.0005323613110125403 grad: 0.006624513217517061
iteration: 0 loss: 11570.61089130626 grad: 3105.3535327368163
iteration: 10 loss: 0.0005702478096778082 grad: -0.0007777343239282796
iteration: 0 loss: 7423.411440094859 grad: 2579.498985622104
iteration: 0 loss: 5940.963629824969 grad: 2498.2833529979107
iteration: 0 loss: 9657.36425268807 grad: 3076.7430917184956
iteration: 0 loss: 4988.778496910855 grad: 2395.3706136411224
iteration: 0 loss: 6616.718664750588 grad: 2422.332270170562
iteration: 0 loss: 5784.866810050414 grad: 2481.5080048080326
iteration: 0 loss: inf grad: 3162.4922103121844
iteration: 0 loss: 8483.251861362478 grad: 2709.3872846980953
iteration: 10 loss: 0.0005653795061251996 grad: 0.014521900656190872
iteration: 0 loss: 14710.070886176689 grad: 3150.703444142722
iteration: 0 loss: inf grad: 3152.490118291405
iteration: 0 loss: inf grad: 3380.13775036672
iteration: 0 loss: 7513.732080646833 grad: 2680.837117168284
iteration: 0 loss: 7530.187400332689 grad: 2712.0920961493225
iteration: 0 loss: 9851.282860740619 grad: 3046.5169614231145
iteration: 10 loss: 0.0004331763881385665 grad: 0.006930022787550552
iteration: 0 loss: 10062.785907175376 grad: 2949.3165333520633
iteration: 0 loss: inf grad: 3415.5527117046936
iteration: 0 loss: 8425.050897452602 grad: 2890.1335035224565
iteration: 0 loss: 14476.391764294816 grad: 3268.619918306079
iteration: 0 loss: 9465.633592264981 grad: 2869.960956131717
iteration: 10 loss: 0.0004191092792263424 grad: 0.004062671408752599
iteration: 0 loss: 6535.506284309776 grad: 2706.07258120538
iteration: 0 loss: 13539.445462161855 grad: 3208.8466331534837
iteration: 10 loss: 0.0801149022035216 grad: 1.2837557555363384
iteration: 0 loss: 6877.930008004993 grad: 2688.4953507851314
iteration: 0 loss: 16374.543735688656 grad: 3263.7537540351364
iteration: 0 loss: 6061.091986204262 grad: 2555.104111283251
iteration: 0 loss: inf grad: 3107.211084330361
iteration: 0 loss: 6644.441453158369 grad: 2541.0765200024766
iteration: 0 loss: 9983.874628827964 grad: 3090.5649346083937
iteration: 0 loss: 7270.637037016848 grad: 2756.065529489376
iteration: 0 loss: 9870.972857113084 grad: 2870.566026267741
iteration: 10 loss: 0.0005963741859886795 grad: -0.047171413521168715
iteration: 0 loss: 9409.548390256003 grad: 2867.1644406013493
iteration: 0 loss: 5337.354458968875 grad: 2444.557311351671
iteration: 0 loss: inf grad: 2967.346070034646
iteration: 0 loss: 11451.667583346105 grad: 3001.5613794235496
iteration: 10 loss: 0.0006264715100934899 grad: 0.007728950186005955
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:57: RuntimeWarning: invalid value encountered in double_scalars
  diff = np.absolute(f_history[-1]-f_history[-2])
iteration: 0 loss: 1581.0684823096553 grad: 1350.5759068311695
iteration: 10 loss: 0.029624061177111635 grad: 0.20672628910121316
iteration: 0 loss: 1229.3443716159343 grad: 1265.5311221845202
iteration: 0 loss: 1351.5667628633516 grad: 1280.9998317527677
iteration: 0 loss: 1777.8075064351353 grad: 1406.1351852875307
iteration: 10 loss: 0.025381697692037877 grad: 0.19882907464871086
iteration: 0 loss: 1055.1607818509628 grad: 1222.0936213203183
iteration: 0 loss: 714.915663871375 grad: 1086.8994190583321
iteration: 0 loss: 1140.6397903834782 grad: 1259.2187898732204
iteration: 0 loss: 575.3342569145663 grad: 1028.7680742024963
iteration: 0 loss: 1739.9514049410463 grad: 1321.3573155135907
iteration: 0 loss: 1108.108030117661 grad: 1228.566049605938
iteration: 0 loss: 646.5500541069413 grad: 1123.4174557084057
iteration: 0 loss: 1374.745547474455 grad: 1321.5143480461793
iteration: 10 loss: 0.027256150608908767 grad: 0.18064099707030062
iteration: 0 loss: 995.543012527126 grad: 1116.1808061326606
iteration: 0 loss: 1015.4298515322585 grad: 1247.3621528377525
iteration: 0 loss: 1378.260425939526 grad: 1268.3041220778096
iteration: 0 loss: 1970.575409935538 grad: 1437.2646839798101
iteration: 10 loss: 0.03180621280956653 grad: 0.11470287285306816
iteration: 0 loss: 1175.4684841503824 grad: 1232.4040358982884
iteration: 0 loss: 832.8419612187699 grad: 1147.9759874014321
iteration: 0 loss: 2089.2934060723624 grad: 1441.8478568066487
iteration: 10 loss: 0.03387764753841276 grad: 0.23436668203371894
iteration: 0 loss: 970.3289539455556 grad: 1162.9505079042774
iteration: 0 loss: 1407.286420643871 grad: 1319.166820518229
iteration: 0 loss: 1683.52423753525 grad: 1372.82983893643
iteration: 10 loss: 0.03202164915009317 grad: 0.9528334413083632
iteration: 0 loss: 926.7780564387059 grad: 1105.5728645653887
iteration: 0 loss: 980.0755535930056 grad: 1216.64650248106
iteration: 10 loss: 0.02778799163008278 grad: 0.2237705800880486
iteration: 0 loss: 1083.1594332637994 grad: 1277.2195869236064
iteration: 0 loss: 967.1792061349347 grad: 1231.1175228987902
iteration: 10 loss: 0.02940536904299568 grad: 0.0807818704425746
iteration: 0 loss: 2267.401296927707 grad: 1448.806524296112
iteration: 10 loss: 0.030693843432924372 grad: -0.016836309887604234
iteration: 0 loss: 2140.3137320321025 grad: 1401.5876608893498
iteration: 10 loss: 0.10804445635651726 grad: 1.2838314134955484
iteration: 0 loss: 1156.2449883712673 grad: 1260.8589022176639
iteration: 0 loss: 898.3766901139363 grad: 1159.4443318414662
iteration: 0 loss: 2566.0235569620695 grad: 1530.9264189353587
iteration: 10 loss: 0.03929790753252085 grad: 0.18181176025580584
iteration: 0 loss: 1200.1801379449992 grad: 1332.5111973326138
iteration: 0 loss: 1141.7678253389795 grad: 1254.8525481489528
iteration: 0 loss: 1627.881464472207 grad: 1304.1002492610123
iteration: 10 loss: 0.026432945441973225 grad: 0.18158967801023496
iteration: 0 loss: 1596.1090645386219 grad: 1346.9918610000852
iteration: 10 loss: 0.026279071735189063 grad: 0.2381182743490093
iteration: 0 loss: 1314.7515326181353 grad: 1309.0504981237852
iteration: 10 loss: 0.0327541745708996 grad: 0.7398498364488258
iteration: 0 loss: 1265.9941247127508 grad: 1254.2969021237554
iteration: 0 loss: 1981.991190533032 grad: 1450.647617365577
iteration: 10 loss: 0.028617183624274647 grad: 0.4083623246077267
iteration: 0 loss: 958.875965678625 grad: 1193.9336167599802
iteration: 0 loss: 1013.0354933789608 grad: 1216.6346715409823
iteration: 0 loss: 1405.7230785179217 grad: 1275.9278938854236
iteration: 0 loss: 632.4584416799626 grad: 1103.2777544348114
iteration: 0 loss: 2883.63819262096 grad: 1578.9429527670598
iteration: 10 loss: 0.06717011890379415 grad: 0.9481869470726636
iteration: 0 loss: 1142.7905139357968 grad: 1171.6984951034153
iteration: 0 loss: 1484.2481203803884 grad: 1412.002878721479
iteration: 0 loss: 1657.5185752424973 grad: 1303.4309395076048
iteration: 0 loss: 1263.708746385804 grad: 1142.54291378045
iteration: 0 loss: 963.5881419944667 grad: 1236.8111122203445
iteration: 10 loss: 0.025468884878302397 grad: 0.14753857683966093
iteration: 0 loss: 1733.607475995802 grad: 1369.9774351836386
iteration: 10 loss: 0.028216269160276915 grad: 0.32539609876640463
iteration: 0 loss: 1821.0132955018278 grad: 1375.8222146165078
iteration: 10 loss: 0.03171475925608485 grad: 0.2179121852559512
iteration: 0 loss: 2103.4950928776234 grad: 1419.6712114348884
iteration: 10 loss: 0.1290995469508661 grad: 0.6974351487555797
iteration: 0 loss: 1087.4849467620063 grad: 1275.419988764795
iteration: 0 loss: 1885.551549110285 grad: 1454.1545170338654
iteration: 10 loss: 0.027989313078499188 grad: 0.0684564026869005
iteration: 0 loss: 1402.3445928210162 grad: 1301.6261709866626
iteration: 0 loss: 888.3948384767668 grad: 1175.6873124002952
iteration: 0 loss: 1215.6835560464783 grad: 1266.8772262500388
iteration: 0 loss: 861.6950446133203 grad: 1228.6642032874447
iteration: 0 loss: 1368.0796129345347 grad: 1300.2956509349224
iteration: 10 loss: 0.027018347257332855 grad: 0.21475920714193092
iteration: 0 loss: 2099.0993023900833 grad: 1478.0905176592
iteration: 10 loss: 0.027474533523094247 grad: 0.11215213514142647
iteration: 0 loss: 1083.9800993714275 grad: 1250.081273248015
iteration: 0 loss: 1781.1409672157768 grad: 1381.7206917592544
iteration: 10 loss: 0.02959322248319619 grad: 0.14794652998455357
iteration: 0 loss: 1768.232509363461 grad: 1458.7858847274608
iteration: 10 loss: 0.02796910517876809 grad: 0.09469399468953829
iteration: 0 loss: 896.5631522274409 grad: 1079.123487380169
iteration: 0 loss: 904.3317254569826 grad: 1186.494038369985
iteration: 0 loss: 1148.725363564811 grad: 1288.1545170835627
iteration: 0 loss: 1194.0643476803932 grad: 1258.6498439202674
iteration: 0 loss: 914.8929546192172 grad: 1131.7089480858615
iteration: 0 loss: 1639.679349325138 grad: 1387.792552038382
iteration: 10 loss: 0.029537576792790787 grad: 0.22393776665890747
iteration: 0 loss: 1403.4359826559296 grad: 1269.2563162560557
iteration: 10 loss: 0.027577910953616745 grad: 0.12665531588665907
iteration: 0 loss: 1305.1288212227885 grad: 1351.6387932141415
iteration: 0 loss: 1564.2785163227766 grad: 1378.4361218494291
iteration: 0 loss: 2482.359174899914 grad: 1536.7931279503005
iteration: 10 loss: 0.05275170954171864 grad: -0.5546686840491122
iteration: 0 loss: 1017.2270667469661 grad: 1181.1977467480447
iteration: 0 loss: 1660.9268461619902 grad: 1370.36510453138
iteration: 10 loss: 0.026386710108040876 grad: 0.17992602558959647
iteration: 0 loss: 1057.5443368105064 grad: 1230.722377010409
iteration: 0 loss: 834.1991904579031 grad: 1141.1228780983042
iteration: 0 loss: 1098.8977311484186 grad: 1152.894533928038
iteration: 0 loss: 855.8382878105116 grad: 1145.3996272900322
iteration: 0 loss: 1255.7801651191817 grad: 1333.6986559913435
iteration: 0 loss: 1848.7743692198344 grad: 1351.1447965406708
iteration: 10 loss: 0.028699801580924857 grad: 0.09789774161318386
iteration: 0 loss: 2029.8046042668473 grad: 1466.2280839626198
iteration: 10 loss: 0.02619038673715708 grad: 0.3417297439696048
iteration: 0 loss: 1895.7575454159232 grad: 1398.4605992873603
iteration: 10 loss: 0.040775651860961014 grad: -0.8660932905268492
iteration: 0 loss: 1498.102630178444 grad: 1343.2437185897988
iteration: 0 loss: 1966.3928907009627 grad: 1399.007462047742
iteration: 10 loss: 0.028413927326420726 grad: 0.14052944380693055
iteration: 0 loss: 1928.3870142852147 grad: 1403.0999126326617
iteration: 10 loss: 0.030378527445877393 grad: 0.39306147913997
iteration: 0 loss: 1542.4935631542453 grad: 1218.0095375309088
iteration: 10 loss: 0.023731248470787912 grad: 0.0796482135240127
iteration: 0 loss: 2396.2989842967695 grad: 1438.8756757482213
iteration: 10 loss: 0.029502599859948863 grad: 0.17420337295741872
iteration: 0 loss: 1638.4321337430626 grad: 1288.7485109687266
iteration: 10 loss: 0.02687325646713196 grad: 0.11667268421191818
iteration: 0 loss: 793.6273589944892 grad: 1101.9776708535474
iteration: 0 loss: 712.5052384910006 grad: 1044.3033861368226
iteration: 0 loss: 1024.684898070691 grad: 1147.3066026695117
iteration: 0 loss: 1342.576161408403 grad: 1320.8109229684117
iteration: 0 loss: 1242.4612840982904 grad: 1238.3641002228737
iteration: 0 loss: 1000.6441445952612 grad: 1198.1063074625097
iteration: 0 loss: 869.9343559121785 grad: 1161.7048081598355
iteration: 0 loss: 1846.0071173794056 grad: 1462.3631023018352
iteration: 10 loss: 0.025502796937706244 grad: 0.21912236452179062
iteration: 0 loss: 1515.428930006051 grad: 1322.708203545631
iteration: 10 loss: 0.029555752835056574 grad: 0.4733606458333397
iteration: 0 loss: 1453.6764204374851 grad: 1278.7710106177838
iteration: 10 loss: 0.026809930390961446 grad: 0.1411908259646132
iteration: 0 loss: 1218.5343284680625 grad: 1266.8353836426813
iteration: 0 loss: 950.395736963056 grad: 1090.360528746571
iteration: 0 loss: 1463.7318569167908 grad: 1308.1569651373507
iteration: 0 loss: 1078.3178508684064 grad: 1209.8094304172928
iteration: 0 loss: 1049.156462219672 grad: 1272.15467022173
iteration: 0 loss: 2466.873096248807 grad: 1570.6209244867346
iteration: 10 loss: 0.04783232653161659 grad: 0.7112889985593578
iteration: 0 loss: 798.8602483939285 grad: 1146.3820686998097
iteration: 0 loss: 762.8255185313357 grad: 1103.7083778884667
iteration: 0 loss: 940.7470380797225 grad: 1163.5336424013021
iteration: 0 loss: 1169.40571161356 grad: 1296.7024475544729
iteration: 0 loss: 1499.3057495774265 grad: 1331.1912563398291
iteration: 0 loss: 1657.347146062513 grad: 1420.3937290561332
iteration: 10 loss: 0.024939332505957944 grad: 0.14614827824930685
iteration: 0 loss: 2014.9067744734473 grad: 1396.294920895046
iteration: 10 loss: 0.04610766472945794 grad: -0.10074769361826905
iteration: 0 loss: 1566.1499772654256 grad: 1341.89839856659
iteration: 10 loss: 0.04235698494614388 grad: 0.7721408213288132
iteration: 0 loss: 1262.1867749203043 grad: 1275.4767251791668
iteration: 0 loss: 2116.045981839795 grad: 1439.5284785663564
iteration: 10 loss: 0.030068331399062034 grad: -0.02329340941383838
iteration: 0 loss: 1625.895556724762 grad: 1341.745836786602
iteration: 0 loss: 937.548532943852 grad: 1259.0274980000097
iteration: 0 loss: 1210.8617139156672 grad: 1262.9236205443322
iteration: 0 loss: 1063.7358383791448 grad: 1239.7229253204605
iteration: 0 loss: 1857.902105659269 grad: 1441.580319171408
iteration: 10 loss: 0.03992100308360849 grad: 0.9690030821439033
iteration: 0 loss: 954.0597673445883 grad: 1269.665236654302
iteration: 0 loss: 1216.4870020161252 grad: 1209.3036668377852
iteration: 0 loss: 643.5714412127134 grad: 1079.1741177599722
iteration: 0 loss: 1465.6772214235882 grad: 1381.6224418283823
iteration: 0 loss: 1378.0786986521616 grad: 1341.788956960554
iteration: 0 loss: 1375.7625649672982 grad: 1324.520218257123
iteration: 10 loss: 0.025957757227760423 grad: 0.06873983745059269
iteration: 0 loss: 2065.5415888811444 grad: 1373.8344201371622
iteration: 10 loss: 0.025926504257354696 grad: 0.27563080835219617
iteration: 0 loss: 1080.1926702161732 grad: 1245.0034410821709
iteration: 0 loss: 1280.8254384099766 grad: 1297.657574851636
iteration: 0 loss: 2102.8059687759155 grad: 1445.7069525205784
iteration: 10 loss: 0.02894565315694887 grad: -0.08136322749929006
iteration: 0 loss: 1264.6106992451973 grad: 1231.809559307041
iteration: 0 loss: 1734.251453639794 grad: 1320.7109602684613
iteration: 10 loss: 0.02509302078826956 grad: 0.3291620571258041
iteration: 0 loss: 1084.9685142608125 grad: 1259.9830533277109
iteration: 0 loss: 1893.5809888962149 grad: 1452.4584249305822
iteration: 10 loss: 0.027508089111506706 grad: 0.15474358050833736
iteration: 0 loss: 1649.5497851728242 grad: 1376.7261186683345
iteration: 0 loss: 1053.1695703450257 grad: 1139.8328415682035
iteration: 0 loss: 797.233528064136 grad: 1104.8073537549935
iteration: 0 loss: 1265.8252799866123 grad: 1364.9237677050787
iteration: 0 loss: 623.5745843321728 grad: 1059.9942733112207
iteration: 0 loss: 928.6751471623548 grad: 1068.4094598942302
iteration: 0 loss: 803.7334842331119 grad: 1098.2719039483418
iteration: 0 loss: 1936.2601174934723 grad: 1401.7328095630796
iteration: 0 loss: 1188.3657783890897 grad: 1198.9214916003964
iteration: 0 loss: 1956.3968617773478 grad: 1396.1162883161326
iteration: 10 loss: 0.09690186150526808 grad: 0.6849928632482123
iteration: 0 loss: 1926.480431195622 grad: 1398.3552084522291
iteration: 10 loss: 0.024994872042787556 grad: 0.17788010912073401
iteration: 0 loss: 2264.48381057369 grad: 1500.3124365219373
iteration: 10 loss: 0.030665263246522623 grad: 0.2360230935132514
iteration: 0 loss: 990.528579923679 grad: 1185.5855943640877
iteration: 0 loss: 1016.5374429474443 grad: 1201.9804220183698
iteration: 0 loss: 1314.406743991134 grad: 1351.2447867857977
iteration: 10 loss: 0.027067007976174038 grad: 0.2239931205834786
iteration: 0 loss: 1351.102853110563 grad: 1307.7375968587687
iteration: 0 loss: 2616.195166204415 grad: 1514.8400620741518
iteration: 10 loss: 0.03580314197328831 grad: 1.3473553292696705
iteration: 0 loss: 1096.1632321525547 grad: 1280.3751701309543
iteration: 0 loss: 1986.1304641918186 grad: 1450.3978765946556
iteration: 10 loss: 0.032358685283477163 grad: 0.16905199889782785
iteration: 0 loss: 1307.004585117915 grad: 1271.2779077175182
iteration: 0 loss: 819.8233456474453 grad: 1198.7536781194858
iteration: 0 loss: 1916.0598845872264 grad: 1423.1590258372244
iteration: 10 loss: 0.02742007009841142 grad: 0.1382010994976338
iteration: 0 loss: 917.8965487648028 grad: 1190.0136877479329
iteration: 0 loss: 2267.9465335608893 grad: 1447.4523613037227
iteration: 10 loss: 0.12574445693727201 grad: 1.4067112900712657
iteration: 0 loss: 818.7415491240645 grad: 1131.8024297290087
iteration: 0 loss: 1792.3799200654585 grad: 1378.0120756103383
iteration: 0 loss: 897.2226416360492 grad: 1122.3503299402305
iteration: 0 loss: 1347.8255192452136 grad: 1369.0209653457596
iteration: 0 loss: 957.8366607953678 grad: 1219.745003320937
iteration: 0 loss: 1257.9807324108262 grad: 1271.8683470213973
iteration: 10 loss: 0.026597351896767082 grad: 0.13330424111682232
iteration: 0 loss: 1323.9918883763728 grad: 1269.1706383652668
iteration: 0 loss: 699.1676684893532 grad: 1081.4135928682351
iteration: 0 loss: 1664.247139704229 grad: 1313.7309436123185
iteration: 10 loss: 0.026405322466664737 grad: 0.2424355444445972
iteration: 0 loss: 1575.5252551735578 grad: 1331.4573500879023
iteration: 10 loss: 0.03022405011208982 grad: 0.12713655686142403
iteration: 0 loss: 4002.410106484003 grad: 1947.3845402595837
iteration: 10 loss: 0.004415522500163537 grad: -0.10402666973472158
iteration: 0 loss: 3171.642025941139 grad: 1823.0300925139873
iteration: 0 loss: 3504.171591905157 grad: 1847.0803586518643
iteration: 0 loss: 4450.157761003312 grad: 2024.974611201662
iteration: 10 loss: 0.004393204903118947 grad: 0.040334167577284655
iteration: 0 loss: 2707.7823687761806 grad: 1761.1932610188949
iteration: 0 loss: 1846.912701007955 grad: 1569.5773363196658
iteration: 10 loss: 0.003936843179293315 grad: 0.6356471283039944
iteration: 0 loss: 2912.517527430825 grad: 1816.2245146182636
iteration: 0 loss: 1537.7986634053502 grad: 1486.7160289896606
iteration: 0 loss: 4371.553782995384 grad: 1902.6543635556698
iteration: 0 loss: 2844.6207796461063 grad: 1768.6395648897305
iteration: 0 loss: 1744.3158980680246 grad: 1618.748394721969
iteration: 0 loss: 3584.8733800081736 grad: 1904.1295156422136
iteration: 0 loss: 2495.252968544437 grad: 1613.3300724687297
iteration: 0 loss: 2635.326436628754 grad: 1799.2625062552818
iteration: 0 loss: 3515.648932112142 grad: 1827.8511362804966
iteration: 0 loss: 5083.140832800812 grad: 2071.6862923773633
iteration: 10 loss: 0.007029821430809203 grad: -0.007377174330461436
iteration: 0 loss: 2987.5582358481847 grad: 1776.282792045518
iteration: 0 loss: 2115.471028848638 grad: 1655.9291274609982
iteration: 0 loss: 5193.395223688305 grad: 2074.8863774214215
iteration: 10 loss: 0.006775312850632789 grad: 0.08938811195396393
iteration: 0 loss: 2489.5446622804284 grad: 1676.8633413778653
iteration: 0 loss: 3608.4897941745908 grad: 1900.841601953473
iteration: 0 loss: 4361.61324788194 grad: 1977.0997666903506
iteration: 10 loss: 0.029417801229018634 grad: 0.6182754161904641
iteration: 0 loss: 2306.409706308198 grad: 1595.9590190882388
iteration: 0 loss: 2666.7489724273005 grad: 1755.4644435875134
iteration: 0 loss: 2788.9748251578026 grad: 1842.06229961202
iteration: 0 loss: 2427.374309235516 grad: 1775.3499210040143
iteration: 0 loss: 5585.466747729874 grad: 2084.678106906202
iteration: 10 loss: 0.007740883716244504 grad: -0.7132666932641802
iteration: 0 loss: 5388.473887721699 grad: 2018.696142353508
iteration: 10 loss: 0.4672465507226827 grad: -0.9296445997342804
iteration: 0 loss: 3044.973790217233 grad: 1816.8224146716452
iteration: 0 loss: 2319.714805681207 grad: 1672.966313757544
iteration: 10 loss: 0.004126057585050479 grad: 0.17973231712778795
iteration: 0 loss: 6377.340528292972 grad: 2202.4510530829184
iteration: 10 loss: 0.18638101329270285 grad: 0.3348822603849919
iteration: 0 loss: 3081.6624366502297 grad: 1917.9448900439615
iteration: 0 loss: 2960.3419987353004 grad: 1809.3501579074998
iteration: 0 loss: 4150.59078373385 grad: 1879.324329881555
iteration: 10 loss: 0.004032246315380855 grad: 0.48509302775395374
iteration: 0 loss: 4073.9140106400005 grad: 1940.9862516321978
iteration: 10 loss: 0.004252615222702099 grad: 0.02247660545517981
iteration: 0 loss: 3404.3327493113043 grad: 1884.9970533938206
iteration: 10 loss: 0.018525999582462562 grad: 0.8444025299854127
iteration: 0 loss: 3223.5434170059916 grad: 1807.6908006320036
iteration: 10 loss: 0.004388462246094026 grad: -0.0052325983659134495
iteration: 0 loss: 5101.253070287529 grad: 2088.627902773894
iteration: 10 loss: 0.006930619324769147 grad: 0.6139129760082587
iteration: 0 loss: 2429.068094301174 grad: 1720.7097916466741
iteration: 0 loss: 2636.1107099725323 grad: 1753.5657311221062
iteration: 0 loss: 3461.1503400394035 grad: 1837.5857270215663
iteration: 0 loss: 1697.2295107128814 grad: 1594.956647575445
iteration: 0 loss: 7230.831342156197 grad: 2273.751704893131
iteration: 10 loss: 0.5728946431759554 grad: 2.484268977586442
iteration: 0 loss: 2870.9859358933195 grad: 1690.7059344483732
iteration: 0 loss: 3834.6433200051742 grad: 2030.5080910146594
iteration: 0 loss: 4070.2640550612427 grad: 1877.6633774037673
iteration: 0 loss: 3135.062497911757 grad: 1651.040407404673
iteration: 0 loss: 2479.350688209422 grad: 1784.3150196507115
iteration: 0 loss: 4386.83280282224 grad: 1970.2473844875788
iteration: 10 loss: 0.0042187685916856435 grad: -0.3645293132719804
iteration: 0 loss: 4585.368987570572 grad: 1979.1344413108736
iteration: 10 loss: 0.00462690884424691 grad: 0.12020541689224078
iteration: 0 loss: 5318.52256013811 grad: 2043.6401410814465
iteration: 10 loss: 0.4136556087747026 grad: 2.1884410411707966
iteration: 0 loss: 2831.1948280236497 grad: 1839.285456737091
iteration: 0 loss: 4827.284558531148 grad: 2096.764680745792
iteration: 10 loss: 0.00722022065284281 grad: 0.44742792245873286
iteration: 0 loss: 3555.477062249277 grad: 1874.0904958853296
iteration: 0 loss: 2340.879499993421 grad: 1695.0135599263276
iteration: 0 loss: 3152.752484010506 grad: 1826.7890057166196
iteration: 0 loss: 2327.038301654147 grad: 1772.0565588576794
iteration: 0 loss: 3482.683138224957 grad: 1872.8464865459564
iteration: 0 loss: 5243.8821915962335 grad: 2128.0261680150097
iteration: 10 loss: 0.004263617353651419 grad: 0.8258775270282445
iteration: 0 loss: 2800.2320071055765 grad: 1802.1107280228202
iteration: 0 loss: 4538.849216199035 grad: 1990.9242929304182
iteration: 10 loss: 0.027852486963225107 grad: 1.4876500905469578
iteration: 0 loss: 4526.395378932205 grad: 2099.633223029403
iteration: 10 loss: 0.004213050095793072 grad: -0.0190140580405208
iteration: 0 loss: 2325.0232383103325 grad: 1557.1187174857969
iteration: 0 loss: 2426.604500594462 grad: 1711.5134263184964
iteration: 0 loss: 2953.1050114302984 grad: 1856.66516699989
iteration: 0 loss: 3060.998375130094 grad: 1813.58865539147
iteration: 0 loss: 2300.7755954274417 grad: 1633.2500811886005
iteration: 0 loss: 4172.9205324130735 grad: 1999.5662162187668
iteration: 10 loss: 0.0047328663872163316 grad: 0.08572756628697878
iteration: 0 loss: 3453.1666235824746 grad: 1828.292320130555
iteration: 10 loss: 0.004293835663040888 grad: 0.04201991628849699
iteration: 0 loss: 3351.6503640502565 grad: 1946.7933989039877
iteration: 0 loss: 4051.8226901797457 grad: 1986.4458229534628
iteration: 10 loss: 0.004113341821886768 grad: -0.08250516979001812
iteration: 0 loss: 6185.7441602858735 grad: 2210.8844863410145
iteration: 10 loss: 0.15231753525255376 grad: -1.1095243432129034
iteration: 0 loss: 2592.0896829378817 grad: 1706.0775621570065
iteration: 0 loss: 4282.367324185969 grad: 1973.011966765661
iteration: 10 loss: 0.0038217704542065885 grad: 0.10004899978159508
iteration: 0 loss: 2712.038572193968 grad: 1775.5089036042602
iteration: 0 loss: 2135.1767464502873 grad: 1647.1669520970518
iteration: 0 loss: 2728.812820562572 grad: 1663.1433624830347
iteration: 0 loss: 2188.329007279906 grad: 1650.109935391174
iteration: 0 loss: 3217.95105496997 grad: 1919.157747769319
iteration: 0 loss: 4553.954391829037 grad: 1945.50288391195
iteration: 0 loss: 5169.219323017724 grad: 2108.990699706305
iteration: 10 loss: 0.003844655909977684 grad: 0.14968735350866477
iteration: 0 loss: 4778.267692818757 grad: 2012.7867733629082
iteration: 10 loss: 0.04930186176021711 grad: -0.5886270674824609
iteration: 0 loss: 3851.0211552094775 grad: 1937.0905814791097
iteration: 0 loss: 4939.44534584963 grad: 2015.9719165870333
iteration: 10 loss: 0.00522393261384092 grad: -0.06502638569697086
iteration: 0 loss: 4870.2716229604375 grad: 2020.678047720392
iteration: 10 loss: 0.022724745302349966 grad: 1.137203259871385
iteration: 0 loss: 3736.4310053986887 grad: 1755.5037021508783
iteration: 10 loss: 0.004459887712569484 grad: -0.006611198440833549
iteration: 0 loss: 5865.472813406236 grad: 2074.121845736722
iteration: 10 loss: 0.007161514879871075 grad: 1.0723129948990493
iteration: 0 loss: 4016.2352300703346 grad: 1856.1504421967143
iteration: 10 loss: 0.004230070744762832 grad: 0.36543408404007266
iteration: 0 loss: 2085.7220443838155 grad: 1590.5559015496558
iteration: 0 loss: 1844.74551255806 grad: 1510.4203260209583
iteration: 0 loss: 2571.8416840405794 grad: 1655.0781544987533
iteration: 0 loss: 3391.053612803014 grad: 1901.4235324758674
iteration: 0 loss: 3214.6871582667227 grad: 1784.360511130401
iteration: 10 loss: 0.003672347171893555 grad: 0.031739600309556776
iteration: 0 loss: 2641.972205977873 grad: 1729.0300667902143
iteration: 0 loss: 2271.637736363901 grad: 1675.7076561400072
iteration: 0 loss: 4664.8849242920405 grad: 2103.1343600623677
iteration: 0 loss: 3803.380273789886 grad: 1907.9810606590045
iteration: 10 loss: 0.004711075996800157 grad: 0.5193200327943902
iteration: 0 loss: 3552.9431141046225 grad: 1840.5048679390566
iteration: 10 loss: 0.004477232931425202 grad: 0.03185992857725138
iteration: 0 loss: 3118.428318279283 grad: 1824.2487857259198
iteration: 10 loss: 0.004493300179919143 grad: 0.03719923983795547
iteration: 0 loss: 2292.4708835595175 grad: 1572.148145708601
iteration: 0 loss: 3722.974828490383 grad: 1884.5944493942732
iteration: 0 loss: 2752.605059349632 grad: 1744.808344800286
iteration: 0 loss: 2798.249712488356 grad: 1834.3156231193584
iteration: 0 loss: 6284.851720698579 grad: 2260.1682836629398
iteration: 10 loss: 0.05253127430544489 grad: -1.220549581438545
iteration: 0 loss: 2060.629872288073 grad: 1654.3473318956815
iteration: 0 loss: 2023.622907047076 grad: 1592.6671122997006
iteration: 0 loss: 2442.6457373257576 grad: 1676.695248987055
iteration: 0 loss: 3061.088172909968 grad: 1868.6744951515286
iteration: 0 loss: 3758.764016100015 grad: 1916.273437298616
iteration: 10 loss: 0.0038243024887792258 grad: 0.03903115441971264
iteration: 0 loss: 4263.994823926411 grad: 2047.8657725081366
iteration: 10 loss: 0.00387316733130402 grad: -0.051128081785700163
iteration: 0 loss: 5252.359854964337 grad: 2010.550846941948
iteration: 10 loss: 0.1574296822893137 grad: 3.5822636447116007
iteration: 0 loss: 4123.109886209518 grad: 1933.350536849614
iteration: 10 loss: 0.04284841793626335 grad: 0.7230393697574606
iteration: 0 loss: 3238.934220352948 grad: 1837.6184478898435
iteration: 0 loss: 5364.829729250621 grad: 2070.5278368818226
iteration: 10 loss: 0.004815903766112487 grad: 0.6427954892914385
iteration: 0 loss: 4073.0198802121904 grad: 1933.9570366388843
iteration: 0 loss: 2541.5886321029175 grad: 1813.951207707562
iteration: 10 loss: 0.0038447671190211126 grad: 0.004059989380448525
iteration: 0 loss: 3052.5100976543804 grad: 1821.7920201788556
iteration: 0 loss: 2743.19355835853 grad: 1787.2118935987448
iteration: 0 loss: 4811.201135494517 grad: 2075.2627477791048
iteration: 10 loss: 0.06265031736455223 grad: 2.7897036100159394
iteration: 0 loss: 2513.0480665105106 grad: 1828.7113624651734
iteration: 0 loss: 3118.530362097008 grad: 1742.6152153628568
iteration: 0 loss: 1709.7066082760573 grad: 1557.342189028932
iteration: 0 loss: 3672.334295075161 grad: 1988.9962818591637
iteration: 0 loss: 3533.001184204625 grad: 1933.6440531591804
iteration: 0 loss: 3502.6194216088115 grad: 1908.1643195399524
iteration: 10 loss: 0.004503157956192313 grad: -0.11293180451625524
iteration: 0 loss: 5068.278901870352 grad: 1982.4297897361184
iteration: 10 loss: 0.13066883146530017 grad: 1.4901579029904823
iteration: 0 loss: 2820.250200224429 grad: 1793.166876444569
iteration: 0 loss: 3291.397164447367 grad: 1869.4454358026092
iteration: 10 loss: 0.0037036998837720603 grad: 0.02205308941766897
iteration: 0 loss: 5155.675810642346 grad: 2082.1325594549317
iteration: 10 loss: 0.004667926132573153 grad: 0.049374062720203477
iteration: 0 loss: 3179.8949431209885 grad: 1774.450441957686
iteration: 0 loss: 4375.9883207066205 grad: 1901.2040653388608
iteration: 10 loss: 0.004403517253442921 grad: 0.6471792492614794
iteration: 0 loss: 2763.8254982259905 grad: 1816.0446415500448
iteration: 0 loss: 4744.517032576736 grad: 2092.807153185392
iteration: 0 loss: 4103.377629480874 grad: 1983.0052644237962
iteration: 0 loss: 2632.2643524879672 grad: 1644.7510304662633
iteration: 0 loss: 2046.1821541795473 grad: 1594.0318632039452
iteration: 0 loss: 3319.838246291383 grad: 1964.5194646030836
iteration: 0 loss: 1668.6530258606717 grad: 1528.2347500603048
iteration: 0 loss: 2394.774318701758 grad: 1545.1083134891815
iteration: 0 loss: 2061.4219621463517 grad: 1583.8208488186187
iteration: 0 loss: 4896.055367748572 grad: 2018.5013923431502
iteration: 10 loss: 0.003916126965619201 grad: 0.02899428019892499
iteration: 0 loss: 2966.69693683037 grad: 1730.2895655655993
iteration: 0 loss: 5080.079465884491 grad: 2010.4236679235587
iteration: 10 loss: 0.32581190641741903 grad: 1.1793955834321384
iteration: 0 loss: 4840.189811547959 grad: 2013.5696595908578
iteration: 10 loss: 0.004438495497610843 grad: 0.004980480503669012
iteration: 0 loss: 5756.004448715819 grad: 2159.7213241563686
iteration: 10 loss: 0.02631828937922943 grad: 1.652581274249934
iteration: 0 loss: 2562.2073069216926 grad: 1709.2262439199212
iteration: 0 loss: 2610.9933192437265 grad: 1730.7018075214212
iteration: 0 loss: 3405.167436183234 grad: 1945.905147528209
iteration: 0 loss: 3481.2588157316563 grad: 1883.8491787099251
iteration: 0 loss: 6605.816675382832 grad: 2182.0370528944527
iteration: 10 loss: 0.2846134627761785 grad: -0.22420789643799727
iteration: 0 loss: 2884.6125574581038 grad: 1845.3123179358936
iteration: 0 loss: 5060.456224473582 grad: 2088.1174025899286
iteration: 10 loss: 0.005628602007742633 grad: 1.2554850320575752
iteration: 0 loss: 3313.023408968029 grad: 1832.3371575319675
iteration: 0 loss: 2187.7780789494773 grad: 1726.2324982361852
iteration: 0 loss: 4793.819194770456 grad: 2049.478598290867
iteration: 10 loss: 0.004650926746597344 grad: 0.40768943088871895
iteration: 0 loss: 2380.9341664179515 grad: 1715.6722264126217
iteration: 0 loss: 5744.057803226918 grad: 2084.608127213324
iteration: 10 loss: 0.4769797615246551 grad: 3.357844137915194
iteration: 0 loss: 2080.746988457962 grad: 1632.5919367593401
iteration: 0 loss: 4518.8436868589715 grad: 1983.6809832938707
iteration: 0 loss: 2305.3838897336527 grad: 1618.724691861566
iteration: 0 loss: 3459.4023011434106 grad: 1972.8968111976806
iteration: 10 loss: 0.004271109417128504 grad: 0.024949827603685772
iteration: 0 loss: 2513.9581674341343 grad: 1759.9099742854648
iteration: 0 loss: 3363.1284105975155 grad: 1833.3043252733355
iteration: 10 loss: 0.004777169136187612 grad: -0.18907991851429837
iteration: 0 loss: 3315.865808362165 grad: 1829.5405834381672
iteration: 0 loss: 1841.281270437729 grad: 1559.2429931509175
iteration: 0 loss: 4169.040233548629 grad: 1892.3634533689606
iteration: 10 loss: 0.004507194777860687 grad: 0.032914462859033995
iteration: 0 loss: 4034.899694915632 grad: 1916.350553441176
iteration: 10 loss: 0.005312044944730587 grad: 0.07210307620611084
iteration: 0 loss: inf grad: 3130.9768885473586
iteration: 10 loss: 0.0005024066141975874 grad: 0.0183646931931183
iteration: 0 loss: 9703.238523898122 grad: 2942.5051539628475
iteration: 0 loss: 10697.313104507479 grad: 2972.1208530886634
iteration: 10 loss: 0.0005045499313961376 grad: 0.2547671878393911
iteration: 0 loss: inf grad: 3261.9011499838907
iteration: 0 loss: 8386.183125560801 grad: 2835.447664648067
iteration: 0 loss: 5585.7959014226935 grad: 2531.4799809432166
iteration: 10 loss: 0.0003438565671041777 grad: 0.009444390276746008
iteration: 0 loss: 8895.911498172885 grad: 2926.8340527523333
iteration: 0 loss: 4777.47713267171 grad: 2399.288507103447
iteration: 0 loss: inf grad: 3069.0077598338084
iteration: 0 loss: 8685.113159724302 grad: 2852.7684411623295
iteration: 0 loss: 5544.812418099758 grad: 2613.791090665248
iteration: 0 loss: 11094.280967917677 grad: 3070.246636280943
iteration: 10 loss: 0.0004514741065742617 grad: -0.428411260082342
iteration: 0 loss: 7452.907917203952 grad: 2596.6703596734114
iteration: 0 loss: 8145.188514779711 grad: 2899.090646044705
iteration: 0 loss: inf grad: 2944.360373185725
iteration: 0 loss: 15495.779118815395 grad: 3334.2740479575405
iteration: 0 loss: 9177.008711944676 grad: 2863.7572008845505
iteration: 0 loss: 6538.183899078267 grad: 2668.820452286931
iteration: 0 loss: inf grad: 3341.5979348287246
iteration: 0 loss: 7603.937498461208 grad: 2701.8806456161324
iteration: 0 loss: 11016.50292463986 grad: 3061.8702625161313
iteration: 0 loss: 13331.080217308778 grad: 3184.59128990614
iteration: 10 loss: 0.0004799254714849998 grad: -0.7285431443616716
iteration: 0 loss: 6787.381757600245 grad: 2570.532484167885
iteration: 0 loss: 8406.015467215977 grad: 2828.91384231776
iteration: 0 loss: 8598.341223345733 grad: 2968.656639504756
iteration: 0 loss: 7355.736341245458 grad: 2862.587059519335
iteration: 10 loss: 0.0004681725562973456 grad: 0.010035105517732495
iteration: 0 loss: inf grad: 3359.785943030769
iteration: 0 loss: inf grad: 3253.889858399688
iteration: 0 loss: 9459.66479022109 grad: 2925.0765324916815
iteration: 0 loss: 7021.103280079851 grad: 2694.365466204781
iteration: 10 loss: 0.00046318502228876406 grad: -0.0017091879311941615
iteration: 0 loss: inf grad: 3546.4103677284684
iteration: 0 loss: 9418.95536941228 grad: 3092.380850975323
iteration: 0 loss: 9141.946416236793 grad: 2915.479768819753
iteration: 0 loss: inf grad: 3028.8158855822635
iteration: 0 loss: inf grad: 3125.46681262155
iteration: 0 loss: 10334.756717686441 grad: 3037.6795785296517
iteration: 10 loss: 0.0005988075704821808 grad: -1.6523455060163545
iteration: 0 loss: inf grad: 2910.026365043288
iteration: 10 loss: 0.0002825416635129262 grad: 0.3810098115535866
iteration: 0 loss: inf grad: 3362.030000920402
iteration: 0 loss: 7392.3799805585595 grad: 2775.2047865233626
iteration: 0 loss: 8172.657605507393 grad: 2825.25729699802
iteration: 0 loss: inf grad: 2961.4952855375336
iteration: 0 loss: 5295.402268190233 grad: 2569.3329617888694
iteration: 0 loss: inf grad: 3658.5012459514346
iteration: 0 loss: inf grad: 2727.3867068397203
iteration: 10 loss: 0.0004988547536248171 grad: -0.20900386433046142
iteration: 0 loss: 11845.029950971659 grad: 3274.3259633305197
iteration: 10 loss: 0.0004500498391941867 grad: 0.007615358749347111
iteration: 0 loss: inf grad: 3027.4858890196747
iteration: 0 loss: 9214.328979057582 grad: 2662.5733686876183
iteration: 0 loss: 7648.790119527524 grad: 2874.159245386591
iteration: 0 loss: inf grad: 3175.2978406859065
iteration: 0 loss: inf grad: 3191.149883288465
iteration: 0 loss: inf grad: 3292.5381177010904
iteration: 0 loss: 8697.675128149429 grad: 2964.187972033161
iteration: 0 loss: 14695.554750623445 grad: 3375.229063340836
iteration: 0 loss: 10674.082530371927 grad: 3022.4053017654796
iteration: 0 loss: 7268.363059945869 grad: 2728.9944868679245
iteration: 0 loss: 9658.384950911872 grad: 2941.0978471896656
iteration: 10 loss: 0.00048151264623315495 grad: 0.4464889477118088
iteration: 0 loss: 7348.045417277846 grad: 2851.7999242866676
iteration: 0 loss: 10561.548950343129 grad: 3019.171263104167
iteration: 10 loss: 0.0004013974367725578 grad: 0.0020973813028881023
iteration: 0 loss: inf grad: 3426.4119278624985
iteration: 0 loss: 8506.791280399797 grad: 2901.712474624346
iteration: 0 loss: inf grad: 3210.9823439139664
iteration: 0 loss: 13754.908687672729 grad: 3380.331153753047
iteration: 10 loss: 0.0004475967963861132 grad: 0.01296903605595932
iteration: 0 loss: 7158.095713674832 grad: 2513.864088284433
iteration: 0 loss: 7652.743330953664 grad: 2761.1877615331878
iteration: 10 loss: 0.00037435210701501506 grad: 0.0027569210472343127
iteration: 0 loss: 8984.171148844236 grad: 2990.2816545110536
iteration: 0 loss: 9297.982795876324 grad: 2920.9589427258916
iteration: 10 loss: 0.0004763437001119283 grad: -0.004386022092370392
iteration: 0 loss: 6937.050052861739 grad: 2633.2404229649455
iteration: 0 loss: 12634.930350114852 grad: 3217.4849391605903
iteration: 10 loss: 0.0004978925386570733 grad: 0.0204994260728674
iteration: 0 loss: inf grad: 2947.8833155565962
iteration: 10 loss: 0.0003514814201149751 grad: 0.015475736080665905
iteration: 0 loss: 10302.6200900445 grad: 3134.0289773187546
iteration: 10 loss: 0.0003321008195846596 grad: 0.001703189931152387
iteration: 0 loss: 12435.533343216142 grad: 3202.6791479336152
iteration: 10 loss: 0.0005448362155055457 grad: -0.3667957646886344
iteration: 0 loss: inf grad: 3557.465499297822
iteration: 0 loss: 7805.953631399784 grad: 2746.869149244466
iteration: 0 loss: 13025.225057257754 grad: 3178.2082521519314
iteration: 10 loss: 0.00038432992931286043 grad: 0.017330911933595785
iteration: 0 loss: 8209.108071100629 grad: 2861.850162290497
iteration: 10 loss: 0.0003575657425575297 grad: 0.011013814451695999
iteration: 0 loss: 6480.391961863721 grad: 2653.8954650119426
iteration: 0 loss: inf grad: 2680.219671071371
iteration: 0 loss: 6783.33076017226 grad: 2659.198564336084
iteration: 0 loss: 9878.496861615115 grad: 3089.824429169539
iteration: 0 loss: inf grad: 3133.4457610601066
iteration: 0 loss: 15633.610022352312 grad: 3401.088518006918
iteration: 0 loss: inf grad: 3244.704153466144
iteration: 0 loss: inf grad: 3117.857615352986
iteration: 0 loss: inf grad: 3245.3091736503643
iteration: 0 loss: inf grad: 3254.3617585342354
iteration: 0 loss: 10978.057567534854 grad: 2825.498347650849
iteration: 10 loss: 0.11187794086237607 grad: 0.6475544792954614
iteration: 0 loss: inf grad: 3341.053641673778
iteration: 0 loss: inf grad: 2993.0415739792807
iteration: 10 loss: 0.0004570788111199032 grad: 0.6857688330021492
iteration: 0 loss: 6486.540790590553 grad: 2565.744738992066
iteration: 0 loss: 5619.525439024164 grad: 2435.711708402416
iteration: 0 loss: 7786.003303164834 grad: 2669.366143363536
iteration: 0 loss: 10280.46537382077 grad: 3062.828315935584
iteration: 0 loss: inf grad: 2876.403749571451
iteration: 0 loss: 8122.748858966658 grad: 2783.8713071477255
iteration: 0 loss: 7028.927522318539 grad: 2700.8743638433266
iteration: 0 loss: 14210.025363988278 grad: 3385.092035331828
iteration: 10 loss: 0.017053888070883906 grad: -1.413085239865123
iteration: 0 loss: 11427.938750558811 grad: 3073.621261694522
iteration: 10 loss: 0.0004275717894250358 grad: 0.6189367659651324
iteration: 0 loss: 10488.491460475627 grad: 2965.1878042045955
iteration: 10 loss: 0.0004671678550287404 grad: -0.0034644114343867285
iteration: 0 loss: 9559.964658905368 grad: 2938.2513337681726
iteration: 0 loss: 6804.841043017371 grad: 2536.6666105948175
iteration: 0 loss: 11232.699263881112 grad: 3033.9873412338716
iteration: 0 loss: 8414.436592141521 grad: 2811.200762974243
iteration: 0 loss: 8735.196475346911 grad: 2957.763052285781
iteration: 10 loss: 0.00046773222063414073 grad: 0.006083511249839671
iteration: 0 loss: 19043.98892109746 grad: 3641.127185608163
iteration: 0 loss: 6435.34371900589 grad: 2666.779765581207
iteration: 0 loss: 6347.86155458568 grad: 2567.478930028972
iteration: 0 loss: 7470.891937388311 grad: 2704.008573198128
iteration: 0 loss: 9508.762733935066 grad: 3011.7683699801755
iteration: 0 loss: 11322.286328859436 grad: 3089.315268043857
iteration: 10 loss: 0.0003030502464918589 grad: 0.0026586122414868547
iteration: 0 loss: 13024.33977302275 grad: 3295.73333938825
iteration: 10 loss: 0.000408252529186112 grad: -0.007135622137513397
iteration: 0 loss: inf grad: 3237.910011429395
iteration: 0 loss: inf grad: 3111.980087449444
iteration: 0 loss: 9803.429805441498 grad: 2957.851301492857
iteration: 0 loss: inf grad: 3337.6051655900974
iteration: 0 loss: inf grad: 3118.302110673856
iteration: 0 loss: 8089.13402165113 grad: 2922.9984808108215
iteration: 0 loss: 9236.0012602429 grad: 2933.636295037554
iteration: 0 loss: 8405.999888380657 grad: 2880.891593468892
iteration: 10 loss: 0.0004268721412782642 grad: -0.02781290696684996
iteration: 0 loss: inf grad: 3341.0129283953333
iteration: 0 loss: 7789.332000177949 grad: 2945.5412999498694
iteration: 0 loss: 9461.518697986727 grad: 2809.446758353805
iteration: 0 loss: 5251.401391831213 grad: 2511.601898844846
iteration: 0 loss: 11005.387415260255 grad: 3205.093564707805
iteration: 0 loss: 10748.828747753178 grad: 3117.0241418975934
iteration: 10 loss: 0.00042714917270297354 grad: -0.017928708120312904
iteration: 0 loss: 10655.53455935141 grad: 3075.9899238939097
iteration: 10 loss: 0.0006880041830961338 grad: -0.8178421222678938
iteration: 0 loss: inf grad: 3197.4709685112202
iteration: 0 loss: 8668.396437732625 grad: 2891.117717293014
iteration: 0 loss: inf grad: 3013.4504958008583
iteration: 10 loss: 0.0005628082779532468 grad: -0.0006363904287277743
iteration: 0 loss: 15239.590805947024 grad: 3353.043287736644
iteration: 10 loss: 0.0005459647218231112 grad: 0.010131413796108454
iteration: 0 loss: 9733.726881570277 grad: 2865.755109869488
iteration: 10 loss: 0.0004245858978141438 grad: 0.9327924406492647
iteration: 0 loss: 13120.363497527007 grad: 3063.381519642635
iteration: 10 loss: 0.00030308799151415854 grad: 1.0408654998139668
iteration: 0 loss: 8504.746774134263 grad: 2924.891507943781
iteration: 10 loss: 0.0005299577156124128 grad: 0.011929858303916067
iteration: 0 loss: 14356.201054817455 grad: 3372.0161589092427
iteration: 0 loss: inf grad: 3193.658315504794
iteration: 0 loss: 7976.108040240384 grad: 2654.964797733607
iteration: 0 loss: 6299.617687772674 grad: 2571.622726109352
iteration: 0 loss: 10266.388563691908 grad: 3165.381608603538
iteration: 0 loss: 5345.413991726376 grad: 2463.5875006898514
iteration: 0 loss: 7023.609593556955 grad: 2495.228228391959
iteration: 0 loss: 6141.8387953115225 grad: 2555.0570918994244
iteration: 0 loss: inf grad: 3250.4515720408276
iteration: 0 loss: 8956.988300541214 grad: 2787.1616027148475
iteration: 0 loss: 15643.507183116699 grad: 3239.6108279161954
iteration: 0 loss: inf grad: 3244.2161322249603
iteration: 0 loss: inf grad: 3476.276042903067
iteration: 0 loss: 7986.80896808442 grad: 2756.121502751701
iteration: 0 loss: 8055.930130354872 grad: 2794.277764551025
iteration: 0 loss: 10474.379305356471 grad: 3135.600748588282
iteration: 10 loss: 0.00046929027832282537 grad: 0.004099146575668767
iteration: 0 loss: 10649.168567482102 grad: 3035.288352962821
iteration: 0 loss: inf grad: 3513.292400486421
iteration: 0 loss: 9004.45595595158 grad: 2972.3895043364278
iteration: 0 loss: inf grad: 3362.5430131068824
iteration: 0 loss: 10039.89510153865 grad: 2953.2694787980727
iteration: 10 loss: 0.00037308609039014715 grad: 0.005147640485493606
iteration: 0 loss: 6900.706951609376 grad: 2780.959663878433
iteration: 0 loss: inf grad: 3299.7280943779547
iteration: 0 loss: 7291.412010078894 grad: 2768.9501280425898
iteration: 0 loss: inf grad: 3358.6058785666582
iteration: 0 loss: 6496.656094472738 grad: 2632.3062728358295
iteration: 0 loss: inf grad: 3197.2152211255143
iteration: 0 loss: 7031.089711552128 grad: 2611.091577948447
iteration: 0 loss: 10589.167915913737 grad: 3176.498031181146
iteration: 10 loss: 0.00041621413865042 grad: 0.011767327103339126
iteration: 0 loss: 7686.906489347309 grad: 2835.8365712973036
iteration: 0 loss: 10510.233145626093 grad: 2953.229270218376
iteration: 10 loss: 0.0005834299293135039 grad: -0.015041068045742042
iteration: 0 loss: 10008.442563609027 grad: 2949.279492515711
iteration: 0 loss: 5678.978729750172 grad: 2514.1340910032536
iteration: 0 loss: inf grad: 3046.059089678828
iteration: 0 loss: 12267.9322522916 grad: 3088.0341826993276
iteration: 10 loss: 0.0005555432263380764 grad: -0.0025351983975038404
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:57: RuntimeWarning: invalid value encountered in double_scalars
  diff = np.absolute(f_history[-1]-f_history[-2])
iteration: 0 loss: 1325.0208428209125 grad: 1264.7005523256478
iteration: 10 loss: 0.04036224887915358 grad: 0.30787869158185993
iteration: 0 loss: 1033.4145313545782 grad: 1185.9706643884995
iteration: 10 loss: 0.03812057787085492 grad: 0.1545127340219063
iteration: 0 loss: 1125.0405944755053 grad: 1199.572692758251
iteration: 0 loss: 1501.9627117739271 grad: 1317.241442948723
iteration: 10 loss: 0.03678772910610529 grad: 0.2587992681347864
iteration: 0 loss: 890.0025266904552 grad: 1144.1214504908944
iteration: 0 loss: 600.5564473305699 grad: 1016.9787998686008
iteration: 10 loss: 0.038390650357707105 grad: 0.07421267475440618
iteration: 0 loss: 957.0540349240623 grad: 1179.6774464215578
iteration: 0 loss: 480.3249900791083 grad: 964.1940179304124
iteration: 0 loss: 1469.7324637318081 grad: 1237.4301862717482
iteration: 0 loss: 930.9037029733173 grad: 1151.0577486264622
iteration: 0 loss: 546.4731292881581 grad: 1050.4621466561566
iteration: 0 loss: 1150.3913599697125 grad: 1238.5682979934545
iteration: 10 loss: 0.03809979320232577 grad: 0.2566483335974584
iteration: 0 loss: 839.4665292329963 grad: 1045.5858788791138
iteration: 0 loss: 848.8751649845825 grad: 1168.9422236780647
iteration: 0 loss: 1160.0637670217682 grad: 1186.5110817919972
iteration: 0 loss: 1653.0861781289495 grad: 1347.0486952745161
iteration: 10 loss: 0.042893456813379256 grad: 0.27728978200615106
iteration: 0 loss: 985.0102915026018 grad: 1154.4088263258855
iteration: 0 loss: 698.18182795854 grad: 1075.4948720428633
iteration: 0 loss: 1777.9882730514362 grad: 1351.0151970714828
iteration: 10 loss: 0.04448809993184775 grad: 0.3601900934467692
iteration: 0 loss: 810.8768524882772 grad: 1088.887332960693
iteration: 0 loss: 1180.2375749081828 grad: 1235.0569890698762
iteration: 0 loss: 1415.2745488125076 grad: 1286.1464443578532
iteration: 10 loss: 0.043160076054680925 grad: 0.3530856805372616
iteration: 0 loss: 779.7423663206421 grad: 1034.8010194239284
iteration: 0 loss: 823.882755064215 grad: 1141.021741985881
iteration: 10 loss: 0.03829783630333788 grad: 0.16599056484584446
iteration: 0 loss: 913.1192706320543 grad: 1195.854698486428
iteration: 0 loss: 817.6261899099732 grad: 1153.5658956824466
iteration: 10 loss: 0.03865879289746359 grad: 0.12369005787651205
iteration: 0 loss: 1913.937779205623 grad: 1356.7907016689787
iteration: 10 loss: 0.043963106589217205 grad: 0.020001816006931405
iteration: 0 loss: 1801.897192683954 grad: 1313.4590209930852
iteration: 10 loss: 0.11058869646612121 grad: 0.5329552839257118
iteration: 0 loss: 957.6723060505192 grad: 1180.4472781858913
iteration: 0 loss: 760.1150269994833 grad: 1086.2724581631119
iteration: 0 loss: 2172.632790757692 grad: 1434.8727345590498
iteration: 10 loss: 0.045093398965201974 grad: 0.028657440780579635
iteration: 0 loss: 1008.0548177083601 grad: 1247.4454301205246
iteration: 0 loss: 966.1517413520261 grad: 1177.5548035126208
iteration: 10 loss: 0.03690806783329621 grad: 0.1320426439988618
iteration: 0 loss: 1373.9843246563253 grad: 1222.147548739143
iteration: 10 loss: 0.036807075987532946 grad: 0.38159083506441116
iteration: 0 loss: 1354.6273996841462 grad: 1261.9170967870723
iteration: 10 loss: 0.03832390556900381 grad: 0.24877023932679054
iteration: 0 loss: 1110.2002296994615 grad: 1225.564842846365
iteration: 10 loss: 0.044242766014080684 grad: 0.26756605308010517
iteration: 0 loss: 1068.8357966425276 grad: 1175.011270761088
iteration: 10 loss: 0.037297496892279014 grad: 0.19054409702535932
iteration: 0 loss: 1657.408299107707 grad: 1358.2927807848334
iteration: 10 loss: 0.03518937771216522 grad: -0.03132101655628955
iteration: 0 loss: 802.0179205823453 grad: 1117.1875206410987
iteration: 0 loss: 849.1995303663571 grad: 1140.0763537970677
iteration: 0 loss: 1190.5884163194096 grad: 1194.870525188557
iteration: 10 loss: 0.03640074182608673 grad: 0.24139025563897432
iteration: 0 loss: 526.6575164836355 grad: 1035.0300721584206
iteration: 0 loss: 2441.007498275769 grad: 1480.234645464857
iteration: 10 loss: 0.05553257477400952 grad: 0.8289560838888607
iteration: 0 loss: 971.7652784650993 grad: 1097.5370910319234
iteration: 10 loss: 0.0388155738632618 grad: 0.11238469960992842
iteration: 0 loss: 1237.6894859311185 grad: 1321.1539115427067
iteration: 10 loss: 0.03710185323374886 grad: 0.11141769831737111
iteration: 0 loss: 1412.2609543944816 grad: 1221.181152874788
iteration: 10 loss: 0.03658806622512533 grad: 0.2616640633507573
iteration: 0 loss: 1084.8453435278595 grad: 1069.7636774296204
iteration: 0 loss: 810.569617852377 grad: 1159.018252083268
iteration: 0 loss: 1455.879582760853 grad: 1282.6413593709444
iteration: 10 loss: 0.03795739973462663 grad: 0.37983834871789846
iteration: 0 loss: 1549.4581387277415 grad: 1289.0082475302856
iteration: 10 loss: 0.045448166218358725 grad: 0.18671596037271537
iteration: 0 loss: 1756.1033834812108 grad: 1330.1522340954002
iteration: 10 loss: 0.1078559778596338 grad: 1.335046491587282
iteration: 0 loss: 907.2155382131873 grad: 1195.1327371165926
iteration: 10 loss: 0.03827254061153243 grad: 0.2784394117697738
iteration: 0 loss: 1586.6215886270504 grad: 1363.6723621676115
iteration: 10 loss: 0.04173839357529439 grad: 0.12961917886823981
iteration: 0 loss: 1199.2802253701739 grad: 1219.4555060405553
iteration: 0 loss: 746.473136027201 grad: 1101.4012892596857
iteration: 0 loss: 1022.3101399764553 grad: 1187.9079199000944
iteration: 10 loss: 0.03788266421973028 grad: 0.23340169741294572
iteration: 0 loss: 717.3020329720136 grad: 1150.7592485017615
iteration: 0 loss: 1141.040454054958 grad: 1217.9697765741407
iteration: 10 loss: 0.03613340993648639 grad: 0.2712706016568935
iteration: 0 loss: 1774.0131750147762 grad: 1384.499934718522
iteration: 10 loss: 0.03627330419939509 grad: 0.05837351042634169
iteration: 0 loss: 911.1617606628693 grad: 1171.363392341936
iteration: 0 loss: 1496.8090324586594 grad: 1294.2780130137157
iteration: 10 loss: 0.04078432072095562 grad: 0.28756726395472093
iteration: 0 loss: 1476.5265765195816 grad: 1365.505483921545
iteration: 10 loss: 0.036137559447855565 grad: 0.08833227896401097
iteration: 0 loss: 758.0804517176038 grad: 1009.8526652576694
iteration: 0 loss: 748.8180934989662 grad: 1111.8912093947083
iteration: 0 loss: 968.6992803936201 grad: 1206.9429835482192
iteration: 0 loss: 1009.9507639840351 grad: 1179.2314073054529
iteration: 0 loss: 784.2471343812366 grad: 1061.0306076974061
iteration: 0 loss: 1383.3717521578708 grad: 1300.617626580371
iteration: 10 loss: 0.04118228111639672 grad: 0.33300001773699406
iteration: 0 loss: 1204.1998351414202 grad: 1189.1466084338222
iteration: 10 loss: 0.03698589097422717 grad: 0.19474662730931142
iteration: 0 loss: 1100.622532940324 grad: 1267.2271864012646
iteration: 10 loss: 0.04116886937018452 grad: 0.19655239481456196
iteration: 0 loss: 1310.1757437454785 grad: 1290.7696978777321
iteration: 10 loss: 0.03654235534585579 grad: 0.1703611427305577
iteration: 0 loss: 2102.608982716132 grad: 1438.939602593719
iteration: 10 loss: 0.07088923459047113 grad: 0.06464451357989592
iteration: 0 loss: 859.5320504898099 grad: 1108.2770550385146
iteration: 0 loss: 1404.0629381932138 grad: 1283.9178205045441
iteration: 10 loss: 0.03654659566679832 grad: 0.16231235701210617
iteration: 0 loss: 892.6531407431895 grad: 1153.1533358134957
iteration: 0 loss: 701.3886936124458 grad: 1069.6509421170438
iteration: 0 loss: 930.5342524812338 grad: 1080.9325865976723
iteration: 0 loss: 709.0548239864431 grad: 1071.8092445969946
iteration: 0 loss: 1048.813709520327 grad: 1248.5444831461764
iteration: 0 loss: 1556.3351958123915 grad: 1264.5930481447865
iteration: 10 loss: 0.039971253826727414 grad: 0.044755060306815246
iteration: 0 loss: 1709.2994552029822 grad: 1374.0373710447839
iteration: 10 loss: 0.03392001316090252 grad: 0.28184576579470927
iteration: 0 loss: 1609.9911472523302 grad: 1309.6978488653256
iteration: 10 loss: 0.04236323036622542 grad: 0.8326326438776568
iteration: 0 loss: 1257.0686283136936 grad: 1258.1634822723408
iteration: 10 loss: 0.03869447799297632 grad: 0.19306401526795736
iteration: 0 loss: 1660.2377976898433 grad: 1310.1229444847577
iteration: 10 loss: 0.038595976134026634 grad: 0.21351138416322368
iteration: 0 loss: 1625.7911883723891 grad: 1314.5805535386246
iteration: 10 loss: 0.040534883037087806 grad: 0.38797116140563637
iteration: 0 loss: 1315.879003910175 grad: 1141.7993248711828
iteration: 0 loss: 2054.365929714267 grad: 1349.5987088527659
iteration: 10 loss: 0.040503521953558615 grad: -0.0070431805957978585
iteration: 0 loss: 1398.4114761210872 grad: 1207.9701958709725
iteration: 10 loss: 0.03937119412148604 grad: 0.2303616749842431
iteration: 0 loss: 664.800870208527 grad: 1032.185691787623
iteration: 0 loss: 612.8396332965668 grad: 979.890884662722
iteration: 0 loss: 864.0830022519407 grad: 1073.9052549092726
iteration: 0 loss: 1134.8388874825496 grad: 1237.0913096527938
iteration: 0 loss: 1046.3017689711496 grad: 1160.3965532873672
iteration: 0 loss: 829.1598879409456 grad: 1122.7009881474262
iteration: 0 loss: 729.2768303822158 grad: 1088.65591260233
iteration: 0 loss: 1554.7389080013788 grad: 1369.9755822545862
iteration: 10 loss: 0.03810802752427249 grad: 0.12541011510062175
iteration: 0 loss: 1279.178367143681 grad: 1241.1019856981948
iteration: 10 loss: 0.04052833065262265 grad: 0.1870033049891257
iteration: 0 loss: 1239.2364505016935 grad: 1197.0078153243567
iteration: 10 loss: 0.03704617533466345 grad: 0.18582577979878184
iteration: 0 loss: 1025.7121804429414 grad: 1187.2492960240231
iteration: 0 loss: 803.4877812864004 grad: 1022.0012207632873
iteration: 0 loss: 1235.1538580678116 grad: 1225.444875572814
iteration: 0 loss: 901.9009176856929 grad: 1131.4796680444972
iteration: 0 loss: 874.1394217953725 grad: 1192.3813897076047
iteration: 10 loss: 0.04009201437458449 grad: 0.16755665510315354
iteration: 0 loss: 2075.3974738883253 grad: 1471.500660351839
iteration: 10 loss: 0.049491666299052835 grad: 0.6489801676660174
iteration: 0 loss: 668.4623153785727 grad: 1074.6912466985884
iteration: 0 loss: 647.4192349329926 grad: 1034.5018635596573
iteration: 0 loss: 785.1734037711313 grad: 1088.6324954296692
iteration: 0 loss: 977.7709109540315 grad: 1214.5960478020609
iteration: 0 loss: 1264.9393823361395 grad: 1246.383909867508
iteration: 10 loss: 0.03552932628669871 grad: 0.19575111696520098
iteration: 0 loss: 1393.5141457878838 grad: 1331.362651643692
iteration: 10 loss: 0.03627221636204261 grad: 0.31607501037533997
iteration: 0 loss: 1685.1156422640952 grad: 1307.9904230643374
iteration: 10 loss: 0.0702066009180271 grad: 1.689573147917423
iteration: 0 loss: 1307.9652725848264 grad: 1256.6641237297922
iteration: 10 loss: 0.04389433656880018 grad: 0.13700851917520104
iteration: 0 loss: 1063.6639139981073 grad: 1194.7160874037686
iteration: 10 loss: 0.03713774100635254 grad: 0.1968724213634603
iteration: 0 loss: 1791.657456418193 grad: 1347.3710350445267
iteration: 10 loss: 0.04161061258491827 grad: 0.1542378705092377
iteration: 0 loss: 1371.4791243462323 grad: 1256.9203083850075
iteration: 10 loss: 0.03483971477180603 grad: 0.10001689085491054
iteration: 0 loss: 781.440510679699 grad: 1179.0876179508543
iteration: 0 loss: 1006.6485357591769 grad: 1182.4207749473128
iteration: 0 loss: 899.5650393070624 grad: 1161.7556472962985
iteration: 10 loss: 0.04208829033847327 grad: 0.27221164396873865
iteration: 0 loss: 1567.0273530372658 grad: 1351.6421752118392
iteration: 10 loss: 0.04550509107635166 grad: 0.15913559562688756
iteration: 0 loss: 803.2504984281302 grad: 1188.0672250018124
iteration: 0 loss: 1026.8115099021309 grad: 1131.9058717101234
iteration: 0 loss: 543.4754791722047 grad: 1010.7570899231157
iteration: 0 loss: 1240.9875062731762 grad: 1294.3404897491255
iteration: 10 loss: 0.035193013203256254 grad: 0.32753992805638427
iteration: 0 loss: 1151.524684806207 grad: 1256.1817694102479
iteration: 10 loss: 0.0403548226382885 grad: 0.14904395398635942
iteration: 0 loss: 1161.4603359088117 grad: 1240.7417023626567
iteration: 10 loss: 0.03702524876247149 grad: 0.1716475729262642
iteration: 0 loss: 1759.3696559548573 grad: 1287.9551452811506
iteration: 10 loss: 0.03628402857033176 grad: 0.10114325013031322
iteration: 0 loss: 903.5258186576692 grad: 1165.0922940839014
iteration: 0 loss: 1081.3484762557637 grad: 1215.347276184631
iteration: 10 loss: 0.035597864756709896 grad: 0.29164751135110806
iteration: 0 loss: 1788.5706598387058 grad: 1354.5779192291625
iteration: 10 loss: 0.03703700149476803 grad: 0.1294830580884025
iteration: 0 loss: 1070.0703087869476 grad: 1153.6485426269169
iteration: 10 loss: 0.039281416438329456 grad: 0.20728945656996173
iteration: 0 loss: 1461.5316035522467 grad: 1237.1989584746093
iteration: 10 loss: 0.0362837947349445 grad: 0.33085068575076076
iteration: 0 loss: 904.9487330176615 grad: 1179.5075509978733
iteration: 10 loss: 0.04131377517445468 grad: 0.16702773573489488
iteration: 0 loss: 1593.0655399856885 grad: 1359.832731242475
iteration: 10 loss: 0.03857910414146979 grad: 0.22727016789460275
iteration: 0 loss: 1401.6330532727627 grad: 1290.3791475723822
iteration: 10 loss: 0.03593496173562016 grad: 0.19469594281626418
iteration: 0 loss: 899.3521490761748 grad: 1069.2748020199017
iteration: 0 loss: 670.1635910101451 grad: 1033.647226289975
iteration: 0 loss: 1064.1261235907698 grad: 1279.043958737746
iteration: 0 loss: 521.5335883319804 grad: 990.591095167559
iteration: 0 loss: 798.1313940808864 grad: 1002.6341794946651
iteration: 0 loss: 670.6693251543977 grad: 1028.5657485671445
iteration: 0 loss: 1632.2174257952495 grad: 1313.6392368033485
iteration: 10 loss: 0.03492849293847526 grad: 0.1780967548903758
iteration: 0 loss: 997.0849328451375 grad: 1121.661047020064
iteration: 10 loss: 0.03659558757830051 grad: 0.21203919090578516
iteration: 0 loss: 1646.4037786606002 grad: 1307.399519244732
iteration: 10 loss: 0.13922743880869926 grad: 0.7640826940778955
iteration: 0 loss: 1622.5361462034955 grad: 1309.5984798069358
iteration: 10 loss: 0.03475872118096132 grad: 0.2475381518856857
iteration: 0 loss: 1895.3755622754013 grad: 1404.9316744587677
iteration: 10 loss: 0.044527451084088 grad: 0.22033514575324042
iteration: 0 loss: 829.1408103595971 grad: 1110.4664016705856
iteration: 0 loss: 859.2973225763182 grad: 1125.7661720157503
iteration: 0 loss: 1101.837602094376 grad: 1266.1688794766274
iteration: 10 loss: 0.03800716326242599 grad: 0.24201370831504088
iteration: 0 loss: 1139.864128871274 grad: 1225.9019529240643
iteration: 0 loss: 2207.938290184122 grad: 1420.0344541644238
iteration: 10 loss: 0.04492390422654518 grad: 0.5470074138507206
iteration: 0 loss: 911.3598450801948 grad: 1199.0712792666723
iteration: 0 loss: 1669.480841501012 grad: 1359.4239938051069
iteration: 10 loss: 0.041338769863531605 grad: -0.01806234419805091
iteration: 0 loss: 1101.6008116858295 grad: 1191.3003122832515
iteration: 0 loss: 680.1609055349414 grad: 1120.8177704123805
iteration: 0 loss: 1616.430082708259 grad: 1333.5758652981958
iteration: 10 loss: 0.03923821751529679 grad: 0.18651099962357004
iteration: 0 loss: 776.1508436334636 grad: 1114.8790581788799
iteration: 10 loss: 0.039247424091379136 grad: 0.2135563337780317
iteration: 0 loss: 1906.003252104851 grad: 1357.2238979867843
iteration: 10 loss: 0.08762743869572825 grad: 1.5704613822185924
iteration: 0 loss: 684.820868830879 grad: 1058.7878285469228
iteration: 0 loss: 1519.2601521487377 grad: 1291.197978009073
iteration: 10 loss: 0.03410080762015158 grad: 0.24585313728915273
iteration: 0 loss: 754.8265775397041 grad: 1051.2379213117756
iteration: 0 loss: 1136.5248707173205 grad: 1283.8221016769385
iteration: 10 loss: 0.03827339764558763 grad: 0.09830646339103216
iteration: 0 loss: 804.0430025655248 grad: 1142.0280353070168
iteration: 0 loss: 1058.0491600920652 grad: 1191.36264258106
iteration: 10 loss: 0.03672628376741407 grad: 0.15214437877893047
iteration: 0 loss: 1116.606577435907 grad: 1188.5607741849653
iteration: 0 loss: 591.0437982675195 grad: 1011.3024520236552
iteration: 0 loss: 1407.5799624708036 grad: 1231.662787798888
iteration: 10 loss: 0.035506514951107834 grad: 0.24321318632200634
iteration: 0 loss: 1330.3149318704461 grad: 1246.438600294025
iteration: 10 loss: 0.04152282824196101 grad: 0.21468080143447693
iteration: 0 loss: 4850.813083609613 grad: 2116.0881323684357
iteration: 0 loss: 3899.5178597340364 grad: 1984.4406478964274
iteration: 0 loss: 4285.5272954472375 grad: 2008.0077856613796
iteration: 10 loss: 0.002532509621232748 grad: -0.015153438998818092
iteration: 0 loss: 5409.383220523803 grad: 2200.7551009203526
iteration: 10 loss: 0.0033630399239799854 grad: 0.045572203890907534
iteration: 0 loss: 3315.0564965570184 grad: 1914.9744331799939
iteration: 0 loss: 2250.1590427552733 grad: 1707.3503665786675
iteration: 10 loss: 0.0024838311556694944 grad: 0.00621181753068995
iteration: 0 loss: 3584.6663670616836 grad: 1975.8709157132062
iteration: 0 loss: 1889.9438042203415 grad: 1616.6153201586517
iteration: 0 loss: 5314.263624653934 grad: 2071.930536097997
iteration: 0 loss: 3499.594920842776 grad: 1926.7907324105095
iteration: 0 loss: 2166.4633481098163 grad: 1761.712424859224
iteration: 0 loss: 4378.343063231569 grad: 2070.7915363099
iteration: 10 loss: 0.002579568537343717 grad: 0.060526887973705756
iteration: 0 loss: 3056.597962576867 grad: 1753.7785945550593
iteration: 0 loss: 3227.9442784750177 grad: 1956.730724656603
iteration: 0 loss: 4307.536373076865 grad: 1987.7098168787313
iteration: 0 loss: 6209.61913015298 grad: 2252.480761299498
iteration: 10 loss: 0.003970187884988263 grad: 0.2758787202531484
iteration: 0 loss: 3669.8876276763954 grad: 1932.2629711561378
iteration: 0 loss: 2610.086398581714 grad: 1802.5310916512067
iteration: 0 loss: 6309.007822917993 grad: 2255.2036756225766
iteration: 10 loss: 0.007427107057248411 grad: 0.006245142370103131
iteration: 0 loss: 3042.6873012899237 grad: 1825.888323663705
iteration: 0 loss: 4413.705511374876 grad: 2068.0662540926505
iteration: 0 loss: 5333.057663360384 grad: 2149.9425091753014
iteration: 10 loss: 0.014514857564460148 grad: -0.8751318946375083
iteration: 0 loss: 2795.327144962287 grad: 1733.0614436238177
iteration: 0 loss: 3283.4858593534354 grad: 1910.890733666194
iteration: 0 loss: 3424.093391788327 grad: 2002.2487615439816
iteration: 0 loss: 2933.7717942134836 grad: 1933.2429456371724
iteration: 0 loss: 6810.302441511615 grad: 2268.491446708115
iteration: 10 loss: 0.006159953913249245 grad: -0.13663787216774975
iteration: 0 loss: 6578.810913754859 grad: 2195.7821184831273
iteration: 10 loss: 5.259269094062884 grad: -7.305485976716631
iteration: 0 loss: 3739.11637716533 grad: 1974.6546967016998
iteration: 0 loss: 2819.3551684062463 grad: 1818.648651797736
iteration: 0 loss: 7799.458677674757 grad: 2395.674174233188
iteration: 10 loss: 0.09171525951189158 grad: 2.221962982603131
iteration: 0 loss: 3767.1782545297106 grad: 2085.228482460967
iteration: 0 loss: 3631.908960547876 grad: 1969.7938158454137
iteration: 10 loss: 0.002693772644058547 grad: 0.011971890121246777
iteration: 0 loss: 5083.006613004658 grad: 2044.6822837931998
iteration: 10 loss: 0.0030775427506093615 grad: -0.0709331373353748
iteration: 0 loss: 4965.749177235472 grad: 2109.714770467941
iteration: 10 loss: 0.002784128378896805 grad: 0.035133184274241634
iteration: 0 loss: 4187.785442801989 grad: 2050.791185323719
iteration: 10 loss: 0.021447375414109873 grad: -0.7652137399573778
iteration: 0 loss: 3921.7070457783702 grad: 1967.6432729367143
iteration: 10 loss: 0.0025946985789976843 grad: 0.030327980817558626
iteration: 0 loss: 6261.148699493671 grad: 2272.0522072381245
iteration: 10 loss: 0.019857703939619983 grad: 1.2501473289197658
iteration: 0 loss: 2973.4285900760374 grad: 1872.9154922711225
iteration: 0 loss: 3239.3361298749646 grad: 1906.0634504281413
iteration: 10 loss: 0.0028143713921467265 grad: 0.03899966121231743
iteration: 0 loss: 4203.210203346184 grad: 1999.0907768044158
iteration: 0 loss: 2073.3739409221653 grad: 1737.0740595270531
iteration: 0 loss: 8810.911853045669 grad: 2471.3845993433943
iteration: 10 loss: 0.9312904397533699 grad: 5.580952486875737
iteration: 0 loss: 3484.87127035312 grad: 1839.9163744476728
iteration: 0 loss: 4726.1547244746525 grad: 2210.411774240204
iteration: 0 loss: 4967.022198662806 grad: 2042.400356968263
iteration: 0 loss: 3770.422916267735 grad: 1794.4634102909606
iteration: 0 loss: 3043.9325412266944 grad: 1941.385832400746
iteration: 0 loss: 5365.8134603460185 grad: 2143.4030189226614
iteration: 10 loss: 0.002586264347138984 grad: -0.11690686089408818
iteration: 0 loss: 5593.412316218533 grad: 2153.3191822334757
iteration: 10 loss: 0.0046938213861060585 grad: 0.5751549181175569
iteration: 0 loss: 6522.2316736432 grad: 2222.4993708988113
iteration: 10 loss: 0.47285032009875233 grad: 3.2639973419120096
iteration: 0 loss: 3478.4529310092635 grad: 1999.3358003390886
iteration: 0 loss: 5895.625407528605 grad: 2279.7957189326735
iteration: 10 loss: 0.0029423006185838444 grad: -0.014838657892763228
iteration: 0 loss: 4330.672807835936 grad: 2039.5934735536403
iteration: 0 loss: 2883.7504476660874 grad: 1844.5501139084554
iteration: 0 loss: 3858.8471431486223 grad: 1986.3252186576667
iteration: 10 loss: 0.0027811722406609492 grad: 0.059430375954499036
iteration: 0 loss: 2862.325202742496 grad: 1925.1752000916176
iteration: 0 loss: 4263.372921901456 grad: 2038.5418387194745
iteration: 10 loss: 0.0028275566172934223 grad: 0.014889304376383236
iteration: 0 loss: 6388.101202606361 grad: 2314.4755865381403
iteration: 10 loss: 0.002600134448254142 grad: -0.4150392410325815
iteration: 0 loss: 3422.572530523727 grad: 1959.1106731119573
iteration: 0 loss: 5555.289051351408 grad: 2166.262798413891
iteration: 10 loss: 0.031976207176657306 grad: 1.1124871144734891
iteration: 0 loss: 5522.054962248877 grad: 2282.05660015432
iteration: 10 loss: 0.0027561175560218876 grad: 0.028083348223579725
iteration: 0 loss: 2866.6174630562473 grad: 1696.0136796593315
iteration: 0 loss: 2990.479581518698 grad: 1864.6837454298166
iteration: 0 loss: 3584.594313502726 grad: 2018.0551280805148
iteration: 0 loss: 3752.9600287164267 grad: 1972.7220455440624
iteration: 0 loss: 2805.0933878495666 grad: 1776.4313719695206
iteration: 0 loss: 5108.837136904361 grad: 2174.934614592049
iteration: 10 loss: 0.002946202583685094 grad: 0.03186828938123543
iteration: 0 loss: 4147.500286282009 grad: 1991.3652607700913
iteration: 10 loss: 0.002874718767336824 grad: 0.044842515160609345
iteration: 0 loss: 4094.05538201258 grad: 2117.9259268609467
iteration: 0 loss: 4951.428867257814 grad: 2158.621412158479
iteration: 10 loss: 0.002742111959378235 grad: 0.15091094481857492
iteration: 0 loss: 7553.986295672898 grad: 2404.2329354117433
iteration: 10 loss: 0.10591260024838531 grad: -2.545850821534956
iteration: 0 loss: 3141.4763471645088 grad: 1856.2938092095583
iteration: 0 loss: 5235.595275379379 grad: 2146.389996898153
iteration: 10 loss: 0.0030069121805189006 grad: 0.011484267788239851
iteration: 0 loss: 3299.516886601909 grad: 1932.2393375332122
iteration: 10 loss: 0.002968883987870702 grad: 0.07493002256034795
iteration: 0 loss: 2605.8633294786573 grad: 1793.6427566014097
iteration: 0 loss: 3353.8862609052744 grad: 1809.2607132453036
iteration: 0 loss: 2701.2170210424147 grad: 1797.0394122872708
iteration: 0 loss: 3957.1042870468877 grad: 2086.8080726049566
iteration: 0 loss: 5566.024803047185 grad: 2115.906798482387
iteration: 0 loss: 6315.61590085848 grad: 2294.4695196458433
iteration: 10 loss: 0.0023990320041775703 grad: -0.1287148635730681
iteration: 0 loss: 5838.7315218251615 grad: 2189.1017443673572
iteration: 10 loss: 0.03093119964284018 grad: -0.9946304642128918
iteration: 0 loss: 4730.214440376527 grad: 2108.245904134607
iteration: 0 loss: 5990.405384579813 grad: 2191.602659384861
iteration: 10 loss: 0.004891573563906026 grad: 0.573054028977885
iteration: 0 loss: 5927.427369525255 grad: 2196.5987966595912
iteration: 10 loss: 0.05828221226825421 grad: 0.7131163383278775
iteration: 0 loss: 4532.338027082405 grad: 1910.787226715529
iteration: 10 loss: 0.002288563155823133 grad: 0.000920353580268719
iteration: 0 loss: 7132.704379535514 grad: 2256.127158701424
iteration: 10 loss: 0.0053041534133213145 grad: 2.571047983882023
iteration: 0 loss: 4864.631537710691 grad: 2020.9780336663578
iteration: 10 loss: 0.002605837005169385 grad: 0.4693133620791258
iteration: 0 loss: 2585.0461284281328 grad: 1728.266152296332
iteration: 0 loss: 2253.994201415206 grad: 1642.7955320281867
iteration: 0 loss: 3152.294665251545 grad: 1800.4385464952932
iteration: 0 loss: 4154.735433678737 grad: 2068.1578057306133
iteration: 0 loss: 3929.14952751362 grad: 1943.8115680115754
iteration: 0 loss: 3205.0566187485038 grad: 1880.778952660435
iteration: 0 loss: 2782.3870522885777 grad: 1823.3124290070666
iteration: 0 loss: 5707.517286282693 grad: 2285.678089829605
iteration: 10 loss: 0.002586624246429313 grad: -0.00082905519946843
iteration: 0 loss: 4633.879673043148 grad: 2074.3148888114356
iteration: 10 loss: 0.0026022149580636656 grad: -0.4581189793317403
iteration: 0 loss: 4307.200450744124 grad: 2003.9393584064464
iteration: 10 loss: 0.0027310475310183724 grad: 0.003149733387908236
iteration: 0 loss: 3821.333199055774 grad: 1985.6426043768997
iteration: 0 loss: 2818.7404691018364 grad: 1711.5576144430017
iteration: 0 loss: 4557.0521646901625 grad: 2052.417667791614
iteration: 0 loss: 3373.3377752214697 grad: 1896.8851660419691
iteration: 0 loss: 3435.4250436257385 grad: 1995.7915672387232
iteration: 0 loss: 7691.18348735604 grad: 2458.3693644969453
iteration: 10 loss: 0.08992147615406959 grad: 0.43815155418373364
iteration: 0 loss: 2540.7633241457934 grad: 1801.1948181957246
iteration: 0 loss: 2484.2343900292854 grad: 1733.485509523527
iteration: 0 loss: 2987.7245430640546 grad: 1824.8080786480741
iteration: 0 loss: 3766.169155737417 grad: 2033.9576659643735
iteration: 0 loss: 4604.756686134404 grad: 2085.5991890775013
iteration: 10 loss: 0.0022136904233114115 grad: 0.01645671726259816
iteration: 0 loss: 5228.316843540785 grad: 2226.9626377200557
iteration: 10 loss: 0.002284468839537691 grad: 0.10403169505817961
iteration: 0 loss: 6442.9494625697525 grad: 2186.102418174152
iteration: 10 loss: 0.12156199726186143 grad: 1.5606267805615717
iteration: 0 loss: 5039.504884108954 grad: 2101.9872034628916
iteration: 10 loss: 0.015101480969778177 grad: 2.593645293969899
iteration: 0 loss: 3966.7454889234086 grad: 1998.7909474614755
iteration: 0 loss: 6541.569448526797 grad: 2252.8042800650037
iteration: 10 loss: 0.022069933686659417 grad: 1.0700013581447823
iteration: 0 loss: 4973.634713640015 grad: 2104.328237504199
iteration: 0 loss: 3148.3145211820915 grad: 1973.7712437393777
iteration: 0 loss: 3743.55517915506 grad: 1980.1252038594112
iteration: 0 loss: 3344.1802025418747 grad: 1943.1151620630626
iteration: 0 loss: 5897.1973126499015 grad: 2257.297498479341
iteration: 10 loss: 0.05184685608731921 grad: 2.722273820084226
iteration: 0 loss: 3072.6269170392934 grad: 1988.278271065123
iteration: 0 loss: 3794.158809252683 grad: 1897.7819299094267
iteration: 0 loss: 2075.7486163790513 grad: 1694.9857497886003
iteration: 0 loss: 4481.870984481288 grad: 2164.9326925469777
iteration: 0 loss: 4289.789876593414 grad: 2102.6130258136373
iteration: 10 loss: 0.002912474490585737 grad: 0.0476030117132662
iteration: 0 loss: 4277.156175245288 grad: 2074.365309078339
iteration: 10 loss: 0.002566564411443489 grad: 0.335012586882769
iteration: 0 loss: 6148.0256956262665 grad: 2155.378138761116
iteration: 10 loss: 0.26517242661264556 grad: 1.4192877971013378
iteration: 0 loss: 3446.824744280739 grad: 1948.944040125581
iteration: 0 loss: 4035.654726645392 grad: 2033.0541628442097
iteration: 0 loss: 6267.2318514767885 grad: 2262.5535444225607
iteration: 10 loss: 0.0031191929949934342 grad: -0.03464191163302749
iteration: 0 loss: 3894.169670011585 grad: 1931.009931883535
iteration: 10 loss: 0.002885938037335026 grad: -0.0014189995831883594
iteration: 0 loss: 5339.819732079852 grad: 2069.547398726534
iteration: 10 loss: 0.0022458776220446452 grad: 0.7158076736826358
iteration: 0 loss: 3405.4914432948412 grad: 1973.9793867983808
iteration: 0 loss: 5808.962685347839 grad: 2276.810836524961
iteration: 10 loss: 0.0029867160126080057 grad: 0.05702652378924038
iteration: 0 loss: 5022.287628577612 grad: 2156.1832766303323
iteration: 10 loss: 0.002471813002061522 grad: 0.008971049621790728
iteration: 0 loss: 3227.627331464685 grad: 1793.4740189518548
iteration: 0 loss: 2507.576546407127 grad: 1733.9632827740993
iteration: 0 loss: 4082.0922945639627 grad: 2138.4581445941712
iteration: 0 loss: 2056.6523873424835 grad: 1662.320516420269
iteration: 0 loss: 2883.2613016135806 grad: 1684.0793042783823
iteration: 0 loss: 2494.986120550986 grad: 1721.7930686996297
iteration: 0 loss: 5963.08860784135 grad: 2195.5370886559526
iteration: 10 loss: 0.0029846003599232063 grad: 0.7214502378960701
iteration: 0 loss: 3622.774201414618 grad: 1880.0404614646984
iteration: 10 loss: 0.002679252248278565 grad: 0.03598694087467429
iteration: 0 loss: 6230.82027025013 grad: 2187.5181826096596
iteration: 10 loss: 0.38572325111387973 grad: 1.0683248846775395
iteration: 0 loss: 5880.26312745776 grad: 2189.8461650763716
iteration: 10 loss: 0.0029658133545043793 grad: 1.0763011739468553
iteration: 0 loss: 7029.163926564806 grad: 2346.3174183844
iteration: 10 loss: 0.07621038970219987 grad: 4.1013426121051975
iteration: 0 loss: 3150.686421250712 grad: 1858.322751203805
iteration: 0 loss: 3232.2777416600325 grad: 1884.608907323311
iteration: 0 loss: 4178.8934358138995 grad: 2117.2776218374884
iteration: 0 loss: 4270.764694185473 grad: 2048.935615512494
iteration: 0 loss: 8054.20935787128 grad: 2371.3721364413286
iteration: 10 loss: 0.16653747343066658 grad: 2.2000841107070053
iteration: 0 loss: 3536.4773043480523 grad: 2007.0625494422607
iteration: 0 loss: 6210.0777266149435 grad: 2271.3298334804304
iteration: 10 loss: 0.014564449552298438 grad: 0.3126071223566971
iteration: 0 loss: 4059.825053632094 grad: 1992.880494230514
iteration: 0 loss: 2704.938771338196 grad: 1878.7801486294613
iteration: 0 loss: 5833.788198737521 grad: 2229.2268737491036
iteration: 10 loss: 0.0027527535064357585 grad: -0.1513335806860936
iteration: 0 loss: 2917.6357892112665 grad: 1869.5265527633526
iteration: 10 loss: 0.00258263346635927 grad: 0.003693645624422129
iteration: 0 loss: 7027.8030965238895 grad: 2266.5155391738253
iteration: 10 loss: 0.5900573420803994 grad: 2.864785144441801
iteration: 0 loss: 2556.766289265751 grad: 1775.0485485410993
iteration: 0 loss: 5521.542205856262 grad: 2157.845428619432
iteration: 10 loss: 0.007719585993485948 grad: 0.46304142837710044
iteration: 0 loss: 2827.215834531581 grad: 1762.5469852406516
iteration: 0 loss: 4244.026086469404 grad: 2147.258159970021
iteration: 10 loss: 0.002811895061644133 grad: 0.011287859042873527
iteration: 0 loss: 3069.6922158151256 grad: 1913.4962228444783
iteration: 0 loss: 4132.3992782517325 grad: 1992.762555715246
iteration: 0 loss: 4021.3713900805715 grad: 1989.3589609045143
iteration: 0 loss: 2269.819395831335 grad: 1697.663392663096
iteration: 0 loss: 5066.815235611476 grad: 2060.4920009020284
iteration: 10 loss: 0.010278042614862154 grad: 0.37492253672887715
iteration: 0 loss: 4884.456591502088 grad: 2083.403754112267
iteration: 10 loss: 0.002795166095231914 grad: 0.030107439127420574
iteration: 0 loss: inf grad: 3303.678194499048
iteration: 0 loss: inf grad: 3100.094182479124
iteration: 10 loss: 0.00030383323478004473 grad: -0.00553547856975972
iteration: 0 loss: 12045.013337657276 grad: 3134.4353604627486
iteration: 0 loss: inf grad: 3439.4159945225942
iteration: 0 loss: 9282.254948633756 grad: 2989.5371153382634
iteration: 0 loss: 6321.625277590964 grad: 2662.940582603071
iteration: 10 loss: 0.0003478831839112734 grad: -0.6402319667534108
iteration: 0 loss: inf grad: 3086.115927044563
iteration: 0 loss: 5447.58692545197 grad: 2522.852409503043
iteration: 0 loss: inf grad: 3232.160376948812
iteration: 0 loss: 9755.148566225478 grad: 3003.216008471952
iteration: 0 loss: 6219.4847701633635 grad: 2751.193134991937
iteration: 0 loss: 12450.547154700704 grad: 3234.3474515159787
iteration: 10 loss: 0.00032451060410080987 grad: -0.0062110967270982325
iteration: 0 loss: 8418.5570170279 grad: 2741.832310261791
iteration: 0 loss: 9166.47762860637 grad: 3054.721455819634
iteration: 0 loss: inf grad: 3104.8575556429914
iteration: 0 loss: inf grad: 3516.6360269380907
iteration: 0 loss: inf grad: 3017.091314667611
iteration: 0 loss: 7289.2293515851115 grad: 2811.8770252947124
iteration: 0 loss: inf grad: 3520.9074240648615
iteration: 0 loss: 8625.875125000914 grad: 2851.147443974221
iteration: 0 loss: inf grad: 3227.6303082380837
iteration: 0 loss: inf grad: 3359.190985595908
iteration: 0 loss: 7755.455956838935 grad: 2707.120768688119
iteration: 0 loss: 9491.897170861248 grad: 2982.9198684245202
iteration: 0 loss: 9679.602517880196 grad: 3126.5439686313853
iteration: 0 loss: inf grad: 3016.6163834743456
iteration: 0 loss: inf grad: 3538.802568904832
iteration: 0 loss: inf grad: 3428.6432525641185
iteration: 0 loss: 10669.333893832138 grad: 3079.950110519912
iteration: 0 loss: 8031.089552952795 grad: 2842.129576331261
iteration: 0 loss: inf grad: 3737.5784138734352
iteration: 0 loss: 10565.019789204343 grad: 3257.64114272014
iteration: 0 loss: 10187.804894131397 grad: 3073.0169540569623
iteration: 0 loss: inf grad: 3192.274568556675
iteration: 0 loss: inf grad: 3294.563516221312
iteration: 0 loss: inf grad: 3200.876840676613
iteration: 0 loss: inf grad: 3068.7034393648005
iteration: 0 loss: inf grad: 3542.853717023151
iteration: 0 loss: 8246.477802068424 grad: 2920.6407632870196
iteration: 0 loss: 9193.93097884975 grad: 2978.1280796061947
iteration: 0 loss: inf grad: 3122.21235713599
iteration: 0 loss: 5982.396012521124 grad: 2712.299022435638
iteration: 0 loss: inf grad: 3855.945724684341
iteration: 0 loss: inf grad: 2872.006875787666
iteration: 0 loss: 13317.683911257878 grad: 3447.9056815759664
iteration: 0 loss: inf grad: 3190.669488829419
iteration: 0 loss: inf grad: 2803.001494790442
iteration: 0 loss: 8598.470756640858 grad: 3031.11860815581
iteration: 0 loss: inf grad: 3345.9272530759936
iteration: 0 loss: inf grad: 3360.4963318006858
iteration: 0 loss: inf grad: 3471.45933553015
iteration: 0 loss: 9785.576006815158 grad: 3119.9151124130394
iteration: 0 loss: inf grad: 3556.553099169135
iteration: 0 loss: inf grad: 3184.3516393853197
iteration: 0 loss: 8217.548682958155 grad: 2878.2150259613627
iteration: 0 loss: inf grad: 3101.9090130418194
iteration: 10 loss: 0.0003477100214116614 grad: -0.4460719490276918
iteration: 0 loss: 8295.379190138 grad: 3010.6229960399714
iteration: 0 loss: 11852.61708867133 grad: 3178.409696440086
iteration: 10 loss: 0.00041119923381219536 grad: 0.6901540930869389
iteration: 0 loss: inf grad: 3610.458714402907
iteration: 0 loss: 9636.535364813528 grad: 3060.1686112097877
iteration: 0 loss: inf grad: 3382.7228176293506
iteration: 0 loss: 15474.41614548178 grad: 3562.8786160444465
iteration: 10 loss: 0.0004360493092628365 grad: 0.016142688152899393
iteration: 0 loss: 8075.114673859882 grad: 2648.8425595766857
iteration: 0 loss: 8596.371228227594 grad: 2911.9269254317
iteration: 0 loss: 10084.735998556991 grad: 3148.5944218576487
iteration: 0 loss: 10457.324999772647 grad: 3080.733074616811
iteration: 0 loss: 7799.6598767952 grad: 2772.628243662136
iteration: 0 loss: inf grad: 3394.8605042653253
iteration: 0 loss: inf grad: 3106.8889226471647
iteration: 10 loss: 0.00029657099036161196 grad: 0.0016323794030626815
iteration: 0 loss: 11551.946872495302 grad: 3305.269351639973
iteration: 0 loss: inf grad: 3368.4376287365435
iteration: 0 loss: inf grad: 3751.0713703620577
iteration: 0 loss: 8724.021288649952 grad: 2897.496095848963
iteration: 0 loss: inf grad: 3347.667564507747
iteration: 0 loss: 9163.177371775284 grad: 3011.830915732772
iteration: 10 loss: 0.0003149960777425969 grad: 0.009777369305939969
iteration: 0 loss: 7347.033296171154 grad: 2796.947498278775
iteration: 0 loss: inf grad: 2823.777143722136
iteration: 0 loss: 7638.503601940524 grad: 2804.36525814798
iteration: 0 loss: 11123.216018232812 grad: 3258.7771420425197
iteration: 0 loss: inf grad: 3303.347408297108
iteration: 0 loss: inf grad: 3581.5340323233436
iteration: 0 loss: inf grad: 3418.0395561943733
iteration: 0 loss: inf grad: 3289.9004346603383
iteration: 0 loss: inf grad: 3423.026561226038
iteration: 0 loss: inf grad: 3430.4514951036563
iteration: 0 loss: inf grad: 2977.1023771867513
iteration: 10 loss: 0.0007172450317408551 grad: 0.18930280034113495
iteration: 0 loss: inf grad: 3519.3339868363737
iteration: 0 loss: inf grad: 3147.9941127853667
iteration: 0 loss: 7283.705321600582 grad: 2699.9952208196337
iteration: 0 loss: 6331.20438088506 grad: 2566.2513395097926
iteration: 0 loss: 8677.605704153895 grad: 2812.5919870653497
iteration: 0 loss: 11558.786681249703 grad: 3230.179740137194
iteration: 0 loss: inf grad: 3032.1309589404455
iteration: 0 loss: inf grad: 2936.7021644528786
iteration: 0 loss: 7874.601583883083 grad: 2845.8344956616465
iteration: 0 loss: inf grad: 3568.8523841479982
iteration: 0 loss: inf grad: 3237.7005399694335
iteration: 0 loss: inf grad: 3124.562520674148
iteration: 10 loss: 0.0003331016262315891 grad: 0.15946928198305924
iteration: 0 loss: 10759.884095947245 grad: 3099.697000703218
iteration: 0 loss: 7651.213983580285 grad: 2671.07644435975
iteration: 0 loss: inf grad: 3199.8758519773937
iteration: 0 loss: 9487.426256315312 grad: 2961.938192932866
iteration: 0 loss: inf grad: 3115.187598388299
iteration: 10 loss: 0.00030605913806622 grad: 0.004803984565981377
iteration: 0 loss: inf grad: 3834.3781749650625
iteration: 0 loss: 7247.072672880505 grad: 2812.1826431925797
iteration: 0 loss: 7137.985224938778 grad: 2709.1606414734883
iteration: 0 loss: 8389.953857967457 grad: 2847.4282243666844
iteration: 0 loss: inf grad: 3171.7388412129585
iteration: 0 loss: inf grad: 3256.740202374044
iteration: 10 loss: 0.0003227961344898424 grad: -0.395054848797886
iteration: 0 loss: inf grad: 3473.638888783774
iteration: 0 loss: inf grad: 3410.0283777520317
iteration: 0 loss: inf grad: 3279.7760624999332
iteration: 0 loss: 11117.777963456321 grad: 3119.6151158716125
iteration: 0 loss: inf grad: 3516.7355845860657
iteration: 0 loss: inf grad: 3282.964099666503
iteration: 0 loss: 9104.110293190832 grad: 3079.5754949337834
iteration: 10 loss: 0.00033184509216384453 grad: 0.01213464783456821
iteration: 0 loss: inf grad: 3091.559503744861
iteration: 10 loss: 0.0003140064416749572 grad: -0.009226638871112126
iteration: 0 loss: inf grad: 3034.3738371896525
iteration: 10 loss: 0.0003839483542833477 grad: 0.004943868016437099
iteration: 0 loss: inf grad: 3522.874497194115
iteration: 0 loss: 8696.447558277017 grad: 3107.124928492536
iteration: 10 loss: 0.00031943678252652967 grad: 0.0006847099502611191
iteration: 0 loss: 10591.664623178494 grad: 2961.984476831095
iteration: 0 loss: 5979.769120409812 grad: 2646.1506499002408
iteration: 10 loss: 0.0004189792433118617 grad: -0.0029692477678593247
iteration: 0 loss: inf grad: 3374.686830910952
iteration: 10 loss: 0.00030527540895325893 grad: 0.009866659438482247
iteration: 0 loss: inf grad: 3283.721266607829
iteration: 0 loss: inf grad: 3241.3727827667335
iteration: 0 loss: inf grad: 3364.8217397410776
iteration: 0 loss: 9793.003099237134 grad: 3043.76941331165
iteration: 0 loss: inf grad: 3174.743385793533
iteration: 10 loss: 0.0004082839057611471 grad: 0.019251211210583274
iteration: 0 loss: inf grad: 3533.452136261442
iteration: 0 loss: inf grad: 3013.5596126750625
iteration: 10 loss: 0.0004837212830104611 grad: 0.09485248933868838
iteration: 0 loss: inf grad: 3228.56433301409
iteration: 0 loss: 9501.666520663257 grad: 3082.2013978447835
iteration: 0 loss: inf grad: 3550.601236849553
iteration: 0 loss: inf grad: 3366.352694773563
iteration: 0 loss: 8846.93261565919 grad: 2792.933206326171
iteration: 10 loss: 0.0002787596604321152 grad: -0.0014120128614908
iteration: 0 loss: 7052.550052598332 grad: 2711.6655385577506
iteration: 0 loss: 11587.895766763868 grad: 3340.9089334167134
iteration: 0 loss: 5956.116014746579 grad: 2597.425102263339
iteration: 0 loss: 7820.867968506607 grad: 2627.998445989842
iteration: 0 loss: 7046.747375150755 grad: 2695.364131419119
iteration: 0 loss: inf grad: 3428.031960999616
iteration: 0 loss: 10069.974597584413 grad: 2937.220045453443
iteration: 0 loss: inf grad: 3416.9260834011648
iteration: 0 loss: inf grad: 3418.779837761399
iteration: 0 loss: inf grad: 3664.541444463316
iteration: 0 loss: 8929.315333267448 grad: 2901.4730624219137
iteration: 0 loss: 9031.952635048907 grad: 2940.0294542314314
iteration: 0 loss: inf grad: 3302.073683982011
iteration: 0 loss: 12054.779041896776 grad: 3199.2751539177184
iteration: 0 loss: inf grad: 3700.3422128788948
iteration: 0 loss: 10057.862560262465 grad: 3134.896839872279
iteration: 10 loss: 0.0002648613987151872 grad: 0.007097489378317015
iteration: 0 loss: inf grad: 3545.7651784082536
iteration: 0 loss: 11253.039189649708 grad: 3107.57255630053
iteration: 10 loss: 0.0002797154011204839 grad: 0.0029446093252968684
iteration: 0 loss: 7782.396092341046 grad: 2930.6355905170576
iteration: 0 loss: inf grad: 3477.1271698889122
iteration: 0 loss: 8189.681070913484 grad: 2913.6393360329266
iteration: 0 loss: inf grad: 3538.3344475030062
iteration: 0 loss: 7224.9571295225505 grad: 2772.9889819173823
iteration: 0 loss: inf grad: 3369.3274013758023
iteration: 0 loss: 7960.150887957928 grad: 2750.9116281652973
iteration: 0 loss: 11916.975237282384 grad: 3349.043298581245
iteration: 10 loss: 0.003195477948545224 grad: 0.25068865341863006
iteration: 0 loss: inf grad: 2982.8223192156593
iteration: 0 loss: 11811.11728762203 grad: 3112.491414341027
iteration: 10 loss: 0.000376626282972707 grad: -0.0036972761616157024
iteration: 0 loss: inf grad: 3108.4403413059763
iteration: 0 loss: 6481.364870206736 grad: 2651.914074322276
iteration: 0 loss: inf grad: 3215.3124171715554
iteration: 0 loss: inf grad: 3255.133002164068
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:57: RuntimeWarning: invalid value encountered in double_scalars
  diff = np.absolute(f_history[-1]-f_history[-2])
iteration: 0 loss: 891.1672516546896 grad: 1094.2621371758219
iteration: 10 loss: 0.08145250662147935 grad: 0.48558458024296525
iteration: 0 loss: 687.2079533807057 grad: 1025.3450355388572
iteration: 10 loss: 0.07561357192885639 grad: 0.30564461078890204
iteration: 0 loss: 737.8996174273793 grad: 1035.817545467513
iteration: 10 loss: 0.07109171006992586 grad: 0.3796550212088026
iteration: 0 loss: 1010.7030211435224 grad: 1139.9844968359273
iteration: 10 loss: 0.06987653899837327 grad: 0.3437114017136895
iteration: 0 loss: 595.6420456249587 grad: 990.4399486738935
iteration: 10 loss: 0.06987661959812447 grad: 0.25621005158552423
iteration: 0 loss: 406.4714717024159 grad: 878.9522040560155
iteration: 10 loss: 0.07848795148228502 grad: 0.10378142010633494
iteration: 0 loss: 638.1846237365571 grad: 1020.6718131971267
iteration: 10 loss: 0.06445777888637333 grad: 0.2285399174107252
iteration: 0 loss: 316.63773476733746 grad: 833.8957621883139
iteration: 10 loss: 0.0809897800362788 grad: 0.17193715449844366
iteration: 0 loss: 1000.2919232985575 grad: 1071.0591663499351
iteration: 10 loss: 0.06904924233954675 grad: 0.3885046196504266
iteration: 0 loss: 619.3125522047175 grad: 994.9044050711562
iteration: 10 loss: 0.06869362897619298 grad: 0.27145838169111514
iteration: 0 loss: 355.4996113523861 grad: 909.0406402336062
iteration: 10 loss: 0.07257849225144045 grad: 0.14441960302699922
iteration: 0 loss: 751.9441185639863 grad: 1070.3915940571862
iteration: 10 loss: 0.07596178067251458 grad: 0.43388186417056573
iteration: 0 loss: 564.4543614688013 grad: 904.0537990037185
iteration: 10 loss: 0.06831625804112594 grad: 0.15331733433281708
iteration: 0 loss: 559.240774038944 grad: 1009.7088652987553
iteration: 10 loss: 0.07261729001220374 grad: 0.2618282947729039
iteration: 0 loss: 775.290446475377 grad: 1026.2194797931058
iteration: 10 loss: 0.0646076291327931 grad: 0.3430751554655982
iteration: 0 loss: 1093.0667026833767 grad: 1165.5191529452236
iteration: 10 loss: 0.08117390869930122 grad: 0.47155170617019143
iteration: 0 loss: 663.4545948883834 grad: 998.4257885774057
iteration: 10 loss: 0.07309269802730838 grad: 0.2166781224725577
iteration: 0 loss: 462.0984199986917 grad: 929.5578089744395
iteration: 10 loss: 0.07244077525253735 grad: 0.1973169963147714
iteration: 0 loss: 1201.2595596711587 grad: 1169.7342435611986
iteration: 10 loss: 0.0863543240911068 grad: 0.4671421985680539
iteration: 0 loss: 539.8146384127886 grad: 941.165467570202
iteration: 10 loss: 0.07727550544173441 grad: 0.14241736213873118
iteration: 0 loss: 778.2953465331044 grad: 1069.1850296397165
iteration: 10 loss: 0.07001808029566416 grad: 0.21472106627992926
iteration: 0 loss: 931.9996626433884 grad: 1112.7945751897696
iteration: 10 loss: 0.08082548046182968 grad: 0.4277974282164992
iteration: 0 loss: 541.1118325242774 grad: 894.9949104192207
iteration: 10 loss: 0.06262824096054166 grad: 0.1921490336502784
iteration: 0 loss: 523.3243425603104 grad: 987.0408214950281
iteration: 10 loss: 0.07972567010851477 grad: 0.42025038710901685
iteration: 0 loss: 604.3052451508204 grad: 1034.470655566783
iteration: 10 loss: 0.07119755073505406 grad: 0.32680603754046045
iteration: 0 loss: 549.7297975668276 grad: 996.6656812059505
iteration: 10 loss: 0.0792969855997812 grad: 0.17705946844984533
iteration: 0 loss: 1300.912479859118 grad: 1175.1444920182741
iteration: 10 loss: 0.08299667171244933 grad: 0.18203703712718466
iteration: 0 loss: 1204.7073168298466 grad: 1134.5779289006841
iteration: 10 loss: 0.13813321302718992 grad: 0.1824884430907668
iteration: 0 loss: 624.9248991720438 grad: 1021.0687341902466
iteration: 10 loss: 0.07285014072245262 grad: 0.2797014610577601
iteration: 0 loss: 510.89825261840485 grad: 938.8584485108875
iteration: 10 loss: 0.07535834691952914 grad: 0.23738802790431404
iteration: 0 loss: 1467.5478757321625 grad: 1241.3223214168684
iteration: 10 loss: 0.09468098620643352 grad: -0.05244556476558421
iteration: 0 loss: 669.6100029061209 grad: 1079.0554390647812
iteration: 10 loss: 0.06835411437664747 grad: 0.23325704096373032
iteration: 0 loss: 638.5489638454369 grad: 1016.8238007051955
iteration: 10 loss: 0.07575209899947698 grad: 0.2361431465430665
iteration: 0 loss: 909.2294502773055 grad: 1056.6942573108363
iteration: 10 loss: 0.07268831122564734 grad: 0.4266571926233948
iteration: 0 loss: 904.2201679071791 grad: 1092.240522665727
iteration: 10 loss: 0.07406133656431022 grad: 0.41766234472528224
iteration: 0 loss: 734.3953135411301 grad: 1060.4808424552198
iteration: 10 loss: 0.08868515347421635 grad: 0.8558135572720491
iteration: 0 loss: 717.681045240302 grad: 1017.0423965867099
iteration: 10 loss: 0.0731135238552683 grad: 0.3364747212510989
iteration: 0 loss: 1093.679386388332 grad: 1176.143188218784
iteration: 10 loss: 0.07233081178260777 grad: 0.5329066398615145
iteration: 0 loss: 536.4557432133173 grad: 966.2013627063113
iteration: 10 loss: 0.07537220568817495 grad: 0.1779851383628027
iteration: 0 loss: 563.600294809679 grad: 984.8674434910057
iteration: 10 loss: 0.07055503908155845 grad: 0.31470620432222457
iteration: 0 loss: 820.1519723462734 grad: 1034.4404041518912
iteration: 10 loss: 0.07450591041957853 grad: 0.4066245939704578
iteration: 0 loss: 347.6200481326239 grad: 893.8196694562714
iteration: 10 loss: 0.07473301674442402 grad: 0.14931238011683276
iteration: 0 loss: 1632.2050670756564 grad: 1281.2608435130205
iteration: 10 loss: 0.08869857467288966 grad: 0.36591553743762334
iteration: 0 loss: 665.3996164567362 grad: 948.701958944381
iteration: 10 loss: 0.07590383023026251 grad: 0.22327211708375094
iteration: 0 loss: 811.2429149294353 grad: 1142.864632252099
iteration: 10 loss: 0.07088863055369901 grad: 0.32420230335141026
iteration: 0 loss: 961.4033108538862 grad: 1056.5786096909715
iteration: 10 loss: 0.0766327557010465 grad: 0.41868807383652507
iteration: 0 loss: 742.8240737166747 grad: 924.1207767439137
iteration: 10 loss: 0.07550502895025023 grad: 0.29309076587771155
iteration: 0 loss: 535.1858952394642 grad: 1001.327010795337
iteration: 10 loss: 0.07174696569497675 grad: 0.19241108953694347
iteration: 0 loss: 973.0707717130069 grad: 1110.9802037993231
iteration: 10 loss: 0.07554208686501211 grad: 0.4075467410193573
iteration: 0 loss: 1044.4471684039747 grad: 1115.0383601205435
iteration: 10 loss: 0.08176894398010601 grad: 0.5045906965619593
iteration: 0 loss: 1175.3935057035553 grad: 1151.976078324029
iteration: 10 loss: 0.09064738210872747 grad: 0.6391033174496163
iteration: 0 loss: 602.7158815422753 grad: 1033.7812157406781
iteration: 10 loss: 0.07639224561419476 grad: 0.4083186880852677
iteration: 0 loss: 1050.4249500851026 grad: 1178.9522003189609
iteration: 10 loss: 0.0785651993137435 grad: 0.31966957290273623
iteration: 0 loss: 807.4456875189899 grad: 1055.6632670376675
iteration: 10 loss: 0.07256490762136475 grad: 0.3413984698939372
iteration: 0 loss: 487.8450025039974 grad: 951.3343842654981
iteration: 10 loss: 0.06462445462784672 grad: 0.11817644321366037
iteration: 0 loss: 673.3290018775239 grad: 1027.6067726596202
iteration: 10 loss: 0.07472922281307216 grad: 0.37896775159873386
iteration: 0 loss: 460.4598128628932 grad: 994.8089813801773
iteration: 10 loss: 0.06918914302266123 grad: 0.23561398643583473
iteration: 0 loss: 762.6586262212818 grad: 1053.324396867403
iteration: 10 loss: 0.07361167258039297 grad: 0.4654249403732501
iteration: 0 loss: 1199.5558763242784 grad: 1199.3122601016432
iteration: 10 loss: 0.07317474469329516 grad: 0.35168585193349755
iteration: 0 loss: 604.5478056345996 grad: 1013.2022760030313
iteration: 10 loss: 0.07140473316989797 grad: 0.1459109543529718
iteration: 0 loss: 992.8216810544341 grad: 1119.874140467147
iteration: 10 loss: 0.07698461587278871 grad: 0.6567907735942702
iteration: 0 loss: 976.762716852071 grad: 1181.8137119203634
iteration: 10 loss: 0.07257179506930035 grad: 0.27556550788462214
iteration: 0 loss: 496.4795809232148 grad: 873.2730736818648
iteration: 10 loss: 0.06997046829350108 grad: 0.23194964658661096
iteration: 0 loss: 482.86936791117495 grad: 961.5099890002275
iteration: 10 loss: 0.06965463864335933 grad: 0.35206411832867845
iteration: 0 loss: 641.9215315924687 grad: 1044.397702891406
iteration: 10 loss: 0.071985722453643 grad: 0.18667944848353366
iteration: 0 loss: 669.5537678795737 grad: 1020.1305656014091
iteration: 10 loss: 0.06879641865667839 grad: 0.2099599691982847
iteration: 0 loss: 526.2005595532628 grad: 915.9133082657543
iteration: 10 loss: 0.06444917683852096 grad: 0.1700056365416956
iteration: 0 loss: 917.2895105609479 grad: 1124.674086573225
iteration: 10 loss: 0.07951363971171138 grad: 0.4855128462430881
iteration: 0 loss: 821.0528171381524 grad: 1028.3825902402677
iteration: 10 loss: 0.07853725341159258 grad: 0.37331157017594313
iteration: 0 loss: 723.436184397186 grad: 1095.3439374014742
iteration: 10 loss: 0.07960818116109956 grad: 0.33932979822565246
iteration: 0 loss: 863.1807060234853 grad: 1117.2836110476908
iteration: 10 loss: 0.07238091472456952 grad: 0.33612404568712845
iteration: 0 loss: 1412.872181827418 grad: 1246.3504009152443
iteration: 10 loss: 0.09728807799051124 grad: 0.5490972206004755
iteration: 0 loss: 576.6404985807491 grad: 956.7503426290261
iteration: 10 loss: 0.0741002084913899 grad: 0.22877707627216476
iteration: 0 loss: 926.5374629830568 grad: 1110.704378359892
iteration: 10 loss: 0.07461824382698978 grad: 0.3410444450140056
iteration: 0 loss: 605.4310004755185 grad: 996.8711793438795
iteration: 10 loss: 0.07760699765904891 grad: 0.1678913765498397
iteration: 0 loss: 470.31874129978956 grad: 924.6549228923705
iteration: 10 loss: 0.06897543156165656 grad: 0.14041889857299822
iteration: 0 loss: 629.5540905201993 grad: 934.5145453321244
iteration: 10 loss: 0.06996813466635517 grad: 0.3798074914645859
iteration: 0 loss: 465.15470519436315 grad: 926.8226966992023
iteration: 10 loss: 0.07456019512937928 grad: 0.21089198057129444
iteration: 0 loss: 690.2297161192583 grad: 1080.8697435657036
iteration: 10 loss: 0.07230592898552211 grad: 0.2525723872670467
iteration: 0 loss: 1057.3151253138358 grad: 1094.3158116828351
iteration: 10 loss: 0.07689486024008313 grad: 0.5413521812442154
iteration: 0 loss: 1139.4276889578725 grad: 1189.0596067533052
iteration: 10 loss: 0.06817304179639905 grad: 0.3981890455483735
iteration: 0 loss: 1083.1997196558614 grad: 1133.8015869892338
iteration: 10 loss: 0.07649258201623028 grad: -0.018607317877458937
iteration: 0 loss: 833.086425621167 grad: 1088.7326960559121
iteration: 10 loss: 0.07544457179425411 grad: 0.37202110606657135
iteration: 0 loss: 1116.9295500484352 grad: 1133.7416116181216
iteration: 10 loss: 0.07339635901768236 grad: 0.3912658356702907
iteration: 0 loss: 1088.2779766372362 grad: 1137.5048936576393
iteration: 10 loss: 0.07734530592808203 grad: 0.475784503211645
iteration: 0 loss: 904.119668278355 grad: 986.6096804936324
iteration: 10 loss: 0.06870145225911983 grad: 0.22215034257881372
iteration: 0 loss: 1400.29293798869 grad: 1167.1371627334229
iteration: 10 loss: 0.07894662331818836 grad: 0.23211720147977988
iteration: 0 loss: 955.2648110157429 grad: 1044.5755451914695
iteration: 10 loss: 0.07901734143524664 grad: 0.3514672718719696
iteration: 0 loss: 433.8544695560135 grad: 891.5953461470152
iteration: 10 loss: 0.07368537829130549 grad: 0.17295350170153367
iteration: 0 loss: 407.7052506711382 grad: 847.095000576807
iteration: 10 loss: 0.07531421070862879 grad: 0.04630477903414133
iteration: 0 loss: 576.8596226987407 grad: 928.218432774909
iteration: 10 loss: 0.06710008344965569 grad: 0.10873997174707385
iteration: 0 loss: 752.0227465967179 grad: 1069.631702961781
iteration: 10 loss: 0.06743069820285283 grad: 0.15621225996784066
iteration: 0 loss: 695.1240842525623 grad: 1003.7958163444098
iteration: 10 loss: 0.0660755258139199 grad: 0.3985552070880926
iteration: 0 loss: 546.9502723522025 grad: 969.1013508767926
iteration: 10 loss: 0.07159079903083859 grad: 0.26733565556586314
iteration: 0 loss: 479.0628360051052 grad: 940.5517553784072
iteration: 10 loss: 0.06674275110146316 grad: 0.10525864882420091
iteration: 0 loss: 1040.4378091893168 grad: 1184.3781005180554
iteration: 10 loss: 0.07096736654917053 grad: 0.3369058590728913
iteration: 0 loss: 862.4117358159896 grad: 1074.002622774178
iteration: 10 loss: 0.07809004448658512 grad: 0.5180083479754262
iteration: 0 loss: 846.2259727307963 grad: 1034.8891478886426
iteration: 10 loss: 0.07343196328226688 grad: 0.2647552908100289
iteration: 0 loss: 682.2434125931678 grad: 1026.8179320276197
iteration: 10 loss: 0.07033940435030672 grad: 0.26176715984334864
iteration: 0 loss: 552.1168806469054 grad: 882.0936964671455
iteration: 10 loss: 0.06715322574382299 grad: 0.0746386424442166
iteration: 0 loss: 832.0853046510281 grad: 1059.4900457260637
iteration: 10 loss: 0.06962982004816612 grad: 0.328240227270712
iteration: 0 loss: 593.1747695420046 grad: 979.3132867119289
iteration: 10 loss: 0.07374092610751343 grad: 0.2051338303707083
iteration: 0 loss: 570.5546625396729 grad: 1031.1240610617056
iteration: 10 loss: 0.08035828167422468 grad: 0.32996449438808956
iteration: 0 loss: 1376.8667509797592 grad: 1273.1989317172774
iteration: 10 loss: 0.08727146754873419 grad: 0.7822684195090328
iteration: 0 loss: 444.4293926059661 grad: 929.6020009846118
iteration: 10 loss: 0.06644754192083466 grad: 0.16120020821045894
iteration: 0 loss: 418.26443318871515 grad: 892.4303648419352
iteration: 10 loss: 0.07352733621056276 grad: 0.1810035689657422
iteration: 0 loss: 520.557684843695 grad: 941.5245335366342
iteration: 10 loss: 0.07049058451511056 grad: 0.19886038694950892
iteration: 0 loss: 637.8109102034784 grad: 1051.4428930461104
iteration: 10 loss: 0.07625776840557758 grad: 0.2919201916657452
iteration: 0 loss: 843.0903835760321 grad: 1077.2916138659098
iteration: 10 loss: 0.06975368456022063 grad: 0.3092360473493398
iteration: 0 loss: 919.2021836051152 grad: 1152.418349714355
iteration: 10 loss: 0.07027644849916793 grad: 0.35388134724674847
iteration: 0 loss: 1101.8592189041008 grad: 1132.233752856319
iteration: 10 loss: 0.09992873462165748 grad: 1.2720549028439467
iteration: 0 loss: 852.4880902681648 grad: 1087.3829447515363
iteration: 10 loss: 0.08526896175846393 grad: 0.34726113638179007
iteration: 0 loss: 701.261811363437 grad: 1032.91804814436
iteration: 10 loss: 0.07271992827405814 grad: 0.3759907196651616
iteration: 0 loss: 1200.953267294471 grad: 1167.3525408455473
iteration: 10 loss: 0.08209299964900393 grad: 0.4921439703072491
iteration: 0 loss: 922.4947112156253 grad: 1087.1687316835191
iteration: 10 loss: 0.07637803174987891 grad: 0.1888014953012298
iteration: 0 loss: 503.7381648192733 grad: 1020.4754329046078
iteration: 10 loss: 0.07380759437662411 grad: 0.33325020961106533
iteration: 0 loss: 690.9128706031818 grad: 1023.3861572389382
iteration: 10 loss: 0.06928146354981767 grad: 0.20584333616490533
iteration: 0 loss: 596.8659263053495 grad: 1004.560066039804
iteration: 10 loss: 0.07966010951614325 grad: 0.3844125553342699
iteration: 0 loss: 1027.177124600944 grad: 1169.3317011445902
iteration: 10 loss: 0.08725639672312271 grad: 0.7872546740653348
iteration: 0 loss: 529.9859433343736 grad: 1027.6463962527914
iteration: 10 loss: 0.07472166960228192 grad: 0.24283824785627028
iteration: 0 loss: 698.1217752237999 grad: 979.6763269293294
iteration: 10 loss: 0.07230045538199192 grad: 0.40495105516042235
iteration: 0 loss: 356.2976136462783 grad: 874.6138329448091
iteration: 10 loss: 0.08210487437794324 grad: 0.18081703046632436
iteration: 0 loss: 841.638204396299 grad: 1119.2907893846736
iteration: 10 loss: 0.07133891370077353 grad: 0.4354274171787845
iteration: 0 loss: 770.4230490549552 grad: 1087.2749032211013
iteration: 10 loss: 0.07773629817397291 grad: 0.21688609114291796
iteration: 0 loss: 772.1918059313757 grad: 1072.8780507231131
iteration: 10 loss: 0.07519663590144111 grad: 0.3666294589208719
iteration: 0 loss: 1206.5031643319837 grad: 1114.18426132713
iteration: 10 loss: 0.073147781023925 grad: 0.25112054742314344
iteration: 0 loss: 594.687717479183 grad: 1007.9139604931946
iteration: 10 loss: 0.07177047068795549 grad: 0.3095891809471457
iteration: 0 loss: 723.177952996472 grad: 1051.6515602316254
iteration: 10 loss: 0.0712688804920948 grad: 0.38155612184022375
iteration: 0 loss: 1220.6374107118522 grad: 1172.6076055708486
iteration: 10 loss: 0.07593108189294898 grad: 0.3138002060537847
iteration: 0 loss: 707.5327980676709 grad: 996.0569097415307
iteration: 10 loss: 0.07786689335096691 grad: 0.2941828577112618
iteration: 0 loss: 972.879895138206 grad: 1068.7881607172576
iteration: 10 loss: 0.07126099439946385 grad: 0.3887328916079236
iteration: 0 loss: 603.4065217465137 grad: 1020.504554150039
iteration: 10 loss: 0.07985391783139759 grad: 0.3489935648078224
iteration: 0 loss: 1069.3723443194738 grad: 1177.897083063745
iteration: 10 loss: 0.07514489904629458 grad: 0.37848739973036494
iteration: 0 loss: 944.3746477571539 grad: 1116.3104929611382
iteration: 10 loss: 0.07150732181881639 grad: 0.2639510625912038
iteration: 0 loss: 604.8915293665273 grad: 922.37140534966
iteration: 10 loss: 0.06759528093756341 grad: 0.1727799239122694
iteration: 0 loss: 447.6348958211575 grad: 893.0055457864116
iteration: 10 loss: 0.07394015054474452 grad: 0.17663714106930106
iteration: 0 loss: 692.8434987232312 grad: 1106.186614189348
iteration: 10 loss: 0.06868707293192115 grad: 0.30188849613980373
iteration: 0 loss: 340.10820163167426 grad: 856.2362986901427
iteration: 10 loss: 0.07517627310120258 grad: 0.21030619225789737
iteration: 0 loss: 560.3428384328973 grad: 866.051492557091
iteration: 10 loss: 0.07159498753704115 grad: 0.25204862893706675
iteration: 0 loss: 456.69760283610213 grad: 888.7030254484955
iteration: 10 loss: 0.07000039782790489 grad: 0.23681324255545608
iteration: 0 loss: 1098.2346903571308 grad: 1136.451934643366
iteration: 10 loss: 0.07334669984232857 grad: 0.3731557144501495
iteration: 0 loss: 677.6688244623056 grad: 970.5204744889817
iteration: 10 loss: 0.07556217601797305 grad: 0.3302866837739779
iteration: 0 loss: 1074.5916289932723 grad: 1130.7161741153545
iteration: 10 loss: 0.07624167435510158 grad: 0.3756455380173591
iteration: 0 loss: 1094.6673404787186 grad: 1133.6639773354332
iteration: 10 loss: 0.0677579857031312 grad: 0.3614001028730619
iteration: 0 loss: 1257.4842524775058 grad: 1216.5839979826897
iteration: 10 loss: 0.08018360364704594 grad: 0.6360086561669839
iteration: 0 loss: 542.5250727503461 grad: 959.7325899341316
iteration: 10 loss: 0.07498426169629303 grad: 0.26866856689969915
iteration: 0 loss: 567.0498717653048 grad: 973.0340772624448
iteration: 10 loss: 0.07194919154922404 grad: 0.26217882334175374
iteration: 0 loss: 726.689223714069 grad: 1095.4342531882369
iteration: 10 loss: 0.07489872890238819 grad: 0.3275241971541659
iteration: 0 loss: 748.3114797333129 grad: 1058.8457546466257
iteration: 10 loss: 0.07195688847952458 grad: 0.35294076778951616
iteration: 0 loss: 1472.1997388282953 grad: 1229.0814979102795
iteration: 10 loss: 0.08049304544990511 grad: 0.36187310723415206
iteration: 0 loss: 596.735069762747 grad: 1037.645835124901
iteration: 10 loss: 0.07310873475084505 grad: 0.3734314981758672
iteration: 0 loss: 1104.8052417480021 grad: 1176.6879160231292
iteration: 10 loss: 0.08604934861366266 grad: 0.5386526193430667
iteration: 0 loss: 732.664396294006 grad: 1029.343260613693
iteration: 10 loss: 0.06891119678932062 grad: 0.25208423411505254
iteration: 0 loss: 439.1434902710891 grad: 970.0656492853834
iteration: 10 loss: 0.06988464327084577 grad: 0.1556488783853856
iteration: 0 loss: 1091.3203599271721 grad: 1154.497150750175
iteration: 10 loss: 0.07333347121576397 grad: 0.3837841940139325
iteration: 0 loss: 517.0566296537173 grad: 965.0919048843248
iteration: 10 loss: 0.08075345195224641 grad: 0.36154140647261146
iteration: 0 loss: 1269.5936291089072 grad: 1174.162649242594
iteration: 10 loss: 0.08231724309271984 grad: 0.7166237985505404
iteration: 0 loss: 460.210756333256 grad: 914.9870889849989
iteration: 10 loss: 0.07750409526786163 grad: 0.21047531439336725
iteration: 0 loss: 1013.1575678847552 grad: 1117.2987951650061
iteration: 10 loss: 0.06763261510323818 grad: 0.2930381791703103
iteration: 0 loss: 501.6225372669258 grad: 907.3388795043957
iteration: 10 loss: 0.07156165871988791 grad: 0.24571351531205105
iteration: 0 loss: 752.9761289620128 grad: 1109.6887625951908
iteration: 10 loss: 0.07479150178253556 grad: 0.2156290850128845
iteration: 0 loss: 531.6308179879113 grad: 987.6366209360438
iteration: 10 loss: 0.06970482814672736 grad: 0.32940281777870745
iteration: 0 loss: 683.2665489705641 grad: 1030.7763608254886
iteration: 10 loss: 0.07167433245525569 grad: 0.40460189802701396
iteration: 0 loss: 755.1598875088749 grad: 1028.4990492449347
iteration: 10 loss: 0.07053862134257956 grad: 0.35652712251351737
iteration: 0 loss: 388.6885537916476 grad: 874.6094954850214
iteration: 10 loss: 0.06880986767084157 grad: 0.1216910473297419
iteration: 0 loss: 946.0771989687414 grad: 1064.4662984259355
iteration: 10 loss: 0.06767195355529441 grad: 0.3054823879168631
iteration: 0 loss: 885.7648469995215 grad: 1078.0903483851578
iteration: 10 loss: 0.08043320969359642 grad: 0.38153087726052565
iteration: 0 loss: 4414.733149818788 grad: 2029.4940867890264
iteration: 10 loss: 0.003922756140871736 grad: 0.028505995383913036
iteration: 0 loss: 3530.397020297333 grad: 1903.6434159106316
iteration: 0 loss: 3902.7445589772296 grad: 1925.706985573955
iteration: 0 loss: 4952.024041239452 grad: 2113.7510116302838
iteration: 10 loss: 0.0032304489180784335 grad: 0.034671672436623904
iteration: 0 loss: 3021.848577857774 grad: 1838.1779499228808
iteration: 0 loss: 2045.3021583899344 grad: 1635.8527558502503
iteration: 10 loss: 0.0033493255508470943 grad: -0.004628162029584388
iteration: 0 loss: 3249.6858648517054 grad: 1896.0575304035867
iteration: 0 loss: 1697.5622848391724 grad: 1550.1882858480562
iteration: 0 loss: 4840.50644838245 grad: 1985.7850155042954
iteration: 0 loss: 3155.174618180441 grad: 1842.4464658013624
iteration: 0 loss: 1962.6578010040105 grad: 1688.7932550430287
iteration: 0 loss: 3992.4026079595324 grad: 1985.516866137942
iteration: 10 loss: 0.003477943611373617 grad: 0.008853367283764669
iteration: 0 loss: 2762.7820134143553 grad: 1680.3375109615777
iteration: 0 loss: 2936.39247626562 grad: 1875.2311453983498
iteration: 0 loss: 3881.7605580376885 grad: 1904.7850201409756
iteration: 0 loss: 5639.407629301111 grad: 2159.449850935979
iteration: 10 loss: 0.004685641741327179 grad: 0.29608149494379454
iteration: 0 loss: 3308.745505509512 grad: 1850.3356534822385
iteration: 0 loss: 2356.9614138200754 grad: 1728.2206629028456
iteration: 0 loss: 5750.439591777612 grad: 2165.3511628121832
iteration: 10 loss: 0.006393360456885685 grad: -0.016241027798881723
iteration: 0 loss: 2773.703516698622 grad: 1749.7439386718356
iteration: 0 loss: 4021.2473837615644 grad: 1982.1911745452303
iteration: 0 loss: 4851.232641183389 grad: 2061.7425977021053
iteration: 10 loss: 0.0123281296427277 grad: -0.48122024174271144
iteration: 0 loss: 2497.810997345502 grad: 1661.7033370804772
iteration: 0 loss: 2969.2873742614956 grad: 1830.912372297896
iteration: 0 loss: 3107.456652261994 grad: 1920.384776433189
iteration: 0 loss: 2668.329572195848 grad: 1851.6948207433102
iteration: 10 loss: 0.004031054548580538 grad: 0.03412462060449052
iteration: 0 loss: 6202.006286478139 grad: 2175.343723213391
iteration: 10 loss: 0.01381920228198975 grad: 1.5425524149497907
iteration: 0 loss: 5985.206325113774 grad: 2106.567326340666
iteration: 10 loss: 9.56471279304771 grad: 21.04464540551511
iteration: 0 loss: 3409.1888642317235 grad: 1895.3152472151532
iteration: 0 loss: 2560.230009760969 grad: 1743.7495640356792
iteration: 0 loss: 7085.412237556628 grad: 2300.3491733602714
iteration: 10 loss: 0.12306692058336921 grad: 1.9220199374173133
iteration: 0 loss: 3434.681516322901 grad: 2001.8616676509482
iteration: 0 loss: 3265.0667224012327 grad: 1887.0367149893334
iteration: 10 loss: 0.0035069839749749835 grad: 0.01591138469650276
iteration: 0 loss: 4645.762216043568 grad: 1962.3790519720978
iteration: 10 loss: 0.00360456661358264 grad: 0.34683154846255876
iteration: 0 loss: 4499.157730482338 grad: 2022.4003996093847
iteration: 0 loss: 3773.4356272977416 grad: 1966.6799009117647
iteration: 10 loss: 0.033393063726676206 grad: 0.9798206771822516
iteration: 0 loss: 3593.575426173776 grad: 1884.106799049519
iteration: 0 loss: 5678.504490301701 grad: 2178.738040975574
iteration: 10 loss: 0.008263349698443728 grad: 0.6388139530874938
iteration: 0 loss: 2720.1814159843634 grad: 1794.3679969330542
iteration: 0 loss: 2950.0649090388442 grad: 1828.9969999650757
iteration: 0 loss: 3818.1267937168336 grad: 1918.529606350311
iteration: 0 loss: 1888.7454624651252 grad: 1664.5983789285362
iteration: 0 loss: 8039.322773616362 grad: 2372.273800127096
iteration: 10 loss: 0.7303898161732253 grad: 0.11126684457288327
iteration: 0 loss: 3145.9859631599425 grad: 1762.341928175291
iteration: 0 loss: 4294.371996731074 grad: 2120.2100041404065
iteration: 0 loss: 4486.295191054263 grad: 1959.7309985971724
iteration: 0 loss: 3397.2187108998787 grad: 1720.3957602026367
iteration: 0 loss: 2765.8213969337817 grad: 1860.3330271243028
iteration: 0 loss: 4876.894783891299 grad: 2055.7612101766385
iteration: 10 loss: 0.0036830695740222423 grad: 0.16848685454027162
iteration: 0 loss: 5068.675344403124 grad: 2066.207443245148
iteration: 10 loss: 0.07781445235278542 grad: 0.3948261398150828
iteration: 0 loss: 5924.2969589235245 grad: 2133.085625601001
iteration: 10 loss: 0.3843497289820913 grad: 2.4488953002963023
iteration: 0 loss: 3150.675786519181 grad: 1920.579080359534
iteration: 10 loss: 0.0033893502689104275 grad: 0.061701271821450895
iteration: 0 loss: 5392.175147970065 grad: 2188.4050233363955
iteration: 10 loss: 0.004781387727813457 grad: 0.9953919918732337
iteration: 0 loss: 3948.6566000835196 grad: 1956.5194584482258
iteration: 0 loss: 2612.268613517046 grad: 1768.3627412074163
iteration: 0 loss: 3501.692121165266 grad: 1905.4019826426143
iteration: 0 loss: 2592.2754352206325 grad: 1847.6941620943257
iteration: 0 loss: 3878.7127856627617 grad: 1953.9515514923467
iteration: 10 loss: 0.002882872825218576 grad: -0.020809192714478897
iteration: 0 loss: 5806.137302538632 grad: 2219.4283582787602
iteration: 10 loss: 0.004001013894925232 grad: 0.01444222494071826
iteration: 0 loss: 3102.266221641298 grad: 1880.032168970415
iteration: 0 loss: 5073.896806411934 grad: 2076.7746861337905
iteration: 10 loss: 0.024999063002972187 grad: 0.41701811587291615
iteration: 0 loss: 5034.039098270297 grad: 2189.814169477184
iteration: 10 loss: 0.0037646371929440647 grad: -0.0068852057544944376
iteration: 0 loss: 2598.5765621080127 grad: 1625.32264223298
iteration: 0 loss: 2708.524103022845 grad: 1785.2508462297146
iteration: 0 loss: 3297.2691326268246 grad: 1937.0087834687004
iteration: 0 loss: 3400.9872539390444 grad: 1894.2114995450502
iteration: 0 loss: 2572.9230466335353 grad: 1701.441568178345
iteration: 0 loss: 4645.720973413853 grad: 2085.0322422050945
iteration: 10 loss: 0.003657400056412867 grad: 0.5115957796375106
iteration: 0 loss: 3785.8295705362684 grad: 1908.8246157124188
iteration: 10 loss: 0.003230951562396843 grad: 0.013495107681166057
iteration: 0 loss: 3743.136973205428 grad: 2030.7588907895024
iteration: 0 loss: 4506.1117142140465 grad: 2070.2353966476157
iteration: 10 loss: 0.0038381468740143728 grad: -0.019465217680705447
iteration: 0 loss: 6892.64810518532 grad: 2305.8292853826047
iteration: 10 loss: 0.1038110325673849 grad: -0.3243109867464531
iteration: 0 loss: 2860.9335421542582 grad: 1779.1079928473232
iteration: 0 loss: 4749.924559442033 grad: 2058.708454411323
iteration: 0 loss: 2980.793595998513 grad: 1851.2435994429713
iteration: 0 loss: 2364.2469341665906 grad: 1716.7829909732689
iteration: 0 loss: 3104.019117863965 grad: 1734.617599581083
iteration: 0 loss: 2438.482030595326 grad: 1722.9481728441847
iteration: 0 loss: 3589.038938499757 grad: 2003.6905429287253
iteration: 0 loss: 5082.54637503665 grad: 2031.0502654055458
iteration: 10 loss: 0.004508217573112977 grad: 0.9435693606928696
iteration: 0 loss: 5733.997833476362 grad: 2201.538448229915
iteration: 10 loss: 0.0033189703760647467 grad: -0.09212934376189214
iteration: 0 loss: 5312.042413426702 grad: 2099.448315360526
iteration: 10 loss: 0.00698076308419166 grad: 0.3405372546932346
iteration: 0 loss: 4283.299679124728 grad: 2020.7289624348423
iteration: 0 loss: 5465.762106176529 grad: 2102.6410020266594
iteration: 10 loss: 0.0034171735182595016 grad: 0.00937670567515409
iteration: 0 loss: 5418.905359238139 grad: 2108.4122001455225
iteration: 10 loss: 0.04580163905418224 grad: 1.22474211402917
iteration: 0 loss: 4128.402982010449 grad: 1829.2410494914925
iteration: 10 loss: 0.0029711336160703054 grad: 0.015499466184758934
iteration: 0 loss: 6479.259037601995 grad: 2161.2609120959914
iteration: 10 loss: 0.0044292215461609885 grad: 0.4634692259053121
iteration: 0 loss: 4401.3642371408505 grad: 1938.14076539177
iteration: 0 loss: 2325.9717466577963 grad: 1658.5008629963575
iteration: 0 loss: 2068.526636239274 grad: 1576.358408269044
iteration: 0 loss: 2850.199994350161 grad: 1724.6338072181616
iteration: 0 loss: 3779.367240258302 grad: 1982.1323294500355
iteration: 0 loss: 3594.798208537249 grad: 1862.2387595089883
iteration: 0 loss: 2920.5570152415735 grad: 1802.888532615657
iteration: 0 loss: 2521.44045837303 grad: 1748.4624546028233
iteration: 0 loss: 5194.424363321999 grad: 2194.5399771816437
iteration: 10 loss: 0.009641560531401245 grad: -0.2925407815430275
iteration: 0 loss: 4246.97678458926 grad: 1990.3393836937148
iteration: 10 loss: 0.004287535625652791 grad: 0.5218205116121765
iteration: 0 loss: 3923.5084953448754 grad: 1920.4951670500518
iteration: 10 loss: 0.003240840987101282 grad: 0.031831345577191306
iteration: 0 loss: 3473.4490644746256 grad: 1903.3805800926286
iteration: 0 loss: 2538.936429765019 grad: 1641.3577213460085
iteration: 0 loss: 4139.64525457097 grad: 1966.266029046882
iteration: 0 loss: 3071.1562419792926 grad: 1818.3326485910961
iteration: 0 loss: 3106.8965894814332 grad: 1913.2035344208102
iteration: 0 loss: 6984.982591872228 grad: 2358.1048687688176
iteration: 10 loss: 0.035185623073960996 grad: -1.0939304702015022
iteration: 0 loss: 2334.4566078190696 grad: 1726.2269004596346
iteration: 0 loss: 2238.587577465973 grad: 1659.5563964761277
iteration: 0 loss: 2697.806924097931 grad: 1750.049167816578
iteration: 0 loss: 3417.568473699656 grad: 1949.727252865074
iteration: 0 loss: 4189.871583904379 grad: 1998.4316536295382
iteration: 0 loss: 4751.301586633202 grad: 2136.197090787325
iteration: 10 loss: 0.003763722045484676 grad: 0.003974229521278638
iteration: 0 loss: 5853.946563263279 grad: 2097.3170215637783
iteration: 10 loss: 0.043452815124923785 grad: -0.19811322864646408
iteration: 0 loss: 4578.384634951208 grad: 2016.2278145146356
iteration: 10 loss: 0.004762446704923853 grad: 0.5708337118003323
iteration: 0 loss: 3579.402218621699 grad: 1914.944990579799
iteration: 0 loss: 5988.489475128183 grad: 2161.5748126826256
iteration: 10 loss: 0.0032442638700806788 grad: 1.325359841076473
iteration: 0 loss: 4533.714076308271 grad: 2016.931035408512
iteration: 0 loss: 2846.7038596084863 grad: 1891.5508258481796
iteration: 0 loss: 3373.7145608081933 grad: 1898.8888527752201
iteration: 0 loss: 3033.761181682542 grad: 1864.426142964094
iteration: 0 loss: 5353.3968231797835 grad: 2166.7869698121604
iteration: 10 loss: 0.0060483594372106545 grad: 0.8333837457737772
iteration: 0 loss: 2779.1786133047467 grad: 1906.929609987666
iteration: 0 loss: 3432.87928225542 grad: 1817.6028004604366
iteration: 0 loss: 1907.1901091267353 grad: 1623.3369935594408
iteration: 0 loss: 4076.6821992300047 grad: 2074.5495469703633
iteration: 0 loss: 3895.6928746441113 grad: 2018.48797753476
iteration: 10 loss: 0.0035241110480538655 grad: 0.04636880346408864
iteration: 0 loss: 3895.325717897805 grad: 1991.4671105547452
iteration: 0 loss: 5574.851491838377 grad: 2067.5872534411506
iteration: 10 loss: 0.14113617166797418 grad: 0.8184528647284253
iteration: 0 loss: 3161.3077196684103 grad: 1870.3032994980938
iteration: 0 loss: 3664.569715094644 grad: 1951.2027772894407
iteration: 0 loss: 5728.736015541686 grad: 2171.8642817549157
iteration: 10 loss: 0.0035626187865008515 grad: -0.028487566175096
iteration: 0 loss: 3541.413638911463 grad: 1852.655411824184
iteration: 10 loss: 0.0034598714601915244 grad: 0.019725660001951747
iteration: 0 loss: 4842.347830812112 grad: 1983.8027433891875
iteration: 0 loss: 3089.11966763919 grad: 1891.6061322595774
iteration: 0 loss: 5300.245854840766 grad: 2184.779538652494
iteration: 10 loss: 0.0035145367331675848 grad: 0.02642736156908799
iteration: 0 loss: 4546.079529091146 grad: 2067.313612900498
iteration: 0 loss: 2909.470726239972 grad: 1713.2078025851092
iteration: 0 loss: 2284.0282055195826 grad: 1662.8869817214677
iteration: 0 loss: 3700.6461599804284 grad: 2048.382007762314
iteration: 0 loss: 1854.5511883868248 grad: 1594.4365952737694
iteration: 0 loss: 2631.8354632820233 grad: 1609.6049002621257
iteration: 0 loss: 2236.493603075783 grad: 1649.3933907382648
iteration: 0 loss: 5427.346446002461 grad: 2106.592659914039
iteration: 10 loss: 0.0034380805062193594 grad: 0.013334532242020154
iteration: 0 loss: 3295.4565870562296 grad: 1798.857483364818
iteration: 0 loss: 5682.3695427798075 grad: 2099.4974172922884
iteration: 10 loss: 0.3410822067327734 grad: 1.8157776806403432
iteration: 0 loss: 5335.264165368979 grad: 2098.502122430481
iteration: 10 loss: 0.0036630835767771878 grad: -0.04826218050580865
iteration: 0 loss: 6399.420484666253 grad: 2252.258951382908
iteration: 10 loss: 0.0157549726678884 grad: -0.5957527457166486
iteration: 0 loss: 2861.660611234515 grad: 1782.6940395894455
iteration: 0 loss: 2929.7681424127777 grad: 1808.218351167838
iteration: 0 loss: 3786.466922076617 grad: 2031.5107211149314
iteration: 0 loss: 3883.751973861552 grad: 1964.3220382824427
iteration: 0 loss: 7351.865879129154 grad: 2278.3615430390846
iteration: 10 loss: 0.25850836717687675 grad: 0.0635011236189541
iteration: 0 loss: 3220.427583816473 grad: 1922.5662735617868
iteration: 0 loss: 5645.359494719952 grad: 2178.6977142831606
iteration: 10 loss: 0.003949229473205792 grad: -0.7306905939286116
iteration: 0 loss: 3689.7560098265767 grad: 1911.1812693457587
iteration: 0 loss: 2448.5272388004746 grad: 1800.3141951668688
iteration: 0 loss: 5298.317759798194 grad: 2138.5527909782613
iteration: 10 loss: 0.0034664155894213104 grad: -0.1631510842470822
iteration: 0 loss: 2642.0079676491337 grad: 1790.488274348709
iteration: 0 loss: 6404.01857549465 grad: 2175.896110573034
iteration: 10 loss: 0.5649862285168968 grad: 3.3555583494805123
iteration: 0 loss: 2326.360188863949 grad: 1700.8484770566745
iteration: 0 loss: 5027.592017471975 grad: 2071.3995130834155
iteration: 10 loss: 0.0031109958892391824 grad: 0.06776500990679146
iteration: 0 loss: 2574.1162293147604 grad: 1689.35655461342
iteration: 0 loss: 3843.494077403686 grad: 2058.2740620011245
iteration: 10 loss: 0.0038321756457497195 grad: 0.03407456326794439
iteration: 0 loss: 2778.8257253388465 grad: 1834.3498980537074
iteration: 0 loss: 3742.272968400152 grad: 1912.1323041437827
iteration: 10 loss: 0.0033305305299687793 grad: 0.003993252655666503
iteration: 0 loss: 3674.473400027142 grad: 1908.962333956557
iteration: 0 loss: 2044.9760034479839 grad: 1627.3030816352953
iteration: 0 loss: 4597.651490724115 grad: 1974.0019424254165
iteration: 10 loss: 0.0034904217199337754 grad: 0.35089909476998954
iteration: 0 loss: 4465.224324013378 grad: 2001.6005487646803
iteration: 10 loss: 0.0035233605581610886 grad: 0.02613647464205343
iteration: 0 loss: inf grad: 3217.7163810586635
iteration: 0 loss: 10309.204664880976 grad: 3021.065315196093
iteration: 0 loss: 11423.696754523758 grad: 3054.409437214004
iteration: 10 loss: 0.0003224277064542879 grad: -0.004747716020788355
iteration: 0 loss: inf grad: 3350.263408939503
iteration: 0 loss: 8817.541218915116 grad: 2919.1109052106576
iteration: 0 loss: 5954.581916797953 grad: 2600.223760759861
iteration: 0 loss: inf grad: 3009.6557504562475
iteration: 0 loss: 5072.926577348023 grad: 2458.7629512039266
iteration: 0 loss: inf grad: 3148.8475678604364
iteration: 0 loss: 9218.04994335747 grad: 2928.610440552809
iteration: 0 loss: 5823.0667545056385 grad: 2685.767693777228
iteration: 0 loss: 11772.907511299343 grad: 3150.9709062562474
iteration: 10 loss: 0.09769379442017949 grad: 2.1906791331849536
iteration: 0 loss: 8025.914985513233 grad: 2667.625145808851
iteration: 0 loss: 8665.528496453388 grad: 2977.5403213999343
iteration: 0 loss: inf grad: 3025.348177422251
iteration: 0 loss: 16446.61135769924 grad: 3424.91625267774
iteration: 0 loss: 9761.691913411205 grad: 2938.0819281806894
iteration: 10 loss: 0.0003937825862191279 grad: 0.01293510725402284
iteration: 0 loss: 6882.53689693205 grad: 2742.551173808092
iteration: 0 loss: inf grad: 3430.8259506545037
iteration: 0 loss: 8114.739097815246 grad: 2777.9504473768393
iteration: 0 loss: 11723.208177632752 grad: 3149.6380548687066
iteration: 0 loss: inf grad: 3272.329781243488
iteration: 0 loss: 7256.9962880199755 grad: 2640.3068439510207
iteration: 0 loss: 8953.672071813384 grad: 2904.5100944476208
iteration: 0 loss: 9146.49560813472 grad: 3048.781161213271
iteration: 10 loss: 0.0003348128071097149 grad: 0.007124973335281383
iteration: 0 loss: 7785.0359516323815 grad: 2939.516494277072
iteration: 0 loss: inf grad: 3450.2550564843777
iteration: 0 loss: inf grad: 3339.312395680397
iteration: 0 loss: 10084.256633296473 grad: 3006.40924963988
iteration: 0 loss: 7524.122285265913 grad: 2767.858385717148
iteration: 0 loss: inf grad: 3644.585491752334
iteration: 0 loss: 9992.218211716592 grad: 3172.283514278681
iteration: 0 loss: 9580.043212827159 grad: 2994.0582222549388
iteration: 10 loss: 0.0004104556441730396 grad: 0.010558730402896468
iteration: 0 loss: inf grad: 3112.7050309243386
iteration: 0 loss: inf grad: 3206.8211935972167
iteration: 0 loss: inf grad: 3120.593821228449
iteration: 0 loss: inf grad: 2992.4516064687878
iteration: 10 loss: 0.00046822857587936926 grad: 0.28610372893284824
iteration: 0 loss: inf grad: 3454.226329192952
iteration: 0 loss: 7828.470797147815 grad: 2849.728232355381
iteration: 0 loss: 8680.859734936157 grad: 2902.2815472518323
iteration: 0 loss: inf grad: 3043.00991376937
iteration: 0 loss: 5628.373279384779 grad: 2637.5878599726016
iteration: 0 loss: inf grad: 3759.4291657154145
iteration: 0 loss: inf grad: 2796.978172258073
iteration: 10 loss: 0.00040372896844267166 grad: -0.5614704049677156
iteration: 0 loss: 12565.63113051199 grad: 3361.4013890553047
iteration: 10 loss: 0.00034059633352709085 grad: 0.009207351218864478
iteration: 0 loss: inf grad: 3110.551678978268
iteration: 0 loss: 9705.712825603347 grad: 2731.8501942089024
iteration: 0 loss: 8139.722510055193 grad: 2950.683807243384
iteration: 0 loss: inf grad: 3261.6587937217455
iteration: 0 loss: inf grad: 3277.1354735833493
iteration: 0 loss: inf grad: 3381.4851562531694
iteration: 0 loss: 9250.815643922855 grad: 3042.1614308118305
iteration: 0 loss: inf grad: 3469.5953684821657
iteration: 0 loss: inf grad: 3103.243241756541
iteration: 0 loss: 7776.8585417218355 grad: 2803.3516474592734
iteration: 0 loss: 10349.978311552415 grad: 3023.5723385385263
iteration: 10 loss: 0.00043504926989870995 grad: 0.009077285111517317
iteration: 0 loss: 7803.799552201914 grad: 2932.4086647125714
iteration: 0 loss: 11215.904731309238 grad: 3099.4590481210016
iteration: 10 loss: 0.000404556969773363 grad: 0.003731159415919197
iteration: 0 loss: inf grad: 3520.687659374529
iteration: 0 loss: 9081.467911087897 grad: 2980.337621150345
iteration: 0 loss: inf grad: 3295.303305108697
iteration: 0 loss: 14619.352487425473 grad: 3473.4774257276904
iteration: 10 loss: 0.0003162740242921493 grad: 0.2479943454596396
iteration: 0 loss: 7597.176539153868 grad: 2582.0828975869335
iteration: 0 loss: 8162.260325524213 grad: 2833.7449049114075
iteration: 0 loss: 9573.459024324759 grad: 3067.152017093006
iteration: 0 loss: 9932.376625983017 grad: 3002.5978343012857
iteration: 0 loss: 7364.497487378787 grad: 2703.349218280079
iteration: 0 loss: inf grad: 3310.3446220367696
iteration: 0 loss: inf grad: 3028.6497319136142
iteration: 10 loss: 0.06070039277123711 grad: -0.17821828350531987
iteration: 0 loss: 10889.405873796288 grad: 3218.465248848971
iteration: 10 loss: 0.00039301227661781013 grad: -0.004856492246281981
iteration: 0 loss: 13162.805621313997 grad: 3284.748998491097
iteration: 0 loss: inf grad: 3652.8578477491574
iteration: 0 loss: 8266.907576867856 grad: 2822.3013686256572
iteration: 0 loss: inf grad: 3263.0729940416977
iteration: 0 loss: 8635.696960120817 grad: 2939.762122188283
iteration: 10 loss: 0.0004837298659946431 grad: 0.017362964937435655
iteration: 0 loss: 6952.740319864843 grad: 2725.6319634458605
iteration: 0 loss: inf grad: 2753.873465588551
iteration: 0 loss: 7258.613767396761 grad: 2732.720957331697
iteration: 0 loss: 10529.482987289086 grad: 3175.955580713623
iteration: 0 loss: inf grad: 3220.664783219443
iteration: 0 loss: inf grad: 3490.6896410921536
iteration: 0 loss: inf grad: 3332.9181970217987
iteration: 0 loss: inf grad: 3202.4475688458474
iteration: 0 loss: inf grad: 3333.873207897734
iteration: 0 loss: inf grad: 3342.8192757631095
iteration: 0 loss: 11671.620863150474 grad: 2908.104800860139
iteration: 10 loss: 0.00030586884134787727 grad: 0.013465396455111956
iteration: 0 loss: inf grad: 3428.2380820935045
iteration: 0 loss: inf grad: 3074.2303933354306
iteration: 0 loss: 6931.732192103193 grad: 2634.8844781898615
iteration: 0 loss: 5995.584814018654 grad: 2500.9985178366273
iteration: 0 loss: 8302.200306025103 grad: 2740.215980831481
iteration: 0 loss: 10922.991078409506 grad: 3147.1933217481437
iteration: 0 loss: inf grad: 2957.5607709476867
iteration: 0 loss: 8585.557762468226 grad: 2861.939617658135
iteration: 0 loss: 7375.420748232579 grad: 2774.715667110804
iteration: 0 loss: 15024.439399709652 grad: 3476.8008701608774
iteration: 0 loss: 12116.4222539326 grad: 3155.3518004348393
iteration: 10 loss: 0.00043294272406704045 grad: 0.5153569864163464
iteration: 0 loss: 11125.984953745758 grad: 3048.4966976631313
iteration: 10 loss: 0.00039556597634641963 grad: 0.015622019635304648
iteration: 0 loss: 10174.772088236596 grad: 3023.751592758924
iteration: 10 loss: 0.00043847393614917314 grad: 0.001382692988113287
iteration: 0 loss: 7266.241101318913 grad: 2605.341199458707
iteration: 0 loss: inf grad: 3120.4256406732447
iteration: 0 loss: 8938.217217370873 grad: 2884.3814043091093
iteration: 0 loss: inf grad: 3036.253628047706
iteration: 10 loss: 0.00034433562805960804 grad: 0.008266110205117826
iteration: 0 loss: inf grad: 3741.70344700347
iteration: 0 loss: 6899.97450872766 grad: 2741.947332977638
iteration: 0 loss: 6732.440364004965 grad: 2635.8360980461675
iteration: 0 loss: 8006.015057938102 grad: 2776.5514448405506
iteration: 0 loss: 10046.459635600688 grad: 3093.797663919111
iteration: 10 loss: 0.0003013938390226527 grad: 0.0010415794355093583
iteration: 0 loss: inf grad: 3174.513723755645
iteration: 10 loss: 0.0004717837675178254 grad: 3.3858418407358565e-05
iteration: 0 loss: 13832.997235882749 grad: 3388.498910786797
iteration: 10 loss: 0.00037616922583600336 grad: 1.6048687692565946
iteration: 0 loss: inf grad: 3324.3836290150903
iteration: 0 loss: inf grad: 3200.7330997637155
iteration: 0 loss: 10483.445416098653 grad: 3037.7515641122327
iteration: 0 loss: inf grad: 3427.629399283514
iteration: 0 loss: inf grad: 3200.1940785342085
iteration: 0 loss: 8597.168406451734 grad: 3004.7235974009177
iteration: 10 loss: 0.0004538680818355219 grad: -0.011675963818908373
iteration: 0 loss: 9765.457826667895 grad: 3012.1852500700097
iteration: 0 loss: 8928.706319622586 grad: 2956.802647392841
iteration: 0 loss: inf grad: 3432.0685772880024
iteration: 0 loss: 8225.307863331267 grad: 3026.157093881328
iteration: 0 loss: 9961.747918878735 grad: 2883.052859245665
iteration: 0 loss: 5682.4877370504755 grad: 2579.965784449866
iteration: 10 loss: 0.00032496685691347176 grad: 0.2156665281988308
iteration: 0 loss: inf grad: 3290.1712342215114
iteration: 10 loss: 0.00031948405773040247 grad: 0.022942269641239
iteration: 0 loss: inf grad: 3200.671383805515
iteration: 0 loss: inf grad: 3156.3080948990837
iteration: 0 loss: inf grad: 3277.306052503086
iteration: 0 loss: 9280.64527858095 grad: 2967.1987262894318
iteration: 0 loss: inf grad: 3094.667724554864
iteration: 0 loss: inf grad: 3443.0067039743394
iteration: 0 loss: inf grad: 2936.7149466084084
iteration: 10 loss: 0.00046087291874838144 grad: -0.012657333444665074
iteration: 0 loss: inf grad: 3148.288713865545
iteration: 0 loss: 9048.71070928068 grad: 3001.537413583676
iteration: 0 loss: inf grad: 3462.2734938459525
iteration: 0 loss: inf grad: 3280.5110132804143
iteration: 0 loss: 8415.055471229261 grad: 2725.080939385847
iteration: 0 loss: 6673.782259645013 grad: 2641.2914329435325
iteration: 0 loss: 10918.829241320762 grad: 3251.38845585698
iteration: 0 loss: 5627.234647042791 grad: 2531.5844747322217
iteration: 0 loss: 7417.3199282381365 grad: 2559.816337218088
iteration: 0 loss: 6545.8730735623185 grad: 2622.042036878559
iteration: 0 loss: inf grad: 3338.644429085662
iteration: 0 loss: 9470.218059882674 grad: 2859.691904117536
iteration: 10 loss: 0.000353002351899208 grad: 0.00791439734665184
iteration: 0 loss: inf grad: 3327.420311278757
iteration: 0 loss: inf grad: 3330.856460992036
iteration: 0 loss: inf grad: 3569.0938583679167
iteration: 0 loss: 8467.926907995921 grad: 2831.7371479786048
iteration: 0 loss: 8550.142494400054 grad: 2867.4793454757755
iteration: 0 loss: 11152.098979475326 grad: 3222.1421221160113
iteration: 10 loss: 0.00027338365114039993 grad: 0.13473994842768752
iteration: 0 loss: 11299.444433981367 grad: 3118.4165752549197
iteration: 10 loss: 0.00045548253481022334 grad: -0.0038527905280167658
iteration: 0 loss: inf grad: 3608.7936483065027
iteration: 0 loss: 9514.83985791261 grad: 3055.37260594738
iteration: 0 loss: inf grad: 3453.923847635477
iteration: 0 loss: 10684.119017175382 grad: 3034.2969422507877
iteration: 10 loss: 0.00024033234116028655 grad: 0.0014614853463452962
iteration: 0 loss: 7330.334418237929 grad: 2859.146657894252
iteration: 0 loss: inf grad: 3390.2073560134304
iteration: 0 loss: 7754.018508452815 grad: 2844.584599028341
iteration: 0 loss: inf grad: 3450.8057047257557
iteration: 0 loss: 6824.757932195414 grad: 2702.427316793631
iteration: 0 loss: inf grad: 3285.2199864882614
iteration: 0 loss: 7486.0589630948125 grad: 2684.4317730086905
iteration: 0 loss: 11226.406826438633 grad: 3265.5741137722516
iteration: 10 loss: 0.00047755486130799085 grad: -0.04501610104349262
iteration: 0 loss: 8245.843651533914 grad: 2912.663131528068
iteration: 0 loss: 11101.507620516177 grad: 3030.9030453518276
iteration: 10 loss: 0.00040563352839936584 grad: 0.01256393929480153
iteration: 0 loss: 10533.184187970048 grad: 3027.3922557355463
iteration: 0 loss: 6090.859753455138 grad: 2584.748273028301
iteration: 0 loss: inf grad: 3132.283587358487
iteration: 0 loss: inf grad: 3171.6680368450134
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:57: RuntimeWarning: invalid value encountered in double_scalars
  diff = np.absolute(f_history[-1]-f_history[-2])
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 80.40507294495366 grad: 0.0
iteration: 0 loss: 3208.557258831385 grad: 1773.0651192408109
iteration: 10 loss: 0.006952846475707536 grad: 0.057680450381492485
iteration: 0 loss: 2534.8408596393497 grad: 1664.1677168834876
iteration: 0 loss: 2784.3643436725743 grad: 1683.6052483954174
iteration: 0 loss: 3563.6039048880775 grad: 1846.6499537936334
iteration: 10 loss: 0.006771599895728286 grad: 0.026125013639657472
iteration: 0 loss: 2166.5667027506893 grad: 1606.4756262393946
iteration: 0 loss: 1476.9577790453732 grad: 1429.952005141996
iteration: 0 loss: 2333.0503727091195 grad: 1655.9398950337288
iteration: 0 loss: 1230.7174042569466 grad: 1352.436667909157
iteration: 0 loss: 3506.518701663635 grad: 1735.3454223514066
iteration: 0 loss: 2272.098462884131 grad: 1614.9101146805424
iteration: 0 loss: 1376.0483602155896 grad: 1474.9294448076782
iteration: 0 loss: 2855.823969705134 grad: 1737.1291798815196
iteration: 10 loss: 0.008685130228431344 grad: -0.1472417215565714
iteration: 0 loss: 1997.2376610248016 grad: 1468.8120175512738
iteration: 0 loss: 2114.035097883307 grad: 1639.3422985265315
iteration: 0 loss: 2805.810544188384 grad: 1666.3543586191067
iteration: 0 loss: 4050.986385424693 grad: 1888.8086266920955
iteration: 10 loss: 0.008523192090003497 grad: 0.05475136435326759
iteration: 0 loss: 2407.847413875126 grad: 1620.8102640093343
iteration: 0 loss: 1697.556042803074 grad: 1509.1499664920216
iteration: 0 loss: 4161.408954533533 grad: 1893.9923940059607
iteration: 10 loss: 0.029491864386231595 grad: 0.1272833795559183
iteration: 0 loss: 1994.956730075158 grad: 1530.7917908578856
iteration: 0 loss: 2888.8112210015374 grad: 1733.6721358645766
iteration: 0 loss: 3489.4208250082665 grad: 1802.691230992294
iteration: 10 loss: 0.0077576996343189175 grad: 0.841119277382271
iteration: 0 loss: 1855.174083075295 grad: 1454.0679260782342
iteration: 0 loss: 2116.304976359975 grad: 1602.1046811873393
iteration: 0 loss: 2232.2446498900085 grad: 1679.407207193768
iteration: 0 loss: 1945.633310955647 grad: 1618.7423033259834
iteration: 10 loss: 0.007718330988692204 grad: 0.03583449405757151
iteration: 0 loss: 4485.659899258661 grad: 1900.085702094192
iteration: 10 loss: 0.011337794254359324 grad: -0.29974790498356807
iteration: 0 loss: 4318.274238127775 grad: 1841.4969009205988
iteration: 10 loss: 1.7567812024241325 grad: 16.244359172875704
iteration: 0 loss: 2425.4362259203085 grad: 1656.9057036103732
iteration: 0 loss: 1838.6693797444489 grad: 1524.323968181977
iteration: 0 loss: 5143.499191042648 grad: 2009.754970482439
iteration: 10 loss: 0.07858996605169853 grad: 1.3466215224029705
iteration: 0 loss: 2463.458202078427 grad: 1750.2175323896804
iteration: 0 loss: 2363.6944512118685 grad: 1650.6135403091216
iteration: 0 loss: 3323.1829688510525 grad: 1714.9374248024678
iteration: 10 loss: 0.007951872540367978 grad: -0.01282050781649155
iteration: 0 loss: 3245.34311579559 grad: 1770.5366456089564
iteration: 10 loss: 0.007466109910720578 grad: 0.047857264150172926
iteration: 0 loss: 2700.873055185449 grad: 1720.9741504170204
iteration: 10 loss: 0.026640640825710514 grad: 0.7615785024910557
iteration: 0 loss: 2595.579656865879 grad: 1646.6368474997364
iteration: 10 loss: 0.00730948655241677 grad: 0.07695560898594897
iteration: 0 loss: 4086.018965864209 grad: 1905.4968492631665
iteration: 10 loss: 0.008034813089497302 grad: 0.6983019224886479
iteration: 0 loss: 1922.0156447673842 grad: 1569.0001890125754
iteration: 0 loss: 2108.3493336590695 grad: 1599.4914774372146
iteration: 0 loss: 2762.7843762337666 grad: 1676.2742316183696
iteration: 0 loss: 1338.5819487662927 grad: 1454.5104657274412
iteration: 0 loss: 5813.060187836538 grad: 2073.623284564582
iteration: 10 loss: 0.3485352548463693 grad: -0.022372981037883988
iteration: 0 loss: 2307.9918519248063 grad: 1541.7102058872745
iteration: 0 loss: 3065.0990442343214 grad: 1851.5059021887864
iteration: 0 loss: 3261.1462669144453 grad: 1711.438324246625
iteration: 0 loss: 2513.583181630084 grad: 1504.0195500536797
iteration: 0 loss: 1998.2757555798141 grad: 1626.7246003793946
iteration: 0 loss: 3536.8150838720826 grad: 1797.1541460049714
iteration: 10 loss: 0.006904510455735197 grad: 0.02933173491818776
iteration: 0 loss: 3687.4949852458444 grad: 1806.9502341229877
iteration: 10 loss: 0.00875416212776062 grad: 0.5668257480311552
iteration: 0 loss: 4258.8582606841455 grad: 1865.7157959547135
iteration: 10 loss: 0.2493848579688347 grad: 1.8170972860763586
iteration: 0 loss: 2256.908870655103 grad: 1674.749668228249
iteration: 0 loss: 3867.9470090460277 grad: 1912.5177892395473
iteration: 10 loss: 0.006902493131003046 grad: 0.3884196644567886
iteration: 0 loss: 2852.902457865927 grad: 1711.9348397259698
iteration: 0 loss: 1865.4520425887727 grad: 1546.8385984386873
iteration: 0 loss: 2507.5847739425753 grad: 1666.4227975140584
iteration: 0 loss: 1841.1248577893725 grad: 1615.5135084084523
iteration: 0 loss: 2795.0416249524624 grad: 1709.001489158006
iteration: 10 loss: 0.006040052053735549 grad: 0.034683163598167865
iteration: 0 loss: 4217.5427250451075 grad: 1942.052720081462
iteration: 10 loss: 0.006794776392449752 grad: 0.012205249486492264
iteration: 0 loss: 2231.3209939410726 grad: 1644.07196207612
iteration: 0 loss: 3637.2985621481016 grad: 1815.4322982738536
iteration: 10 loss: 0.03473723826292288 grad: 0.5862233163551849
iteration: 0 loss: 3609.957607212569 grad: 1913.9142272153604
iteration: 0 loss: 1867.9302319003707 grad: 1422.2138596780183
iteration: 0 loss: 1931.7561007223828 grad: 1561.593503116812
iteration: 0 loss: 2375.568951800276 grad: 1692.3152068221884
iteration: 0 loss: 2465.6881810177297 grad: 1655.372014759546
iteration: 0 loss: 1847.5925487753893 grad: 1489.4513881956877
iteration: 0 loss: 3339.3692914900266 grad: 1822.365134777283
iteration: 10 loss: 0.007741364317718597 grad: 0.07873072397954137
iteration: 0 loss: 2766.3747735508837 grad: 1666.7979958746687
iteration: 10 loss: 0.007447803200524173 grad: 0.02434416600194023
iteration: 0 loss: 2692.348055619258 grad: 1776.9009669019156
iteration: 0 loss: 3220.421671930264 grad: 1810.3285605439705
iteration: 10 loss: 0.006403937588567549 grad: -0.15554522119844857
iteration: 0 loss: 4967.805568995439 grad: 2016.4372077335297
iteration: 10 loss: 0.030654527749654582 grad: 0.7723901348195941
iteration: 0 loss: 2062.499755588667 grad: 1554.0517542336852
iteration: 0 loss: 3408.1034682307322 grad: 1800.2079743221675
iteration: 10 loss: 0.006363603528536094 grad: 0.008515793552894124
iteration: 0 loss: 2166.3955515272723 grad: 1616.4313593596862
iteration: 0 loss: 1705.9196808762665 grad: 1500.7136444942848
iteration: 0 loss: 2205.36621094205 grad: 1515.7915733534473
iteration: 0 loss: 1753.7260429579162 grad: 1506.8932915796074
iteration: 0 loss: 2573.5032729273994 grad: 1752.1369864638812
iteration: 0 loss: 3698.797584845695 grad: 1775.0182830638266
iteration: 10 loss: 0.006682613237485798 grad: 0.15462291390239147
iteration: 0 loss: 4137.662424282434 grad: 1926.3700458066392
iteration: 10 loss: 0.006055894269428047 grad: 0.3070434154188925
iteration: 0 loss: 3833.1748714665596 grad: 1838.1749325955425
iteration: 10 loss: 0.009851746878253338 grad: 0.9272587321073925
iteration: 0 loss: 3075.409825415327 grad: 1766.8079502457354
iteration: 0 loss: 3952.0128823027453 grad: 1838.103156174502
iteration: 10 loss: 0.008123590374783485 grad: -0.06119816125996305
iteration: 0 loss: 3885.136019320411 grad: 1842.6238641110926
iteration: 10 loss: 0.02251990031072637 grad: 0.6641471385100026
iteration: 0 loss: 3000.8907225976395 grad: 1600.2335202518316
iteration: 0 loss: 4744.588992734586 grad: 1892.6244339580223
iteration: 10 loss: 0.009645253404001282 grad: 0.8484215705253793
iteration: 0 loss: 3242.82051525563 grad: 1694.4694134971705
iteration: 10 loss: 0.006782585617408834 grad: -0.01535102169832919
iteration: 0 loss: 1652.6594310269422 grad: 1447.5791201291468
iteration: 0 loss: 1473.4455522768294 grad: 1375.8576277099346
iteration: 0 loss: 2057.566562460079 grad: 1508.2191022383986
iteration: 0 loss: 2730.130783346003 grad: 1736.8570401103184
iteration: 0 loss: 2581.1039088288967 grad: 1627.3470521991853
iteration: 0 loss: 2086.332764254061 grad: 1576.6847566059582
iteration: 0 loss: 1804.3752218447082 grad: 1528.2921895012832
iteration: 0 loss: 3754.807144581432 grad: 1920.367608497477
iteration: 10 loss: 0.006921025244413282 grad: -0.2062693623185415
iteration: 0 loss: 3060.4964768193518 grad: 1740.66499285625
iteration: 10 loss: 0.013614117091823242 grad: -0.6951802452777125
iteration: 0 loss: 2848.2807023240043 grad: 1677.9204754875275
iteration: 10 loss: 0.006712554577626923 grad: 0.029179524167867756
iteration: 0 loss: 2493.237236265877 grad: 1664.600744503803
iteration: 0 loss: 1867.574722939192 grad: 1434.471128270426
iteration: 0 loss: 3003.550977331359 grad: 1722.0058750485875
iteration: 0 loss: 2188.217726138907 grad: 1588.8949124164474
iteration: 0 loss: 2218.585349951737 grad: 1672.3826724239661
iteration: 0 loss: 5029.982134414073 grad: 2062.630064087883
iteration: 10 loss: 0.051350508923695255 grad: -0.05927142213901263
iteration: 0 loss: 1650.4714596273773 grad: 1509.6830099748684
iteration: 0 loss: 1607.9409691217268 grad: 1452.1938543137317
iteration: 0 loss: 1946.4341954555491 grad: 1530.2310421839045
iteration: 0 loss: 2439.965290525201 grad: 1703.836899858822
iteration: 0 loss: 3017.4016037406955 grad: 1748.4026882780247
iteration: 10 loss: 0.006584411291855345 grad: 0.05938817012645844
iteration: 0 loss: 3402.802211959386 grad: 1868.0652872318317
iteration: 10 loss: 0.006418364086130168 grad: 0.0582382596870088
iteration: 0 loss: 4204.0619411916205 grad: 1834.597631052642
iteration: 10 loss: 0.038365877529775556 grad: 0.9348073002061559
iteration: 0 loss: 3269.5091906773187 grad: 1762.543107567346
iteration: 10 loss: 0.022143518917304886 grad: 0.35194973041799926
iteration: 0 loss: 2590.2444336164936 grad: 1676.5714406055731
iteration: 0 loss: 4326.162498018406 grad: 1890.7109793898003
iteration: 10 loss: 0.01744137138633629 grad: -0.5723888240937501
iteration: 0 loss: 3287.1234593712334 grad: 1764.1631893816716
iteration: 0 loss: 2023.972907954132 grad: 1654.5960039451347
iteration: 0 loss: 2443.7360975256966 grad: 1660.9893536451846
iteration: 0 loss: 2182.7435216770245 grad: 1630.201585149829
iteration: 0 loss: 3833.4281705623334 grad: 1893.2686741608568
iteration: 10 loss: 0.04016559374246175 grad: 1.5174482179561788
iteration: 0 loss: 2000.1682657393017 grad: 1668.693911206681
iteration: 0 loss: 2484.2146678898675 grad: 1589.627042640202
iteration: 0 loss: 1366.9919310761136 grad: 1419.3939799262034
iteration: 0 loss: 2955.4567023507784 grad: 1814.6622706361495
iteration: 0 loss: 2815.1184678886452 grad: 1762.9708852729598
iteration: 0 loss: 2805.233355062493 grad: 1740.3853653586048
iteration: 10 loss: 0.007130308616483076 grad: 0.06736738305516504
iteration: 0 loss: 4078.299337855646 grad: 1807.5110931649278
iteration: 10 loss: 0.007322470487534619 grad: 1.2367625963609823
iteration: 0 loss: 2247.456089264258 grad: 1635.2792672285966
iteration: 0 loss: 2637.6074399841605 grad: 1705.454049832446
iteration: 0 loss: 4169.595874690335 grad: 1899.7170787012715
iteration: 10 loss: 0.006717452670272905 grad: -0.23570376987434932
iteration: 0 loss: 2562.5389342353506 grad: 1620.689010077223
iteration: 0 loss: 3509.8586087673957 grad: 1734.4466926044158
iteration: 0 loss: 2204.893018133346 grad: 1653.9618343310274
iteration: 0 loss: 3819.4974709901503 grad: 1909.3062493433754
iteration: 0 loss: 3290.1749926676202 grad: 1807.1600289937396
iteration: 10 loss: 0.006711128564678471 grad: 0.035530701770938644
iteration: 0 loss: 2121.828892365455 grad: 1498.8052720635596
iteration: 0 loss: 1633.7959498740129 grad: 1452.7339246482275
iteration: 0 loss: 2647.8025283462575 grad: 1792.5339361414353
iteration: 0 loss: 1316.1386865898123 grad: 1392.2870146309924
iteration: 0 loss: 1950.2852275854507 grad: 1408.3329208896414
iteration: 0 loss: 1626.1325213762104 grad: 1444.865865324607
iteration: 0 loss: 3943.617703971519 grad: 1843.412432346363
iteration: 10 loss: 0.006742650535174603 grad: 0.036926260440213944
iteration: 0 loss: 2398.4392157705274 grad: 1576.3310320501105
iteration: 0 loss: 4057.8536047889384 grad: 1834.7534667639804
iteration: 10 loss: 0.27540718552137894 grad: 0.5230209129920804
iteration: 0 loss: 3865.025461967758 grad: 1836.2966965206303
iteration: 10 loss: 0.007329828258662019 grad: -0.0002987996794390074
iteration: 0 loss: 4608.581476758783 grad: 1971.0050361387052
iteration: 10 loss: 0.010672608707285359 grad: 1.625509067910229
iteration: 0 loss: 2048.913844332416 grad: 1559.0834421974862
iteration: 0 loss: 2103.2050244697766 grad: 1578.9574823665998
iteration: 0 loss: 2716.837277476801 grad: 1775.7262640241102
iteration: 0 loss: 2786.4875916668466 grad: 1717.912915524004
iteration: 0 loss: 5291.298453034257 grad: 1990.0611608367449
iteration: 10 loss: 0.1312158419856463 grad: 3.194536221918637
iteration: 0 loss: 2294.6009176934917 grad: 1682.4511977618877
iteration: 0 loss: 4056.3096507818523 grad: 1906.3873880322762
iteration: 10 loss: 0.007665035709744023 grad: -0.6060329702369962
iteration: 0 loss: 2645.9918428723377 grad: 1670.7082669362558
iteration: 0 loss: 1735.6504870774713 grad: 1573.4498013336538
iteration: 0 loss: 3835.8925939243654 grad: 1868.850732178074
iteration: 10 loss: 0.007179804152904332 grad: -0.13832357316926935
iteration: 0 loss: 1900.6043635414242 grad: 1564.6546206247908
iteration: 0 loss: 4609.58913026684 grad: 1901.3869057011032
iteration: 10 loss: 0.4536035906203324 grad: 2.376718849766831
iteration: 0 loss: 1660.8314322958686 grad: 1486.1653077218239
iteration: 0 loss: 3622.8909321287774 grad: 1810.655243743316
iteration: 0 loss: 1844.5424374587894 grad: 1477.322843914085
iteration: 0 loss: 2774.1006150443445 grad: 1799.82173516301
iteration: 10 loss: 0.007165471168594773 grad: 0.05974686904646472
iteration: 0 loss: 1988.7135106561125 grad: 1605.1485610801296
iteration: 0 loss: 2670.8870398093222 grad: 1671.7966031789156
iteration: 10 loss: 0.006912627034497828 grad: 0.19443476227403494
iteration: 0 loss: 2643.6198359192817 grad: 1670.000948544371
iteration: 0 loss: 1473.1647618556694 grad: 1422.316229072715
iteration: 0 loss: 3345.9424302156494 grad: 1726.6274355602366
iteration: 10 loss: 0.006514905356611548 grad: 0.025786829357663316
iteration: 0 loss: 3206.590625852792 grad: 1746.6225398914116
iteration: 10 loss: 0.006300666665827686 grad: -0.012944730834144946
iteration: 0 loss: 10599.634833170581 grad: 2960.0354261643506
iteration: 10 loss: 0.003254488975719803 grad: 0.6367064052767611
iteration: 0 loss: 8584.436302960647 grad: 2782.434596623206
iteration: 0 loss: 9440.704715820144 grad: 2813.504498346897
iteration: 10 loss: 0.0004733666719403118 grad: -0.006112333916990547
iteration: 0 loss: 11780.121893415484 grad: 3086.984100670379
iteration: 10 loss: 0.0005893266167153012 grad: 0.013971060280107823
iteration: 0 loss: 7309.01428835023 grad: 2683.8279962749507
iteration: 0 loss: 4962.155801920692 grad: 2394.0913107578067
iteration: 0 loss: 7828.99003567918 grad: 2767.0288273066435
iteration: 0 loss: 4226.4520914243 grad: 2267.166757268048
iteration: 0 loss: 11710.706802713556 grad: 2901.417518635237
iteration: 10 loss: 0.0005413542438129133 grad: 0.013035386095456201
iteration: 0 loss: 7647.115930098625 grad: 2697.502892972425
iteration: 0 loss: 4813.795524023307 grad: 2469.953135994868
iteration: 0 loss: 9776.660105334142 grad: 2902.6085501512466
iteration: 10 loss: 0.0005027367945083163 grad: -0.39705705917076345
iteration: 0 loss: 6628.946388431079 grad: 2460.6692163860253
iteration: 0 loss: 7192.388816239167 grad: 2744.118829699784
iteration: 0 loss: 9393.393136065042 grad: 2783.7744307634953
iteration: 10 loss: 0.0005397648174866018 grad: -0.00058658340061494
iteration: 0 loss: 13675.177537208172 grad: 3155.4139796456843
iteration: 0 loss: 8164.0623353515175 grad: 2709.2855995492446
iteration: 10 loss: 0.0006173386328472671 grad: 0.0027059618523370627
iteration: 0 loss: 5695.135036257953 grad: 2523.7774754984566
iteration: 0 loss: 13671.012253414527 grad: 3159.194575692228
iteration: 10 loss: 0.0012320006075738506 grad: 1.501824996895783
iteration: 0 loss: 6757.930527110158 grad: 2559.639425699784
iteration: 0 loss: 9683.331697948537 grad: 2897.3386984224226
iteration: 0 loss: 11785.07568218081 grad: 3011.313433671178
iteration: 10 loss: 0.0007209728831763972 grad: -0.5189279827316127
iteration: 0 loss: 6045.71131363558 grad: 2433.167567781914
iteration: 0 loss: 7398.018060726064 grad: 2677.3036413685377
iteration: 0 loss: 7550.413942989507 grad: 2803.876436849745
iteration: 0 loss: 6496.366856439389 grad: 2706.738160106246
iteration: 0 loss: inf grad: 3173.992555718747
iteration: 0 loss: inf grad: 3075.0680082346435
iteration: 0 loss: 8274.682168436564 grad: 2762.574995466335
iteration: 10 loss: 0.0005599182966927236 grad: 0.06149389523298317
iteration: 0 loss: 6262.276132697876 grad: 2548.888551677517
iteration: 0 loss: inf grad: 3356.3667158745657
iteration: 0 loss: 8287.046609697905 grad: 2922.655902046038
iteration: 0 loss: 8031.329919011073 grad: 2759.528873187619
iteration: 0 loss: 11128.036818888068 grad: 2864.7870563420183
iteration: 10 loss: 0.0007161398375914855 grad: -0.2603271670654222
iteration: 0 loss: inf grad: 2956.3196482794483
iteration: 0 loss: 9227.771364577691 grad: 2873.1168719002817
iteration: 10 loss: 0.019762027175271545 grad: 1.0638309594798263
iteration: 0 loss: 8627.727936331832 grad: 2751.597165838045
iteration: 10 loss: 0.01528184113770046 grad: -0.33447199306941083
iteration: 0 loss: 13766.089083399944 grad: 3180.7812819164774
iteration: 10 loss: 0.03626511193049902 grad: 1.866741349073801
iteration: 0 loss: 6489.350415476291 grad: 2623.57719192284
iteration: 0 loss: 7188.237362084875 grad: 2670.8085724168054
iteration: 0 loss: 9045.993231034749 grad: 2798.114739353588
iteration: 10 loss: 0.0006482293538283557 grad: 0.011193052589957354
iteration: 0 loss: 4671.200676904268 grad: 2429.9032526043184
iteration: 0 loss: 19086.70201586986 grad: 3460.71472926539
iteration: 0 loss: 7594.153896818298 grad: 2577.5306854636337
iteration: 0 loss: 10424.942289855942 grad: 3094.698492475467
iteration: 10 loss: 0.0006955471762921661 grad: -0.02740527483338983
iteration: 0 loss: 10651.960583311198 grad: 2859.4804891731155
iteration: 0 loss: 8204.494526708302 grad: 2518.510138516408
iteration: 0 loss: 6734.738687621208 grad: 2717.4100266811943
iteration: 0 loss: 11779.768689164463 grad: 3002.0430815096443
iteration: 10 loss: 0.000578933382216333 grad: -0.04037522233264208
iteration: 0 loss: 12252.79687070927 grad: 3018.1451265989326
iteration: 10 loss: 0.015572364161595364 grad: 0.7269022365091009
iteration: 0 loss: inf grad: 3114.325328593184
iteration: 0 loss: 7717.951599893463 grad: 2800.300740025792
iteration: 0 loss: 12961.680072987538 grad: 3194.358963814476
iteration: 10 loss: 0.0006111879801292988 grad: 0.3109098764402333
iteration: 0 loss: 9457.177138248366 grad: 2860.0084436405978
iteration: 0 loss: 6409.391413964297 grad: 2584.2596955224153
iteration: 0 loss: 8510.83265980059 grad: 2778.7955186805293
iteration: 10 loss: 0.0005723322074945 grad: 0.9768355083614796
iteration: 0 loss: 6454.778922085137 grad: 2700.3858391710273
iteration: 0 loss: 9292.263357511663 grad: 2851.2009108124876
iteration: 0 loss: inf grad: 3240.255152236686
iteration: 0 loss: 7540.381320667776 grad: 2746.5561495157726
iteration: 0 loss: 12195.51776394084 grad: 3033.565251848069
iteration: 10 loss: 0.011812425107398833 grad: 0.3206741762551303
iteration: 0 loss: 12141.35894687099 grad: 3198.0332333322494
iteration: 10 loss: 0.0005206644498023459 grad: 0.2178358771725903
iteration: 0 loss: 6345.366156534091 grad: 2373.614426138366
iteration: 0 loss: 6718.8447853214775 grad: 2609.830095136873
iteration: 0 loss: 7987.255271046193 grad: 2827.060729670424
iteration: 0 loss: 8200.985772783331 grad: 2764.9531362812054
iteration: 0 loss: 6132.099148586581 grad: 2491.4234319235757
iteration: 0 loss: 11173.392973051728 grad: 3046.3522082567106
iteration: 10 loss: 0.0004841045354789292 grad: -0.0027906420910440285
iteration: 0 loss: 9034.057552212886 grad: 2787.885433087661
iteration: 10 loss: 0.0005037697582420977 grad: -0.1293371122084154
iteration: 0 loss: 9075.251278240874 grad: 2966.6156576088542
iteration: 0 loss: 10888.637143834094 grad: 3023.1987772187535
iteration: 10 loss: 0.000596692870286378 grad: 0.009538061079737543
iteration: 0 loss: inf grad: 3365.6871070017473
iteration: 0 loss: 6860.401006415567 grad: 2599.6727280753767
iteration: 0 loss: 11507.023835491358 grad: 3003.03989412582
iteration: 10 loss: 0.0006701986210166731 grad: 0.40732549770771037
iteration: 0 loss: 7242.395474223574 grad: 2703.1800533493024
iteration: 0 loss: 5735.460226458682 grad: 2511.230533317953
iteration: 0 loss: 7309.678804813358 grad: 2534.159203677018
iteration: 0 loss: 6021.833464489234 grad: 2517.818271749714
iteration: 0 loss: 8720.958292784482 grad: 2923.566580112106
iteration: 0 loss: 12119.034471389981 grad: 2963.114113954234
iteration: 10 loss: 0.0005873922623736275 grad: 0.0027297816959025393
iteration: 0 loss: 13761.357827056647 grad: 3215.6414611389146
iteration: 10 loss: 0.010790510068711063 grad: 1.4662936527912773
iteration: 0 loss: inf grad: 3067.8750413478538
iteration: 0 loss: 10400.074554226798 grad: 2950.914208119293
iteration: 0 loss: 12992.330103609427 grad: 3069.2389975689184
iteration: 10 loss: 0.0006967388299844143 grad: 0.9986261139655604
iteration: 0 loss: 12942.103197605185 grad: 3078.440502680214
iteration: 10 loss: 0.14958061090543529 grad: 2.181419107462954
iteration: 0 loss: 9702.887140911036 grad: 2675.698926034067
iteration: 10 loss: 0.0004396804910026152 grad: -0.10213438206298986
iteration: 0 loss: inf grad: 3161.2938914862702
iteration: 0 loss: 10585.046215186449 grad: 2831.026879434179
iteration: 10 loss: 0.000579115024513819 grad: 0.0011191712269983485
iteration: 0 loss: 5764.040289096243 grad: 2424.8434640594883
iteration: 0 loss: 4978.338304517574 grad: 2304.3188603655403
iteration: 0 loss: 6856.065723159764 grad: 2524.4353435373982
iteration: 0 loss: 9094.481819578308 grad: 2898.0669209031976
iteration: 0 loss: inf grad: 2717.1759515740973
iteration: 10 loss: 0.00037753512151539326 grad: 0.008394973247121329
iteration: 0 loss: 7121.331172364115 grad: 2635.033080364113
iteration: 0 loss: 6154.6290386647925 grad: 2554.224425570866
iteration: 0 loss: 12531.7625589323 grad: 3203.023181608339
iteration: 10 loss: 0.04341054074624977 grad: -0.9623468414619247
iteration: 0 loss: 10088.279519653875 grad: 2905.936465363024
iteration: 10 loss: 0.004446580008582466 grad: -0.6402501620843204
iteration: 0 loss: 9256.071191556895 grad: 2805.853458268412
iteration: 10 loss: 0.0006486718960910697 grad: -0.0035902269478172107
iteration: 0 loss: 8425.215242792021 grad: 2779.302700954837
iteration: 10 loss: 0.000525331616782668 grad: 0.12303883933920196
iteration: 0 loss: 6035.93447764043 grad: 2398.4619612892457
iteration: 0 loss: 9872.335499906192 grad: 2872.5634926121184
iteration: 0 loss: 7437.164962848109 grad: 2655.315640068492
iteration: 10 loss: 0.0005522080609807745 grad: -0.009724103935205883
iteration: 0 loss: 7693.007890510259 grad: 2795.3034066574714
iteration: 0 loss: 16778.69815027621 grad: 3440.9008674370248
iteration: 0 loss: 5645.662333985202 grad: 2522.2208192316793
iteration: 0 loss: 5569.405722979018 grad: 2425.073706721526
iteration: 0 loss: 6554.42988572579 grad: 2554.7906812218844
iteration: 0 loss: 8348.169703567444 grad: 2849.1677975972398
iteration: 0 loss: 10012.436313146307 grad: 2922.2070051323353
iteration: 10 loss: 0.0005605108348060061 grad: 0.00441874781135973
iteration: 0 loss: 11513.13785801537 grad: 3118.9516195969927
iteration: 10 loss: 0.0005758693997367201 grad: -0.042068802633315326
iteration: 0 loss: 14232.420714646187 grad: 3061.129426259427
iteration: 0 loss: 11206.242322665263 grad: 2943.124992140456
iteration: 10 loss: 0.041069370567459955 grad: 3.467491609553926
iteration: 0 loss: 8718.437826772692 grad: 2798.6894725510656
iteration: 0 loss: 14266.101372411033 grad: 3153.656084217945
iteration: 0 loss: 10801.91746309298 grad: 2947.2207446993743
iteration: 10 loss: 0.0005744172614702785 grad: 0.007433780055913284
iteration: 0 loss: 7072.957563152929 grad: 2764.1879420173573
iteration: 10 loss: 0.0006057540079134262 grad: -1.028169679751489
iteration: 0 loss: 8122.426821186548 grad: 2775.466405217305
iteration: 0 loss: 7452.559405683368 grad: 2721.6677954713173
iteration: 0 loss: 13038.095943533863 grad: 3160.9419889106666
iteration: 10 loss: 0.16942624793790112 grad: -0.27577759339911073
iteration: 0 loss: 6816.417465031827 grad: 2786.326725700516
iteration: 10 loss: 0.0006135148842903701 grad: -0.007052918314977767
iteration: 0 loss: 8263.983956613745 grad: 2657.558821676951
iteration: 0 loss: 4668.741668614909 grad: 2373.4678158106376
iteration: 0 loss: 9693.95622446162 grad: 3030.2837546524397
iteration: 0 loss: 9432.012139913348 grad: 2945.386532027978
iteration: 10 loss: 0.0007768654005221007 grad: 0.00938558234542243
iteration: 0 loss: 9420.6697620562 grad: 2908.727919995185
iteration: 10 loss: 0.0005412895618726245 grad: -0.6981477533522159
iteration: 0 loss: inf grad: 3020.9756102899246
iteration: 0 loss: 7697.681077447577 grad: 2730.8918999741973
iteration: 0 loss: 8824.349471182933 grad: 2847.461274580443
iteration: 10 loss: 0.0005305236845742911 grad: 0.02002385059821776
iteration: 0 loss: 13442.89102687012 grad: 3169.365861657724
iteration: 10 loss: 0.0004845249006228352 grad: -0.16755704795661505
iteration: 0 loss: 8565.604276253172 grad: 2707.6202811062635
iteration: 10 loss: 0.0006204994913952595 grad: 0.0017053611764494751
iteration: 0 loss: 11604.744051559079 grad: 2899.6749693425445
iteration: 10 loss: 0.0005745812165762552 grad: -0.008644348428896722
iteration: 0 loss: 7496.106601975571 grad: 2766.5953766959387
iteration: 10 loss: 0.00047248094183900815 grad: 0.002696803401087844
iteration: 0 loss: 12642.938277494255 grad: 3187.717985225296
iteration: 0 loss: 10817.576454863813 grad: 3020.775364010542
iteration: 0 loss: 6921.469647670707 grad: 2507.4349628783984
iteration: 0 loss: 5531.190418992682 grad: 2430.8672230418824
iteration: 0 loss: 9020.487269239897 grad: 2994.515308740098
iteration: 0 loss: 4672.259859992372 grad: 2331.423565920477
iteration: 0 loss: 6196.812492656018 grad: 2357.4470122956186
iteration: 0 loss: 5401.809299829376 grad: 2416.694349506537
iteration: 0 loss: inf grad: 3075.476536991891
iteration: 0 loss: 7866.163419442645 grad: 2632.511210568
iteration: 0 loss: 13743.917147318285 grad: 3063.0025255088904
iteration: 0 loss: inf grad: 3067.2485190301563
iteration: 0 loss: inf grad: 3287.392229159749
iteration: 0 loss: 7055.070239447415 grad: 2609.080031568615
iteration: 0 loss: 7065.776933768432 grad: 2640.0890843782618
iteration: 0 loss: 9250.879077853728 grad: 2964.8251762268037
iteration: 10 loss: 0.0005592192544347861 grad: 0.008858971653296611
iteration: 0 loss: 9346.838823721495 grad: 2869.2237215713494
iteration: 10 loss: 0.0006388574997386472 grad: 0.008218781597179741
iteration: 0 loss: inf grad: 3325.0997887819867
iteration: 0 loss: 7907.846542724897 grad: 2811.8358191780185
iteration: 0 loss: 13611.531521085983 grad: 3178.091848774983
iteration: 0 loss: 8930.094056780405 grad: 2795.1973588333403
iteration: 0 loss: 6059.5634075432345 grad: 2630.0267039055975
iteration: 0 loss: 12665.366287187771 grad: 3119.0685752430327
iteration: 10 loss: 0.0006169618149711328 grad: 0.8572604548064418
iteration: 0 loss: 6423.048640582244 grad: 2614.7438153298826
iteration: 0 loss: 15339.71397015435 grad: 3174.713783058528
iteration: 0 loss: 5656.1598590133135 grad: 2486.820542541519
iteration: 0 loss: inf grad: 3023.897568316068
iteration: 0 loss: 6189.851114558574 grad: 2469.9394096747747
iteration: 0 loss: 9330.680574877384 grad: 3007.2265895416085
iteration: 10 loss: 0.0004468332917895168 grad: 0.005841428120705568
iteration: 0 loss: 6809.63403189826 grad: 2680.107031696184
iteration: 0 loss: 9248.442022676545 grad: 2795.538799194339
iteration: 10 loss: 0.0005853756606070833 grad: -0.24572676060265203
iteration: 0 loss: 8750.846124029149 grad: 2790.15104297796
iteration: 0 loss: 5039.6988635551525 grad: 2379.1946848543193
iteration: 0 loss: inf grad: 2884.2161109974413
iteration: 10 loss: 0.0034311434560963376 grad: 0.5048094564445892
iteration: 0 loss: 10752.20199611584 grad: 2921.0240293693078
iteration: 10 loss: 0.000521605503639545 grad: 0.5717859553033118
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:57: RuntimeWarning: invalid value encountered in double_scalars
  diff = np.absolute(f_history[-1]-f_history[-2])
iteration: 0 loss: 692.3626069675986 grad: 1009.369643678286
iteration: 10 loss: 0.1128453567967385 grad: 0.5667427310169917
iteration: 0 loss: 535.5982437675033 grad: 944.1397237214343
iteration: 10 loss: 0.11188204642993779 grad: 0.3434420258871925
iteration: 0 loss: 568.8336598133698 grad: 955.8806777170987
iteration: 10 loss: 0.10177636016156942 grad: 0.4227294209755288
iteration: 0 loss: 797.2561718400192 grad: 1050.9884879629603
iteration: 10 loss: 0.10135799644641255 grad: 0.5290966978973739
iteration: 0 loss: 472.0452236470812 grad: 912.0076608136646
iteration: 10 loss: 0.10506180310304361 grad: 0.3252778778865676
iteration: 0 loss: 324.7480037055142 grad: 809.3575342513004
iteration: 10 loss: 0.11457716257064021 grad: 0.22507396243271022
iteration: 0 loss: 498.2910661459287 grad: 939.0003558603578
iteration: 10 loss: 0.09371005393751891 grad: 0.26877105989418
iteration: 0 loss: 244.14846407152825 grad: 767.0489637936715
iteration: 10 loss: 0.1216525747123259 grad: 0.21225128792201434
iteration: 0 loss: 789.0064309677934 grad: 987.0476072525473
iteration: 10 loss: 0.1014865309597636 grad: 0.5375879605374074
iteration: 0 loss: 480.28892348610765 grad: 917.3135437328365
iteration: 10 loss: 0.09919672834505698 grad: 0.37624849695639495
iteration: 0 loss: 272.1775485485368 grad: 835.920402317243
iteration: 10 loss: 0.10957691467873784 grad: 0.23623103336832108
iteration: 0 loss: 580.2789539954222 grad: 987.0026451135998
iteration: 10 loss: 0.1098840756911324 grad: 0.5256459790322533
iteration: 0 loss: 447.27527824217464 grad: 833.6167168708943
iteration: 10 loss: 0.10288247344165442 grad: 0.20250405863069604
iteration: 0 loss: 428.9843158425264 grad: 930.6865033436413
iteration: 10 loss: 0.10478759182528434 grad: 0.3075691889191937
iteration: 0 loss: 604.9950576803599 grad: 946.143202927951
iteration: 10 loss: 0.09555274153882981 grad: 0.3924813162753452
iteration: 0 loss: 842.0724441721607 grad: 1076.0886137883592
iteration: 10 loss: 0.11444273346767825 grad: 0.5620110689748588
iteration: 0 loss: 516.5698303049458 grad: 920.9415420457775
iteration: 10 loss: 0.10857245622339012 grad: 0.31980804653196604
iteration: 0 loss: 363.26080049956107 grad: 855.6532872565458
iteration: 10 loss: 0.10748570609427009 grad: 0.22549411304331657
iteration: 0 loss: 940.5366784563735 grad: 1078.066573787883
iteration: 10 loss: 0.12270033365976732 grad: 0.48405029368915564
iteration: 0 loss: 422.9660154422029 grad: 868.0293821219441
iteration: 10 loss: 0.11490030040295096 grad: 0.22653065360392766
iteration: 0 loss: 607.7907548729991 grad: 984.5893678529926
iteration: 10 loss: 0.10223284979309194 grad: 0.2862325194852154
iteration: 0 loss: 713.5063845288532 grad: 1024.7494708572647
iteration: 10 loss: 0.11545783927017923 grad: 0.5913179734173919
iteration: 0 loss: 423.6112276781238 grad: 823.6984274006857
iteration: 10 loss: 0.09266202174215075 grad: 0.22023427907742746
iteration: 0 loss: 396.2822416318299 grad: 909.7855453687306
iteration: 10 loss: 0.11457354428570067 grad: 0.4628429017301852
iteration: 0 loss: 469.6921248967665 grad: 952.6877999756588
iteration: 10 loss: 0.09892672815393863 grad: 0.3749401664012829
iteration: 0 loss: 433.77005657299264 grad: 919.3209290902414
iteration: 10 loss: 0.11517740719351753 grad: 0.22491376319633088
iteration: 0 loss: 1030.9273247755991 grad: 1083.2444283121456
iteration: 10 loss: 0.113330207971475 grad: 0.43462057265217047
iteration: 0 loss: 951.7294081535088 grad: 1047.2560474900317
iteration: 10 loss: 0.17394438999855827 grad: 0.34491352252437607
iteration: 0 loss: 477.6411516844037 grad: 941.3328285223771
iteration: 10 loss: 0.10296038181315287 grad: 0.36533167014581447
iteration: 0 loss: 400.2731282985458 grad: 865.5371220017928
iteration: 10 loss: 0.11267025374251137 grad: 0.24397945406897784
iteration: 0 loss: 1153.9271171904422 grad: 1145.193821810808
iteration: 10 loss: 0.11321279956775304 grad: 0.5039518238773125
iteration: 0 loss: 520.8417170497634 grad: 995.2997708640521
iteration: 10 loss: 0.0994025802675341 grad: 0.24421097844537154
iteration: 0 loss: 497.9881794684994 grad: 937.4390560266872
iteration: 10 loss: 0.10963043301357671 grad: 0.36796525770645394
iteration: 0 loss: 709.7784628597275 grad: 974.6259451794671
iteration: 10 loss: 0.1015422695891175 grad: 0.45663187540142536
iteration: 0 loss: 702.5166973880158 grad: 1006.8274749290089
iteration: 10 loss: 0.10644024547400477 grad: 0.469633747957256
iteration: 0 loss: 575.297934334313 grad: 978.216870041606
iteration: 10 loss: 0.12129139262899778 grad: 0.8536167256770543
iteration: 0 loss: 559.6691850632213 grad: 936.1212252462577
iteration: 10 loss: 0.10323132637743336 grad: 0.46553995163175516
iteration: 0 loss: 842.8227723672388 grad: 1083.932488199801
iteration: 10 loss: 0.10173135676880272 grad: 0.3069305143906278
iteration: 0 loss: 423.8277467504249 grad: 890.6719770541966
iteration: 10 loss: 0.10575758936945269 grad: 0.23831714861804598
iteration: 0 loss: 437.41710285118785 grad: 907.9881719023485
iteration: 10 loss: 0.10007632248280887 grad: 0.33025633712835967
iteration: 0 loss: 658.7472401836272 grad: 953.0878295774737
iteration: 10 loss: 0.10832743489663699 grad: 0.4689148201885885
iteration: 0 loss: 267.7561207286126 grad: 823.6760152915579
iteration: 10 loss: 0.11782021223569411 grad: 0.21045198170204082
iteration: 0 loss: 1283.8768502168227 grad: 1182.0985590748742
iteration: 10 loss: 0.12157708846974526 grad: 0.3968057556285717
iteration: 0 loss: 529.5286063407679 grad: 875.405897140065
iteration: 10 loss: 0.1087738311877977 grad: 0.29260165970369933
iteration: 0 loss: 629.257356366992 grad: 1054.8188503583638
iteration: 10 loss: 0.10285741036237926 grad: 0.3927261682360461
iteration: 0 loss: 764.5080537412501 grad: 973.8900135795182
iteration: 10 loss: 0.1063607220294216 grad: 0.5735601153551155
iteration: 0 loss: 595.5912367892572 grad: 852.0390535947419
iteration: 10 loss: 0.10778741544560365 grad: 0.3173170067312209
iteration: 0 loss: 418.17808675000396 grad: 923.2869864182728
iteration: 10 loss: 0.10816879834915612 grad: 0.26586622983867014
iteration: 0 loss: 765.2810853682756 grad: 1023.8902503177511
iteration: 10 loss: 0.10532187824669977 grad: 0.5538270138217507
iteration: 0 loss: 822.2966396817342 grad: 1028.3801888044252
iteration: 10 loss: 0.11558295168668503 grad: 0.6570050280523667
iteration: 0 loss: 920.2768408115752 grad: 1061.1519330517349
iteration: 10 loss: 0.12976763540427783 grad: 0.7383043687599522
iteration: 0 loss: 463.7809060336867 grad: 953.7258479335435
iteration: 10 loss: 0.1079357275828608 grad: 0.44790922638108166
iteration: 0 loss: 813.0438652431377 grad: 1087.124269563379
iteration: 10 loss: 0.1099844253420253 grad: 0.42257170130847677
iteration: 0 loss: 633.8405473960155 grad: 972.7985490968667
iteration: 10 loss: 0.10504281030377817 grad: 0.40189826650135174
iteration: 0 loss: 377.5961020154431 grad: 876.9676311418635
iteration: 10 loss: 0.09725388023484398 grad: 0.12101284442381763
iteration: 0 loss: 520.840324375769 grad: 946.464594522226
iteration: 10 loss: 0.10879585876980193 grad: 0.49467330993853875
iteration: 0 loss: 352.07908527903214 grad: 916.9557059599225
iteration: 10 loss: 0.10222760205182237 grad: 0.21207978031331692
iteration: 0 loss: 596.5346627708907 grad: 972.1863496240313
iteration: 10 loss: 0.10237772205403996 grad: 0.543657999897415
iteration: 0 loss: 944.7982919322784 grad: 1105.1896967157238
iteration: 10 loss: 0.10301112421538247 grad: 0.5991457062765595
iteration: 0 loss: 470.69560713246557 grad: 933.3647981117215
iteration: 10 loss: 0.10476818631667068 grad: 0.2085642191051692
iteration: 0 loss: 771.5314218023458 grad: 1032.2346901832627
iteration: 10 loss: 0.10799486009669966 grad: 0.7863952162123999
iteration: 0 loss: 758.4121709451257 grad: 1089.595405549868
iteration: 10 loss: 0.10239083793071173 grad: 0.36274385494093675
iteration: 0 loss: 390.9564958231691 grad: 803.4691306715965
iteration: 10 loss: 0.10315200882128424 grad: 0.30935634979000187
iteration: 0 loss: 368.74714384453176 grad: 884.9664972699243
iteration: 10 loss: 0.1064772647473133 grad: 0.4410171052932089
iteration: 0 loss: 504.5375298807248 grad: 962.9584425586353
iteration: 10 loss: 0.10403939168910173 grad: 0.2176215854768308
iteration: 0 loss: 525.1950692340652 grad: 939.922280713507
iteration: 10 loss: 0.09983485719980804 grad: 0.3311269966260536
iteration: 0 loss: 417.28045621753625 grad: 843.4127904915294
iteration: 10 loss: 0.0946233419172958 grad: 0.21465503811544256
iteration: 0 loss: 718.9834506826215 grad: 1036.7541368140573
iteration: 10 loss: 0.11230361408128044 grad: 0.581969095544792
iteration: 0 loss: 657.751109507662 grad: 947.8403789118119
iteration: 10 loss: 0.11144396554531837 grad: 0.49533055203661125
iteration: 0 loss: 565.4957377362912 grad: 1010.5590589003973
iteration: 10 loss: 0.11249624081045279 grad: 0.40081403903538515
iteration: 0 loss: 664.7970603235276 grad: 1029.0865690593723
iteration: 10 loss: 0.10225492308067756 grad: 0.4802158277129755
iteration: 0 loss: 1111.7829422928999 grad: 1149.5155382634316
iteration: 10 loss: 0.1349052356761769 grad: 0.8609300462365467
iteration: 0 loss: 462.84881301628445 grad: 883.1972200006569
iteration: 10 loss: 0.11071938070083641 grad: 0.3270030599715973
iteration: 0 loss: 717.0115931010648 grad: 1023.6435580184203
iteration: 10 loss: 0.10997380486316449 grad: 0.4597692181107912
iteration: 0 loss: 473.6943871880439 grad: 919.5414626347579
iteration: 10 loss: 0.11245127717881008 grad: 0.23188641653892822
iteration: 0 loss: 370.1368829456815 grad: 851.519343043191
iteration: 10 loss: 0.10095078164720855 grad: 0.2238373824829465
iteration: 0 loss: 506.46830745891106 grad: 859.9728786755692
iteration: 10 loss: 0.10339692276912418 grad: 0.4855495417027955
iteration: 0 loss: 362.8688040674606 grad: 855.1367717326211
iteration: 10 loss: 0.10774410514419618 grad: 0.21936679225278688
iteration: 0 loss: 537.4758002047345 grad: 996.3460114921439
iteration: 10 loss: 0.10363574177129274 grad: 0.3215405326922311
iteration: 0 loss: 834.4408014506799 grad: 1009.2868152774582
iteration: 10 loss: 0.10928390477337499 grad: 0.5807085105205156
iteration: 0 loss: 888.2566823911308 grad: 1096.1831947278838
iteration: 10 loss: 0.09735477304771352 grad: 0.6277732029783263
iteration: 0 loss: 847.2567197736159 grad: 1045.2772805836241
iteration: 10 loss: 0.10541594135528695 grad: 0.531603618703384
iteration: 0 loss: 648.8093977827244 grad: 1002.844994243383
iteration: 10 loss: 0.10467373128746905 grad: 0.41920105167255767
iteration: 0 loss: 876.5924891596719 grad: 1045.2715182513587
iteration: 10 loss: 0.10140384707913407 grad: 0.4456624656156749
iteration: 0 loss: 850.734920051305 grad: 1049.5748439795564
iteration: 10 loss: 0.10584540997131792 grad: 0.4245429995089708
iteration: 0 loss: 725.9592747795009 grad: 909.7882942228878
iteration: 10 loss: 0.10087530723219308 grad: 0.30742445312675276
iteration: 0 loss: 1115.7106983759936 grad: 1076.0239859316823
iteration: 10 loss: 0.11188665850402851 grad: 0.4064270842970759
iteration: 0 loss: 761.9200222711795 grad: 962.6076321959752
iteration: 10 loss: 0.1093448212959629 grad: 0.4355465785885467
iteration: 0 loss: 338.492620371153 grad: 821.1028300265398
iteration: 10 loss: 0.1097285326188946 grad: 0.22126560168729648
iteration: 0 loss: 320.56350889833067 grad: 778.5946589072565
iteration: 10 loss: 0.11288128883263032 grad: 0.057485178233747866
iteration: 0 loss: 461.13802743018783 grad: 854.839916067225
iteration: 10 loss: 0.09769472739456185 grad: 0.11362313835197342
iteration: 0 loss: 594.544855308894 grad: 986.3103554888819
iteration: 10 loss: 0.1000654339503936 grad: 0.18337046133717969
iteration: 0 loss: 540.1763025251292 grad: 924.5451876971063
iteration: 10 loss: 0.0989335929969622 grad: 0.4934157350823768
iteration: 0 loss: 425.8037603437749 grad: 894.5090854039918
iteration: 10 loss: 0.10643948813032528 grad: 0.30223588051731204
iteration: 0 loss: 373.6316240640069 grad: 866.3768229620601
iteration: 10 loss: 0.10101165283264311 grad: 0.18170793615761882
iteration: 0 loss: 813.580576307389 grad: 1093.8185428524307
iteration: 10 loss: 0.09819732312759945 grad: 0.3910220262368048
iteration: 0 loss: 681.8520709056734 grad: 989.2492262284152
iteration: 10 loss: 0.11259108414527542 grad: 0.4952387309573443
iteration: 0 loss: 676.695579589432 grad: 954.3081345194903
iteration: 10 loss: 0.1073044994143195 grad: 0.38366811889976216
iteration: 0 loss: 532.1179187850594 grad: 946.1705397703759
iteration: 10 loss: 0.1034005301720423 grad: 0.30067327211939154
iteration: 0 loss: 440.30700735187565 grad: 812.4308998908234
iteration: 10 loss: 0.101152937653751 grad: 0.07559869348093548
iteration: 0 loss: 649.677134221697 grad: 978.2030665900915
iteration: 10 loss: 0.09812424765682821 grad: 0.3820102465242241
iteration: 0 loss: 465.17203935940574 grad: 902.3387521042422
iteration: 10 loss: 0.10762701578906589 grad: 0.22494745367030222
iteration: 0 loss: 430.9360379802904 grad: 948.9392301309085
iteration: 10 loss: 0.1130553111677066 grad: 0.4042919311333796
iteration: 0 loss: 1068.930786731851 grad: 1174.8586728594337
iteration: 10 loss: 0.11858126968185058 grad: 0.7722594928020239
iteration: 0 loss: 348.95239890850735 grad: 855.0772369148976
iteration: 10 loss: 0.09846857415124594 grad: 0.13298803883159904
iteration: 0 loss: 323.0161560562754 grad: 823.2824564279938
iteration: 10 loss: 0.10924405044120662 grad: 0.15496990168778102
iteration: 0 loss: 405.4423595282388 grad: 867.882397227356
iteration: 10 loss: 0.10397012700625717 grad: 0.1918098612905719
iteration: 0 loss: 492.4695408211548 grad: 968.0527488986713
iteration: 10 loss: 0.10828882076202717 grad: 0.40597972890775036
iteration: 0 loss: 663.1135039171635 grad: 993.8500287561371
iteration: 10 loss: 0.09799724715892145 grad: 0.3866551445392551
iteration: 0 loss: 714.7726088967097 grad: 1063.4744369576144
iteration: 10 loss: 0.10030803222533988 grad: 0.4356750823206774
iteration: 0 loss: 846.2197542872805 grad: 1044.1072309823455
iteration: 10 loss: 0.12759190957876854 grad: 0.8060915953861509
iteration: 0 loss: 657.1590609454847 grad: 1002.6959823479812
iteration: 10 loss: 0.11896807905064964 grad: 0.6293502923439386
iteration: 0 loss: 548.2350441131983 grad: 952.55365341573
iteration: 10 loss: 0.10493021480297929 grad: 0.4765987380795054
iteration: 0 loss: 944.3174265967573 grad: 1076.1008073469166
iteration: 10 loss: 0.11147326560537542 grad: 0.48772166937892836
iteration: 0 loss: 725.0695013180663 grad: 1002.6905122615086
iteration: 10 loss: 0.10908935043979992 grad: 0.37266750091272677
iteration: 0 loss: 379.2527440748292 grad: 940.2067771162125
iteration: 10 loss: 0.10750392665944343 grad: 0.34906164521916194
iteration: 0 loss: 536.3355530928258 grad: 942.3367920103003
iteration: 10 loss: 0.0996514632009745 grad: 0.18597288122031158
iteration: 0 loss: 466.91746703312873 grad: 925.707762961028
iteration: 10 loss: 0.11442503614357107 grad: 0.4287261946888292
iteration: 0 loss: 793.6078261027135 grad: 1078.6125060248391
iteration: 10 loss: 0.12119353096502229 grad: 0.3706273466964177
iteration: 0 loss: 408.8971565956266 grad: 948.1312794457486
iteration: 10 loss: 0.10684247433908406 grad: 0.2687980546589224
iteration: 0 loss: 537.774718103045 grad: 901.7548179365115
iteration: 10 loss: 0.10852074125994643 grad: 0.40622465377779166
iteration: 0 loss: 279.4792038126867 grad: 804.6704983885346
iteration: 10 loss: 0.12211055674627212 grad: 0.27496357919704806
iteration: 0 loss: 665.4611317715318 grad: 1032.5263160151221
iteration: 10 loss: 0.10204833889292786 grad: 0.6303986136660494
iteration: 0 loss: 600.6132675638563 grad: 1002.493347570593
iteration: 10 loss: 0.1140060526881412 grad: 0.35125075175898424
iteration: 0 loss: 601.4134847557848 grad: 988.2826260683997
iteration: 10 loss: 0.10625165854832051 grad: 0.45911812794383283
iteration: 0 loss: 963.144673848336 grad: 1027.5730949639587
iteration: 10 loss: 0.1034274704533428 grad: 0.4024501337266786
iteration: 0 loss: 458.3309839258088 grad: 929.014473415602
iteration: 10 loss: 0.10632292321133718 grad: 0.36966898935471315
iteration: 0 loss: 561.4558079769612 grad: 968.804604481505
iteration: 10 loss: 0.10279601695484333 grad: 0.4460038362958525
iteration: 0 loss: 975.2921061199777 grad: 1081.8092791728138
iteration: 10 loss: 0.10922901584655681 grad: 0.4840863044114851
iteration: 0 loss: 555.5833847439669 grad: 919.2261391377773
iteration: 10 loss: 0.1136828048789043 grad: 0.4429167125366724
iteration: 0 loss: 765.8658458320991 grad: 985.4642727799098
iteration: 10 loss: 0.10082110693507605 grad: 0.5047511465492451
iteration: 0 loss: 468.68885139256906 grad: 940.2592834637569
iteration: 10 loss: 0.11197498634282965 grad: 0.3928974260847205
iteration: 0 loss: 842.2883551738428 grad: 1085.2432140368794
iteration: 10 loss: 0.10561892900758804 grad: 0.42881313645455454
iteration: 0 loss: 744.7995379309333 grad: 1028.4079327783345
iteration: 10 loss: 0.10338626668437262 grad: 0.3481833685382397
iteration: 0 loss: 479.8204051457012 grad: 850.1749321569355
iteration: 10 loss: 0.10361685571528638 grad: 0.2285031583302641
iteration: 0 loss: 351.6738699263442 grad: 823.4229986442215
iteration: 10 loss: 0.10939550101846346 grad: 0.21330886465200036
iteration: 0 loss: 531.631307073457 grad: 1020.3187280227985
iteration: 10 loss: 0.09913887696166057 grad: 0.371733020221911
iteration: 0 loss: 257.6003995987226 grad: 787.4858443629024
iteration: 10 loss: 0.11789944558057125 grad: 0.21032839015008845
iteration: 0 loss: 438.3128282264037 grad: 795.8359189386547
iteration: 10 loss: 0.09975526976896534 grad: 0.2750912650912091
iteration: 0 loss: 355.53461798477014 grad: 817.7696852156434
iteration: 10 loss: 0.10654763859483038 grad: 0.28890330962087896
iteration: 0 loss: 867.4594169517848 grad: 1048.7848966053855
iteration: 10 loss: 0.10300150971802809 grad: 0.47539390400826254
iteration: 0 loss: 535.2761145109293 grad: 893.9017576092886
iteration: 10 loss: 0.10860285880831229 grad: 0.3672208167708775
iteration: 0 loss: 830.8839489780134 grad: 1043.1377411201838
iteration: 10 loss: 0.10468181125510125 grad: 0.522311898737237
iteration: 0 loss: 853.4259582078095 grad: 1044.2861642895762
iteration: 10 loss: 0.09901723294999763 grad: 0.4794801500070859
iteration: 0 loss: 983.664660380845 grad: 1122.4585148657216
iteration: 10 loss: 0.1126267832782601 grad: 0.7056335962047702
iteration: 0 loss: 421.2391039444985 grad: 884.6837198675672
iteration: 10 loss: 0.10873905354095438 grad: 0.26095411201341134
iteration: 0 loss: 441.5601355374002 grad: 895.8696909173683
iteration: 10 loss: 0.1055713473918083 grad: 0.3775192965089724
iteration: 0 loss: 564.8110418460957 grad: 1010.3718233190696
iteration: 10 loss: 0.10582257994230365 grad: 0.4401597713421955
iteration: 0 loss: 583.2071341178633 grad: 976.3689369680703
iteration: 10 loss: 0.10557696043460138 grad: 0.43836122452727033
iteration: 0 loss: 1152.1170576840946 grad: 1133.8391570184476
iteration: 10 loss: 0.10952449667622038 grad: 0.3715382704829221
iteration: 0 loss: 460.67199521149183 grad: 954.9423112338698
iteration: 10 loss: 0.10550762741794725 grad: 0.43844350944364163
iteration: 0 loss: 857.2871699118488 grad: 1085.319319391648
iteration: 10 loss: 0.12076541799317718 grad: 0.6669981790397825
iteration: 0 loss: 572.611840608147 grad: 948.1451930278299
iteration: 10 loss: 0.09959775516703799 grad: 0.32801827930675775
iteration: 0 loss: 332.310941864613 grad: 893.6525688724906
iteration: 10 loss: 0.10371635110343712 grad: 0.16809177443305007
iteration: 0 loss: 855.8622481823288 grad: 1063.271169000664
iteration: 10 loss: 0.1020099593849657 grad: 0.40559547094927895
iteration: 0 loss: 402.0391951709718 grad: 887.3706382328648
iteration: 10 loss: 0.11769344528908153 grad: 0.4333245707489808
iteration: 0 loss: 994.284031456221 grad: 1082.6967658151648
iteration: 10 loss: 0.11573751093998329 grad: 0.6234400686761109
iteration: 0 loss: 356.38433957591275 grad: 841.7357777971611
iteration: 10 loss: 0.11514657675533355 grad: 0.24502087858731464
iteration: 0 loss: 801.4626622968543 grad: 1030.3133012041326
iteration: 10 loss: 0.09650506416487174 grad: 0.4213112226880315
iteration: 0 loss: 392.44623649291776 grad: 836.988135380261
iteration: 10 loss: 0.10668593535509982 grad: 0.2747071986030843
iteration: 0 loss: 587.7330079092574 grad: 1023.6008119561945
iteration: 10 loss: 0.10649804721188874 grad: 0.31835617477193057
iteration: 0 loss: 412.80658266483533 grad: 910.935821027369
iteration: 10 loss: 0.1048783455917146 grad: 0.3982673549494923
iteration: 0 loss: 524.1116448154348 grad: 949.6550498980716
iteration: 10 loss: 0.10294817765679909 grad: 0.38664943196549056
iteration: 0 loss: 600.8062540087545 grad: 947.9402227964179
iteration: 10 loss: 0.10428544771176265 grad: 0.4050746886971235
iteration: 0 loss: 302.6559429988403 grad: 805.8700827347135
iteration: 10 loss: 0.10651933380673158 grad: 0.17578241167408565
iteration: 0 loss: 742.8551595949664 grad: 981.1575313757007
iteration: 10 loss: 0.10115234324604733 grad: 0.45849374511528157
iteration: 0 loss: 688.2050094211505 grad: 993.8093969011663
iteration: 10 loss: 0.11316061537879497 grad: 0.5177454725405084
iteration: 0 loss: 7453.041806634055 grad: 2539.0145684992294
iteration: 10 loss: 0.0011642742267047818 grad: 0.011324204814461758
iteration: 0 loss: 5993.879312134845 grad: 2379.601603093708
iteration: 0 loss: 6623.01979446288 grad: 2410.2656328797675
iteration: 10 loss: 0.0010099583861037072 grad: 0.07839855365479757
iteration: 0 loss: 8303.703361161672 grad: 2641.131923962075
iteration: 10 loss: 0.0010832912203940477 grad: 0.027173680912864796
iteration: 0 loss: 5138.404364613165 grad: 2297.5599182029673
iteration: 0 loss: 3480.1281722482 grad: 2049.736976644125
iteration: 10 loss: 0.0011518735700519755 grad: -0.5211167948455919
iteration: 0 loss: 5525.490108464356 grad: 2370.00199727805
iteration: 0 loss: 2944.565526392285 grad: 1941.7334185303164
iteration: 0 loss: 8252.076627438224 grad: 2483.0974501101564
iteration: 10 loss: 0.001130291288559833 grad: -0.0019429030842716011
iteration: 0 loss: 5372.143182922162 grad: 2312.5828860282218
iteration: 0 loss: 3373.202918721414 grad: 2115.0963835849298
iteration: 0 loss: 6837.962495507294 grad: 2486.521928911633
iteration: 10 loss: 0.0013296517969468948 grad: -0.02186814006506596
iteration: 0 loss: 4665.578929064315 grad: 2103.853937875799
iteration: 0 loss: 5004.013358616831 grad: 2347.0046545224477
iteration: 0 loss: 6608.168452909903 grad: 2384.437272203256
iteration: 10 loss: 0.000909489583467472 grad: 0.009515145996052438
iteration: 0 loss: 9616.018355620716 grad: 2702.9495267745287
iteration: 10 loss: 0.05237355950521305 grad: 0.24460397694600164
iteration: 0 loss: 5759.63013492165 grad: 2320.5182590745735
iteration: 0 loss: 4017.5506157243453 grad: 2162.479419577365
iteration: 0 loss: 9648.125715408783 grad: 2708.019397841559
iteration: 10 loss: 0.00171988995465323 grad: 1.091996366200687
iteration: 0 loss: 4696.079229619375 grad: 2188.309893350023
iteration: 0 loss: 6839.44551687343 grad: 2483.417419293147
iteration: 0 loss: 8287.3197407836 grad: 2580.5732665391074
iteration: 10 loss: 0.04122204477343158 grad: -2.4216280304756164
iteration: 0 loss: 4244.373162677648 grad: 2079.394772826162
iteration: 0 loss: 5138.363774253406 grad: 2290.94496948576
iteration: 0 loss: 5281.981200174997 grad: 2403.3874648708234
iteration: 0 loss: 4545.437121075233 grad: 2315.609230143573
iteration: 10 loss: 0.0011969647550193424 grad: -0.014871298124503197
iteration: 0 loss: 10399.04075816174 grad: 2722.3023405162667
iteration: 10 loss: 0.0012425550997828725 grad: 1.4562181057057333
iteration: 0 loss: 10096.253177112027 grad: 2631.505195004481
iteration: 10 loss: 1.4691414623309604 grad: 6.693441879633509
iteration: 0 loss: 5843.119056738556 grad: 2370.6985508856114
iteration: 0 loss: 4367.552035328882 grad: 2183.640888823458
iteration: 0 loss: 11936.12605997189 grad: 2875.819785807509
iteration: 10 loss: 0.08980613774672913 grad: 1.9526876688435344
iteration: 0 loss: 5832.717288601703 grad: 2502.1247176309885
iteration: 0 loss: 5619.1784321036575 grad: 2361.1052797622724
iteration: 0 loss: 7832.302679272418 grad: 2452.1939302963174
iteration: 10 loss: 0.07184440223367843 grad: -0.7218656039239557
iteration: 0 loss: 7685.580392402515 grad: 2532.1104520756735
iteration: 10 loss: 0.0014466263660737736 grad: -0.02021345422993415
iteration: 0 loss: 6473.117853849396 grad: 2461.134700148887
iteration: 10 loss: 0.015677239769667558 grad: 1.177362046004026
iteration: 0 loss: 6075.512656062056 grad: 2356.723594303316
iteration: 10 loss: 0.0012534850552170114 grad: 0.025192903302496635
iteration: 0 loss: 9684.422091789114 grad: 2724.5077487705585
iteration: 10 loss: 0.0012895440397022123 grad: 1.3925236507747494
iteration: 0 loss: 4489.905949282493 grad: 2246.9627519402384
iteration: 0 loss: 5052.05403401365 grad: 2285.640661006805
iteration: 0 loss: 6380.803967146005 grad: 2399.0274389867636
iteration: 0 loss: 3212.496591983401 grad: 2083.373223723735
iteration: 0 loss: 13498.671266250858 grad: 2965.4937195567713
iteration: 10 loss: 1.552935810069109 grad: 1.5442257644016613
iteration: 0 loss: 5377.9296055995155 grad: 2204.639484436163
iteration: 0 loss: 7344.433540795871 grad: 2653.0858774335975
iteration: 0 loss: 7614.443771496913 grad: 2452.873168691928
iteration: 0 loss: 5804.485846573774 grad: 2152.728118270453
iteration: 0 loss: 4694.143917916675 grad: 2327.1490012856993
iteration: 0 loss: 8289.08211155013 grad: 2572.0914597152596
iteration: 10 loss: 0.0011913360834693197 grad: -0.04107080980710376
iteration: 0 loss: 8657.974662778828 grad: 2587.0054442162636
iteration: 10 loss: 0.13283624224475882 grad: 0.5859958274391062
iteration: 0 loss: 10022.763767059241 grad: 2668.803336733286
iteration: 10 loss: 0.9032580879231152 grad: 2.907798087496078
iteration: 0 loss: 5390.427924683749 grad: 2399.005346535185
iteration: 10 loss: 0.0012193911560726437 grad: -0.03663468639850863
iteration: 0 loss: 9081.208667662786 grad: 2733.249237115987
iteration: 10 loss: 0.0013689792990176516 grad: 0.47397012101784675
iteration: 0 loss: 6656.141071695878 grad: 2448.790037553217
iteration: 0 loss: 4505.697180867488 grad: 2214.561421273206
iteration: 0 loss: 5960.244704349842 grad: 2384.5853168981475
iteration: 10 loss: 0.001321820542216301 grad: 0.016583656958779336
iteration: 0 loss: 4493.96930502541 grad: 2312.048456757888
iteration: 0 loss: 6561.817089726007 grad: 2444.0903767275786
iteration: 10 loss: 0.001273958586600863 grad: -0.08263616957434419
iteration: 0 loss: 9725.348333413594 grad: 2774.983895774528
iteration: 10 loss: 0.0010441351928007366 grad: -0.021239548702534965
iteration: 0 loss: 5288.452801083839 grad: 2351.5559708200317
iteration: 0 loss: 8580.228535973407 grad: 2599.4219351750367
iteration: 10 loss: 0.010040098123929718 grad: 0.34821463499030547
iteration: 0 loss: 8538.845999359768 grad: 2739.60958628421
iteration: 0 loss: 4428.676060704028 grad: 2034.2632746601048
iteration: 0 loss: 4702.545386797167 grad: 2238.2413335952274
iteration: 0 loss: 5567.485784596009 grad: 2421.2471494785586
iteration: 0 loss: 5781.801441651876 grad: 2368.6951711397883
iteration: 0 loss: 4336.575080756832 grad: 2133.1140145400477
iteration: 0 loss: 7871.93182841834 grad: 2605.3531682751654
iteration: 10 loss: 0.0010401080521246927 grad: 0.19488906864058145
iteration: 0 loss: 6367.966873900355 grad: 2389.4135814306387
iteration: 10 loss: 0.0010710735580439425 grad: 0.005412948381175168
iteration: 0 loss: 6368.457019601285 grad: 2540.3645023352137
iteration: 0 loss: 7683.639710158279 grad: 2590.8245917218037
iteration: 0 loss: 11595.59024041363 grad: 2884.513532337845
iteration: 10 loss: 0.0705344127940903 grad: -5.150715150901622
iteration: 0 loss: 4829.976240769117 grad: 2226.453298811784
iteration: 0 loss: 8112.59846549487 grad: 2576.4642977770077
iteration: 10 loss: 0.0012539520605721257 grad: -0.005003923760292158
iteration: 0 loss: 5060.270163287779 grad: 2317.8414130639917
iteration: 0 loss: 4010.171440764339 grad: 2148.707808570002
iteration: 0 loss: 5155.845287849205 grad: 2172.718742356651
iteration: 0 loss: 4210.055021262334 grad: 2157.0830124837
iteration: 0 loss: 6139.902710049995 grad: 2506.442785363396
iteration: 0 loss: 8524.392400587014 grad: 2540.4237578141892
iteration: 10 loss: 0.0013480892732993445 grad: -0.022339346283411974
iteration: 0 loss: 9715.230580008296 grad: 2754.7515787657926
iteration: 10 loss: 0.0009565559170865031 grad: 1.121744643510603
iteration: 0 loss: 8967.02882940471 grad: 2627.7035540032552
iteration: 10 loss: 0.007507898683384569 grad: 0.4615822251078454
iteration: 0 loss: 7294.782978343565 grad: 2527.637850245679
iteration: 0 loss: 9215.672770362859 grad: 2632.2623643783727
iteration: 10 loss: 0.005409349057696421 grad: 1.0041018708151137
iteration: 0 loss: 9131.213947737742 grad: 2637.3480677588127
iteration: 10 loss: 0.017658210550011558 grad: 0.9491132406748704
iteration: 0 loss: 6882.8276773318585 grad: 2292.8208955837954
iteration: 10 loss: 0.0008670625062553551 grad: 0.001164364415817026
iteration: 0 loss: 10804.817284487921 grad: 2705.9036867155282
iteration: 10 loss: 0.09970276282084259 grad: 2.82234565586992
iteration: 0 loss: 7432.194738789453 grad: 2423.2743299775184
iteration: 0 loss: 4017.345142053949 grad: 2075.547027213631
iteration: 0 loss: 3478.519423180675 grad: 1970.7485014217395
iteration: 0 loss: 4760.288976625651 grad: 2159.2559499133285
iteration: 0 loss: 6400.798290926792 grad: 2482.951183809326
iteration: 0 loss: 6086.451086287662 grad: 2329.3390784443723
iteration: 10 loss: 0.0009979943451154131 grad: 0.01831083485005972
iteration: 0 loss: 4975.77155667091 grad: 2254.583456843933
iteration: 10 loss: 0.001226895457048985 grad: 0.02453589451636761
iteration: 0 loss: 4312.379262350526 grad: 2188.3143191729855
iteration: 0 loss: 8835.512865614828 grad: 2742.8914721816473
iteration: 10 loss: 0.0012062047872777011 grad: 0.06390903049253051
iteration: 0 loss: 7153.868577055101 grad: 2491.047175769712
iteration: 10 loss: 0.0012621627750129185 grad: 0.037823640068808666
iteration: 0 loss: 6569.120430441294 grad: 2402.579640077366
iteration: 10 loss: 0.0011818279255583714 grad: 0.011502064968770995
iteration: 0 loss: 5901.566148544302 grad: 2381.618517548761
iteration: 10 loss: 0.0012642341885525225 grad: 0.08972692681005252
iteration: 0 loss: 4259.912275270231 grad: 2054.29848207359
iteration: 0 loss: 6978.607763583161 grad: 2462.2011391054175
iteration: 0 loss: 5224.283828292087 grad: 2273.530656820686
iteration: 0 loss: 5362.637575329615 grad: 2394.933852698114
iteration: 0 loss: 11836.1491479128 grad: 2949.5787201141675
iteration: 10 loss: 0.014867627601647242 grad: 1.536450489858815
iteration: 0 loss: 3984.7897831873443 grad: 2158.5395309494356
iteration: 0 loss: 3900.8129256191414 grad: 2080.3784707053946
iteration: 0 loss: 4634.224216231015 grad: 2191.2008751517005
iteration: 0 loss: 5863.663549441944 grad: 2438.0016368227202
iteration: 0 loss: 7085.687215719872 grad: 2503.9332957968018
iteration: 10 loss: 0.0010853697709866208 grad: 0.009040580457203073
iteration: 0 loss: 8079.069711528175 grad: 2670.2584995321795
iteration: 10 loss: 0.0010190018365392461 grad: 0.04158596347446632
iteration: 0 loss: 9999.204412981113 grad: 2623.469018247818
iteration: 10 loss: 0.2800580179609824 grad: 2.3864042875983067
iteration: 0 loss: 7834.452506722503 grad: 2523.217051178066
iteration: 10 loss: 0.05155338839871216 grad: 2.7667727868470475
iteration: 0 loss: 6110.27315399264 grad: 2397.6321047826073
iteration: 0 loss: 10089.267311309042 grad: 2703.897452681512
iteration: 10 loss: 0.0756792685093718 grad: -0.9060866119583899
iteration: 0 loss: 7626.197110557263 grad: 2524.790335680994
iteration: 0 loss: 4935.681962043815 grad: 2368.9795657754776
iteration: 10 loss: 0.0011075784330552613 grad: -0.11828949630766995
iteration: 0 loss: 5744.257338028613 grad: 2377.88811682367
iteration: 0 loss: 5214.850873946624 grad: 2332.6031456857563
iteration: 0 loss: 9156.34404035431 grad: 2709.2624392616876
iteration: 10 loss: 0.0033739031191957606 grad: 3.2155064314318538
iteration: 0 loss: 4754.164956722345 grad: 2385.8493549397035
iteration: 0 loss: 5829.413800326022 grad: 2276.4951319941465
iteration: 0 loss: 3245.6030338405603 grad: 2033.913633996808
iteration: 0 loss: 6855.530151227386 grad: 2595.0499832552614
iteration: 0 loss: 6607.359158777989 grad: 2522.0270331028123
iteration: 10 loss: 0.0012966383536960084 grad: -0.8133084085945049
iteration: 0 loss: 6613.643731944888 grad: 2490.4019681443688
iteration: 10 loss: 0.0010865614947926422 grad: -0.015723534204890387
iteration: 0 loss: 9447.50207667317 grad: 2587.3218869672296
iteration: 10 loss: 0.3764459658810996 grad: 2.4619401168104824
iteration: 0 loss: 5354.236169943207 grad: 2339.372151350653
iteration: 0 loss: 6200.10494821211 grad: 2440.859623554996
iteration: 10 loss: 0.0011363071138145062 grad: -0.011146377665636349
iteration: 0 loss: 9588.649535769237 grad: 2717.2687602894175
iteration: 10 loss: 0.006755177836484191 grad: -0.13924644101983047
iteration: 0 loss: 6045.094664110281 grad: 2318.0459365334023
iteration: 0 loss: 8150.706075772254 grad: 2481.9360211330622
iteration: 10 loss: 0.0013182053040840071 grad: 0.029050664160713606
iteration: 0 loss: 5234.854341218524 grad: 2369.4288096466425
iteration: 0 loss: 8912.215072881152 grad: 2730.2369470984013
iteration: 10 loss: 0.03779071153787134 grad: -0.21141529973676615
iteration: 0 loss: 7666.839454123832 grad: 2586.659227488328
iteration: 10 loss: 0.0009159316012466496 grad: 0.02420681891357712
iteration: 0 loss: 4943.061437284297 grad: 2148.551148376811
iteration: 0 loss: 3881.116025863579 grad: 2083.0546566476146
iteration: 0 loss: 6319.383160989083 grad: 2563.1014722191485
iteration: 0 loss: 3229.9478961424393 grad: 1995.2243080261264
iteration: 0 loss: 4385.83472921907 grad: 2018.4379371977316
iteration: 0 loss: 3813.4022301565465 grad: 2070.991792283855
iteration: 0 loss: 9173.582998125408 grad: 2634.977675288409
iteration: 10 loss: 0.0011437323912766508 grad: 0.8305541126643342
iteration: 0 loss: 5579.856137640519 grad: 2257.6081563663433
iteration: 0 loss: 9653.76523630019 grad: 2626.183124619619
iteration: 10 loss: 0.6227601091761078 grad: 1.961757757110372
iteration: 0 loss: 9069.571952316664 grad: 2629.0222695955904
iteration: 10 loss: 0.007926000362865372 grad: 1.5891566621301974
iteration: 0 loss: 10851.149582054037 grad: 2818.9170126447198
iteration: 10 loss: 0.25495688298277935 grad: 2.859030971041495
iteration: 0 loss: 4921.189447281649 grad: 2233.3050421490784
iteration: 0 loss: 5005.109460270981 grad: 2259.2903454540337
iteration: 0 loss: 6495.543712070733 grad: 2542.3851586578294
iteration: 10 loss: 0.0012789426387330009 grad: 0.07266902802627331
iteration: 0 loss: 6586.030587962043 grad: 2458.559971504728
iteration: 10 loss: 0.0013120176471685145 grad: 0.016736859465778783
iteration: 0 loss: 12407.390278490493 grad: 2847.837549751529
iteration: 10 loss: 0.44867420352370985 grad: 3.484541630596821
iteration: 0 loss: 5519.654365576541 grad: 2407.3869829733503
iteration: 10 loss: 0.0013358952432595702 grad: 0.05918367063911785
iteration: 0 loss: 9573.161266761936 grad: 2725.829839486415
iteration: 10 loss: 0.05470529828876765 grad: 2.063431640216471
iteration: 0 loss: 6261.907387987171 grad: 2392.047383137932
iteration: 10 loss: 0.0009508427399189466 grad: -0.0005931029373225337
iteration: 0 loss: 4240.747676471434 grad: 2254.416222187533
iteration: 0 loss: 8943.212879157778 grad: 2674.122038068479
iteration: 10 loss: 0.001345717910391448 grad: 0.8201655080881728
iteration: 0 loss: 4478.848116215651 grad: 2240.341520326985
iteration: 0 loss: 10831.336270213009 grad: 2720.598593529546
iteration: 10 loss: 0.7499747158713448 grad: 2.949649010613218
iteration: 0 loss: 3944.4391370370854 grad: 2127.895543908046
iteration: 0 loss: 8460.048376340319 grad: 2592.375387696492
iteration: 10 loss: 0.0012809789345324548 grad: 0.5686433450737858
iteration: 0 loss: 4334.425012092612 grad: 2111.3745571536574
iteration: 0 loss: 6553.156220541313 grad: 2577.0015733473656
iteration: 10 loss: 0.0011154086532769725 grad: 0.0044088904429561366
iteration: 0 loss: 4794.019671329467 grad: 2296.6297435446295
iteration: 0 loss: 6431.135422433787 grad: 2390.5759673481452
iteration: 10 loss: 0.001224848092533648 grad: -0.006343777986371523
iteration: 0 loss: 6172.795800299637 grad: 2388.765589530022
iteration: 10 loss: 0.0010982838747705418 grad: 0.01204086708282388
iteration: 0 loss: 3550.9645426396382 grad: 2038.7395520186838
iteration: 0 loss: 7811.074142726217 grad: 2472.975885062616
iteration: 10 loss: 0.0016005723988531497 grad: 0.4301740031418564
iteration: 0 loss: 7540.044304944183 grad: 2501.5934148657966
iteration: 10 loss: 0.0012805374320702288 grad: 0.3035791046568193
iteration: 0 loss: inf grad: 3641.1992226670973
iteration: 0 loss: inf grad: 3419.1931263201277
iteration: 0 loss: inf grad: 3458.0232956805253
iteration: 0 loss: inf grad: 3794.5680291124677
iteration: 0 loss: inf grad: 3303.3611437217105
iteration: 0 loss: 7799.597213707369 grad: 2947.0434879032255
iteration: 10 loss: 0.00011623260004191913 grad: 0.000723004508748606
iteration: 0 loss: inf grad: 3406.1227805580074
iteration: 0 loss: 6777.024652724515 grad: 2787.5636826157966
iteration: 0 loss: inf grad: 3565.0622373445494
iteration: 0 loss: inf grad: 3320.3021770347027
iteration: 0 loss: 7716.946464000872 grad: 3036.2492941590563
iteration: 0 loss: inf grad: 3566.590947466112
iteration: 0 loss: 10427.55217575167 grad: 3021.736504930569
iteration: 0 loss: inf grad: 3374.719292503701
iteration: 0 loss: inf grad: 3420.4155515455814
iteration: 0 loss: inf grad: 3878.068156536344
iteration: 0 loss: inf grad: 3329.7782249642446
iteration: 0 loss: inf grad: 3102.888274821884
iteration: 0 loss: inf grad: 3884.29872390523
iteration: 0 loss: 10630.21073006561 grad: 3141.905308854086
iteration: 0 loss: inf grad: 3560.7569984398433
iteration: 0 loss: inf grad: 3701.021362856674
iteration: 0 loss: inf grad: 2987.597286427046
iteration: 0 loss: inf grad: 3287.897964966828
iteration: 0 loss: inf grad: 3449.8841396893467
iteration: 0 loss: inf grad: 3327.050935869292
iteration: 0 loss: inf grad: 3902.663467018029
iteration: 0 loss: inf grad: 3782.8725411959554
iteration: 0 loss: inf grad: 3404.0391687773426
iteration: 0 loss: inf grad: 3133.7292969067767
iteration: 10 loss: 0.00024092226522043347 grad: -0.36115163293577807
iteration: 0 loss: inf grad: 4123.578841956024
iteration: 0 loss: inf grad: 3592.550867993784
iteration: 0 loss: inf grad: 3396.4033070955015
iteration: 0 loss: inf grad: 3521.5192563032106
iteration: 0 loss: inf grad: 3629.286972775542
iteration: 0 loss: inf grad: 3533.0060944384068
iteration: 0 loss: inf grad: 3387.231772524581
iteration: 0 loss: inf grad: 3907.7099785669284
iteration: 0 loss: inf grad: 3228.8290030938288
iteration: 0 loss: inf grad: 3284.7703767856874
iteration: 0 loss: inf grad: 3442.7871679346626
iteration: 0 loss: 7417.932071696309 grad: 2987.437113340512
iteration: 0 loss: inf grad: 4253.329251079022
iteration: 0 loss: inf grad: 3168.4247923582366
iteration: 0 loss: inf grad: 3805.6915351550037
iteration: 0 loss: inf grad: 3520.1861148871767
iteration: 0 loss: inf grad: 3098.0880503024755
iteration: 0 loss: 10649.597146224482 grad: 3341.6835814364067
iteration: 0 loss: inf grad: 3690.2208011500666
iteration: 0 loss: inf grad: 3710.199261886187
iteration: 0 loss: inf grad: 3827.5942477642757
iteration: 0 loss: inf grad: 3439.350916072357
iteration: 0 loss: inf grad: 3925.311115405161
iteration: 0 loss: inf grad: 3516.548443633674
iteration: 0 loss: 10217.475190382875 grad: 3176.760234768066
iteration: 0 loss: inf grad: 3423.6893165740853
iteration: 0 loss: 10335.933363911465 grad: 3317.5504866023675
iteration: 0 loss: inf grad: 3512.3934336044676
iteration: 0 loss: inf grad: 3982.872745904221
iteration: 0 loss: 11982.510966743544 grad: 3375.776078831465
iteration: 0 loss: inf grad: 3729.2189643551123
iteration: 0 loss: inf grad: 3928.984375191765
iteration: 0 loss: inf grad: 2920.9551308865293
iteration: 0 loss: inf grad: 3210.2574707173308
iteration: 0 loss: inf grad: 3477.9081069939507
iteration: 0 loss: inf grad: 3397.0976008624993
iteration: 0 loss: 9706.35065927515 grad: 3062.5714950790343
iteration: 0 loss: inf grad: 3741.2328502240875
iteration: 0 loss: inf grad: 3427.401046492393
iteration: 0 loss: inf grad: 3646.3279684369004
iteration: 0 loss: inf grad: 3719.197623446125
iteration: 0 loss: inf grad: 4136.41919305332
iteration: 0 loss: inf grad: 3194.804209183486
iteration: 0 loss: inf grad: 3697.6062970513476
iteration: 0 loss: inf grad: 3327.306933443425
iteration: 10 loss: 0.00023844841846518895 grad: 0.0985791939242904
iteration: 0 loss: 9096.54367856345 grad: 3085.747906262173
iteration: 0 loss: inf grad: 3119.3089008117195
iteration: 0 loss: inf grad: 3097.1500364330327
iteration: 0 loss: inf grad: 3598.0141390022836
iteration: 0 loss: inf grad: 3645.579838497035
iteration: 0 loss: inf grad: 3949.8826384546373
iteration: 0 loss: inf grad: 3770.7248197067156
iteration: 0 loss: inf grad: 3627.4415003236804
iteration: 0 loss: inf grad: 3771.873957453191
iteration: 0 loss: inf grad: 3779.1853747011864
iteration: 0 loss: inf grad: 3285.854988945752
iteration: 0 loss: inf grad: 3885.821579292165
iteration: 0 loss: inf grad: 3477.7666216152174
iteration: 0 loss: inf grad: 2976.1012134168227
iteration: 0 loss: 7927.695340561817 grad: 2836.53716410134
iteration: 0 loss: inf grad: 3102.293408257371
iteration: 0 loss: inf grad: 3563.341742866799
iteration: 0 loss: inf grad: 3341.342766355355
iteration: 0 loss: inf grad: 3246.5614253985996
iteration: 0 loss: inf grad: 3143.402979737259
iteration: 0 loss: inf grad: 3936.3307035112957
iteration: 0 loss: inf grad: 3570.306327925544
iteration: 0 loss: inf grad: 3451.6034526772764
iteration: 0 loss: inf grad: 3424.02949148875
iteration: 0 loss: 9452.774185727629 grad: 2951.5688091282645
iteration: 0 loss: inf grad: 3534.114543477719
iteration: 0 loss: inf grad: 3264.5886137658485
iteration: 0 loss: inf grad: 3437.786690320105
iteration: 0 loss: inf grad: 4230.650604254514
iteration: 0 loss: inf grad: 3099.4082324109413
iteration: 0 loss: inf grad: 2990.514202291866
iteration: 0 loss: inf grad: 3146.4973660499245
iteration: 0 loss: inf grad: 3505.7924406310394
iteration: 0 loss: inf grad: 3596.638718995626
iteration: 0 loss: inf grad: 3834.6504549402307
iteration: 0 loss: inf grad: 3762.6350875058492
iteration: 0 loss: inf grad: 3620.455254251055
iteration: 0 loss: 13788.015618157988 grad: 3442.76745263781
iteration: 10 loss: 0.00023728950129059905 grad: -4.3400342499353936e-05
iteration: 0 loss: inf grad: 3882.214899013863
iteration: 0 loss: inf grad: 3620.363216900715
iteration: 0 loss: inf grad: 3396.7777885894075
iteration: 0 loss: inf grad: 3412.9811310677615
iteration: 0 loss: inf grad: 3349.107898123888
iteration: 0 loss: inf grad: 3886.1096312328846
iteration: 0 loss: inf grad: 3426.462892148594
iteration: 10 loss: 0.00021209080163812772 grad: 0.002946055908087346
iteration: 0 loss: inf grad: 3266.5636747510043
iteration: 0 loss: inf grad: 2919.1061342388402
iteration: 10 loss: 0.0002448581349612637 grad: -0.0016196933808236167
iteration: 0 loss: inf grad: 3724.4666049265747
iteration: 0 loss: inf grad: 3621.6007466997958
iteration: 0 loss: inf grad: 3573.8692325507973
iteration: 0 loss: inf grad: 3709.468077948143
iteration: 0 loss: inf grad: 3359.5990622569234
iteration: 0 loss: inf grad: 3504.296565931682
iteration: 0 loss: inf grad: 3897.986324103189
iteration: 0 loss: inf grad: 3326.39065104762
iteration: 0 loss: inf grad: 3558.153355290181
iteration: 0 loss: inf grad: 3399.1499432454248
iteration: 0 loss: inf grad: 3915.945185810711
iteration: 0 loss: inf grad: 3710.8708996768855
iteration: 0 loss: inf grad: 3086.924576668175
iteration: 0 loss: 8740.448003807292 grad: 2987.8756002417076
iteration: 0 loss: inf grad: 3681.0189341070977
iteration: 0 loss: inf grad: 2868.6173783285126
iteration: 0 loss: inf grad: 2901.962723900339
iteration: 0 loss: inf grad: 2969.737152232399
iteration: 0 loss: inf grad: 3779.2205247119687
iteration: 0 loss: inf grad: 3243.102294818194
iteration: 10 loss: 0.0180369743056045 grad: 1.0836233190149251
iteration: 0 loss: inf grad: 3765.735805786999
iteration: 0 loss: inf grad: 3773.4216161743643
iteration: 0 loss: inf grad: 4039.6085704930147
iteration: 0 loss: inf grad: 3203.659960120691
iteration: 0 loss: inf grad: 3245.110243287669
iteration: 0 loss: inf grad: 3642.1615200620636
iteration: 0 loss: inf grad: 3529.849117569322
iteration: 0 loss: inf grad: 4086.4335335032342
iteration: 0 loss: inf grad: 3458.410486423962
iteration: 0 loss: inf grad: 3908.9995304101258
iteration: 0 loss: inf grad: 3435.0907703243556
iteration: 0 loss: 9730.179916029963 grad: 3235.4666237885
iteration: 0 loss: inf grad: 3836.6750152959357
iteration: 0 loss: inf grad: 3218.028422038016
iteration: 0 loss: inf grad: 3902.6403282042374
iteration: 0 loss: inf grad: 3061.2981543964133
iteration: 0 loss: inf grad: 3715.171877535097
iteration: 0 loss: inf grad: 3036.1957322833296
iteration: 0 loss: inf grad: 3693.252147210729
iteration: 0 loss: inf grad: 3299.278931532066
iteration: 0 loss: inf grad: 3432.936452967424
iteration: 0 loss: inf grad: 3429.783965034122
iteration: 0 loss: 7977.052114874515 grad: 2922.3011898514224
iteration: 0 loss: inf grad: 3548.084641522035
iteration: 0 loss: inf grad: 3590.9582447209523
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:57: RuntimeWarning: invalid value encountered in double_scalars
  diff = np.absolute(f_history[-1]-f_history[-2])
iteration: 0 loss: 393.4639401766402 grad: 837.9572668725273
iteration: 10 loss: 0.2284308822646298 grad: 0.7466673887159518
iteration: 20 loss: 0.12551638090223027 grad: 0.31253782755448667
iteration: 0 loss: 291.43230890475184 grad: 783.5528002366945
iteration: 10 loss: 0.26145560581459326 grad: 0.45342500455362034
iteration: 20 loss: 0.15370718658645768 grad: 0.19937061606675777
iteration: 0 loss: 309.07564419533065 grad: 792.894564973851
iteration: 10 loss: 0.24798159455977756 grad: 0.5081747511393433
iteration: 20 loss: 0.1464250347378931 grad: 0.22121072417746754
iteration: 0 loss: 448.0327963578891 grad: 872.7127768101611
iteration: 10 loss: 0.2234894183126347 grad: 0.6244684147884317
iteration: 20 loss: 0.12512367445693104 grad: 0.24748393674139885
iteration: 0 loss: 263.200203594821 grad: 756.5749415937805
iteration: 10 loss: 0.2513779132684497 grad: 0.29986678670025335
iteration: 20 loss: 0.15390266516938442 grad: 0.11187858396240778
iteration: 0 loss: 185.89251601012867 grad: 670.4564862275122
iteration: 10 loss: 0.27295866179396067 grad: 0.45777209574293587
iteration: 20 loss: 0.16704651395998968 grad: 0.22808286560158367
iteration: 0 loss: 274.46531535337436 grad: 779.2854996714415
iteration: 10 loss: 0.23521846375032593 grad: 0.28928626447800665
iteration: 20 loss: 0.14420435468987722 grad: 0.14085272718593755
iteration: 0 loss: 134.45964703192269 grad: 635.3241712763529
iteration: 10 loss: 0.33268721244902816 grad: 0.32148681656716005
iteration: 20 loss: 0.2135534959412656 grad: 0.16981365936021858
iteration: 0 loss: 453.08439038966736 grad: 818.4944062643689
iteration: 10 loss: 0.25441924200905225 grad: 0.6231262824680581
iteration: 20 loss: 0.1496312483905058 grad: 0.2347466731955489
iteration: 0 loss: 267.48049479302466 grad: 761.2498403957612
iteration: 10 loss: 0.23924475602093895 grad: 0.38751875330589436
iteration: 20 loss: 0.14668072415290886 grad: 0.1706659783782901
iteration: 0 loss: 152.4514042121596 grad: 692.4012699753017
iteration: 10 loss: 0.28870550934599043 grad: 0.3499404501124602
iteration: 20 loss: 0.18449964278079278 grad: 0.1669658372684586
iteration: 0 loss: 312.64597917188564 grad: 818.7995303103393
iteration: 10 loss: 0.2465868619006373 grad: 0.528889577526968
iteration: 20 loss: 0.1431677137195286 grad: 0.21242591511320136
iteration: 0 loss: 259.9624038380884 grad: 690.4788735546581
iteration: 10 loss: 0.25596532573738595 grad: 0.3328819512074441
iteration: 20 loss: 0.15784406518179656 grad: 0.1687881979081722
iteration: 0 loss: 236.8786661022895 grad: 771.6968583128034
iteration: 10 loss: 0.24881992829713537 grad: 0.2969150244609487
iteration: 20 loss: 0.15213616534410077 grad: 0.11688704567737226
iteration: 0 loss: 338.74761791365256 grad: 785.7473336885901
iteration: 10 loss: 0.22716107454752174 grad: 0.44816228778823985
iteration: 20 loss: 0.1354204232320626 grad: 0.19036996549681082
iteration: 0 loss: 452.2928155886255 grad: 893.1257756610565
iteration: 10 loss: 0.24084255312812852 grad: 0.8022510698450012
iteration: 20 loss: 0.13095101769563786 grad: 0.2911948701760285
iteration: 0 loss: 291.4465553786042 grad: 764.0621101520668
iteration: 10 loss: 0.24805537084086443 grad: 0.3609324994787134
iteration: 20 loss: 0.14867359189371532 grad: 0.14640617720825994
iteration: 0 loss: 204.3200671637541 grad: 708.3681454867154
iteration: 10 loss: 0.2699318926954881 grad: 0.33940093004909516
iteration: 20 loss: 0.16831989338267953 grad: 0.14539599797122804
iteration: 0 loss: 537.8928762486597 grad: 896.0264504932322
iteration: 10 loss: 0.2526881684516411 grad: 0.6339835939876073
iteration: 20 loss: 0.13267725018092527 grad: 0.24797676085849046
iteration: 0 loss: 236.19045841760098 grad: 718.310769505463
iteration: 10 loss: 0.2723728149540792 grad: 0.23452885272336005
iteration: 20 loss: 0.16657130602991607 grad: 0.13470303002010658
iteration: 0 loss: 333.8729000232946 grad: 817.1199222847135
iteration: 10 loss: 0.22969872869444688 grad: 0.33368577183343917
iteration: 20 loss: 0.13687166601977713 grad: 0.14626317681032328
iteration: 0 loss: 382.21303701683263 grad: 851.6183408636964
iteration: 10 loss: 0.24881172631004037 grad: 0.7576094881411477
iteration: 20 loss: 0.13701493042902835 grad: 0.3051273590452549
iteration: 0 loss: 261.29329932852585 grad: 682.2644881456044
iteration: 10 loss: 0.24548327651973523 grad: 0.47546288007602916
iteration: 20 loss: 0.15294172713716267 grad: 0.20309076472196225
iteration: 0 loss: 202.55637420226728 grad: 754.8996719400261
iteration: 10 loss: 0.2771519171069072 grad: 0.4249779816923744
iteration: 20 loss: 0.167410237196567 grad: 0.15904919100620374
iteration: 0 loss: 259.8872910200495 grad: 790.3398407888544
iteration: 10 loss: 0.23153671650189525 grad: 0.3935623065693152
iteration: 20 loss: 0.1398348861659377 grad: 0.15638490105764052
iteration: 0 loss: 248.538646589169 grad: 761.9284136799834
iteration: 10 loss: 0.2627188680709713 grad: 0.312904827107521
iteration: 20 loss: 0.15734205287778671 grad: 0.1387000589977341
iteration: 0 loss: 589.6624039361966 grad: 900.4882399999898
iteration: 10 loss: 0.23785622647102916 grad: 0.8064599210310422
iteration: 20 loss: 0.12368753461669257 grad: 0.316409093313499
iteration: 0 loss: 533.8367466344838 grad: 869.0162141697768
iteration: 10 loss: 0.2988047365566176 grad: 0.7447969742455833
iteration: 20 loss: 0.14393541913374397 grad: 0.29632375598366256
iteration: 0 loss: 255.39108132930372 grad: 781.9730975171108
iteration: 10 loss: 0.24868055411835402 grad: 0.3566669817661807
iteration: 20 loss: 0.1506415006598135 grad: 0.12882107897973238
iteration: 0 loss: 229.66846150587372 grad: 717.9983911748056
iteration: 10 loss: 0.26479263589251123 grad: 0.32269427169684795
iteration: 20 loss: 0.16147587371200312 grad: 0.14673968412572402
iteration: 0 loss: 655.3174411531307 grad: 951.4896407313274
iteration: 10 loss: 0.22660668083600202 grad: 0.5968943881585316
iteration: 20 loss: 0.11402182561522006 grad: 0.24000223959403155
iteration: 0 loss: 285.4236269591216 grad: 825.5703611501713
iteration: 10 loss: 0.23031020595755597 grad: 0.10570839138144411
iteration: 20 loss: 0.13966135754526476 grad: 0.03543064687321422
iteration: 0 loss: 276.0153188493217 grad: 776.5981813145493
iteration: 10 loss: 0.2655792066876033 grad: 0.3263427990967863
iteration: 20 loss: 0.1594114722740249 grad: 0.13716500337942075
iteration: 0 loss: 389.1095701328925 grad: 807.8969023916111
iteration: 10 loss: 0.23759446916053342 grad: 0.6579102054656436
iteration: 20 loss: 0.13770948098185312 grad: 0.27611095099963645
iteration: 0 loss: 391.4944098551179 grad: 836.0501826634942
iteration: 10 loss: 0.24799698001085993 grad: 0.5746484614069276
iteration: 20 loss: 0.14166918961398717 grad: 0.23597968094171315
iteration: 0 loss: 316.139204227421 grad: 811.5096404272119
iteration: 10 loss: 0.2619460953233515 grad: 0.7522159147246306
iteration: 20 loss: 0.1503984236410774 grad: 0.28471074897802384
iteration: 0 loss: 317.43608864773233 grad: 777.2696301508506
iteration: 10 loss: 0.23338589496498488 grad: 0.5859494600350412
iteration: 20 loss: 0.13794019753946818 grad: 0.24077537062100607
iteration: 0 loss: 447.16133191323155 grad: 900.3887679518991
iteration: 10 loss: 0.22357878206722406 grad: 0.7113685295214587
iteration: 20 loss: 0.12053237531736985 grad: 0.29051269474659125
iteration: 0 loss: 242.00892970445685 grad: 738.599480837488
iteration: 10 loss: 0.25351317031196563 grad: 0.45644884631196947
iteration: 20 loss: 0.1554466228170151 grad: 0.22239205608614465
iteration: 0 loss: 238.70344680140295 grad: 753.4277527784219
iteration: 10 loss: 0.24738687061901807 grad: 0.2890571711015676
iteration: 20 loss: 0.1512655863045273 grad: 0.11345343388287224
iteration: 0 loss: 389.4467628611376 grad: 790.9680606808715
iteration: 10 loss: 0.2544040259829423 grad: 0.6992787258378457
iteration: 20 loss: 0.14580946645781462 grad: 0.29989487265509357
iteration: 0 loss: 148.3175744700349 grad: 681.7027375318539
iteration: 10 loss: 0.3127073257870837 grad: 0.2516501074128463
iteration: 20 loss: 0.1981104026493664 grad: 0.1064442123595043
iteration: 0 loss: 715.6635977884804 grad: 982.7924431272343
iteration: 10 loss: 0.23451642212851517 grad: 0.6305252221757789
iteration: 20 loss: 0.11383878405550521 grad: 0.20518693940479368
iteration: 0 loss: 311.71425823052556 grad: 725.4125433766104
iteration: 10 loss: 0.25225359377360623 grad: 0.49329580306300996
iteration: 20 loss: 0.1491191376298957 grad: 0.2206685348430173
iteration: 0 loss: 332.89927039062934 grad: 875.8146821324084
iteration: 10 loss: 0.23269931736335406 grad: 0.4081733947298272
iteration: 20 loss: 0.13449193776349685 grad: 0.17894092288066962
iteration: 0 loss: 446.0524154702098 grad: 807.1752890553223
iteration: 10 loss: 0.2322166886171379 grad: 0.8072385247438238
iteration: 20 loss: 0.13066188328362346 grad: 0.35013617424105736
iteration: 0 loss: 359.1090454140382 grad: 705.0203289026526
iteration: 10 loss: 0.25637383846829337 grad: 0.5460291108136328
iteration: 20 loss: 0.15215157702666396 grad: 0.24103789771329365
iteration: 0 loss: 237.53016581033054 grad: 766.3364357460006
iteration: 10 loss: 0.2526450638738011 grad: 0.3334965310838375
iteration: 20 loss: 0.15466657656559785 grad: 0.14085120352886796
iteration: 0 loss: 424.2591375239506 grad: 850.1904170399898
iteration: 10 loss: 0.23665922127011899 grad: 0.7886669032937206
iteration: 20 loss: 0.13257246301476366 grad: 0.326990577425454
iteration: 0 loss: 465.0990519112019 grad: 854.355977921253
iteration: 10 loss: 0.25315808602962614 grad: 0.8695555527457168
iteration: 20 loss: 0.13904719492976172 grad: 0.34021865268398144
iteration: 0 loss: 510.16482631064406 grad: 882.0920401196482
iteration: 10 loss: 0.26244643248537614 grad: 0.8706675116732212
iteration: 20 loss: 0.13767427282172404 grad: 0.2475443058326967
iteration: 0 loss: 252.5748375541096 grad: 790.6325458903466
iteration: 10 loss: 0.24090542230128034 grad: 0.491945421643496
iteration: 20 loss: 0.14295937827046146 grad: 0.21010432972355206
iteration: 0 loss: 442.4192321330566 grad: 903.0189438634764
iteration: 10 loss: 0.23455961252032217 grad: 0.5654990202226722
iteration: 20 loss: 0.13031585423033737 grad: 0.22961327140278942
iteration: 0 loss: 357.896999114705 grad: 807.6911899115661
iteration: 10 loss: 0.23942535668621448 grad: 0.5685641703523466
iteration: 20 loss: 0.14023160737931242 grad: 0.23400423012081162
iteration: 0 loss: 204.31735451587602 grad: 726.7411944458341
iteration: 10 loss: 0.25236308442650957 grad: -0.13440859464995267
iteration: 20 loss: 0.16104950595317033 grad: -0.05891432673761133
iteration: 0 loss: 286.91098998490145 grad: 785.6693078177035
iteration: 10 loss: 0.25061162115558877 grad: 0.5734725159630156
iteration: 20 loss: 0.14883021983818748 grad: 0.24846034778360504
iteration: 0 loss: 180.66040763262671 grad: 759.951065892274
iteration: 10 loss: 0.26322357638466864 grad: 0.004020468074243236
iteration: 20 loss: 0.1668367207848033 grad: -0.01898961631749667
iteration: 0 loss: 328.7384204131693 grad: 806.8301974725879
iteration: 10 loss: 0.22740226212326062 grad: 0.568955942315338
iteration: 20 loss: 0.13369142997874983 grad: 0.22101174426543174
iteration: 0 loss: 529.2095308200611 grad: 918.4848979197934
iteration: 10 loss: 0.22123202861687538 grad: 0.7566226404124508
iteration: 20 loss: 0.1195495854815156 grad: 0.3125722648028098
iteration: 0 loss: 259.1163948388426 grad: 774.035428294959
iteration: 10 loss: 0.2455647877691676 grad: 0.14600340539026516
iteration: 20 loss: 0.15014382799398102 grad: 0.06621037669067045
iteration: 0 loss: 424.06765993143 grad: 856.7411985059737
iteration: 10 loss: 0.22299934126352705 grad: 0.8763979344469616
iteration: 20 loss: 0.12086834218991332 grad: 0.3234947884939156
iteration: 0 loss: 407.3744904136582 grad: 905.8835279636181
iteration: 10 loss: 0.2261674519208596 grad: 0.3734247410622893
iteration: 20 loss: 0.12785277995267694 grad: 0.1482830574778905
iteration: 0 loss: 211.9161912058872 grad: 664.3020587660518
iteration: 10 loss: 0.2601821495699452 grad: 0.2809837262817098
iteration: 20 loss: 0.1624892780478562 grad: 0.15094431179991138
iteration: 0 loss: 191.66021766150897 grad: 733.2718305972834
iteration: 10 loss: 0.27158670342561725 grad: 0.314400175761959
iteration: 20 loss: 0.16970026955285986 grad: 0.14330919956610733
iteration: 0 loss: 274.5882136648221 grad: 799.0638037184203
iteration: 10 loss: 0.23988319281042178 grad: 0.11255020978206637
iteration: 20 loss: 0.14430357468600877 grad: 0.06896117484189865
iteration: 0 loss: 292.42801023729083 grad: 780.3570152543494
iteration: 10 loss: 0.24052191288534563 grad: 0.4891606993670983
iteration: 20 loss: 0.14529959045665403 grad: 0.2105404296869065
iteration: 0 loss: 239.62362735976694 grad: 698.1540506248633
iteration: 10 loss: 0.23988302290044058 grad: 0.3187864626451063
iteration: 20 loss: 0.15157374796697035 grad: 0.13794832921265032
iteration: 0 loss: 393.86757135297506 grad: 861.2790136628892
iteration: 10 loss: 0.24239677139815863 grad: 0.7180439732855131
iteration: 20 loss: 0.13506346581925754 grad: 0.2831324210154999
iteration: 0 loss: 394.60409794037474 grad: 786.7670546957625
iteration: 10 loss: 0.2516351248648954 grad: 0.7134767271617706
iteration: 20 loss: 0.14436720622093577 grad: 0.2864958560552776
iteration: 0 loss: 313.68657050038144 grad: 839.0624290200975
iteration: 10 loss: 0.23981574684859713 grad: 0.452493264212693
iteration: 20 loss: 0.13861961615244087 grad: 0.19858986918181823
iteration: 0 loss: 355.55503709457025 grad: 854.5588072645694
iteration: 10 loss: 0.239368310558017 grad: 0.5259733798018638
iteration: 20 loss: 0.1364665571772176 grad: 0.22520581898472186
iteration: 0 loss: 623.584253155932 grad: 955.4849492476078
iteration: 10 loss: 0.2615333409756369 grad: 0.9412549610379121
iteration: 20 loss: 0.12513079509021321 grad: 0.3098209187002128
iteration: 0 loss: 264.9454196101771 grad: 731.4890542342878
iteration: 10 loss: 0.27025708186920383 grad: 0.4341609824855955
iteration: 20 loss: 0.1653875767948524 grad: 0.1841161454992539
iteration: 0 loss: 390.4226774583856 grad: 850.5579139808688
iteration: 10 loss: 0.24225517278930717 grad: 0.6087127061320365
iteration: 20 loss: 0.13682995717119245 grad: 0.26639726898969907
iteration: 0 loss: 270.08357785760165 grad: 763.0543931741609
iteration: 10 loss: 0.25725360746301623 grad: 0.29107567250916877
iteration: 20 loss: 0.1544415604497441 grad: 0.10941200644783453
iteration: 0 loss: 210.40349133762655 grad: 705.7188119525024
iteration: 10 loss: 0.2511292172103739 grad: 0.32784043198575663
iteration: 20 loss: 0.15579361031475195 grad: 0.14973461342895583
iteration: 0 loss: 287.86523566830493 grad: 713.2514942866169
iteration: 10 loss: 0.24073695122007682 grad: 0.6286945364109758
iteration: 20 loss: 0.14558017958419936 grad: 0.28585669614134607
iteration: 0 loss: 198.67670249229957 grad: 708.9886523438772
iteration: 10 loss: 0.27017282242914836 grad: 0.31857119959137314
iteration: 20 loss: 0.16775882970870787 grad: 0.1503249594714303
iteration: 0 loss: 290.39785540015345 grad: 827.4833497021905
iteration: 10 loss: 0.23543716823058441 grad: 0.2423452424257408
iteration: 20 loss: 0.14002512252740085 grad: 0.09289497314776696
iteration: 0 loss: 473.23136922888193 grad: 837.8018277972798
iteration: 10 loss: 0.24160159894943883 grad: 0.6821121660936866
iteration: 20 loss: 0.1343431883140676 grad: 0.2722078121870826
iteration: 0 loss: 487.9100392843882 grad: 911.5408824891772
iteration: 10 loss: 0.21021172209326655 grad: 0.7231996875964242
iteration: 20 loss: 0.1140909391094531 grad: 0.28171428325174686
iteration: 0 loss: 476.3963768032943 grad: 867.9399447692576
iteration: 10 loss: 0.23005589182503172 grad: 0.7077185474454397
iteration: 20 loss: 0.12636706831935127 grad: 0.2803230794608968
iteration: 0 loss: 355.4628608254059 grad: 833.102762485462
iteration: 10 loss: 0.2386706733223863 grad: 0.4397774383175598
iteration: 20 loss: 0.1396828741521445 grad: 0.1948891524584927
iteration: 0 loss: 494.108203228532 grad: 867.7438215788388
iteration: 10 loss: 0.2193929246983647 grad: 0.5896438088599485
iteration: 20 loss: 0.12134150164572267 grad: 0.2596596260738845
iteration: 0 loss: 473.3274381618545 grad: 871.4774512632549
iteration: 10 loss: 0.23678331767306107 grad: 0.5882912981199137
iteration: 20 loss: 0.12946372476703386 grad: 0.2532688245408806
iteration: 0 loss: 434.6566871964506 grad: 754.8645021459749
iteration: 10 loss: 0.23759801175550488 grad: 0.4750828104805752
iteration: 20 loss: 0.13992914591951625 grad: 0.22373762178730167
iteration: 0 loss: 654.2990237408405 grad: 894.0153811421827
iteration: 10 loss: 0.2428500587281054 grad: 0.5307048773606179
iteration: 20 loss: 0.12797691781485232 grad: 0.24058607624335984
iteration: 0 loss: 454.10988428533693 grad: 798.5703737984203
iteration: 10 loss: 0.24717026498431302 grad: 0.6556164386090115
iteration: 20 loss: 0.13956610845509038 grad: 0.26942203768358225
iteration: 0 loss: 184.79323252165133 grad: 681.0463881422986
iteration: 10 loss: 0.28359516362053744 grad: 0.2726405335785129
iteration: 20 loss: 0.17791348280818717 grad: 0.13769264607018553
iteration: 0 loss: 181.81888894736701 grad: 643.4661826450592
iteration: 10 loss: 0.2880409139051236 grad: 0.24719419608966742
iteration: 20 loss: 0.18073668616133629 grad: 0.1300142737685062
iteration: 0 loss: 267.78029160809587 grad: 708.5437958931143
iteration: 10 loss: 0.2441378675175656 grad: 0.22818218075494237
iteration: 20 loss: 0.1506445140853843 grad: 0.11620299367408309
iteration: 0 loss: 331.3989035603417 grad: 819.095078757489
iteration: 10 loss: 0.24115697757125334 grad: 0.1601814002254529
iteration: 20 loss: 0.1452015941727454 grad: 0.05778926414134462
iteration: 0 loss: 300.48779951977104 grad: 766.51136086366
iteration: 10 loss: 0.2331928604762652 grad: 0.5320885552268346
iteration: 20 loss: 0.14011326154385725 grad: 0.2587325345136845
iteration: 0 loss: 232.69424536916415 grad: 741.7042300744462
iteration: 10 loss: 0.25743930774495477 grad: 0.22800500484925024
iteration: 20 loss: 0.15768404182397883 grad: 0.09602088885438659
iteration: 0 loss: 204.49923579537474 grad: 718.5410795753189
iteration: 10 loss: 0.2557046815581998 grad: 0.10105162359247666
iteration: 20 loss: 0.16186120516150285 grad: 0.045550383881064235
iteration: 0 loss: 447.678301709103 grad: 909.154013223347
iteration: 10 loss: 0.21366332236787208 grad: 0.398087741340534
iteration: 20 loss: 0.11889485621494797 grad: 0.18187733788917898
iteration: 0 loss: 385.8826845991201 grad: 821.5038804198282
iteration: 10 loss: 0.24716462498095512 grad: 0.7240671605405036
iteration: 20 loss: 0.1394111413628707 grad: 0.310555446156743
iteration: 0 loss: 401.3263276445236 grad: 792.6254890236617
iteration: 10 loss: 0.2405895824371824 grad: 0.48389875473780225
iteration: 20 loss: 0.13936357555625858 grad: 0.20710594532317053
iteration: 0 loss: 296.14568541247246 grad: 785.8221550630178
iteration: 10 loss: 0.238577011603907 grad: 0.33980827046534096
iteration: 20 loss: 0.1429162268723303 grad: 0.15305562868562525
iteration: 0 loss: 264.1493770983197 grad: 673.6182978265854
iteration: 10 loss: 0.2546558637399027 grad: 0.23795638254541307
iteration: 20 loss: 0.15865846462673455 grad: 0.11789077397946522
iteration: 0 loss: 369.4641895482774 grad: 810.9720870041052
iteration: 10 loss: 0.2246983849203058 grad: 0.5068189341918202
iteration: 20 loss: 0.13023918995068842 grad: 0.20630396100833723
iteration: 0 loss: 254.39608593056107 grad: 749.1799856273459
iteration: 10 loss: 0.2580701270842176 grad: 0.2081226565766583
iteration: 20 loss: 0.1563218998665186 grad: 0.0899140814256579
iteration: 0 loss: 231.40595468550882 grad: 787.2794323062244
iteration: 10 loss: 0.27364889245506097 grad: 0.47478980108941315
iteration: 20 loss: 0.16253463392521636 grad: 0.21493088372486707
iteration: 0 loss: 577.5468884800761 grad: 977.3736735471736
iteration: 10 loss: 0.23264599571749303 grad: 0.6411234163775904
iteration: 20 loss: 0.11778511422446834 grad: 0.2301093534330968
iteration: 0 loss: 194.85375365539377 grad: 709.0008096894378
iteration: 10 loss: 0.2648528183548148 grad: 0.21973060466597394
iteration: 20 loss: 0.16650919378133205 grad: 0.09741353053006321
iteration: 0 loss: 177.34806475299362 grad: 682.1742005565967
iteration: 10 loss: 0.27961130181707133 grad: 0.25725054729729724
iteration: 20 loss: 0.1770688092871172 grad: 0.13446791805812977
iteration: 0 loss: 222.48042225999018 grad: 720.1181332122997
iteration: 10 loss: 0.25810887467502075 grad: 0.09805864145677068
iteration: 20 loss: 0.1597823454288697 grad: 0.03965626340263788
iteration: 0 loss: 259.655368148114 grad: 803.1222168676896
iteration: 10 loss: 0.252834868096298 grad: 0.3002232007103485
iteration: 20 loss: 0.15138082385072432 grad: 0.13255674161704994
iteration: 0 loss: 374.9794983514876 grad: 824.2369829640008
iteration: 10 loss: 0.2215283846200526 grad: 0.4946730310318974
iteration: 20 loss: 0.12496995856632896 grad: 0.20586008537752057
iteration: 0 loss: 388.24389024252196 grad: 882.8246426594442
iteration: 10 loss: 0.23059334819018576 grad: 0.4885154610597219
iteration: 20 loss: 0.13248951844566778 grad: 0.19443542239324518
iteration: 0 loss: 447.0898075738984 grad: 867.7808802654871
iteration: 10 loss: 0.25377363122143753 grad: 0.8604256482640716
iteration: 20 loss: 0.12991851458437767 grad: 0.3219292505902779
iteration: 0 loss: 349.07777276990123 grad: 832.2769329198899
iteration: 10 loss: 0.25745958157197246 grad: 0.7580009747139929
iteration: 20 loss: 0.14164623632231352 grad: 0.2959525310019954
iteration: 0 loss: 306.3524638501918 grad: 790.3183504361043
iteration: 10 loss: 0.23993643691590644 grad: 0.5498150401319861
iteration: 20 loss: 0.14315862631304876 grad: 0.22850747407084063
iteration: 0 loss: 522.90667554885 grad: 894.3840165819252
iteration: 10 loss: 0.2392628856268857 grad: 0.6460972990763294
iteration: 20 loss: 0.12642149370301262 grad: 0.21802124317217636
iteration: 0 loss: 409.06950105876587 grad: 831.6790118861971
iteration: 10 loss: 0.24060830789924445 grad: 0.5642732769521048
iteration: 20 loss: 0.1365121511994816 grad: 0.25402901975243075
iteration: 0 loss: 195.60007654414005 grad: 780.343914801175
iteration: 10 loss: 0.25384431956345826 grad: 0.23334100757850618
iteration: 20 loss: 0.15585360295600284 grad: 0.0947575446969472
iteration: 0 loss: 304.4736192910559 grad: 782.985648746346
iteration: 10 loss: 0.22819612439161574 grad: 0.2518768268249263
iteration: 20 loss: 0.1376225186332621 grad: 0.11346190850132455
iteration: 0 loss: 262.44061369187733 grad: 767.9754146996389
iteration: 10 loss: 0.2644930372400929 grad: 0.5193764899345577
iteration: 20 loss: 0.15695287194665236 grad: 0.2110369905715378
iteration: 0 loss: 423.2347484656095 grad: 896.4329265370866
iteration: 10 loss: 0.24913154959408115 grad: 0.574269971761195
iteration: 20 loss: 0.13137508749621868 grad: 0.19882702696785226
iteration: 0 loss: 223.94921548975353 grad: 786.5504399595402
iteration: 10 loss: 0.25461502983109147 grad: 0.2671917142597963
iteration: 20 loss: 0.15645629141909012 grad: 0.10581775942630753
iteration: 0 loss: 311.6492649360858 grad: 747.3680243671358
iteration: 10 loss: 0.2599206032907453 grad: 0.5020160964912902
iteration: 20 loss: 0.15782946017679933 grad: 0.2192684024703711
iteration: 0 loss: 158.76341819096325 grad: 666.1481731165493
iteration: 10 loss: 0.30961835671173915 grad: 0.46656215980448523
iteration: 20 loss: 0.19295205650343467 grad: 0.21569715912290757
iteration: 0 loss: 382.57153574569014 grad: 857.3602462783476
iteration: 10 loss: 0.22054557712511846 grad: 0.7561756739587364
iteration: 20 loss: 0.1280920172723034 grad: 0.3075628869617966
iteration: 0 loss: 334.42434867364483 grad: 831.7638302931897
iteration: 10 loss: 0.24959115161569076 grad: 0.4653247002928439
iteration: 20 loss: 0.14436426947494593 grad: 0.1906199757136291
iteration: 0 loss: 331.8814614286739 grad: 819.7384584581242
iteration: 10 loss: 0.24205779552804332 grad: 0.5022954463435424
iteration: 20 loss: 0.14165166483375782 grad: 0.21653110724009494
iteration: 0 loss: 563.4597595032023 grad: 852.3701415186409
iteration: 10 loss: 0.2406938566215434 grad: 0.5195694673577085
iteration: 20 loss: 0.13490435406691342 grad: 0.21949071921851857
iteration: 0 loss: 246.7359243898312 grad: 770.7409983600787
iteration: 10 loss: 0.2503248153306314 grad: 0.31065887239358253
iteration: 20 loss: 0.1515386882530782 grad: 0.1356187592300872
iteration: 0 loss: 315.5922424848234 grad: 804.0842170463803
iteration: 10 loss: 0.2282851593013633 grad: 0.5780317000290298
iteration: 20 loss: 0.13544788661205198 grad: 0.2522991473383697
iteration: 0 loss: 564.823164248966 grad: 899.3992020767901
iteration: 10 loss: 0.2358911692425119 grad: 0.6864170558069524
iteration: 20 loss: 0.12762822806590438 grad: 0.27891844842316693
iteration: 0 loss: 312.7670207670989 grad: 762.0357073877241
iteration: 10 loss: 0.25000229451407824 grad: 0.5797753166995296
iteration: 20 loss: 0.14603933604402383 grad: 0.2542280557960182
iteration: 0 loss: 428.29725409582994 grad: 817.5345835760746
iteration: 10 loss: 0.23487627381671866 grad: 0.7038303619505072
iteration: 20 loss: 0.13361599126751986 grad: 0.31086809172870644
iteration: 0 loss: 263.5077573448098 grad: 779.3807509455692
iteration: 10 loss: 0.2624029210717047 grad: 0.4587963013723697
iteration: 20 loss: 0.1560165878356863 grad: 0.1807676886988029
iteration: 0 loss: 472.83134162027784 grad: 902.0738414721869
iteration: 10 loss: 0.2250731128738566 grad: 0.49200841510581134
iteration: 20 loss: 0.12362361332022621 grad: 0.21165693914910355
iteration: 0 loss: 428.2581384278623 grad: 854.7617268541079
iteration: 10 loss: 0.22796590825782914 grad: 0.414110141816035
iteration: 20 loss: 0.13015079830339263 grad: 0.1775996979397932
iteration: 0 loss: 279.41477904744954 grad: 704.5053703111652
iteration: 10 loss: 0.25521348923187115 grad: 0.29181135941616737
iteration: 20 loss: 0.15829096532781312 grad: 0.13095145702578373
iteration: 0 loss: 198.2950196511151 grad: 681.3387734503567
iteration: 10 loss: 0.28256511232005077 grad: 0.24595492865309426
iteration: 20 loss: 0.17841075027271555 grad: 0.10919737569177224
iteration: 0 loss: 281.51101970145925 grad: 846.2876878823375
iteration: 10 loss: 0.23119842984471523 grad: 0.4902856360210676
iteration: 20 loss: 0.13727783829893847 grad: 0.20438964148881975
iteration: 0 loss: 140.83639553187038 grad: 652.1371028532637
iteration: 10 loss: 0.30990733687915717 grad: 0.1624609599588463
iteration: 20 loss: 0.19898120017048562 grad: 0.0687556678143588
iteration: 0 loss: 272.4483490424453 grad: 660.0262136371173
iteration: 10 loss: 0.22811140254088969 grad: 0.4770503577372421
iteration: 20 loss: 0.13590188614861373 grad: 0.21379806393384176
iteration: 0 loss: 208.57475938863064 grad: 676.0109821270053
iteration: 10 loss: 0.27632148411777796 grad: 0.40781293237973293
iteration: 20 loss: 0.17104005605456638 grad: 0.18336307831128185
iteration: 0 loss: 486.15515158833546 grad: 870.9388903697933
iteration: 10 loss: 0.2326687405013879 grad: 0.5901982548321041
iteration: 20 loss: 0.13005501344626033 grad: 0.25008075574821315
iteration: 0 loss: 310.40559916743047 grad: 742.3593078797103
iteration: 10 loss: 0.2469158500678697 grad: 0.5348813935970056
iteration: 20 loss: 0.14686185519547673 grad: 0.23515770292150673
iteration: 0 loss: 444.8599189596316 grad: 866.5305582446751
iteration: 10 loss: 0.23644249046992627 grad: 0.654846027056998
iteration: 20 loss: 0.13009048316201766 grad: 0.252148019930083
iteration: 0 loss: 480.7347032942226 grad: 868.4252610388694
iteration: 10 loss: 0.22647872688304976 grad: 0.6061464609164342
iteration: 20 loss: 0.12876408643883638 grad: 0.2595522563344195
iteration: 0 loss: 528.6497827443717 grad: 933.1779152618094
iteration: 10 loss: 0.2280704274283132 grad: 0.5788899184525347
iteration: 20 loss: 0.1209735110619063 grad: 0.22664133158278527
iteration: 0 loss: 227.00277221479973 grad: 733.8380759098445
iteration: 10 loss: 0.26148475189215376 grad: 0.18918682236759707
iteration: 20 loss: 0.16018746574007475 grad: 0.07770007431779245
iteration: 0 loss: 243.9939717415873 grad: 743.4456844397355
iteration: 10 loss: 0.25751025050108595 grad: 0.33362418919682846
iteration: 20 loss: 0.158508376592925 grad: 0.13722703271560063
iteration: 0 loss: 303.2061798963692 grad: 837.968651137056
iteration: 10 loss: 0.238751102140172 grad: 0.4497008835256948
iteration: 20 loss: 0.13897872708978617 grad: 0.1687073448488268
iteration: 0 loss: 317.1729228790049 grad: 810.0190453200421
iteration: 10 loss: 0.23311701598520979 grad: 0.4797457319164536
iteration: 20 loss: 0.13648011265021606 grad: 0.1933310099953997
iteration: 0 loss: 636.3987356552886 grad: 943.1177754404084
iteration: 10 loss: 0.2250481386287323 grad: 0.7319539361205152
iteration: 20 loss: 0.11399401260148972 grad: 0.2712830904199904
iteration: 0 loss: 243.47143084420506 grad: 792.64422269376
iteration: 10 loss: 0.26330337401056814 grad: 0.409074892743043
iteration: 20 loss: 0.15947926552888186 grad: 0.15879940867753323
iteration: 0 loss: 469.1962811683446 grad: 901.7112403447582
iteration: 10 loss: 0.24683821691409816 grad: 0.7115021901011329
iteration: 20 loss: 0.12838425913700616 grad: 0.31008608302589585
iteration: 0 loss: 318.0203379566597 grad: 786.0235729083963
iteration: 10 loss: 0.2478536064926712 grad: 0.4144414835588244
iteration: 20 loss: 0.14987284746745322 grad: 0.1741664674857441
iteration: 0 loss: 174.64294332294 grad: 741.2460362932799
iteration: 10 loss: 0.26594901915128005 grad: -0.012145941100890881
iteration: 20 loss: 0.16963332902163197 grad: -0.01004952465455088
iteration: 0 loss: 485.22297909713467 grad: 884.3655234289455
iteration: 10 loss: 0.2310455771808351 grad: 0.6238583100142285
iteration: 20 loss: 0.12819367012905952 grad: 0.2580641082018239
iteration: 0 loss: 229.01672742616054 grad: 736.2458428431021
iteration: 10 loss: 0.2751661043421336 grad: 0.5940822768797872
iteration: 20 loss: 0.16621253056142782 grad: 0.2594680812338282
iteration: 0 loss: 544.9142750606676 grad: 899.2881036915828
iteration: 10 loss: 0.2322696898789383 grad: 0.7010205081798757
iteration: 20 loss: 0.12251793493107305 grad: 0.27143506016676544
iteration: 0 loss: 202.98718327316544 grad: 698.0839590058894
iteration: 10 loss: 0.2779506823179955 grad: 0.4269316415940361
iteration: 20 loss: 0.16939511263287635 grad: 0.19530796014553262
iteration: 0 loss: 445.8660710708085 grad: 855.6614487299945
iteration: 10 loss: 0.21659116688748659 grad: 0.5263649340642289
iteration: 20 loss: 0.12310620560327318 grad: 0.2261492325208522
iteration: 0 loss: 216.66339843224458 grad: 692.5616227372616
iteration: 10 loss: 0.26190648677319667 grad: 0.3714945052467223
iteration: 20 loss: 0.16367151994448 grad: 0.18146733511513127
iteration: 0 loss: 326.4119729663413 grad: 849.2306444896774
iteration: 10 loss: 0.24977474706230912 grad: 0.30003301888965506
iteration: 20 loss: 0.14824082777109157 grad: 0.11232352319913874
iteration: 0 loss: 227.64647204100345 grad: 753.8469323094322
iteration: 10 loss: 0.25762996873419813 grad: 0.4884633036835836
iteration: 20 loss: 0.15870918722000418 grad: 0.21841454003833013
iteration: 0 loss: 275.9151853132629 grad: 789.0727303846685
iteration: 10 loss: 0.24557625982858802 grad: 0.5761348661811991
iteration: 20 loss: 0.14469710225454369 grad: 0.24226522128341638
iteration: 0 loss: 347.65078421651634 grad: 786.1842459836271
iteration: 10 loss: 0.23745631719611993 grad: 0.5328124063892239
iteration: 20 loss: 0.14038415113910477 grad: 0.23372115314416475
iteration: 0 loss: 168.55758149229058 grad: 668.0321176317486
iteration: 10 loss: 0.27383579431617283 grad: 0.23681244185888808
iteration: 20 loss: 0.17613504178941433 grad: 0.11244598972613133
iteration: 0 loss: 424.4172426928229 grad: 814.9779009264876
iteration: 10 loss: 0.2313056549039836 grad: 0.6005779902966629
iteration: 20 loss: 0.13222249793331933 grad: 0.24805137497781374
iteration: 0 loss: 389.14145964659707 grad: 825.2881533777363
iteration: 10 loss: 0.2501493965186951 grad: 0.5524694975901748
iteration: 20 loss: 0.14062329532796206 grad: 0.2245323358754731
iteration: 0 loss: 6904.521288462077 grad: 2453.3474209314863
iteration: 10 loss: 0.0010097568566297095 grad: 0.01660931872374973
iteration: 0 loss: 5560.918173770503 grad: 2302.9521737817263
iteration: 0 loss: 6108.236102842429 grad: 2329.5054086304817
iteration: 10 loss: 0.0012869530156339433 grad: 0.003562621920514742
iteration: 0 loss: 7671.389918644872 grad: 2553.594565138183
iteration: 10 loss: 0.001426053623700599 grad: -0.004440761646034798
iteration: 0 loss: 4727.797136639848 grad: 2221.216320509586
iteration: 0 loss: 3219.5469542315445 grad: 1981.6552303283613
iteration: 0 loss: 5106.91165853443 grad: 2292.573947921409
iteration: 0 loss: 2727.853985064458 grad: 1875.8394877663252
iteration: 0 loss: 7617.607748635238 grad: 2401.868798293224
iteration: 10 loss: 0.0013895060930570419 grad: 0.017477474842253607
iteration: 0 loss: 4971.503083203864 grad: 2234.0752147665726
iteration: 0 loss: 3105.6321018465246 grad: 2046.8475916902203
iteration: 0 loss: 6312.673159228961 grad: 2404.0085394928196
iteration: 10 loss: 0.0013337182807101106 grad: -0.3315803915919289
iteration: 0 loss: 4321.010043330801 grad: 2035.4905818338411
iteration: 0 loss: 4652.842567871586 grad: 2272.1027452457743
iteration: 0 loss: 6100.848636385618 grad: 2306.350945134207
iteration: 0 loss: 8853.685164317749 grad: 2611.462052455713
iteration: 10 loss: 0.0021021130259296942 grad: -0.00996949524257703
iteration: 0 loss: 5292.4549347219445 grad: 2242.7028083888945
iteration: 0 loss: 3711.49414973103 grad: 2087.8602839025834
iteration: 0 loss: 8938.359882975434 grad: 2615.648588510262
iteration: 10 loss: 0.006011904267573052 grad: 0.1095109669858212
iteration: 0 loss: 4341.414278079265 grad: 2117.5838172777844
iteration: 0 loss: 6295.576480205418 grad: 2398.848815002703
iteration: 0 loss: 7635.455913859734 grad: 2492.494417868394
iteration: 10 loss: 0.03854624906274363 grad: -1.0655561666206412
iteration: 0 loss: 3964.320868661415 grad: 2012.0132406429993
iteration: 0 loss: 4724.384175298261 grad: 2215.8249603531003
iteration: 10 loss: 0.0014452695997368376 grad: 0.014545185326736591
iteration: 0 loss: 4914.267083158739 grad: 2322.721704172878
iteration: 0 loss: 4179.630258225531 grad: 2239.6833917193385
iteration: 10 loss: 0.0015014220235488292 grad: 0.009123241891491711
iteration: 0 loss: 9594.593657667649 grad: 2630.9993620914406
iteration: 10 loss: 0.025506331752033228 grad: -0.2573975821338421
iteration: 0 loss: 9361.956347125113 grad: 2545.189726040771
iteration: 10 loss: 1.9484695638662246 grad: -14.657136408253761
iteration: 0 loss: 5348.5528737421255 grad: 2287.496361117195
iteration: 0 loss: 4064.484844451741 grad: 2111.0317502219614
iteration: 0 loss: 11027.471657851991 grad: 2779.1921855020187
iteration: 10 loss: 0.1335388693339403 grad: 1.1582237342150772
iteration: 0 loss: 5383.847540895115 grad: 2420.879769266451
iteration: 0 loss: 5165.066896340905 grad: 2283.13973766417
iteration: 0 loss: 7220.787511636055 grad: 2371.3180313655275
iteration: 10 loss: 0.0012730294243242083 grad: -0.01973987663170435
iteration: 0 loss: 7096.289598298903 grad: 2447.575935333847
iteration: 10 loss: 0.0015616976293544708 grad: 0.022241989914456018
iteration: 0 loss: 5961.403887786045 grad: 2380.332356274784
iteration: 10 loss: 0.0013626826389967887 grad: -0.3366265469445127
iteration: 0 loss: 5593.687945253663 grad: 2278.0698478615263
iteration: 10 loss: 0.0011749935431659899 grad: 0.04030442955684477
iteration: 0 loss: 8924.46123145149 grad: 2634.3853231464573
iteration: 10 loss: 0.02183507696926509 grad: 0.3996126785800358
iteration: 0 loss: 4201.7385330938405 grad: 2168.6071181583725
iteration: 0 loss: 4650.601685869472 grad: 2212.10285290074
iteration: 0 loss: 5929.93691788625 grad: 2318.688929836686
iteration: 10 loss: 0.0012140244361944497 grad: 0.007430510174004808
iteration: 0 loss: 3008.357565288059 grad: 2011.7802515868311
iteration: 0 loss: 12479.657419227473 grad: 2868.6652623912014
iteration: 10 loss: 1.3100050499885005 grad: 5.797309825100981
iteration: 0 loss: 4956.896818692657 grad: 2133.62719724532
iteration: 0 loss: 6735.830557915353 grad: 2561.9513978092273
iteration: 0 loss: 7014.488129135472 grad: 2371.5976034035352
iteration: 10 loss: 0.0011547400486465035 grad: -0.0032575829325398997
iteration: 0 loss: 5314.3633524802635 grad: 2083.0585370568588
iteration: 0 loss: 4347.0591374506575 grad: 2249.3159779879848
iteration: 0 loss: 7625.543579497806 grad: 2486.966676019311
iteration: 10 loss: 0.0018067886134800078 grad: -1.0944013611417056
iteration: 0 loss: 7976.627143561575 grad: 2500.0760819691113
iteration: 10 loss: 0.03914603376919827 grad: 0.9944178780182468
iteration: 0 loss: 9263.031788931377 grad: 2578.5577268097322
iteration: 10 loss: 0.742386384353846 grad: 3.521987564902009
iteration: 0 loss: 4979.969914185709 grad: 2317.116021941619
iteration: 10 loss: 0.001262474324903451 grad: 0.03712883150390241
iteration: 0 loss: 8390.608393076582 grad: 2643.763431117453
iteration: 10 loss: 0.0014459801671116359 grad: 0.6730920501372792
iteration: 0 loss: 6140.9284896906065 grad: 2365.9151673557108
iteration: 0 loss: 4163.101812871943 grad: 2143.6628614913548
iteration: 0 loss: 5528.691450999111 grad: 2304.615339024077
iteration: 10 loss: 0.0014510451133405281 grad: 0.024492151593065517
iteration: 0 loss: 4136.300033429062 grad: 2234.338318486153
iteration: 0 loss: 6063.288515443023 grad: 2364.1850613279466
iteration: 10 loss: 0.0013141170189059762 grad: 0.025610782907790973
iteration: 0 loss: 9012.0644368858 grad: 2684.301268505932
iteration: 10 loss: 0.001538091295515187 grad: -0.005341257547478736
iteration: 0 loss: 4871.617294327681 grad: 2273.262772280897
iteration: 0 loss: 7910.700411893759 grad: 2511.8073679081745
iteration: 10 loss: 0.0013442824764007871 grad: 0.7191829169221748
iteration: 0 loss: 7885.807850271573 grad: 2648.860362566944
iteration: 10 loss: 0.0014138753864575517 grad: 0.7887907742343021
iteration: 0 loss: 4092.320341046907 grad: 1969.5472140486036
iteration: 0 loss: 4313.125243210535 grad: 2161.4956695698984
iteration: 0 loss: 5132.745356674641 grad: 2339.4103610590564
iteration: 0 loss: 5300.676113232464 grad: 2286.6366142767406
iteration: 0 loss: 4011.6954276905058 grad: 2060.50738422664
iteration: 0 loss: 7265.97503518736 grad: 2523.26828217444
iteration: 10 loss: 0.0017402932267445563 grad: 0.18578700422679945
iteration: 0 loss: 5889.644694623308 grad: 2311.8424234872214
iteration: 10 loss: 0.0013982460685920987 grad: 0.01521931041320115
iteration: 0 loss: 5877.435736854646 grad: 2457.5074264902573
iteration: 0 loss: 7084.141833080482 grad: 2504.2923928621526
iteration: 10 loss: 0.0017022940512767739 grad: -0.05931733809466809
iteration: 0 loss: 10666.456676627626 grad: 2787.1376406962154
iteration: 10 loss: 0.06528530891360292 grad: -0.6425550986583464
iteration: 0 loss: 4497.992514255974 grad: 2153.169695890248
iteration: 0 loss: 7446.814347815892 grad: 2485.890497883008
iteration: 10 loss: 0.001741954550115307 grad: -0.45837720563908396
iteration: 0 loss: 4673.079881602108 grad: 2236.358212662421
iteration: 0 loss: 3702.5113563156833 grad: 2077.728868415083
iteration: 0 loss: 4722.777952063535 grad: 2099.0131214689236
iteration: 0 loss: 3884.8408888792455 grad: 2083.06848076819
iteration: 0 loss: 5664.04040642355 grad: 2422.0059709236925
iteration: 0 loss: 7911.556387126216 grad: 2453.691211986111
iteration: 10 loss: 0.0014979366956553845 grad: 0.0035493996255742963
iteration: 0 loss: 8969.16056139988 grad: 2661.595446074588
iteration: 10 loss: 0.001586420768448575 grad: 0.034652698401427054
iteration: 0 loss: 8313.743331082202 grad: 2539.869297434975
iteration: 10 loss: 0.002400974631944502 grad: 0.90632283221724
iteration: 0 loss: 6707.444454542375 grad: 2441.9970738837665
iteration: 0 loss: 8491.43509405129 grad: 2541.2006925279948
iteration: 10 loss: 0.002230570729642006 grad: 1.3991077277811024
iteration: 0 loss: 8424.112748366191 grad: 2548.52174432882
iteration: 10 loss: 0.10352675109143919 grad: 2.6113086273677193
iteration: 0 loss: 6381.194826512407 grad: 2217.28581947762
iteration: 10 loss: 0.001242926934289492 grad: -0.30965651744093337
iteration: 0 loss: 10022.698547211141 grad: 2615.0520602548063
iteration: 10 loss: 0.01725248713840053 grad: 2.3698376085506982
iteration: 0 loss: 6880.751744506453 grad: 2343.0256651588916
iteration: 10 loss: 0.0017145273146029053 grad: 0.4219143669606123
iteration: 0 loss: 3654.223043544966 grad: 2007.49766696202
iteration: 0 loss: 3210.603435335348 grad: 1907.232076503501
iteration: 0 loss: 4471.236738900996 grad: 2089.437928081686
iteration: 0 loss: 5913.316435890854 grad: 2400.155342714059
iteration: 0 loss: 5633.246125546817 grad: 2252.334630612807
iteration: 10 loss: 0.0009684380956142294 grad: 0.022830670304375845
iteration: 0 loss: 4600.023156573862 grad: 2180.4918508699307
iteration: 0 loss: 3970.8014706713716 grad: 2114.2997029201497
iteration: 0 loss: 8154.028879819409 grad: 2650.817855159143
iteration: 10 loss: 0.0013341362672773275 grad: -0.17510928161585693
iteration: 0 loss: 6565.434861313703 grad: 2404.4873200059474
iteration: 10 loss: 0.001990625404604626 grad: 0.5806507964630756
iteration: 0 loss: 6083.1533722067625 grad: 2325.82370197373
iteration: 10 loss: 0.0014849074280143461 grad: 0.011438236172236408
iteration: 0 loss: 5477.5858728088115 grad: 2302.985159704748
iteration: 0 loss: 3984.799133614497 grad: 1984.8472747223618
iteration: 0 loss: 6474.193848271066 grad: 2378.2390545767535
iteration: 0 loss: 4820.270586388022 grad: 2198.2269857969795
iteration: 0 loss: 4946.188518307961 grad: 2312.7829579368517
iteration: 10 loss: 0.0015549789637919855 grad: 0.009281298310360047
iteration: 0 loss: 10912.569038140316 grad: 2850.335253063079
iteration: 10 loss: 0.0982473798940191 grad: 0.8445849729170918
iteration: 0 loss: 3665.32368819441 grad: 2088.8325948426636
iteration: 0 loss: 3564.27748939031 grad: 2009.977682599261
iteration: 0 loss: 4270.589385371089 grad: 2117.8469602120495
iteration: 0 loss: 5393.0218745270695 grad: 2358.574049102854
iteration: 0 loss: 6535.779497966797 grad: 2418.467065013128
iteration: 10 loss: 0.0014488608364693143 grad: 0.0024294559374587613
iteration: 0 loss: 7432.930398144582 grad: 2582.4042975986795
iteration: 10 loss: 0.0013325673776721073 grad: 1.1304072548132496
iteration: 0 loss: 9186.285375334115 grad: 2534.405264950763
iteration: 10 loss: 0.2627234120630998 grad: 3.9489166671416926
iteration: 0 loss: 7237.058919673736 grad: 2438.834046675573
iteration: 10 loss: 0.06544330937851389 grad: 0.7253798690749038
iteration: 0 loss: 5651.614586706622 grad: 2317.6348340249115
iteration: 0 loss: 9316.884102588823 grad: 2612.5462497820636
iteration: 10 loss: 0.07191485214703293 grad: -0.6794381614663176
iteration: 0 loss: 7026.978432308819 grad: 2441.304140451431
iteration: 0 loss: 4555.322665104128 grad: 2290.0510437151943
iteration: 10 loss: 0.0015215564040276645 grad: 0.2065058554634761
iteration: 0 loss: 5284.661151150631 grad: 2296.2669518971825
iteration: 0 loss: 4802.779648589233 grad: 2253.5639617748097
iteration: 0 loss: 8421.99836821016 grad: 2617.3211624645646
iteration: 10 loss: 0.024476986146923577 grad: 3.065196300990717
iteration: 0 loss: 4408.901031979935 grad: 2307.6193053602137
iteration: 0 loss: 5408.659479135785 grad: 2201.7546945492895
iteration: 0 loss: 3015.0481857950754 grad: 1966.4447616069124
iteration: 0 loss: 6291.511466826796 grad: 2506.700684537786
iteration: 0 loss: 6128.564276091609 grad: 2439.4035910096172
iteration: 10 loss: 0.0014323421703672714 grad: -0.007882798750405807
iteration: 0 loss: 6104.6559624343845 grad: 2407.0086311726473
iteration: 10 loss: 0.0014221719175111502 grad: 0.010576430771809232
iteration: 0 loss: 8657.267399024147 grad: 2499.8724264098664
iteration: 10 loss: 0.30988504745817574 grad: 1.515133592400761
iteration: 0 loss: 4973.203994590306 grad: 2262.778133967813
iteration: 0 loss: 5760.403608422446 grad: 2361.2995561500384
iteration: 10 loss: 0.001354640767782588 grad: 0.0006487579687874511
iteration: 0 loss: 8811.190240934588 grad: 2625.823001412931
iteration: 10 loss: 0.0013892727279612286 grad: 0.2874570262112772
iteration: 0 loss: 5559.6131023866 grad: 2239.6349480152144
iteration: 10 loss: 0.0014182333173018626 grad: 0.0030640779008624655
iteration: 0 loss: 7540.480145707825 grad: 2399.462068993662
iteration: 10 loss: 0.001177586253933524 grad: 0.8055306607553689
iteration: 0 loss: 4845.739816232417 grad: 2290.6406321750364
iteration: 0 loss: 8247.583770021094 grad: 2640.757102601032
iteration: 0 loss: 7069.273292727059 grad: 2501.2052616342726
iteration: 0 loss: 4554.858948361528 grad: 2074.4080210261995
iteration: 0 loss: 3587.8277326966204 grad: 2010.9945917446814
iteration: 0 loss: 5826.658201381917 grad: 2479.5214301844508
iteration: 0 loss: 2979.4836372407585 grad: 1931.3159765042305
iteration: 0 loss: 4008.077150900565 grad: 1950.7348689764092
iteration: 0 loss: 3556.8747138043323 grad: 2001.1718610275248
iteration: 0 loss: 8465.61727859428 grad: 2547.0375150439595
iteration: 10 loss: 0.0012302210009445182 grad: 0.7903649805645201
iteration: 0 loss: 5139.248633264841 grad: 2180.0749400528903
iteration: 10 loss: 0.0012404390634566714 grad: 0.015685346838885977
iteration: 0 loss: 8914.494015075099 grad: 2539.0142131378793
iteration: 10 loss: 0.5775973662278954 grad: 1.9726735284132781
iteration: 0 loss: 8324.442606831262 grad: 2536.618246538158
iteration: 10 loss: 0.0015016750892920588 grad: 1.3369576935158707
iteration: 0 loss: 9989.419975124112 grad: 2723.1495138351993
iteration: 10 loss: 0.17861718986585567 grad: 4.649144394368104
iteration: 0 loss: 4544.518775336387 grad: 2160.371006444685
iteration: 0 loss: 4602.256625361562 grad: 2184.6974695544427
iteration: 0 loss: 5966.747864396177 grad: 2454.1247958924787
iteration: 10 loss: 0.001450481134022332 grad: 0.04492763520976811
iteration: 0 loss: 6088.982900568356 grad: 2377.4855166012876
iteration: 0 loss: 11431.073108036027 grad: 2752.922262240704
iteration: 10 loss: 0.5077667135959597 grad: 4.299686693102913
iteration: 0 loss: 5095.6970728557635 grad: 2329.465932705462
iteration: 0 loss: 8812.314973209779 grad: 2635.4058616449875
iteration: 10 loss: 0.008312490295131945 grad: 2.748416197375289
iteration: 0 loss: 5788.237515450944 grad: 2313.0785722240767
iteration: 0 loss: 3916.477177633466 grad: 2178.7094786635894
iteration: 0 loss: 8277.318576856936 grad: 2584.4591685142927
iteration: 10 loss: 0.0016122467442817817 grad: -0.3258103522635589
iteration: 0 loss: 4127.82547316428 grad: 2164.136700211795
iteration: 0 loss: 10002.697562897007 grad: 2632.564470996355
iteration: 10 loss: 0.705376192157432 grad: 4.792853928346562
iteration: 0 loss: 3660.9099016919295 grad: 2060.2249717749455
iteration: 0 loss: 7816.475060544178 grad: 2503.6351225226167
iteration: 10 loss: 0.002592854003358463 grad: 0.5641929710874904
iteration: 0 loss: 3993.682429841234 grad: 2041.8163284671334
iteration: 0 loss: 6043.136143110116 grad: 2488.1134961162497
iteration: 10 loss: 0.0013176246310732413 grad: 0.015668758303455696
iteration: 0 loss: 4426.038648632513 grad: 2219.209049960463
iteration: 0 loss: 5940.701844191045 grad: 2310.976542456343
iteration: 10 loss: 0.0015208839229837229 grad: 0.008270550613333122
iteration: 0 loss: 5713.802844526148 grad: 2307.150350275724
iteration: 0 loss: 3243.166725872921 grad: 1969.663312092016
iteration: 0 loss: 7213.351701548399 grad: 2387.6153381280233
iteration: 10 loss: 0.0017575198885011064 grad: 0.41361355540928935
iteration: 0 loss: 7006.8163318778215 grad: 2418.201650836263
iteration: 10 loss: 0.001600891661258753 grad: 0.7341348986456746
iteration: 0 loss: inf grad: 3387.3096139709705
iteration: 0 loss: inf grad: 3176.6308617255636
iteration: 10 loss: 0.00027018178794109684 grad: 0.007373856019724611
iteration: 0 loss: 12735.09859408735 grad: 3215.323101601626
iteration: 10 loss: 0.0003418541322885589 grad: -0.08087065328989572
iteration: 0 loss: inf grad: 3527.655006017455
iteration: 0 loss: 9889.056334038054 grad: 3065.475631617556
iteration: 0 loss: 6632.761552143187 grad: 2734.3530346422667
iteration: 10 loss: 0.000278070971200412 grad: 0.2547672583825532
iteration: 0 loss: inf grad: 3162.7201414127167
iteration: 0 loss: 5709.812679826363 grad: 2590.8283463633343
iteration: 0 loss: inf grad: 3316.9552983685962
iteration: 0 loss: 10345.715886067519 grad: 3081.651099358841
iteration: 0 loss: 6550.577313351766 grad: 2826.2874859178655
iteration: 0 loss: 13160.73236566824 grad: 3316.9404590562704
iteration: 10 loss: 0.00027447482518089765 grad: -0.002313719656219292
iteration: 0 loss: 8875.347011736329 grad: 2808.865760914978
iteration: 0 loss: inf grad: 3133.4839247036825
iteration: 0 loss: inf grad: 3179.6140076792763
iteration: 0 loss: inf grad: 3606.679889007997
iteration: 0 loss: inf grad: 3095.9099259931027
iteration: 10 loss: 0.0003683846576181664 grad: 0.0025327034362040085
iteration: 0 loss: 7694.688269326865 grad: 2888.5170747926913
iteration: 0 loss: inf grad: 3613.626479837765
iteration: 0 loss: 9056.944714955276 grad: 2925.629804182651
iteration: 0 loss: inf grad: 3311.5172502579203
iteration: 0 loss: inf grad: 3441.4287912855007
iteration: 0 loss: inf grad: 2778.9926807662923
iteration: 0 loss: 9923.050528995842 grad: 3056.3854019311234
iteration: 10 loss: 0.0002915728625587442 grad: -0.0011947834269816658
iteration: 0 loss: 10226.044991119135 grad: 3210.722584924083
iteration: 0 loss: inf grad: 3096.5952991037684
iteration: 0 loss: inf grad: 3632.922349544072
iteration: 0 loss: inf grad: 3514.1287862310965
iteration: 0 loss: 11273.023514533394 grad: 3164.2122571541177
iteration: 0 loss: 8358.854687946026 grad: 2912.058881176632
iteration: 0 loss: inf grad: 3835.7337848885268
iteration: 0 loss: 11188.937467197882 grad: 3342.2033866375677
iteration: 10 loss: 0.00024430010076188904 grad: 0.006728353372210732
iteration: 0 loss: 10845.342663658199 grad: 3156.100440672504
iteration: 0 loss: inf grad: 3272.84937387636
iteration: 0 loss: inf grad: 3378.8171760339915
iteration: 0 loss: inf grad: 3284.2475551191656
iteration: 0 loss: inf grad: 3148.77616611079
iteration: 0 loss: inf grad: 3632.8506910650726
iteration: 0 loss: 8602.571408144879 grad: 2998.99550198795
iteration: 0 loss: 9758.235505176292 grad: 3051.5730649529523
iteration: 0 loss: inf grad: 3204.0296384264325
iteration: 0 loss: 6307.30389633606 grad: 2782.9953383674565
iteration: 0 loss: inf grad: 3953.4153998304655
iteration: 0 loss: inf grad: 2945.95627800892
iteration: 0 loss: inf grad: 3535.8267496914636
iteration: 0 loss: inf grad: 3272.242701118089
iteration: 0 loss: inf grad: 2879.7812317891457
iteration: 0 loss: 9032.541045340675 grad: 3107.8919147893007
iteration: 0 loss: inf grad: 3432.1024707926335
iteration: 0 loss: inf grad: 3449.7088522179647
iteration: 0 loss: inf grad: 3560.826859236131
iteration: 0 loss: 10447.319475790211 grad: 3203.9439612020956
iteration: 10 loss: 0.0002349850758698515 grad: 0.002506520628030344
iteration: 0 loss: inf grad: 3648.1555231726097
iteration: 0 loss: inf grad: 3270.8384077426063
iteration: 10 loss: 0.0003140406228009273 grad: 0.020235619289822666
iteration: 0 loss: 8692.658448129887 grad: 2955.787250391588
iteration: 0 loss: inf grad: 3183.6259593907344
iteration: 0 loss: 8721.876454132143 grad: 3087.1115970970804
iteration: 0 loss: inf grad: 3263.7497522462277
iteration: 0 loss: inf grad: 3708.027571211967
iteration: 0 loss: 10192.61829138903 grad: 3140.4890042027755
iteration: 0 loss: inf grad: 3465.318152662934
iteration: 0 loss: inf grad: 3656.6908594729944
iteration: 0 loss: 8497.088580825803 grad: 2715.8890347642932
iteration: 0 loss: 9112.511423544742 grad: 2984.7742920241544
iteration: 10 loss: 0.00026278075264682144 grad: 0.00015297273506946485
iteration: 0 loss: 10664.674786399422 grad: 3228.8162909777166
iteration: 0 loss: inf grad: 3162.0471657987755
iteration: 0 loss: 8284.163399189954 grad: 2849.1528399560734
iteration: 0 loss: inf grad: 3481.823573309466
iteration: 0 loss: inf grad: 3187.91431383791
iteration: 0 loss: 12250.518893052551 grad: 3388.199219117746
iteration: 0 loss: inf grad: 3459.177770437751
iteration: 0 loss: inf grad: 3846.588829908545
iteration: 0 loss: 9249.499588239372 grad: 2971.497510086744
iteration: 0 loss: inf grad: 3437.0930983953494
iteration: 0 loss: 9673.398847989594 grad: 3088.211235959614
iteration: 10 loss: 0.00043961095401424575 grad: -0.00016509124637313437
iteration: 0 loss: 7691.449405206649 grad: 2868.3837971967305
iteration: 0 loss: inf grad: 2900.021682841245
iteration: 0 loss: 8078.896916381126 grad: 2878.0319582833095
iteration: 0 loss: 11810.511073451002 grad: 3346.1588713450783
iteration: 0 loss: inf grad: 3390.3623573158075
iteration: 0 loss: inf grad: 3676.7110666663602
iteration: 0 loss: inf grad: 3507.2438982755066
iteration: 0 loss: inf grad: 3374.9422294484702
iteration: 0 loss: inf grad: 3512.2850798630907
iteration: 0 loss: inf grad: 3518.5379665890096
iteration: 0 loss: inf grad: 3059.5477083940796
iteration: 0 loss: inf grad: 3610.8738876553193
iteration: 0 loss: inf grad: 3236.1662956292203
iteration: 0 loss: 7743.4959351705675 grad: 2770.2482265805565
iteration: 0 loss: 6706.377213928089 grad: 2635.519235155509
iteration: 0 loss: 9170.567904678075 grad: 2883.112588645022
iteration: 0 loss: 12244.692181996857 grad: 3313.3837349119794
iteration: 10 loss: 0.0002823683131613176 grad: 0.004804380932576656
iteration: 0 loss: inf grad: 3110.34622505929
iteration: 0 loss: inf grad: 3013.912883570848
iteration: 0 loss: 8335.68640321 grad: 2919.7006726193395
iteration: 0 loss: inf grad: 3659.99184903258
iteration: 0 loss: inf grad: 3322.7332917145613
iteration: 0 loss: inf grad: 3207.454861001413
iteration: 0 loss: 11320.907660316123 grad: 3177.553685494695
iteration: 0 loss: 8055.80768172363 grad: 2741.099938296591
iteration: 0 loss: inf grad: 3285.91602688939
iteration: 0 loss: 10038.796807136143 grad: 3037.49341989171
iteration: 0 loss: inf grad: 3195.4534228501434
iteration: 0 loss: inf grad: 3934.2114937585784
iteration: 0 loss: inf grad: 2887.0519703718173
iteration: 10 loss: 0.00020468429482372647 grad: -0.0029768270353341527
iteration: 0 loss: 7512.4645467845285 grad: 2778.683680276682
iteration: 0 loss: 8952.887134068937 grad: 2925.1742787204444
iteration: 0 loss: inf grad: 3254.6272124908546
iteration: 0 loss: inf grad: 3339.2620374496305
iteration: 0 loss: inf grad: 3565.155326203536
iteration: 0 loss: inf grad: 3498.5593320569615
iteration: 0 loss: inf grad: 3364.1993336344985
iteration: 0 loss: 11728.04036576755 grad: 3200.997765906631
iteration: 10 loss: 0.00029881178422577 grad: 0.010233491896645462
iteration: 0 loss: inf grad: 3609.49497605612
iteration: 0 loss: inf grad: 3369.911387853038
iteration: 0 loss: inf grad: 3162.3349952415724
iteration: 10 loss: 0.00020299725954166868 grad: -0.182084263281652
iteration: 0 loss: inf grad: 3172.207728610494
iteration: 0 loss: inf grad: 3113.1498320672176
iteration: 10 loss: 0.0003073531023057347 grad: -0.011317195167729747
iteration: 0 loss: inf grad: 3613.3277598266823
iteration: 0 loss: inf grad: 3185.8059093974225
iteration: 10 loss: 0.00030740118977105754 grad: -0.005643364986634725
iteration: 0 loss: 11228.709441503333 grad: 3039.390743791362
iteration: 10 loss: 0.0003422429065474055 grad: 0.006299815533424333
iteration: 0 loss: 6234.716762270658 grad: 2713.45816634092
iteration: 0 loss: inf grad: 3465.211419924123
iteration: 0 loss: inf grad: 3367.2022098375
iteration: 0 loss: inf grad: 3323.44677161833
iteration: 0 loss: inf grad: 3455.2290554033407
iteration: 0 loss: 10340.992525627493 grad: 3123.725904009089
iteration: 0 loss: inf grad: 3258.1370827443293
iteration: 0 loss: inf grad: 3622.567188316385
iteration: 0 loss: inf grad: 3095.7268114883454
iteration: 10 loss: 0.0002974923170933669 grad: -0.034560870617838434
iteration: 0 loss: inf grad: 3316.060843643391
iteration: 0 loss: inf grad: 3163.3347101636255
iteration: 0 loss: inf grad: 3645.1396968658846
iteration: 0 loss: inf grad: 3452.0470832987858
iteration: 0 loss: 9327.82911428806 grad: 2866.637931473936
iteration: 0 loss: 7456.522620320335 grad: 2780.0046014587933
iteration: 0 loss: 12261.997790565814 grad: 3423.7341471757804
iteration: 10 loss: 0.00036104825663972986 grad: 0.012805908431217768
iteration: 0 loss: 6347.759250655058 grad: 2670.275847962807
iteration: 0 loss: 8370.41363636714 grad: 2697.77451320187
iteration: 0 loss: 7398.606027221485 grad: 2764.6423764264614
iteration: 0 loss: inf grad: 3513.799531772872
iteration: 0 loss: inf grad: 3010.989663714905
iteration: 10 loss: 0.00027379105978814715 grad: 0.011454192887881645
iteration: 0 loss: inf grad: 3506.727029038235
iteration: 0 loss: inf grad: 3507.82284151679
iteration: 0 loss: inf grad: 3760.10381399131
iteration: 0 loss: 9495.656382077445 grad: 2978.8364938585355
iteration: 0 loss: 9631.810517947399 grad: 3020.753988361851
iteration: 0 loss: inf grad: 3392.3371708322684
iteration: 0 loss: inf grad: 3280.4894260395467
iteration: 0 loss: inf grad: 3797.201765142635
iteration: 0 loss: 10717.579592195703 grad: 3217.741352528711
iteration: 10 loss: 0.00023955615787682208 grad: 0.011639474689624266
iteration: 0 loss: inf grad: 3635.0572870734627
iteration: 0 loss: 11951.45211667929 grad: 3195.2372219598255
iteration: 0 loss: 8293.738563016866 grad: 3012.0164549792275
iteration: 0 loss: inf grad: 3569.089184682037
iteration: 0 loss: 8575.842833674888 grad: 2991.0189799695527
iteration: 10 loss: 0.00036537955483337015 grad: 0.1655458173975023
iteration: 0 loss: inf grad: 3630.180418527274
iteration: 0 loss: 7648.285465741538 grad: 2843.4109581264147
iteration: 0 loss: inf grad: 3457.1385693392176
iteration: 0 loss: 8402.329163818129 grad: 2823.3533315990535
iteration: 0 loss: inf grad: 3436.756807733729
iteration: 0 loss: inf grad: 3066.4656896802053
iteration: 0 loss: 12493.494595566113 grad: 3195.839577300323
iteration: 10 loss: 0.00030131693347357213 grad: -0.17988073284214026
iteration: 0 loss: inf grad: 3193.8165584470485
iteration: 0 loss: 6804.050497335312 grad: 2719.235662335353
iteration: 0 loss: inf grad: 3296.671311954903
iteration: 0 loss: inf grad: 3335.6504795879205
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:57: RuntimeWarning: invalid value encountered in double_scalars
  diff = np.absolute(f_history[-1]-f_history[-2])
iteration: 0 loss: 523.095542806912 grad: 923.8768048162493
iteration: 10 loss: 0.1590265364110647 grad: 0.6435099421726185
iteration: 0 loss: 399.7098104870332 grad: 864.4023249987846
iteration: 10 loss: 0.16732837845235984 grad: 0.36679976935437075
iteration: 0 loss: 422.2908062180361 grad: 874.2172056960686
iteration: 10 loss: 0.1550728287933122 grad: 0.4627331550051937
iteration: 0 loss: 602.140013815195 grad: 962.5217935145972
iteration: 10 loss: 0.14758723002308793 grad: 0.5169600892075972
iteration: 0 loss: 357.2041455128167 grad: 834.7726064371734
iteration: 10 loss: 0.15634169246351864 grad: 0.3346555197006043
iteration: 0 loss: 248.62885523203443 grad: 740.8223615614377
iteration: 10 loss: 0.1728635413712089 grad: 0.2815424184335322
iteration: 0 loss: 373.00200591720653 grad: 859.1286501574908
iteration: 10 loss: 0.1422573749917898 grad: 0.30360230836636176
iteration: 0 loss: 184.27660821484162 grad: 701.3908344569661
iteration: 10 loss: 0.2000886513851583 grad: 0.17639336066470285
iteration: 0 loss: 604.324396108895 grad: 903.7205553524709
iteration: 10 loss: 0.15449035817800905 grad: 0.5224453501579733
iteration: 0 loss: 365.72578553536874 grad: 839.722068258485
iteration: 10 loss: 0.14777785105649277 grad: 0.40153913041968325
iteration: 0 loss: 203.42377682815052 grad: 765.1701990334395
iteration: 10 loss: 0.16853546887432458 grad: 0.30382242744414734
iteration: 0 loss: 429.32631856996215 grad: 903.6434353645668
iteration: 10 loss: 0.1644967489997229 grad: 0.5157149543018353
iteration: 0 loss: 340.54696677172524 grad: 761.0317801150595
iteration: 10 loss: 0.1569833310634501 grad: 0.2567015356044486
iteration: 0 loss: 325.2840162896669 grad: 851.6350827507312
iteration: 10 loss: 0.15811105919304033 grad: 0.31543489646360207
iteration: 0 loss: 453.72001367659226 grad: 866.2263340229745
iteration: 10 loss: 0.1440196562059348 grad: 0.45069144590620536
iteration: 0 loss: 625.2033265712294 grad: 985.092572743866
iteration: 10 loss: 0.16279265703286167 grad: 0.707869187963629
iteration: 0 loss: 390.07445940819076 grad: 843.1807484388581
iteration: 10 loss: 0.1629634843750616 grad: 0.37778716657159
iteration: 0 loss: 272.6521836031496 grad: 782.5190094308897
iteration: 10 loss: 0.16637318200462853 grad: 0.2527871083364406
iteration: 0 loss: 718.4426253008456 grad: 987.6099105259725
iteration: 10 loss: 0.17330841818296572 grad: 0.580606904101434
iteration: 0 loss: 317.6565130740868 grad: 793.0221753210337
iteration: 10 loss: 0.1707211132091867 grad: 0.2069083789016534
iteration: 0 loss: 454.2963502398577 grad: 901.119137642247
iteration: 10 loss: 0.15057205610223304 grad: 0.34626653559150633
iteration: 0 loss: 528.3087836006284 grad: 938.6611117102739
iteration: 10 loss: 0.16487020932072483 grad: 0.6027790459512734
iteration: 0 loss: 339.37855911493756 grad: 754.6591282251364
iteration: 10 loss: 0.156191573982803 grad: 0.34605282161282647
iteration: 0 loss: 289.6033044520915 grad: 832.859234873996
iteration: 10 loss: 0.17333585981677144 grad: 0.5071657146420808
iteration: 0 loss: 349.5904273792572 grad: 870.972783787489
iteration: 10 loss: 0.14771628013510385 grad: 0.3813877915447508
iteration: 0 loss: 332.4774647447033 grad: 839.9751369996493
iteration: 10 loss: 0.17145285138609656 grad: 0.2764316859719739
iteration: 0 loss: 784.5492778462612 grad: 991.8894331702672
iteration: 10 loss: 0.1595014427189612 grad: 0.6248036777228785
iteration: 0 loss: 723.6691207032646 grad: 959.0926331061951
iteration: 10 loss: 0.22476074817181638 grad: 0.3849687404057151
iteration: 0 loss: 351.40190997745395 grad: 862.1335767494895
iteration: 10 loss: 0.15661027997587304 grad: 0.4291411315823115
iteration: 0 loss: 304.6166988468232 grad: 792.10112078475
iteration: 10 loss: 0.1694450396012557 grad: 0.30821313325553223
iteration: 0 loss: 878.675679325248 grad: 1047.8222728625742
iteration: 10 loss: 0.15601937888475897 grad: 0.41030599665451073
iteration: 0 loss: 387.35484389853104 grad: 911.2618251507365
iteration: 10 loss: 0.14873261035708393 grad: 0.14720904254047967
iteration: 0 loss: 373.7402271030625 grad: 857.2312676024671
iteration: 10 loss: 0.16523891625729448 grad: 0.3952168209266416
iteration: 0 loss: 531.9577033770524 grad: 892.267487045987
iteration: 10 loss: 0.15552546649930513 grad: 0.6481897626756241
iteration: 0 loss: 528.53990260326 grad: 921.4176665694488
iteration: 10 loss: 0.16086824694105994 grad: 0.4916383079818597
iteration: 0 loss: 431.2213664176406 grad: 895.4232975156488
iteration: 10 loss: 0.17566159550898935 grad: 0.5282707344819257
iteration: 0 loss: 428.82224976321464 grad: 858.0032033885282
iteration: 10 loss: 0.15190899578473446 grad: 0.5335083745430598
iteration: 0 loss: 620.2251635593192 grad: 992.6645933684007
iteration: 10 loss: 0.14601566134677507 grad: 0.5422163230153005
iteration: 0 loss: 328.1257707007662 grad: 814.9090912197894
iteration: 10 loss: 0.15934208588309542 grad: 0.31158688636139775
iteration: 0 loss: 325.20522363455 grad: 830.5192852289232
iteration: 10 loss: 0.15587264753643962 grad: 0.31880570539914194
iteration: 0 loss: 511.0449472521344 grad: 872.0484516939158
iteration: 10 loss: 0.16234150095988298 grad: 0.5695254206151605
iteration: 0 loss: 203.1565210382907 grad: 753.4830848831298
iteration: 10 loss: 0.18488638296681034 grad: 0.2543782978297069
iteration: 0 loss: 964.9363496915956 grad: 1082.2711586049882
iteration: 10 loss: 0.1624419151237155 grad: 0.566939246411369
iteration: 0 loss: 404.9106995129198 grad: 801.5310743496532
iteration: 10 loss: 0.16486805425450834 grad: 0.38183207516155204
iteration: 0 loss: 458.67065600142416 grad: 965.3349782784109
iteration: 10 loss: 0.151818119224448 grad: 0.43712695063147644
iteration: 0 loss: 590.1244775233371 grad: 891.0103683037489
iteration: 10 loss: 0.15401746598432842 grad: 0.6433132717980705
iteration: 0 loss: 460.66576064411805 grad: 778.9748222398733
iteration: 10 loss: 0.16107846220818994 grad: 0.45990095215243876
iteration: 0 loss: 319.80869942255777 grad: 844.7905051156025
iteration: 10 loss: 0.15892625415339848 grad: 0.2685646587037035
iteration: 0 loss: 574.2828843952183 grad: 937.3864691434019
iteration: 10 loss: 0.15338112456794822 grad: 0.6534239318791273
iteration: 0 loss: 625.6570744806037 grad: 941.7636016654966
iteration: 10 loss: 0.1711160890889997 grad: 0.6940836215016126
iteration: 0 loss: 691.2101218172164 grad: 972.2102578272737
iteration: 10 loss: 0.18158208421201297 grad: 0.8489728074344284
iteration: 0 loss: 346.3202184411382 grad: 872.160574590567
iteration: 10 loss: 0.1615849654949257 grad: 0.4981149686314674
iteration: 0 loss: 610.2323511546975 grad: 996.1622377991678
iteration: 10 loss: 0.16127444751013667 grad: 0.5109280555060604
iteration: 0 loss: 480.3865274926198 grad: 891.3609311126766
iteration: 10 loss: 0.1540706628402404 grad: 0.45703228506497706
iteration: 0 loss: 277.63480600026395 grad: 802.16317807993
iteration: 10 loss: 0.15282647435428356 grad: -0.05151178892996036
iteration: 0 loss: 385.95720517290357 grad: 866.4485618480435
iteration: 10 loss: 0.1607597464039414 grad: 0.5411422951759548
iteration: 0 loss: 254.3667847840027 grad: 839.5231762199699
iteration: 10 loss: 0.15744117793474288 grad: 0.08790100992918173
iteration: 0 loss: 448.8421152766432 grad: 888.8775250641869
iteration: 10 loss: 0.15018598301701952 grad: 0.6063584993794888
iteration: 0 loss: 711.1005692425898 grad: 1012.618615923482
iteration: 10 loss: 0.1442611420230109 grad: 0.6627572058532817
iteration: 0 loss: 349.37757268074586 grad: 853.5725404709508
iteration: 10 loss: 0.15342665382459844 grad: 0.12391361610480049
iteration: 0 loss: 574.9249992200536 grad: 944.8551644504819
iteration: 10 loss: 0.1516231412692419 grad: 0.7679400727274628
iteration: 0 loss: 559.2143975092988 grad: 997.9900847142833
iteration: 10 loss: 0.1478719763118451 grad: 0.36195359257122833
iteration: 0 loss: 288.8247882402024 grad: 735.014488231216
iteration: 10 loss: 0.159315534638319 grad: 0.27133615182556986
iteration: 0 loss: 269.49936736632804 grad: 810.4314850919377
iteration: 10 loss: 0.16367695866624257 grad: 0.41845684912057224
iteration: 0 loss: 376.6480109202134 grad: 881.7055567836045
iteration: 10 loss: 0.15534323618091797 grad: 0.178676354279789
iteration: 0 loss: 393.06551973853277 grad: 860.4289739289817
iteration: 10 loss: 0.14918715316492703 grad: 0.34063641529464256
iteration: 0 loss: 316.7554888965999 grad: 771.404123555969
iteration: 10 loss: 0.14905459049954184 grad: 0.2958127778464437
iteration: 0 loss: 533.6403156897176 grad: 948.9355860265891
iteration: 10 loss: 0.16111313282023978 grad: 0.6504314043555329
iteration: 0 loss: 513.88309224989 grad: 867.6397600055873
iteration: 10 loss: 0.16578137261504625 grad: 0.5820503231153047
iteration: 0 loss: 420.70012151007086 grad: 925.5771134104359
iteration: 10 loss: 0.15964723253786028 grad: 0.3917188951887993
iteration: 0 loss: 491.4916213787734 grad: 942.7745577759947
iteration: 10 loss: 0.15454453050402994 grad: 0.44894777382013307
iteration: 0 loss: 840.7997397043536 grad: 1052.506594858436
iteration: 10 loss: 0.17852573401118702 grad: 0.8918172212689824
iteration: 0 loss: 348.570185817048 grad: 806.4101986519037
iteration: 10 loss: 0.16961736345778758 grad: 0.3801928420193675
iteration: 0 loss: 532.2715834900426 grad: 937.3854955171798
iteration: 10 loss: 0.15728735020853823 grad: 0.6025431467243864
iteration: 0 loss: 360.5837611187484 grad: 841.4205112892209
iteration: 10 loss: 0.16827390836409573 grad: 0.26755589721376194
iteration: 0 loss: 277.5482611099265 grad: 779.0065544437064
iteration: 10 loss: 0.15648982718415605 grad: 0.2938967041818802
iteration: 0 loss: 385.3595440123618 grad: 786.849190083412
iteration: 10 loss: 0.1556725306691078 grad: 0.5364377494911283
iteration: 0 loss: 269.1848020276688 grad: 781.7129192131043
iteration: 10 loss: 0.1658048205006873 grad: 0.26335704362373713
iteration: 0 loss: 399.4024350302582 grad: 912.0543399686808
iteration: 10 loss: 0.15282003353240725 grad: 0.2942220176009186
iteration: 0 loss: 638.9484411433673 grad: 924.0902053918529
iteration: 10 loss: 0.1586178051083235 grad: 0.6265629212310955
iteration: 0 loss: 664.813675359624 grad: 1004.0231634615996
iteration: 10 loss: 0.1417649266239261 grad: 0.651911077645143
iteration: 0 loss: 635.9988064158448 grad: 956.8705733847969
iteration: 10 loss: 0.15210729016673047 grad: 0.5974945574343622
iteration: 0 loss: 481.9214624524717 grad: 918.4139480781309
iteration: 10 loss: 0.15378190519434362 grad: 0.4231645005433519
iteration: 0 loss: 666.3771826171709 grad: 956.7447447269651
iteration: 10 loss: 0.14354186055656595 grad: 0.5744146427547063
iteration: 0 loss: 638.2245725351099 grad: 960.7005590641047
iteration: 10 loss: 0.15733131802285658 grad: 0.5330982531410963
iteration: 0 loss: 565.3954686372722 grad: 832.5349453170319
iteration: 10 loss: 0.15517201218591997 grad: 0.3912087015854233
iteration: 0 loss: 864.2779483886676 grad: 985.6556649940101
iteration: 10 loss: 0.16330741139112928 grad: 0.4441072335213109
iteration: 0 loss: 595.3354828656925 grad: 880.5202227061744
iteration: 10 loss: 0.16421020047610826 grad: 0.5518430541575078
iteration: 0 loss: 248.51406769389632 grad: 751.5124742261
iteration: 10 loss: 0.17216181312919202 grad: 0.233589633957211
iteration: 0 loss: 241.69691904358098 grad: 711.6489814431286
iteration: 10 loss: 0.17706290232143312 grad: 0.0570372786105955
iteration: 0 loss: 355.686042721292 grad: 782.8374174780186
iteration: 10 loss: 0.14948975405522189 grad: 0.184758865249983
iteration: 0 loss: 447.73572550785195 grad: 902.7848452386
iteration: 10 loss: 0.14927190564545031 grad: 0.19652311575803905
iteration: 0 loss: 407.097585301297 grad: 845.1922069370889
iteration: 10 loss: 0.14568089139803223 grad: 0.5069917698555753
iteration: 0 loss: 320.62026539187286 grad: 818.0025038244713
iteration: 10 loss: 0.16543186584699884 grad: 0.2603198983146693
iteration: 0 loss: 277.8554758218527 grad: 792.9191185119698
iteration: 10 loss: 0.15191800253705878 grad: 0.10102599115562383
iteration: 0 loss: 604.2640064441489 grad: 1002.1397347250108
iteration: 10 loss: 0.1429210774137821 grad: 0.3800443360941992
iteration: 0 loss: 517.051013345108 grad: 906.0210289173561
iteration: 10 loss: 0.16731592670525555 grad: 0.7009853736769058
iteration: 0 loss: 522.3015896061719 grad: 872.8998037820422
iteration: 10 loss: 0.16214908958466945 grad: 0.45685737971031215
iteration: 0 loss: 398.0381090629439 grad: 866.6077730015907
iteration: 10 loss: 0.15350793410189825 grad: 0.3005011622743606
iteration: 0 loss: 344.5272688209105 grad: 743.502326163213
iteration: 10 loss: 0.15629948176815975 grad: 0.17467965661176632
iteration: 0 loss: 499.9808684541001 grad: 894.7801121013454
iteration: 10 loss: 0.14512137481870013 grad: 0.45027088714474
iteration: 0 loss: 343.62857609900004 grad: 826.6607172099662
iteration: 10 loss: 0.16270245024861651 grad: 0.20366412675449946
iteration: 0 loss: 320.09904463632853 grad: 869.4832801676275
iteration: 10 loss: 0.17400540381673968 grad: 0.4586096644215992
iteration: 0 loss: 793.2310194455539 grad: 1076.2503849395482
iteration: 10 loss: 0.16114777313338313 grad: 0.728880116072345
iteration: 0 loss: 264.31347268650336 grad: 783.5954884998757
iteration: 10 loss: 0.15813952552095792 grad: 0.17299072964429285
iteration: 0 loss: 239.60628405256094 grad: 753.4258134597427
iteration: 10 loss: 0.16824841361068882 grad: 0.15417164423383553
iteration: 0 loss: 300.8650001622438 grad: 793.374463777744
iteration: 10 loss: 0.16017947007044728 grad: 0.1545069713722445
iteration: 0 loss: 360.46714242303875 grad: 886.3334735052786
iteration: 10 loss: 0.1576702726832247 grad: 0.37527724420868197
iteration: 0 loss: 503.90850027903724 grad: 909.5601077707365
iteration: 10 loss: 0.14879031441126822 grad: 0.4412067248896448
iteration: 0 loss: 534.6547577348548 grad: 973.8155649510173
iteration: 10 loss: 0.15229155986708065 grad: 0.4967463146722283
iteration: 0 loss: 619.8533010366472 grad: 956.1548343469489
iteration: 10 loss: 0.17548007934981963 grad: 0.7402108163833071
iteration: 0 loss: 478.0749274625212 grad: 917.0018930191046
iteration: 10 loss: 0.16756817715296082 grad: 0.7467068189660038
iteration: 0 loss: 413.47881945700186 grad: 870.811432541202
iteration: 10 loss: 0.15584550913130937 grad: 0.48246436576215174
iteration: 0 loss: 712.9229904062596 grad: 985.3636511971786
iteration: 10 loss: 0.16455908011762818 grad: 0.5982787591668961
iteration: 0 loss: 549.5244859595579 grad: 918.258274221344
iteration: 10 loss: 0.16009064838809453 grad: 0.5146216462630102
iteration: 0 loss: 273.70696916154014 grad: 860.6453817211246
iteration: 10 loss: 0.15964181369401392 grad: 0.26869041022732415
iteration: 0 loss: 408.1744181419768 grad: 863.9261960499475
iteration: 10 loss: 0.1469727619379145 grad: 0.2634165624412496
iteration: 0 loss: 351.12863055997235 grad: 846.8515448047119
iteration: 10 loss: 0.1676643979671181 grad: 0.5317416498721346
iteration: 0 loss: 587.4948029459623 grad: 987.7506358715368
iteration: 10 loss: 0.16903127618554042 grad: 0.40972991486078914
iteration: 0 loss: 301.764106842051 grad: 866.9649723953029
iteration: 10 loss: 0.16131233040736828 grad: 0.2060612445400452
iteration: 0 loss: 417.39959435685137 grad: 825.3692703099334
iteration: 10 loss: 0.16087550403161996 grad: 0.4664404331499925
iteration: 0 loss: 211.59331291876714 grad: 735.3856525115774
iteration: 10 loss: 0.19244334153121267 grad: 0.3354793015438594
iteration: 0 loss: 506.5543918647136 grad: 945.0515269527575
iteration: 10 loss: 0.14731845408410169 grad: 0.6915583899070471
iteration: 0 loss: 454.6219466876942 grad: 917.0979760829016
iteration: 10 loss: 0.1635562169632811 grad: 0.40230462420201685
iteration: 0 loss: 451.2074843970273 grad: 904.7014245066445
iteration: 10 loss: 0.15674326696700353 grad: 0.5099492127784624
iteration: 0 loss: 745.1438785704344 grad: 940.0702413937602
iteration: 10 loss: 0.15868366983489646 grad: 0.43332876576426566
iteration: 0 loss: 340.9751825998805 grad: 850.53090863592
iteration: 10 loss: 0.1607956195915573 grad: 0.34160876500218035
iteration: 0 loss: 426.1835035037537 grad: 886.1177734623436
iteration: 10 loss: 0.15007085250908858 grad: 0.5166261381121038
iteration: 0 loss: 749.19769230802 grad: 990.6545846399184
iteration: 10 loss: 0.15744645476380273 grad: 0.6368644607238134
iteration: 0 loss: 422.43084743952136 grad: 842.0377582850655
iteration: 10 loss: 0.16554801708711733 grad: 0.5499243276849728
iteration: 0 loss: 577.1703666895047 grad: 901.9414068761662
iteration: 10 loss: 0.15273292623880314 grad: 0.6067029737594785
iteration: 0 loss: 354.5423882111082 grad: 860.5973711882872
iteration: 10 loss: 0.16896118122928205 grad: 0.4241622671261011
iteration: 0 loss: 635.7294665917029 grad: 994.4999616510981
iteration: 10 loss: 0.15112318239781333 grad: 0.4843068004156412
iteration: 0 loss: 570.0632148462903 grad: 942.4450755739877
iteration: 10 loss: 0.14734178591921096 grad: 0.41445792139333165
iteration: 0 loss: 366.80006036104663 grad: 777.7585353751138
iteration: 10 loss: 0.1588610825320674 grad: 0.3002094959801749
iteration: 0 loss: 266.79112854222495 grad: 753.1904367672603
iteration: 10 loss: 0.16778638298455917 grad: 0.1901826393017135
iteration: 0 loss: 389.1032116611755 grad: 933.0668194762087
iteration: 10 loss: 0.14907752653479375 grad: 0.4319044094966154
iteration: 0 loss: 192.7150167764493 grad: 720.5992753002303
iteration: 10 loss: 0.18745214941240687 grad: 0.21755678695558325
iteration: 0 loss: 364.5238889501356 grad: 729.0410820344302
iteration: 10 loss: 0.15608751701322696 grad: 0.40701993914916085
iteration: 0 loss: 274.7637862373779 grad: 748.4109376204647
iteration: 10 loss: 0.16426241346644846 grad: 0.3378822752825784
iteration: 0 loss: 654.1698136360966 grad: 959.8662164866723
iteration: 10 loss: 0.15134838250246088 grad: 0.5826946847644849
iteration: 0 loss: 405.13830575943155 grad: 818.5418555698369
iteration: 10 loss: 0.15968217572497617 grad: 0.4311716975212214
iteration: 0 loss: 612.6310998774054 grad: 955.0899596713964
iteration: 10 loss: 0.15339949851427678 grad: 0.4875175824168809
iteration: 0 loss: 647.5194182550039 grad: 956.5555326020396
iteration: 10 loss: 0.14465313334205182 grad: 0.5630099353973166
iteration: 0 loss: 723.8061637088598 grad: 1027.2504535857229
iteration: 10 loss: 0.15511207543386263 grad: 0.4435596945162613
iteration: 0 loss: 310.82871811405147 grad: 809.9931027537734
iteration: 10 loss: 0.1656709093278385 grad: 0.20999215574683006
iteration: 0 loss: 329.82111886681287 grad: 820.3696598654839
iteration: 10 loss: 0.16067391789676383 grad: 0.3617670478009505
iteration: 0 loss: 417.42476071940115 grad: 924.6774116202952
iteration: 10 loss: 0.15904799026032956 grad: 0.44307749470345
iteration: 0 loss: 430.34021251430715 grad: 893.6432106387167
iteration: 10 loss: 0.15029533953218727 grad: 0.4709724170783956
iteration: 0 loss: 860.7837903442753 grad: 1038.4200139865052
iteration: 10 loss: 0.15192926961572084 grad: 0.4348722219572461
iteration: 0 loss: 336.99573323505723 grad: 874.0738186664208
iteration: 10 loss: 0.16016125839384834 grad: 0.48562087287865174
iteration: 0 loss: 643.3011725366864 grad: 993.4246097861947
iteration: 10 loss: 0.17110914965831564 grad: 0.6623814897136873
iteration: 0 loss: 432.0475776810538 grad: 867.1022342610652
iteration: 10 loss: 0.15071850953675245 grad: 0.40693185739066096
iteration: 0 loss: 240.29783042489933 grad: 816.8762279208756
iteration: 10 loss: 0.16292108396390342 grad: 0.11645630435348225
iteration: 0 loss: 650.1948248030757 grad: 974.0270380866568
iteration: 10 loss: 0.1511554941128204 grad: 0.5498798559053079
iteration: 0 loss: 303.8074218766178 grad: 813.1389134059293
iteration: 10 loss: 0.17710803701232758 grad: 0.5074848060929207
iteration: 0 loss: 745.8170360720422 grad: 991.1831694082575
iteration: 10 loss: 0.16056249307555184 grad: 0.5945745707592891
iteration: 0 loss: 269.1202547631665 grad: 770.9705420889784
iteration: 10 loss: 0.17014437558381443 grad: 0.34774322399252855
iteration: 0 loss: 600.2239095799266 grad: 944.1167218452154
iteration: 10 loss: 0.14212108028153703 grad: 0.5465988805899609
iteration: 0 loss: 294.80455065669315 grad: 764.8826201394186
iteration: 10 loss: 0.16401910174516326 grad: 0.3706124296039811
iteration: 0 loss: 437.283169261478 grad: 936.0381331133245
iteration: 10 loss: 0.16181845465441505 grad: 0.3565681812921202
iteration: 0 loss: 308.0126134149572 grad: 832.6320600592263
iteration: 10 loss: 0.16175064049540114 grad: 0.3901757528182255
iteration: 0 loss: 383.81803232843544 grad: 869.9734146156891
iteration: 10 loss: 0.15470734183988746 grad: 0.45232050828534
iteration: 0 loss: 459.72176669558036 grad: 867.112406135163
iteration: 10 loss: 0.1535250821512322 grad: 0.45249431129473416
iteration: 0 loss: 228.3249010188785 grad: 736.9139381995436
iteration: 10 loss: 0.1634216948292636 grad: 0.16836449866666048
iteration: 0 loss: 566.9269158995456 grad: 898.1632487633052
iteration: 10 loss: 0.14727854064999105 grad: 0.5484954895198755
iteration: 0 loss: 525.4225466497595 grad: 910.4432592821026
iteration: 10 loss: 0.16470768066904962 grad: 0.5116544515708288
iteration: 0 loss: 6349.981403064784 grad: 2370.1749815976136
iteration: 10 loss: 0.0018240642331709917 grad: -0.4959304702853294
iteration: 0 loss: 5105.842091399853 grad: 2222.719971427659
iteration: 0 loss: 5625.514291029025 grad: 2250.4469215449167
iteration: 10 loss: 0.0015968395124549386 grad: -0.012458779034910435
iteration: 0 loss: 7085.3231623986 grad: 2466.809580300137
iteration: 10 loss: 0.0015215019044593316 grad: -0.029116647467207154
iteration: 0 loss: 4355.74127984522 grad: 2146.1884801102137
iteration: 0 loss: 2926.5020212745226 grad: 1914.1309286813503
iteration: 10 loss: 0.0016882623577575114 grad: -0.009916484725166212
iteration: 0 loss: 4694.00216452069 grad: 2213.332280040711
iteration: 0 loss: 2484.238007050821 grad: 1809.2297525175923
iteration: 0 loss: 6948.615526198087 grad: 2318.5064207684277
iteration: 10 loss: 0.0016476609267887068 grad: 0.008954891620799163
iteration: 0 loss: 4560.209675387714 grad: 2155.513942391275
iteration: 0 loss: 2815.452396879102 grad: 1974.5616154418674
iteration: 0 loss: 5802.341903037029 grad: 2320.734936610103
iteration: 10 loss: 0.0022401712035802616 grad: -0.3014290633923
iteration: 0 loss: 3984.987835854004 grad: 1965.206035028566
iteration: 0 loss: 4253.96868400244 grad: 2191.773211521894
iteration: 0 loss: 5625.239375085115 grad: 2229.267063507461
iteration: 0 loss: 8150.439990602175 grad: 2524.3420773411785
iteration: 10 loss: 0.005271026344747621 grad: 0.4398053795397488
iteration: 0 loss: 4838.721296922657 grad: 2163.050277192403
iteration: 10 loss: 0.001703126219746826 grad: 0.010196046455608879
iteration: 0 loss: 3406.801911415202 grad: 2017.4898694202022
iteration: 0 loss: 8227.654818269411 grad: 2527.2856960209892
iteration: 10 loss: 0.0033270268111002884 grad: 1.2570206850711456
iteration: 0 loss: 4004.6901206741413 grad: 2044.1201971173432
iteration: 0 loss: 5822.800610930134 grad: 2319.6390862866247
iteration: 0 loss: 7027.8106830307115 grad: 2406.4161712889536
iteration: 10 loss: 0.0024508799185489556 grad: 0.7084620898485352
iteration: 0 loss: 3647.429831598478 grad: 1939.5779617971025
iteration: 0 loss: 4357.483109205547 grad: 2140.20740605704
iteration: 10 loss: 0.0016633754103994843 grad: 0.022269462981659313
iteration: 0 loss: 4506.054822579798 grad: 2244.5591876651697
iteration: 0 loss: 3873.4526135880387 grad: 2164.629313704464
iteration: 0 loss: 8857.973913524207 grad: 2539.3368422815875
iteration: 10 loss: 0.023464031987135637 grad: -1.4804427657245915
iteration: 0 loss: 8632.672622418464 grad: 2459.897185348591
iteration: 10 loss: 13.376983204167283 grad: -18.78501874263903
iteration: 0 loss: 4938.777010409141 grad: 2212.567824615915
iteration: 0 loss: 3687.6753658767193 grad: 2038.124195548284
iteration: 10 loss: 0.0013168229488655925 grad: 0.21499409840004557
iteration: 0 loss: 10153.876164949797 grad: 2684.098772438366
iteration: 10 loss: 0.13600183918209618 grad: 2.7678323015896753
iteration: 0 loss: 4962.965460920778 grad: 2337.9416207594104
iteration: 0 loss: 4769.623823616936 grad: 2207.1043123167956
iteration: 10 loss: 0.0017133190465921705 grad: -0.04222170098475631
iteration: 0 loss: 6706.317731067052 grad: 2290.478056663371
iteration: 10 loss: 0.0021291417519519614 grad: 0.6852544509450423
iteration: 0 loss: 6542.6873912369565 grad: 2364.3303090421578
iteration: 10 loss: 0.0016618659428786486 grad: -0.015053793389230196
iteration: 0 loss: 5460.9870417625425 grad: 2297.25528514395
iteration: 10 loss: 0.05291231840965338 grad: -0.9750937197705303
iteration: 0 loss: 5155.148087392788 grad: 2202.203690316398
iteration: 10 loss: 0.0018564155827996067 grad: 0.07957200980663784
iteration: 0 loss: 8220.970957011554 grad: 2544.4443594536215
iteration: 10 loss: 0.001560801786994985 grad: -0.07993554597139796
iteration: 0 loss: 3836.7016942160135 grad: 2096.503446087281
iteration: 0 loss: 4284.112369213657 grad: 2135.6308770977985
iteration: 0 loss: 5468.403847805604 grad: 2239.0285867969924
iteration: 0 loss: 2734.2171646414904 grad: 1944.5086580594361
iteration: 0 loss: 11506.855212065728 grad: 2767.254949538743
iteration: 10 loss: 1.2528532786342443 grad: 3.6696309563088216
iteration: 0 loss: 4528.0610207652535 grad: 2060.2876002358444
iteration: 10 loss: 0.0014655034941494125 grad: -0.009603655146582443
iteration: 0 loss: 6236.060265680706 grad: 2475.374734848626
iteration: 0 loss: 6371.244988992996 grad: 2288.5106934772557
iteration: 0 loss: 4906.421340823927 grad: 2008.5246867333967
iteration: 0 loss: 3993.768251053211 grad: 2173.125599551473
iteration: 10 loss: 0.0013455590868199413 grad: 0.01768783422673316
iteration: 0 loss: 7037.191209104239 grad: 2400.925500057292
iteration: 10 loss: 0.0015443055030614646 grad: -0.1217113774307784
iteration: 0 loss: 7295.229767511294 grad: 2413.2720956964786
iteration: 10 loss: 0.05534641782948959 grad: 0.9544332604189728
iteration: 0 loss: 8568.772585655262 grad: 2490.223151199055
iteration: 10 loss: 0.72967178664112 grad: 3.060009012142415
iteration: 0 loss: 4537.132433710126 grad: 2240.793738628447
iteration: 0 loss: 7743.145919209535 grad: 2553.5341193144964
iteration: 10 loss: 0.0016071400459622964 grad: 0.6345874464007806
iteration: 0 loss: 5654.591725746417 grad: 2284.8110153682683
iteration: 0 loss: 3800.8133243097664 grad: 2066.1000469199043
iteration: 0 loss: 5049.588827137076 grad: 2224.904346766615
iteration: 10 loss: 0.0015600178184368733 grad: 0.023701733722032978
iteration: 0 loss: 3788.8429016639802 grad: 2157.8002949004394
iteration: 0 loss: 5577.892204331025 grad: 2281.277269613758
iteration: 10 loss: 0.0014140541712880474 grad: 0.025083638083821365
iteration: 0 loss: 8320.896169305264 grad: 2592.5267583211207
iteration: 10 loss: 0.0015980090564963493 grad: 0.5513722143611925
iteration: 0 loss: 4486.916909708806 grad: 2194.702392657994
iteration: 0 loss: 7317.21703503447 grad: 2424.243937235312
iteration: 10 loss: 0.018231275326053783 grad: 0.02059132822548501
iteration: 0 loss: 7285.8344823238995 grad: 2558.675262320058
iteration: 10 loss: 0.0012530635417947037 grad: -0.0034870732463851382
iteration: 0 loss: 3773.8203311659386 grad: 1899.6200942332912
iteration: 0 loss: 3960.890032348504 grad: 2088.451813295391
iteration: 0 loss: 4752.258681091337 grad: 2259.74041037531
iteration: 0 loss: 4899.199299926735 grad: 2210.596482947465
iteration: 0 loss: 3678.8170109642274 grad: 1989.9620798474334
iteration: 0 loss: 6696.118545192344 grad: 2434.7741102895952
iteration: 10 loss: 0.0018832344027363104 grad: 0.25164363429255665
iteration: 0 loss: 5409.094387714613 grad: 2231.335306634718
iteration: 0 loss: 5412.491868250417 grad: 2371.6172564912004
iteration: 0 loss: 6504.648849663441 grad: 2418.0044510401294
iteration: 10 loss: 0.0015960733422120525 grad: 0.0015254483790500643
iteration: 0 loss: 9894.568351611764 grad: 2693.142732524081
iteration: 10 loss: 0.20552736087120138 grad: 3.093234679809179
iteration: 0 loss: 4133.3364495218375 grad: 2077.8396426211893
iteration: 0 loss: 6839.926566033638 grad: 2403.0907915585876
iteration: 10 loss: 0.001453034673679874 grad: 0.7005838719575174
iteration: 0 loss: 4329.91430542726 grad: 2163.3148942561043
iteration: 10 loss: 0.0016084197625539011 grad: 0.027541801578321708
iteration: 0 loss: 3436.5376110569778 grad: 2006.531679186972
iteration: 0 loss: 4382.436204380306 grad: 2028.718696751215
iteration: 0 loss: 3544.3878597614485 grad: 2012.7869439643302
iteration: 0 loss: 5203.935408001125 grad: 2340.209223747372
iteration: 0 loss: 7276.826679534252 grad: 2372.016971720798
iteration: 10 loss: 0.003577473218849098 grad: -0.9594383371643653
iteration: 0 loss: 8282.161258268405 grad: 2571.151500974429
iteration: 10 loss: 0.0015281689636388116 grad: 0.17472257392547472
iteration: 0 loss: 7642.555424429587 grad: 2453.3219685685913
iteration: 10 loss: 0.14513889750841455 grad: -2.405132131954742
iteration: 0 loss: 6219.244577675461 grad: 2361.6577200555175
iteration: 0 loss: 7851.77939181723 grad: 2454.5492370353672
iteration: 10 loss: 0.005102254782369445 grad: 0.357962721366649
iteration: 0 loss: 7771.862752233624 grad: 2461.1630459582066
iteration: 10 loss: 0.06849278387513054 grad: 0.7473479650448545
iteration: 0 loss: 5882.568319536752 grad: 2137.7981644004904
iteration: 10 loss: 0.024963587189135564 grad: 0.47803600459026346
iteration: 0 loss: 9246.32920257477 grad: 2524.7161339076515
iteration: 10 loss: 0.028571450810307975 grad: 1.7409883054824777
iteration: 0 loss: 6330.365068143977 grad: 2265.09443471764
iteration: 10 loss: 0.0019356909681598402 grad: 0.06282990008406138
iteration: 0 loss: 3392.035726437521 grad: 1939.951488862604
iteration: 0 loss: 2949.0252550404807 grad: 1842.220791212088
iteration: 0 loss: 4089.54958811239 grad: 2016.9759335787487
iteration: 0 loss: 5427.235171660435 grad: 2317.0402814222725
iteration: 0 loss: 5154.643337623387 grad: 2176.661394864612
iteration: 0 loss: 4258.45576653066 grad: 2108.4646693259742
iteration: 0 loss: 3657.722620241979 grad: 2040.866621699855
iteration: 0 loss: 7486.68927654588 grad: 2561.124509288269
iteration: 10 loss: 0.0016823823838918047 grad: -0.4771110597417811
iteration: 0 loss: 6071.114453699896 grad: 2324.994897784029
iteration: 10 loss: 0.0016741321120538157 grad: 0.025692998362754474
iteration: 0 loss: 5577.446053281948 grad: 2243.120354703299
iteration: 10 loss: 0.0015588314603188667 grad: -0.004985808985478639
iteration: 0 loss: 5025.919592832694 grad: 2224.506790755072
iteration: 0 loss: 3634.376618132467 grad: 1916.8070213335895
iteration: 0 loss: 5950.490824500744 grad: 2299.1105036837507
iteration: 0 loss: 4431.031538651954 grad: 2124.689763098975
iteration: 0 loss: 4524.304033182063 grad: 2233.4695748420363
iteration: 0 loss: 10059.638190842925 grad: 2753.1929120602986
iteration: 10 loss: 0.051851578275091015 grad: -0.019405892557402687
iteration: 0 loss: 3341.661024608335 grad: 2015.3549424512507
iteration: 0 loss: 3300.5003939238522 grad: 1940.0335201378439
iteration: 0 loss: 3932.933090811184 grad: 2045.7483272817094
iteration: 0 loss: 4954.879929693293 grad: 2275.3619515347405
iteration: 0 loss: 6031.879501990682 grad: 2336.899679881789
iteration: 10 loss: 0.001316804438829422 grad: -0.0020316447736897433
iteration: 0 loss: 6866.08947818545 grad: 2493.2613832535744
iteration: 10 loss: 0.001368637195396745 grad: -0.06938931702468815
iteration: 0 loss: 8484.846707060156 grad: 2448.6087498138395
iteration: 10 loss: 0.29990760869854555 grad: 0.31108214965533687
iteration: 0 loss: 6659.827507508683 grad: 2356.647907559339
iteration: 10 loss: 0.012677037850318646 grad: 1.9169839584964947
iteration: 0 loss: 5187.611415088836 grad: 2237.9921219198272
iteration: 0 loss: 8532.926987557828 grad: 2522.958253866854
iteration: 10 loss: 0.041617897906689905 grad: 0.5753300761194864
iteration: 0 loss: 6501.775258817538 grad: 2356.6464180919993
iteration: 0 loss: 4191.643008115496 grad: 2212.568770157346
iteration: 10 loss: 0.0015757227070439099 grad: 0.02159421620413293
iteration: 0 loss: 4864.867128144083 grad: 2218.3197333083895
iteration: 0 loss: 4410.30855553324 grad: 2178.0776403726372
iteration: 0 loss: 7755.748509249286 grad: 2529.190599978301
iteration: 10 loss: 0.0024943639159160243 grad: 3.42267045458151
iteration: 0 loss: 4056.692774676603 grad: 2228.156137185036
iteration: 0 loss: 4950.604551903221 grad: 2123.7454167024766
iteration: 0 loss: 2749.437582964296 grad: 1897.8752453638954
iteration: 0 loss: 5843.109122976497 grad: 2424.036515762343
iteration: 0 loss: 5660.282498482824 grad: 2356.856418723175
iteration: 0 loss: 5622.237143254277 grad: 2324.486888768683
iteration: 10 loss: 0.0019122570149854503 grad: 0.011170983819719052
iteration: 0 loss: 8049.647375494407 grad: 2414.820971897269
iteration: 10 loss: 0.40360194217778783 grad: 0.9973845068794823
iteration: 0 loss: 4554.686573977639 grad: 2185.3377282435695
iteration: 0 loss: 5252.272825882417 grad: 2277.613167073116
iteration: 10 loss: 0.0014307170720051297 grad: -0.0042538763484726285
iteration: 0 loss: 8160.99854808383 grad: 2536.285568733061
iteration: 10 loss: 0.0012510904317340728 grad: 0.18365387268418165
iteration: 0 loss: 5133.474027743213 grad: 2165.347666404542
iteration: 0 loss: 6967.512235878763 grad: 2317.1916071662813
iteration: 10 loss: 0.0017759568727342412 grad: 0.008672728262146465
iteration: 0 loss: 4476.644322085884 grad: 2209.574508105402
iteration: 0 loss: 7577.48204655876 grad: 2548.8494461480745
iteration: 0 loss: 6531.749329801571 grad: 2414.056336587242
iteration: 0 loss: 4173.87564941689 grad: 2004.3285528056158
iteration: 0 loss: 3307.0794843439216 grad: 1944.065751819654
iteration: 0 loss: 5372.371367551852 grad: 2392.7567668682223
iteration: 0 loss: 2733.859970414288 grad: 1866.4519938898748
iteration: 0 loss: 3688.2086074540803 grad: 1884.686092200561
iteration: 0 loss: 3240.2652850282584 grad: 1927.3480399327022
iteration: 0 loss: 7786.84743992989 grad: 2459.9455163352063
iteration: 10 loss: 0.0017799418406370519 grad: 0.7534208117168281
iteration: 0 loss: 4749.348441389873 grad: 2108.0042927987924
iteration: 0 loss: 8216.63547797489 grad: 2449.873431622555
iteration: 10 loss: 0.5507979110724673 grad: 2.0325675462715855
iteration: 0 loss: 7699.130916683888 grad: 2454.5506873383865
iteration: 10 loss: 0.0018793149131604216 grad: 0.03321268173231927
iteration: 0 loss: 9220.59412407557 grad: 2629.001392709565
iteration: 10 loss: 0.03800475663658951 grad: 1.6683604005307202
iteration: 0 loss: 4173.915068700627 grad: 2081.9005053219903
iteration: 0 loss: 4235.092287909934 grad: 2112.613556860335
iteration: 0 loss: 5507.7266833407575 grad: 2372.013219984566
iteration: 10 loss: 0.0016191030564104121 grad: 0.019268062265255893
iteration: 0 loss: 5604.751650636255 grad: 2294.8157100447324
iteration: 0 loss: 10533.965292392539 grad: 2656.545347351185
iteration: 10 loss: 0.1985497657892252 grad: 1.9036826394544752
iteration: 0 loss: 4684.086715308166 grad: 2248.5589947633225
iteration: 0 loss: 8133.744485883301 grad: 2544.096591075402
iteration: 10 loss: 0.0019165589593202722 grad: 2.393565679689729
iteration: 0 loss: 5320.536901500934 grad: 2233.948498709438
iteration: 0 loss: 3571.5758241864373 grad: 2103.2041023337106
iteration: 0 loss: 7641.028163655073 grad: 2497.6347378796963
iteration: 10 loss: 0.002179865738567473 grad: -0.25379842952991677
iteration: 0 loss: 3777.856986985722 grad: 2091.709491473266
iteration: 0 loss: 9221.620993622233 grad: 2539.4507767691703
iteration: 10 loss: 0.6824547844814581 grad: 5.022307803797656
iteration: 0 loss: 3342.94595403554 grad: 1987.1265202330544
iteration: 0 loss: 7192.611596943612 grad: 2417.7256887232234
iteration: 10 loss: 0.0016223555007441478 grad: 0.03486463623070664
iteration: 0 loss: 3697.541138225884 grad: 1973.6662031165451
iteration: 0 loss: 5558.731135842886 grad: 2404.869041047401
iteration: 0 loss: 4041.356759427469 grad: 2142.7204124803466
iteration: 0 loss: 5458.774980464309 grad: 2233.4444986763997
iteration: 10 loss: 0.0019458791159029881 grad: 0.06733713079375345
iteration: 0 loss: 5261.987432902774 grad: 2228.172446598956
iteration: 0 loss: 2994.857809438072 grad: 1902.1402348923934
iteration: 0 loss: 6628.309616351725 grad: 2305.8161474799135
iteration: 10 loss: 0.00218467068572698 grad: 0.41196592515214164
iteration: 0 loss: 6441.578573195576 grad: 2335.518584202152
iteration: 10 loss: 0.0015167806175833737 grad: 0.022652870226441163
iteration: 0 loss: inf grad: 3556.5275521531807
iteration: 0 loss: inf grad: 3333.2352376036592
iteration: 0 loss: inf grad: 3374.7446991973347
iteration: 0 loss: inf grad: 3704.387385553947
iteration: 0 loss: inf grad: 3222.969502870507
iteration: 0 loss: 7398.318226871075 grad: 2872.4137796583655
iteration: 10 loss: 0.00023732412691143426 grad: 0.0013375621385178574
iteration: 0 loss: inf grad: 3321.875779604942
iteration: 0 loss: 6392.511840377955 grad: 2717.010901848447
iteration: 0 loss: inf grad: 3477.8154452414124
iteration: 0 loss: inf grad: 3239.0260739956375
iteration: 0 loss: 7389.104677943256 grad: 2966.10456973296
iteration: 0 loss: inf grad: 3484.3020529600367
iteration: 0 loss: 9928.289143810673 grad: 2951.0089674989567
iteration: 0 loss: inf grad: 3290.7362328514846
iteration: 10 loss: 0.00023091695832342586 grad: 0.30552023164157255
iteration: 0 loss: inf grad: 3340.712599028629
iteration: 0 loss: inf grad: 3784.1264189009044
iteration: 0 loss: inf grad: 3248.4697990645036
iteration: 0 loss: 8569.692790885783 grad: 3028.3547701938605
iteration: 0 loss: inf grad: 3788.390024450479
iteration: 0 loss: 10128.71372112063 grad: 3069.7937173055907
iteration: 0 loss: inf grad: 3477.0908824536837
iteration: 0 loss: inf grad: 3611.218038728814
iteration: 0 loss: inf grad: 2913.43935114776
iteration: 0 loss: inf grad: 3211.989769345313
iteration: 10 loss: 0.0002388769353274256 grad: -0.0010447204856300738
iteration: 0 loss: inf grad: 3371.43552273025
iteration: 0 loss: inf grad: 3252.2492983287148
iteration: 10 loss: 0.00020949471614916217 grad: 0.005687425052520919
iteration: 0 loss: inf grad: 3812.094277495564
iteration: 0 loss: inf grad: 3691.2528524123645
iteration: 0 loss: inf grad: 3318.272230463006
iteration: 0 loss: inf grad: 3058.8003679839417
iteration: 10 loss: 0.00018953866145404226 grad: 0.524313136559273
iteration: 0 loss: inf grad: 4026.318570521438
iteration: 0 loss: 12513.239908107991 grad: 3509.643680485636
iteration: 0 loss: inf grad: 3313.8845949714582
iteration: 0 loss: inf grad: 3436.252419313797
iteration: 0 loss: inf grad: 3543.2543038798367
iteration: 0 loss: inf grad: 3447.8427219826854
iteration: 0 loss: inf grad: 3305.659558714919
iteration: 0 loss: inf grad: 3816.2226238857097
iteration: 0 loss: inf grad: 3150.6919552453082
iteration: 0 loss: inf grad: 3205.9881914514194
iteration: 0 loss: inf grad: 3360.091896746071
iteration: 0 loss: 7085.287040031841 grad: 2918.72103273634
iteration: 0 loss: inf grad: 4153.652110163469
iteration: 0 loss: inf grad: 3094.5136626044355
iteration: 0 loss: inf grad: 3715.138999258242
iteration: 0 loss: inf grad: 3432.9428247438636
iteration: 0 loss: inf grad: 3025.744958623289
iteration: 0 loss: 10189.92617007301 grad: 3262.736888784625
iteration: 0 loss: inf grad: 3604.206682714461
iteration: 0 loss: inf grad: 3621.56305443167
iteration: 0 loss: inf grad: 3735.3715667623264
iteration: 0 loss: inf grad: 3361.184915392955
iteration: 10 loss: 0.0002701074858619408 grad: 0.006309862823269395
iteration: 0 loss: inf grad: 3832.9921348322455
iteration: 0 loss: inf grad: 3430.3875143625455
iteration: 0 loss: 9669.674751192711 grad: 3099.093695981463
iteration: 0 loss: inf grad: 3343.327105917746
iteration: 0 loss: 9779.617430625092 grad: 3241.154813636647
iteration: 0 loss: inf grad: 3427.0017666849762
iteration: 0 loss: inf grad: 3891.3766832481783
iteration: 0 loss: 11305.461004379433 grad: 3294.016299571232
iteration: 0 loss: inf grad: 3637.171976137579
iteration: 0 loss: inf grad: 3835.5451154576604
iteration: 0 loss: inf grad: 2854.137878717183
iteration: 0 loss: inf grad: 3133.316083312896
iteration: 0 loss: inf grad: 3392.0226690005616
iteration: 0 loss: inf grad: 3315.173098303414
iteration: 0 loss: 9223.698490543939 grad: 2985.6625535923367
iteration: 0 loss: inf grad: 3654.0310605703035
iteration: 0 loss: inf grad: 3350.8594939717973
iteration: 0 loss: inf grad: 3558.825322088162
iteration: 0 loss: inf grad: 3632.1998660499257
iteration: 0 loss: inf grad: 4040.0917896917003
iteration: 0 loss: inf grad: 3118.3795989094856
iteration: 0 loss: inf grad: 3605.425196791642
iteration: 0 loss: inf grad: 3243.3738224294066
iteration: 0 loss: 8691.419246576716 grad: 3016.0372013573497
iteration: 0 loss: inf grad: 3041.830853688112
iteration: 0 loss: 9028.474775886161 grad: 3018.710460990311
iteration: 0 loss: inf grad: 3510.3384381096807
iteration: 0 loss: inf grad: 3559.437297676348
iteration: 0 loss: inf grad: 3855.2563659658053
iteration: 0 loss: inf grad: 3681.612286006832
iteration: 0 loss: inf grad: 3540.6468214131282
iteration: 0 loss: inf grad: 3685.5262872274657
iteration: 0 loss: inf grad: 3693.2984948883623
iteration: 0 loss: inf grad: 3210.823746684373
iteration: 0 loss: inf grad: 3789.190560860283
iteration: 0 loss: inf grad: 3396.526915942549
iteration: 0 loss: inf grad: 2909.7301277776346
iteration: 0 loss: 7466.930078349522 grad: 2763.38118884635
iteration: 0 loss: inf grad: 3029.4205525875623
iteration: 0 loss: inf grad: 3476.4042936769633
iteration: 0 loss: inf grad: 3264.023206816442
iteration: 0 loss: inf grad: 3168.7084176486123
iteration: 0 loss: inf grad: 3064.7244483039194
iteration: 0 loss: inf grad: 3844.578117985944
iteration: 0 loss: inf grad: 3486.118746659542
iteration: 0 loss: inf grad: 3366.6934728946644
iteration: 0 loss: inf grad: 3337.4526743139895
iteration: 0 loss: 8984.129987940863 grad: 2879.73192709272
iteration: 0 loss: inf grad: 3453.4603121330215
iteration: 0 loss: 11219.171807763014 grad: 3189.1387084331236
iteration: 0 loss: inf grad: 3353.375297956291
iteration: 0 loss: inf grad: 4132.160383341575
iteration: 0 loss: inf grad: 3027.1143799641054
iteration: 0 loss: inf grad: 2911.7547011612423
iteration: 0 loss: inf grad: 3069.0974326003734
iteration: 10 loss: 0.00024981156721795827 grad: 0.006847550115783354
iteration: 0 loss: inf grad: 3414.7284349452334
iteration: 0 loss: inf grad: 3506.6428066449043
iteration: 0 loss: inf grad: 3744.884780436575
iteration: 0 loss: inf grad: 3675.6409552039586
iteration: 0 loss: inf grad: 3534.076782784334
iteration: 0 loss: 13001.383804785357 grad: 3358.21806813422
iteration: 0 loss: inf grad: 3786.429018723741
iteration: 0 loss: inf grad: 3537.3897569050146
iteration: 0 loss: inf grad: 3320.2244710750906
iteration: 10 loss: 0.0002031660141338679 grad: -0.009973605187217969
iteration: 0 loss: inf grad: 3334.1354072620115
iteration: 10 loss: 0.0002918812732042914 grad: -0.0030446087374446807
iteration: 0 loss: inf grad: 3266.9442422769607
iteration: 10 loss: 0.000361545371230353 grad: -0.00803794355747128
iteration: 0 loss: inf grad: 3793.6762740287813
iteration: 0 loss: inf grad: 3346.14547857621
iteration: 10 loss: 0.00024368820066394454 grad: 0.0028107176118681
iteration: 0 loss: inf grad: 3188.4676990758126
iteration: 0 loss: inf grad: 2848.537638875209
iteration: 0 loss: inf grad: 3633.586020592533
iteration: 0 loss: inf grad: 3535.981481283071
iteration: 0 loss: inf grad: 3487.32838200185
iteration: 0 loss: inf grad: 3620.795754153337
iteration: 0 loss: inf grad: 3278.1770972030663
iteration: 0 loss: inf grad: 3418.2987194599746
iteration: 0 loss: inf grad: 3804.966601027243
iteration: 0 loss: inf grad: 3245.2615792003185
iteration: 0 loss: inf grad: 3477.51307038312
iteration: 0 loss: inf grad: 3319.6588520160044
iteration: 0 loss: inf grad: 3822.7139298210295
iteration: 0 loss: inf grad: 3621.942388266557
iteration: 0 loss: inf grad: 3010.874129644642
iteration: 0 loss: 8372.311694313254 grad: 2918.208624899471
iteration: 0 loss: inf grad: 3592.969600726178
iteration: 0 loss: 7050.995321064952 grad: 2799.558293988025
iteration: 0 loss: inf grad: 2830.9758005039553
iteration: 0 loss: inf grad: 2897.813639705129
iteration: 10 loss: 0.00017896991233680058 grad: -0.00247114547135743
iteration: 0 loss: inf grad: 3692.5745001732594
iteration: 0 loss: inf grad: 3158.82467270658
iteration: 10 loss: 0.00023976301732049748 grad: 0.003689092514753983
iteration: 0 loss: inf grad: 3678.7476455493716
iteration: 0 loss: inf grad: 3680.3504759264424
iteration: 0 loss: inf grad: 3943.2527735920753
iteration: 0 loss: inf grad: 3126.8932023291545
iteration: 0 loss: 10679.92277886894 grad: 3168.292320817528
iteration: 0 loss: inf grad: 3557.2699773483923
iteration: 0 loss: inf grad: 3445.8116617256305
iteration: 0 loss: inf grad: 3987.218258183148
iteration: 0 loss: 11869.382054234591 grad: 3372.5474286910303
iteration: 0 loss: inf grad: 3816.102623502149
iteration: 0 loss: inf grad: 3352.0149887439284
iteration: 0 loss: 9234.105008944642 grad: 3156.431507545499
iteration: 0 loss: inf grad: 3745.932166478923
iteration: 0 loss: inf grad: 3140.9489509030473
iteration: 0 loss: inf grad: 3816.5164505336224
iteration: 0 loss: 8503.918256534596 grad: 2985.3707613101233
iteration: 0 loss: inf grad: 3629.1208280942346
iteration: 0 loss: 9384.05387631542 grad: 2966.8235798300066
iteration: 0 loss: inf grad: 3608.713890096845
iteration: 0 loss: inf grad: 3217.264714669943
iteration: 0 loss: inf grad: 3347.763874818323
iteration: 0 loss: inf grad: 3348.9782477042395
iteration: 0 loss: 7647.945449964336 grad: 2850.1172259513874
iteration: 0 loss: inf grad: 3461.134551248913
iteration: 0 loss: inf grad: 3503.8008954285388
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:57: RuntimeWarning: invalid value encountered in double_scalars
  diff = np.absolute(f_history[-1]-f_history[-2])
iteration: 0 loss: 186.30865304782236 grad: 667.2749204870208
iteration: 10 loss: 0.5931846878441899 grad: 0.6946100171784174
iteration: 20 loss: 0.3583163305577479 grad: 0.2062530505233149
iteration: 30 loss: 0.27073396338230604 grad: 0.09294982793423012
iteration: 0 loss: 132.13887861819492 grad: 621.7189916318075
iteration: 10 loss: 0.7591338332280099 grad: 0.377479654030062
iteration: 20 loss: 0.4732831519497102 grad: 0.07261529804349767
iteration: 30 loss: 0.35705456247887923 grad: 0.003274592544851242
iteration: 40 loss: 0.2906918621242353 grad: -0.022225769712060216
iteration: 0 loss: 133.61458589427713 grad: 630.2391838637418
iteration: 10 loss: 0.7496183440412509 grad: 0.27886899984966607
iteration: 20 loss: 0.4615465812705059 grad: 0.017517818764284245
iteration: 30 loss: 0.34594980649275053 grad: -0.03552894756120406
iteration: 40 loss: 0.2806958407973897 grad: -0.05190498173545739
iteration: 0 loss: 211.85076080795815 grad: 694.9453952365651
iteration: 10 loss: 0.6353847287554427 grad: 0.5406824245482635
iteration: 20 loss: 0.38510136285880103 grad: 0.13617852851818604
iteration: 30 loss: 0.2897985470692881 grad: 0.04989066894846411
iteration: 0 loss: 129.7343768501779 grad: 600.5876029095527
iteration: 10 loss: 0.7768627228174823 grad: 0.3993126341602838
iteration: 20 loss: 0.48834846665092546 grad: 0.07577347235612124
iteration: 30 loss: 0.3690254903562659 grad: 0.0035402062320582066
iteration: 40 loss: 0.300583924345946 grad: -0.02322307288627839
iteration: 0 loss: 98.08040506087937 grad: 530.7787875357301
iteration: 10 loss: 0.8071740758577189 grad: 0.6140516789790527
iteration: 20 loss: 0.5048231632517854 grad: 0.15393960332150688
iteration: 30 loss: 0.3808474941429681 grad: 0.04057862154253465
iteration: 40 loss: 0.30977552363806765 grad: -0.0035911669021669846
iteration: 0 loss: 126.8704488793669 grad: 618.4156345852344
iteration: 10 loss: 0.7409396755912728 grad: 0.4805081427119239
iteration: 20 loss: 0.47136161341950356 grad: 0.1615951273241609
iteration: 30 loss: 0.35839418495602743 grad: 0.06859433687407271
iteration: 40 loss: 0.2930805475950613 grad: 0.027455596717207976
iteration: 0 loss: 67.84062030101627 grad: 503.4301244243886
iteration: 10 loss: 1.0386166868030904 grad: 0.7376478593283244
iteration: 20 loss: 0.6422983229654577 grad: 0.222319358593481
iteration: 30 loss: 0.47631144093098887 grad: 0.08394896396852797
iteration: 40 loss: 0.3818550806439486 grad: 0.026839193336945844
iteration: 50 loss: 0.3200667945817589 grad: -0.0014389678285168178
iteration: 0 loss: 219.98993658632082 grad: 651.8077381586568
iteration: 10 loss: 0.6964733092966278 grad: 0.5173696901276594
iteration: 20 loss: 0.4180919329713685 grad: 0.1119451680791273
iteration: 30 loss: 0.30992289301717724 grad: 0.032697240403775356
iteration: 40 loss: 0.2497558718099423 grad: 0.003729369664229309
iteration: 0 loss: 124.74992880676137 grad: 603.4995794360211
iteration: 10 loss: 0.7656132397620571 grad: 0.4117377033065901
iteration: 20 loss: 0.4835393916700923 grad: 0.10810644962582042
iteration: 30 loss: 0.36621584639165533 grad: 0.0338420214861188
iteration: 40 loss: 0.298730757278463 grad: 0.002374477462664319
iteration: 0 loss: 76.01065124998496 grad: 548.1091694467061
iteration: 10 loss: 0.918261655036903 grad: 0.44137319078766446
iteration: 20 loss: 0.5781737199132212 grad: 0.06724361125092797
iteration: 30 loss: 0.43382719396124064 grad: -0.018183233341143443
iteration: 40 loss: 0.3506984110507884 grad: -0.048361824718008546
iteration: 0 loss: 138.13950137168393 grad: 651.867214459568
iteration: 10 loss: 0.7161685139056896 grad: 0.1279566082189061
iteration: 20 loss: 0.4451086184624067 grad: -0.038031091663770195
iteration: 30 loss: 0.33626806095203804 grad: -0.06868117581543787
iteration: 40 loss: 0.2742710603125405 grad: -0.0762476481428759
iteration: 0 loss: 127.42138802456661 grad: 546.8324904715145
iteration: 10 loss: 0.78679159919011 grad: 0.691151659829067
iteration: 20 loss: 0.494465999801915 grad: 0.29445940348266625
iteration: 30 loss: 0.37301835429215396 grad: 0.1655763075019439
iteration: 40 loss: 0.30337948184718366 grad: 0.10320607676822198
iteration: 0 loss: 110.52559883635716 grad: 612.5341169366993
iteration: 10 loss: 0.7642771191430339 grad: 0.25547649189569716
iteration: 20 loss: 0.4839563660865348 grad: -0.0028255536235036044
iteration: 30 loss: 0.3671502380687922 grad: -0.0523611966444986
iteration: 40 loss: 0.2997623752531085 grad: -0.06734225911294305
iteration: 0 loss: 157.65995169506184 grad: 624.4841170926275
iteration: 10 loss: 0.6895440519092517 grad: 0.42441271500245714
iteration: 20 loss: 0.43352641892438853 grad: 0.0882080943355179
iteration: 30 loss: 0.3287058761244969 grad: 0.01893702053894264
iteration: 40 loss: 0.2686533081615297 grad: -0.006901286490488897
iteration: 0 loss: 192.14442525481346 grad: 710.64579282277
iteration: 10 loss: 0.6530764946010922 grad: 0.46353265495063345
iteration: 20 loss: 0.388097952733915 grad: 0.050670657065839286
iteration: 30 loss: 0.28968799501738185 grad: -0.01577900232499507
iteration: 0 loss: 139.65617007273505 grad: 607.0733515354042
iteration: 10 loss: 0.7267269807360374 grad: 0.3472631143642363
iteration: 20 loss: 0.45456387569224305 grad: 0.05257891310980689
iteration: 30 loss: 0.34379639704457077 grad: -0.008650880693298809
iteration: 40 loss: 0.2805094128343626 grad: -0.030138481356021714
iteration: 0 loss: 99.51015927165376 grad: 561.515286744842
iteration: 10 loss: 0.8536942952048941 grad: 0.5867564339570708
iteration: 20 loss: 0.5368201592196287 grad: 0.1902792386725074
iteration: 30 loss: 0.4042396007183594 grad: 0.07862197994578166
iteration: 40 loss: 0.3279755330740128 grad: 0.029959859782910388
iteration: 0 loss: 253.21000086182494 grad: 714.1671998015211
iteration: 10 loss: 0.6431476538779283 grad: 0.6712393396892532
iteration: 20 loss: 0.3711977277637495 grad: 0.23069825194994487
iteration: 30 loss: 0.2744251195741857 grad: 0.12223553668517276
iteration: 0 loss: 111.61572727159091 grad: 569.1623601524927
iteration: 10 loss: 0.8323447440813073 grad: 0.47028639365585756
iteration: 20 loss: 0.5241287605375956 grad: 0.1557669254625946
iteration: 30 loss: 0.3951856080372057 grad: 0.061484276759814535
iteration: 40 loss: 0.3210513645448042 grad: 0.02015491697969532
iteration: 0 loss: 147.4621954049545 grad: 649.430542385232
iteration: 10 loss: 0.699055964756388 grad: 0.1981399437798187
iteration: 20 loss: 0.4370755215441849 grad: 0.02115392607360776
iteration: 30 loss: 0.33070314215547114 grad: -0.01819011192188594
iteration: 40 loss: 0.2700527322696706 grad: -0.03177215814172769
iteration: 0 loss: 162.7859743980847 grad: 676.5297883911381
iteration: 10 loss: 0.6818539791252491 grad: 0.42006417655513595
iteration: 20 loss: 0.4117000935769035 grad: 0.06957699249633809
iteration: 30 loss: 0.3079907134235162 grad: -0.0024235500015012046
iteration: 0 loss: 136.9008673861729 grad: 541.2863137098252
iteration: 10 loss: 0.7497375013468958 grad: 0.6460973264150238
iteration: 20 loss: 0.4707538421386992 grad: 0.18910189784135348
iteration: 30 loss: 0.3560222486588497 grad: 0.07724904035787028
iteration: 40 loss: 0.2902991532432266 grad: 0.031646559968974176
iteration: 0 loss: 85.3223874197172 grad: 599.0907505083144
iteration: 10 loss: 0.862037945377944 grad: 0.16567367572646086
iteration: 20 loss: 0.5356314147480092 grad: -0.06573853523161008
iteration: 30 loss: 0.4011525346024262 grad: -0.09774465153461678
iteration: 40 loss: 0.32450073895473575 grad: -0.10181324829090013
iteration: 0 loss: 122.98853628277274 grad: 627.3941802800301
iteration: 10 loss: 0.6878965247954263 grad: 0.2828641544922529
iteration: 20 loss: 0.4365738299877855 grad: 0.03422043326722012
iteration: 30 loss: 0.3338502427646249 grad: -0.020311077251839033
iteration: 40 loss: 0.2745145396390647 grad: -0.039077448263694255
iteration: 0 loss: 127.7789826419487 grad: 604.2678236907504
iteration: 10 loss: 0.7727206891733492 grad: 0.5348608061581597
iteration: 20 loss: 0.4783802388179042 grad: 0.15469365422294867
iteration: 30 loss: 0.36047702542344434 grad: 0.05417544298234518
iteration: 40 loss: 0.29339484596667637 grad: 0.011850846166961573
iteration: 0 loss: 277.77845135727745 grad: 717.6872398031269
iteration: 10 loss: 0.6279186099076965 grad: 0.8524856130488433
iteration: 20 loss: 0.3609314766879999 grad: 0.29794261229733676
iteration: 30 loss: 0.266173024765108 grad: 0.16629357270992193
iteration: 0 loss: 251.10098509116526 grad: 691.5158144330605
iteration: 10 loss: 0.7373455226268727 grad: 0.7013214443998934
iteration: 20 loss: 0.3999832036051058 grad: 0.21935477566956219
iteration: 30 loss: 0.29235665568012337 grad: 0.09917857241336253
iteration: 0 loss: 111.07300862540279 grad: 621.946492005838
iteration: 10 loss: 0.7545335662629937 grad: 0.060461957485904785
iteration: 20 loss: 0.47700437085619674 grad: -0.09425293133230764
iteration: 30 loss: 0.36089979030417785 grad: -0.10843137190101752
iteration: 40 loss: 0.2940434856532606 grad: -0.10571309879434926
iteration: 0 loss: 113.30051945465749 grad: 569.8293709429907
iteration: 10 loss: 0.8159410834056292 grad: 0.5112560274211218
iteration: 20 loss: 0.508265670125845 grad: 0.12134439534269513
iteration: 30 loss: 0.38241446312630695 grad: 0.029434504715717305
iteration: 40 loss: 0.3105762784989565 grad: -0.006308547846982763
iteration: 0 loss: 304.574111977194 grad: 758.8629577364383
iteration: 10 loss: 0.5873980536018114 grad: 0.4610645446703566
iteration: 20 loss: 0.3373895492901614 grad: 0.0977144375968589
iteration: 30 loss: 0.25019355887682404 grad: 0.029486325474934474
iteration: 0 loss: 129.96666011480954 grad: 656.2672599196126
iteration: 10 loss: 0.7501966468891458 grad: 0.0703941659773735
iteration: 20 loss: 0.4772694540747166 grad: -0.023738266314531825
iteration: 30 loss: 0.36305696736351156 grad: -0.041093625967782044
iteration: 40 loss: 0.29698268245820975 grad: -0.04686506553623923
iteration: 0 loss: 128.89152431960864 grad: 616.9535424458803
iteration: 10 loss: 0.7785305780124955 grad: 0.39331295381656994
iteration: 20 loss: 0.4842292656281147 grad: 0.07577275836369504
iteration: 30 loss: 0.36451368003632856 grad: 0.004276566607095305
iteration: 40 loss: 0.29623968155944813 grad: -0.022334785950414056
iteration: 0 loss: 179.8921290242184 grad: 642.7137492117429
iteration: 10 loss: 0.7028297604754864 grad: 0.46950815834263687
iteration: 20 loss: 0.429226786131618 grad: 0.13629494313495003
iteration: 30 loss: 0.3215968198606292 grad: 0.04854498512778922
iteration: 40 loss: 0.26103559281091293 grad: 0.012713214612774446
iteration: 0 loss: 178.73748261564143 grad: 665.5317340142418
iteration: 10 loss: 0.6894053500012557 grad: 0.48179502185709283
iteration: 20 loss: 0.4187435950975255 grad: 0.12405932910048423
iteration: 30 loss: 0.31389614566416474 grad: 0.041606457280166244
iteration: 0 loss: 145.3492083410317 grad: 645.149346176317
iteration: 10 loss: 0.7122931492453559 grad: 0.6317446374932654
iteration: 20 loss: 0.4374061985091347 grad: 0.17442816273942435
iteration: 30 loss: 0.32946958756218175 grad: 0.059748349643299545
iteration: 40 loss: 0.2685334698765225 grad: 0.013455542189986104
iteration: 0 loss: 151.12678111446374 grad: 617.6904327439787
iteration: 10 loss: 0.6814215668739714 grad: 0.6723597808815136
iteration: 20 loss: 0.4272589577822972 grad: 0.17970124444155477
iteration: 30 loss: 0.32531579151716206 grad: 0.0634006991871991
iteration: 0 loss: 188.7321025907748 grad: 717.2083981657248
iteration: 10 loss: 0.6175965580178772 grad: 0.37148366758978685
iteration: 20 loss: 0.3746875941205045 grad: 0.06885099486613193
iteration: 30 loss: 0.28229364137107704 grad: 0.00573757902829599
iteration: 0 loss: 122.16279632753081 grad: 585.6729887094365
iteration: 10 loss: 0.7379261868675464 grad: 0.6249259693375491
iteration: 20 loss: 0.46458125349561835 grad: 0.19346630884535393
iteration: 30 loss: 0.3522044330418959 grad: 0.07987655987319567
iteration: 40 loss: 0.28763984870990744 grad: 0.03174115051151208
iteration: 0 loss: 112.078427831679 grad: 598.2605420125328
iteration: 10 loss: 0.7769757816553134 grad: 0.3512608172176096
iteration: 20 loss: 0.4924919848372982 grad: 0.058022233483524394
iteration: 30 loss: 0.37310734393891337 grad: -0.009236876386862714
iteration: 40 loss: 0.3041374617853514 grad: -0.03379716508901766
iteration: 0 loss: 199.3869754326274 grad: 629.5088546823265
iteration: 10 loss: 0.6644624901219603 grad: 0.770501682586054
iteration: 20 loss: 0.40244365615630656 grad: 0.2402861131881838
iteration: 30 loss: 0.3017549049172885 grad: 0.11232385744645912
iteration: 0 loss: 73.61388312179956 grad: 539.5997051423896
iteration: 10 loss: 0.965561186506999 grad: 0.41591995772107127
iteration: 20 loss: 0.5999548623986729 grad: 0.05073039526993164
iteration: 30 loss: 0.4471243658851811 grad: -0.028994931463685064
iteration: 40 loss: 0.35990152563516115 grad: -0.05537698611634583
iteration: 50 loss: 0.3026564855261478 grad: -0.06509121267643356
iteration: 0 loss: 317.9104850264685 grad: 782.8129821211346
iteration: 10 loss: 0.5666223154560364 grad: 0.42459154635858765
iteration: 20 loss: 0.3148546776966958 grad: 0.08102126091185881
iteration: 30 loss: 0.23127510811751895 grad: 0.027904264710410596
iteration: 0 loss: 156.7230082100541 grad: 575.7596106386236
iteration: 10 loss: 0.7166122088714143 grad: 0.7007867390570439
iteration: 20 loss: 0.44297089446519583 grad: 0.24257443510780957
iteration: 30 loss: 0.33388623946848384 grad: 0.12257380513398539
iteration: 40 loss: 0.2720526832070556 grad: 0.06877109315330607
iteration: 0 loss: 140.45483307264232 grad: 696.5863378753888
iteration: 10 loss: 0.6764957856199875 grad: 0.09164239644063801
iteration: 20 loss: 0.4243684906849177 grad: -0.07760084381915784
iteration: 30 loss: 0.3224187389547627 grad: -0.09552759318160524
iteration: 0 loss: 222.44744831086317 grad: 641.7149236373498
iteration: 10 loss: 0.6330119831513499 grad: 0.8763558988283247
iteration: 20 loss: 0.38378363520694786 grad: 0.30187828001861805
iteration: 30 loss: 0.2886162600392378 grad: 0.16167625002524696
iteration: 0 loss: 184.60402365584642 grad: 559.0752324974437
iteration: 10 loss: 0.7094699443387898 grad: 0.8745860452742406
iteration: 20 loss: 0.43515779130013305 grad: 0.28736338137267603
iteration: 30 loss: 0.32708674714591657 grad: 0.14478113275713178
iteration: 40 loss: 0.2661755116778295 grad: 0.08381358951061225
iteration: 0 loss: 113.58423111326556 grad: 608.3205267872297
iteration: 10 loss: 0.7763520121852707 grad: 0.3820576699952519
iteration: 20 loss: 0.4887359088445671 grad: 0.05887738443843388
iteration: 30 loss: 0.3698223989401661 grad: -0.012699680843390918
iteration: 40 loss: 0.30150461135955003 grad: -0.03846918375928973
iteration: 0 loss: 192.63099013380906 grad: 677.1343198638738
iteration: 10 loss: 0.6724703354205882 grad: 0.6179956267231489
iteration: 20 loss: 0.40197235377986956 grad: 0.15822252288629446
iteration: 30 loss: 0.2997928032831293 grad: 0.05518336235796431
iteration: 0 loss: 219.8365010690606 grad: 680.5532451558503
iteration: 10 loss: 0.6607132056857484 grad: 0.7246086828503542
iteration: 20 loss: 0.3886129914800139 grad: 0.19209155506194978
iteration: 30 loss: 0.2883321503354223 grad: 0.08514935484302821
iteration: 0 loss: 230.60872791392384 grad: 702.3921280374775
iteration: 10 loss: 0.6759681686073897 grad: 0.5165047418096607
iteration: 20 loss: 0.3907387479663488 grad: 0.10431563232641561
iteration: 30 loss: 0.2872118491279087 grad: 0.031083422490589856
iteration: 0 loss: 120.15882661145291 grad: 628.8488851239786
iteration: 10 loss: 0.73164500750764 grad: 0.4293905384044841
iteration: 20 loss: 0.4589561900978548 grad: 0.06530987016490657
iteration: 30 loss: 0.348391711353956 grad: -0.008947196134892341
iteration: 40 loss: 0.28491740523763426 grad: -0.03444076588254042
iteration: 0 loss: 192.67680161623295 grad: 718.7390916646739
iteration: 10 loss: 0.6631673003880265 grad: 0.3443070013998475
iteration: 20 loss: 0.39519572244700446 grad: 0.05697182700263472
iteration: 30 loss: 0.2941499278185802 grad: -0.0015599241461356364
iteration: 0 loss: 171.80951460693575 grad: 642.2058732310925
iteration: 10 loss: 0.6911529931555627 grad: 0.5727396186152071
iteration: 20 loss: 0.4272187498671139 grad: 0.17090630343100602
iteration: 30 loss: 0.3222791689888089 grad: 0.0770846103111727
iteration: 40 loss: 0.26281597258986783 grad: 0.03741449622694328
iteration: 0 loss: 92.23612408825979 grad: 576.0212543882285
iteration: 10 loss: 0.8418285091586519 grad: -0.028547452774963483
iteration: 20 loss: 0.5376585890116158 grad: -0.08249257448094804
iteration: 30 loss: 0.4069019802206888 grad: -0.08695403620904543
iteration: 40 loss: 0.33102184536735213 grad: -0.0842256014975071
iteration: 0 loss: 128.2829098185105 grad: 624.2010562840803
iteration: 10 loss: 0.7458622341511562 grad: 0.515058844681727
iteration: 20 loss: 0.4654247098887936 grad: 0.10577738024608214
iteration: 30 loss: 0.35138415836743 grad: 0.014808263823474199
iteration: 40 loss: 0.2862703646352119 grad: -0.018305596758456565
iteration: 0 loss: 77.21748729275413 grad: 602.9091350008615
iteration: 10 loss: 0.8905096803131197 grad: -0.08613091468842826
iteration: 20 loss: 0.5633311021525018 grad: -0.1444592332354569
iteration: 30 loss: 0.42379923342576314 grad: -0.1403029241490451
iteration: 40 loss: 0.3432783194446694 grad: -0.12932492488238617
iteration: 0 loss: 150.39297606941074 grad: 640.7759798399904
iteration: 10 loss: 0.6890092898615463 grad: 0.36478374613624975
iteration: 20 loss: 0.4338182878407844 grad: 0.08490532831099003
iteration: 30 loss: 0.3296604233477906 grad: 0.023597926367557583
iteration: 40 loss: 0.26988219881241093 grad: -0.0011633776840912916
iteration: 0 loss: 240.4222013519967 grad: 731.6928823113115
iteration: 10 loss: 0.6026731882472053 grad: 0.7250485628598548
iteration: 20 loss: 0.3570913728773865 grad: 0.23372171315592638
iteration: 30 loss: 0.2673240465607126 grad: 0.11815315031334798
iteration: 0 loss: 117.37454557419137 grad: 614.6013004864604
iteration: 10 loss: 0.7801411176591997 grad: 0.11299359220678099
iteration: 20 loss: 0.49610679631926113 grad: -0.007674199893161976
iteration: 30 loss: 0.3764256105145746 grad: -0.034401786937620346
iteration: 40 loss: 0.3071842327875526 grad: -0.0434091355487969
iteration: 0 loss: 186.51118057333233 grad: 681.3513084348571
iteration: 10 loss: 0.5804287674176191 grad: 0.6964338528072109
iteration: 20 loss: 0.3474262787190541 grad: 0.16935275953081813
iteration: 30 loss: 0.262054810753067 grad: 0.06047703126924031
iteration: 0 loss: 171.72893651118648 grad: 720.8346945622748
iteration: 10 loss: 0.6483163677038729 grad: 0.024540605254076767
iteration: 20 loss: 0.3980361101080648 grad: -0.06824638199054796
iteration: 30 loss: 0.3004157507214338 grad: -0.07211260021140811
iteration: 0 loss: 100.49924650918948 grad: 526.0017536954635
iteration: 10 loss: 0.8620949603310609 grad: 0.41216174573306374
iteration: 20 loss: 0.5404623324551391 grad: 0.14356534250270225
iteration: 30 loss: 0.40572150843118754 grad: 0.0650506187412745
iteration: 40 loss: 0.3285308925154492 grad: 0.029951390081545952
iteration: 0 loss: 82.88472633145453 grad: 581.9976242587093
iteration: 10 loss: 0.9098152914005947 grad: 0.17618771230958774
iteration: 20 loss: 0.5742415204022109 grad: -0.02735744758953772
iteration: 30 loss: 0.4313923360021528 grad: -0.06880550947546993
iteration: 40 loss: 0.3490348305779414 grad: -0.07990004642294629
iteration: 0 loss: 126.56481097431184 grad: 635.989581264021
iteration: 10 loss: 0.7478203850459977 grad: 0.2508392042458736
iteration: 20 loss: 0.4734214884200066 grad: 0.1031796590344622
iteration: 30 loss: 0.3593261884326156 grad: 0.04680678236235149
iteration: 40 loss: 0.29355182701006427 grad: 0.01870144039969334
iteration: 0 loss: 134.5539821417776 grad: 619.4694658455356
iteration: 10 loss: 0.7394689786844372 grad: 0.3910472836985621
iteration: 20 loss: 0.46558520292596356 grad: 0.0963199189378425
iteration: 30 loss: 0.35275261074845976 grad: 0.027376211241510035
iteration: 40 loss: 0.288065550106942 grad: -0.0006699317604954135
iteration: 0 loss: 120.3437487161958 grad: 553.7877183670792
iteration: 10 loss: 0.7868908450375536 grad: 0.5782964839232698
iteration: 20 loss: 0.5053711023042937 grad: 0.16168227298052268
iteration: 30 loss: 0.38512401764455717 grad: 0.05499249668043586
iteration: 40 loss: 0.315037086812481 grad: 0.011298255660719481
iteration: 0 loss: 175.17194228843272 grad: 685.0762649755835
iteration: 10 loss: 0.675610592687173 grad: 0.4818790813566902
iteration: 20 loss: 0.4104602337400303 grad: 0.09659786226016592
iteration: 30 loss: 0.30833806202826963 grad: 0.01633389885746754
iteration: 0 loss: 201.1099350607172 grad: 624.3177781935647
iteration: 10 loss: 0.6924699173368108 grad: 0.7585755557662639
iteration: 20 loss: 0.4181091668639386 grad: 0.2358728779046178
iteration: 30 loss: 0.31278638780581375 grad: 0.1126186131565458
iteration: 0 loss: 142.63300791357244 grad: 667.7139985476732
iteration: 10 loss: 0.6759554160950177 grad: 0.34660268086772544
iteration: 20 loss: 0.4244477803922564 grad: 0.05905546951584931
iteration: 30 loss: 0.3225153094438362 grad: -0.007527034254247708
iteration: 0 loss: 154.09011217563275 grad: 680.7414042581685
iteration: 10 loss: 0.710664176039741 grad: 0.23450267987472162
iteration: 20 loss: 0.4357038374362839 grad: 0.010987576915379462
iteration: 30 loss: 0.3269875726403949 grad: -0.03209498759775763
iteration: 40 loss: 0.2656873668804771 grad: -0.04567742965321302
iteration: 0 loss: 290.3740804068553 grad: 761.8947302743752
iteration: 10 loss: 0.6184564314834543 grad: 0.835959460968376
iteration: 20 loss: 0.3389897709481705 grad: 0.18985168402451907
iteration: 30 loss: 0.2487276339463113 grad: 0.08693158678511015
iteration: 0 loss: 130.84668048611204 grad: 579.9790217100647
iteration: 10 loss: 0.7943114175294878 grad: 0.5945282320464444
iteration: 20 loss: 0.49877883746739826 grad: 0.1800373478171653
iteration: 30 loss: 0.3769941238938786 grad: 0.07117927459659462
iteration: 40 loss: 0.3071188969355386 grad: 0.025198506606815572
iteration: 0 loss: 173.33824646438887 grad: 676.5879916903491
iteration: 10 loss: 0.6739961793518887 grad: 0.3863204621775125
iteration: 20 loss: 0.41271732050757814 grad: 0.08257109077501772
iteration: 30 loss: 0.3103858657743992 grad: 0.012787048402037173
iteration: 0 loss: 133.65231047322274 grad: 605.2909871769831
iteration: 10 loss: 0.7659846170646151 grad: 0.32371526918704185
iteration: 20 loss: 0.47299451300692785 grad: 0.06530719164397773
iteration: 30 loss: 0.35524632691489055 grad: 0.0059110314690586935
iteration: 40 loss: 0.2884763495037326 grad: -0.017610812188418513
iteration: 0 loss: 104.99938964226459 grad: 560.1273786598485
iteration: 10 loss: 0.7822139288708224 grad: 0.5281865658068141
iteration: 20 loss: 0.49560895573912683 grad: 0.16644893021110096
iteration: 30 loss: 0.37600249714234285 grad: 0.07129550947216157
iteration: 40 loss: 0.3068333629393733 grad: 0.030178712666097125
iteration: 0 loss: 149.1346255616966 grad: 565.8800469696241
iteration: 10 loss: 0.6985884972542739 grad: 0.7231199442339842
iteration: 20 loss: 0.4340670260820513 grad: 0.25482131899237037
iteration: 30 loss: 0.327951066339665 grad: 0.12963541153853753
iteration: 40 loss: 0.26759227326494345 grad: 0.07445527684030151
iteration: 0 loss: 92.25363887380077 grad: 561.7904251558447
iteration: 10 loss: 0.8725370109825896 grad: 0.4852377456925145
iteration: 20 loss: 0.5478275832998737 grad: 0.1387227457724488
iteration: 30 loss: 0.4119204559075859 grad: 0.041031647887473086
iteration: 40 loss: 0.3337916743856278 grad: 0.0004744032775643907
iteration: 0 loss: 130.71581953444064 grad: 657.66307518586
iteration: 10 loss: 0.7078812582548463 grad: 0.21510457892010834
iteration: 20 loss: 0.449388837871993 grad: 0.013452112318360786
iteration: 30 loss: 0.34244146829463384 grad: -0.031140731707353514
iteration: 40 loss: 0.28066155787895364 grad: -0.046840258430009066
iteration: 0 loss: 227.4782543439818 grad: 665.9486563333912
iteration: 10 loss: 0.6489296595008537 grad: 0.6767786021374218
iteration: 20 loss: 0.387968211850586 grad: 0.2017284376850992
iteration: 30 loss: 0.2898073863722278 grad: 0.09176297528716033
iteration: 0 loss: 213.15620370541853 grad: 726.4414660078266
iteration: 10 loss: 0.584991505062879 grad: 0.5129944945308254
iteration: 20 loss: 0.3553602497721923 grad: 0.09747883583664047
iteration: 30 loss: 0.2692972958125624 grad: 0.021086225853600028
iteration: 0 loss: 220.62541478143967 grad: 691.411680809844
iteration: 10 loss: 0.6715318766507401 grad: 0.7025297957359355
iteration: 20 loss: 0.4023427711843562 grad: 0.20073511566356955
iteration: 30 loss: 0.3005969550488523 grad: 0.09053245270027357
iteration: 0 loss: 161.56745783939488 grad: 662.2249520311602
iteration: 10 loss: 0.7277840452257309 grad: 0.25874072541350784
iteration: 20 loss: 0.4473450209491354 grad: 0.028623390769076492
iteration: 30 loss: 0.33519143511908955 grad: -0.01920044729485905
iteration: 40 loss: 0.2718770900266298 grad: -0.03517888128646978
iteration: 0 loss: 232.53063992195874 grad: 689.6160367496342
iteration: 10 loss: 0.6310942791976479 grad: 0.7147695507032253
iteration: 20 loss: 0.3811035735115266 grad: 0.2173313510552183
iteration: 30 loss: 0.28638650876126287 grad: 0.10127452600160908
iteration: 0 loss: 212.28323353277756 grad: 694.0951058532007
iteration: 10 loss: 0.6676526884655025 grad: 0.5523290119531076
iteration: 20 loss: 0.3968662894125869 grad: 0.17065379503378791
iteration: 30 loss: 0.2950505705406282 grad: 0.07636361594426923
iteration: 0 loss: 222.59337565151938 grad: 599.2491415661932
iteration: 10 loss: 0.6768031798603095 grad: 0.7380345789623346
iteration: 20 loss: 0.4116523289196927 grad: 0.29953381710080923
iteration: 30 loss: 0.30819995095234015 grad: 0.17606122868864293
iteration: 0 loss: 323.8659758644447 grad: 711.5310247664812
iteration: 10 loss: 0.6316687574046889 grad: 0.82090479664668
iteration: 20 loss: 0.361143222740049 grad: 0.3036971258618236
iteration: 30 loss: 0.2661844188107187 grad: 0.17172961900294098
iteration: 0 loss: 229.8491919089266 grad: 634.7612311123683
iteration: 10 loss: 0.6646489880709354 grad: 0.821616738722809
iteration: 20 loss: 0.3953492550482141 grad: 0.25647899595533596
iteration: 30 loss: 0.2946157080469592 grad: 0.12571437064747149
iteration: 0 loss: 88.31199876254983 grad: 539.0147213052227
iteration: 10 loss: 0.9067835323816822 grad: 0.5190538897421696
iteration: 20 loss: 0.5685201974686358 grad: 0.15033189840178665
iteration: 30 loss: 0.42593939956185006 grad: 0.04634707174715133
iteration: 40 loss: 0.344175325132958 grad: 0.003501625924122344
iteration: 0 loss: 92.46810722575778 grad: 508.9836122925716
iteration: 10 loss: 0.901015645485462 grad: 0.5128958711158761
iteration: 20 loss: 0.5623542957269323 grad: 0.17501072487108188
iteration: 30 loss: 0.42121040930173465 grad: 0.0748812566682754
iteration: 40 loss: 0.3404751113834705 grad: 0.030250741026004453
iteration: 0 loss: 132.47688591551503 grad: 560.8501486571456
iteration: 10 loss: 0.7697146187403466 grad: 0.6244125875367506
iteration: 20 loss: 0.48827387456623744 grad: 0.2401174816690399
iteration: 30 loss: 0.370166157669866 grad: 0.1261194581248688
iteration: 40 loss: 0.30199262925634685 grad: 0.07283320809487014
iteration: 0 loss: 151.3565345731603 grad: 651.3342414105088
iteration: 10 loss: 0.7218276924058955 grad: -0.026822825369671323
iteration: 20 loss: 0.4506121200273066 grad: -0.054319734235883085
iteration: 30 loss: 0.339983763934883 grad: -0.05651683773535561
iteration: 40 loss: 0.2769673995111258 grad: -0.0552856720115838
iteration: 0 loss: 140.65882001744743 grad: 609.0533780740005
iteration: 10 loss: 0.7155982671902993 grad: 0.4757338961761611
iteration: 20 loss: 0.44812577203877235 grad: 0.14128534286285233
iteration: 30 loss: 0.33889769467433245 grad: 0.054179734540742756
iteration: 40 loss: 0.27644715613339277 grad: 0.018030212433990053
iteration: 0 loss: 107.53051670253105 grad: 588.0777796649628
iteration: 10 loss: 0.803034924416269 grad: 0.32909839819638875
iteration: 20 loss: 0.5057630786509022 grad: 0.044796155768373994
iteration: 30 loss: 0.38192654331396625 grad: -0.015349529697597229
iteration: 40 loss: 0.3107090044391457 grad: -0.03654867976699965
iteration: 0 loss: 95.49642856833952 grad: 569.7046520280329
iteration: 10 loss: 0.8516211330845015 grad: 0.2662317838975523
iteration: 20 loss: 0.541368142937658 grad: 0.05587495289406921
iteration: 30 loss: 0.4085160098143619 grad: -0.0001274545550043766
iteration: 40 loss: 0.3316278544762099 grad: -0.02220912266912039
iteration: 0 loss: 202.25342608702874 grad: 724.6786923857496
iteration: 10 loss: 0.5976263708606976 grad: 0.20675497022404868
iteration: 20 loss: 0.36570355902767915 grad: 0.02388507654791036
iteration: 30 loss: 0.27669657768447825 grad: -0.013122911266333397
iteration: 0 loss: 182.651071515416 grad: 653.4065564472263
iteration: 10 loss: 0.6806486257222456 grad: 0.6908763981020335
iteration: 20 loss: 0.41421059143389855 grad: 0.20954837621189382
iteration: 30 loss: 0.3117639295313976 grad: 0.09174925248765406
iteration: 0 loss: 201.6491276589678 grad: 629.2088397941762
iteration: 10 loss: 0.6785681780656887 grad: 0.6209661500703442
iteration: 20 loss: 0.416315053405428 grad: 0.19990161528196157
iteration: 30 loss: 0.3134849918855748 grad: 0.09631125680705141
iteration: 0 loss: 137.4485393962177 grad: 624.5310977876779
iteration: 10 loss: 0.7237150017687203 grad: 0.3929075306305736
iteration: 20 loss: 0.4558584648305193 grad: 0.08601724455502391
iteration: 30 loss: 0.3456951547942398 grad: 0.01593719990025423
iteration: 40 loss: 0.28248951823678164 grad: -0.011771717635326105
iteration: 0 loss: 138.97351551344124 grad: 533.0511595259716
iteration: 10 loss: 0.768877183665526 grad: 0.7010169699955278
iteration: 20 loss: 0.4854437346388791 grad: 0.25808756579008774
iteration: 30 loss: 0.36801192951936007 grad: 0.12899095362696666
iteration: 40 loss: 0.3004543559883876 grad: 0.0699605202701821
iteration: 0 loss: 178.45106697899652 grad: 645.2639841624664
iteration: 10 loss: 0.6292681861072693 grad: 0.3967583163967644
iteration: 20 loss: 0.3916080145119684 grad: 0.09394699095868393
iteration: 30 loss: 0.2980768603343181 grad: 0.021023554893543173
iteration: 0 loss: 115.48710307542086 grad: 595.1440257383472
iteration: 10 loss: 0.7740280853830964 grad: 0.20563930990115034
iteration: 20 loss: 0.4889431876283032 grad: 0.04513030543830135
iteration: 30 loss: 0.3705160346994903 grad: 0.0033339037672418543
iteration: 40 loss: 0.30232659357956104 grad: -0.014142409247897336
iteration: 0 loss: 99.88192622316373 grad: 626.1131966905168
iteration: 10 loss: 0.8156534956117618 grad: 0.327825444260404
iteration: 20 loss: 0.5094924506104441 grad: 0.013259147859914331
iteration: 30 loss: 0.3834721392966623 grad: -0.04719112542812561
iteration: 40 loss: 0.3113743382060751 grad: -0.06511146165142373
iteration: 0 loss: 245.06865389845555 grad: 779.3419188382799
iteration: 10 loss: 0.5832789776933912 grad: 0.3220620595825501
iteration: 20 loss: 0.33806173426050506 grad: 0.03609005698877343
iteration: 30 loss: 0.2513940402953846 grad: -0.006521485624534453
iteration: 0 loss: 97.90744097137762 grad: 562.4654292598059
iteration: 10 loss: 0.8195458722171183 grad: 0.4877037674140743
iteration: 20 loss: 0.5179968963807782 grad: 0.11740593862750864
iteration: 30 loss: 0.39115158174337755 grad: 0.02267730200627004
iteration: 40 loss: 0.31803800181263464 grad: -0.013756128534147687
iteration: 0 loss: 84.80759488876832 grad: 540.7033400311838
iteration: 10 loss: 0.9029370520980096 grad: 0.4183141289720928
iteration: 20 loss: 0.5668509810691169 grad: 0.10165256869241504
iteration: 30 loss: 0.4251746411867895 grad: 0.018927380581047523
iteration: 40 loss: 0.34378366738664307 grad: -0.013749396566916917
iteration: 0 loss: 103.18798741516126 grad: 571.3226030894862
iteration: 10 loss: 0.8360694647037069 grad: 0.2885843557493728
iteration: 20 loss: 0.5249678969230445 grad: 0.08804370583244212
iteration: 30 loss: 0.3952238616177717 grad: 0.03009980289964911
iteration: 40 loss: 0.32082020319268684 grad: 0.00418073705943528
iteration: 0 loss: 113.88579018509022 grad: 637.0595677209316
iteration: 10 loss: 0.7714456930867031 grad: 0.1246337851686472
iteration: 20 loss: 0.486391644464096 grad: -0.061622925119101404
iteration: 30 loss: 0.367947458443599 grad: -0.08995446098327425
iteration: 40 loss: 0.2997753832546964 grad: -0.0942585684585113
iteration: 0 loss: 179.5032750047453 grad: 654.2006434489829
iteration: 10 loss: 0.6118424784397741 grad: 0.4666141220672337
iteration: 20 loss: 0.37807691720527975 grad: 0.13670777348138854
iteration: 30 loss: 0.2877818280372098 grad: 0.061174865218605316
iteration: 0 loss: 168.65215485611833 grad: 702.2843964860258
iteration: 10 loss: 0.6963177665429612 grad: 0.12052111259790273
iteration: 20 loss: 0.42412962191023823 grad: -0.049134419225396936
iteration: 30 loss: 0.31755512863723695 grad: -0.07221954537501729
iteration: 40 loss: 0.2577497853470215 grad: -0.07537824092628419
iteration: 0 loss: 189.02134744194362 grad: 691.7752648896724
iteration: 10 loss: 0.64281331742272 grad: 0.6254950532122361
iteration: 20 loss: 0.37463443694212567 grad: 0.14606194248075188
iteration: 30 loss: 0.2783851370271474 grad: 0.0474635565560419
iteration: 0 loss: 147.08819962981852 grad: 662.2189796682128
iteration: 10 loss: 0.7055095010947215 grad: 0.5236254241229523
iteration: 20 loss: 0.427192340524512 grad: 0.08460971401284362
iteration: 30 loss: 0.31958373393701384 grad: 0.003666256152555709
iteration: 40 loss: 0.2592779456883198 grad: -0.02368536582616733
iteration: 0 loss: 140.04291121672267 grad: 628.3969231569611
iteration: 10 loss: 0.7122641766004976 grad: 0.44092672496335394
iteration: 20 loss: 0.4448793027129134 grad: 0.11095751922583298
iteration: 30 loss: 0.3362805054408785 grad: 0.03194373325850376
iteration: 40 loss: 0.274272999850754 grad: 0.0003227928572582441
iteration: 0 loss: 240.8139945288306 grad: 712.6931824968868
iteration: 10 loss: 0.6385846727589524 grad: 0.41394116666547975
iteration: 20 loss: 0.37071158951064054 grad: 0.0736311983717454
iteration: 30 loss: 0.2749146150045734 grad: 0.007899179606004478
iteration: 0 loss: 192.03627824461373 grad: 661.3406198951787
iteration: 10 loss: 0.7022836258906451 grad: 0.5072120724291308
iteration: 20 loss: 0.4257997987974291 grad: 0.16842544793379471
iteration: 30 loss: 0.3187652904371961 grad: 0.08171852714734783
iteration: 40 loss: 0.2588734415030558 grad: 0.0437485655975902
iteration: 0 loss: 84.22113792868686 grad: 620.0919975448387
iteration: 10 loss: 0.8400955166755673 grad: 0.10685042213174903
iteration: 20 loss: 0.5313200635605013 grad: -0.09318170389454818
iteration: 30 loss: 0.40074838070573937 grad: -0.11940995613391606
iteration: 40 loss: 0.32537747813950607 grad: -0.12059837261575881
iteration: 0 loss: 146.17963519113565 grad: 621.9590403026839
iteration: 10 loss: 0.692458664503647 grad: 0.3561265151710982
iteration: 20 loss: 0.43957077351668966 grad: 0.0778272889330595
iteration: 30 loss: 0.33573776878479705 grad: 0.010686427898908914
iteration: 40 loss: 0.27581560884713824 grad: -0.01499574527575342
iteration: 0 loss: 123.41903947289624 grad: 610.5020297043155
iteration: 10 loss: 0.7568707529230365 grad: 0.5872735309659842
iteration: 20 loss: 0.473252361405023 grad: 0.13855063851804209
iteration: 30 loss: 0.35826184357351437 grad: 0.034020143829771615
iteration: 40 loss: 0.2924188669659005 grad: -0.006192269208281167
iteration: 0 loss: 181.73996690842097 grad: 713.9291907909799
iteration: 10 loss: 0.6575788227275386 grad: 0.25643601417681017
iteration: 20 loss: 0.39480900961941434 grad: 0.018591963484831275
iteration: 30 loss: 0.2966396485609783 grad: -0.028657694033397903
iteration: 0 loss: 104.61453249877758 grad: 624.2379503338138
iteration: 10 loss: 0.8037247014525003 grad: 0.27638028084105154
iteration: 20 loss: 0.5100942924617068 grad: -0.0019169985726346685
iteration: 30 loss: 0.3865322071136358 grad: -0.055195000282455504
iteration: 40 loss: 0.3150471127266263 grad: -0.07101921176290937
iteration: 0 loss: 153.95310352994102 grad: 593.5886941188671
iteration: 10 loss: 0.7460905009990132 grad: 0.5920583682035433
iteration: 20 loss: 0.46285906447734637 grad: 0.17106302203688248
iteration: 30 loss: 0.3483853222201519 grad: 0.07031719885222878
iteration: 40 loss: 0.2833852207313612 grad: 0.028728117389843745
iteration: 0 loss: 79.75670000535519 grad: 527.6115627738118
iteration: 10 loss: 0.9393059184110164 grad: 0.6702550045839855
iteration: 20 loss: 0.5851798889011248 grad: 0.14969134238247128
iteration: 30 loss: 0.43729739579843724 grad: 0.024384128540035424
iteration: 40 loss: 0.3527119013782464 grad: -0.02193279534795701
iteration: 0 loss: 186.37899980936618 grad: 682.3796477645161
iteration: 10 loss: 0.6168690610956301 grad: 0.659867544689035
iteration: 20 loss: 0.3839025668290891 grad: 0.18506803019049894
iteration: 30 loss: 0.29194731452422606 grad: 0.07819172505990335
iteration: 0 loss: 154.16717341554514 grad: 661.0592726451712
iteration: 10 loss: 0.7097565843407871 grad: 0.2410771070220123
iteration: 20 loss: 0.44114920921434864 grad: 0.010964953810422324
iteration: 30 loss: 0.3334386747344318 grad: -0.02913577621905726
iteration: 40 loss: 0.27213942267016217 grad: -0.041956165227665335
iteration: 0 loss: 152.8955539381739 grad: 651.5944162980486
iteration: 10 loss: 0.7213394575300381 grad: 0.4040701371970165
iteration: 20 loss: 0.4455847450863902 grad: 0.09521337604449272
iteration: 30 loss: 0.335730548300572 grad: 0.022475680962108023
iteration: 40 loss: 0.2733796644210584 grad: -0.006917870982440779
iteration: 0 loss: 282.7180168064751 grad: 678.3169581013079
iteration: 10 loss: 0.6665074129596178 grad: 0.5628941380992035
iteration: 20 loss: 0.39281474721577647 grad: 0.1719581614610535
iteration: 30 loss: 0.2905573270085039 grad: 0.08082824831917408
iteration: 0 loss: 112.0516267862918 grad: 612.7790043910775
iteration: 10 loss: 0.7558600860975275 grad: 0.2993103768172121
iteration: 20 loss: 0.4765840419177865 grad: 0.051269456942665326
iteration: 30 loss: 0.3607569932296015 grad: -0.007274146413217296
iteration: 40 loss: 0.2942201387989123 grad: -0.029859090812932115
iteration: 0 loss: 151.45971429306886 grad: 639.5520022804911
iteration: 10 loss: 0.6396434385241346 grad: 0.3823549538328853
iteration: 20 loss: 0.40309016737550835 grad: 0.0851848281360284
iteration: 30 loss: 0.30715981074800686 grad: 0.013041031168998329
iteration: 0 loss: 278.8324319608745 grad: 715.9975212490019
iteration: 10 loss: 0.6267647659477998 grad: 0.7686404703066382
iteration: 20 loss: 0.36774008268556907 grad: 0.24699059593734607
iteration: 30 loss: 0.2732420629772605 grad: 0.12345059348154407
iteration: 0 loss: 147.1392518753403 grad: 605.7703195516829
iteration: 10 loss: 0.7336809186350696 grad: 0.647858857291244
iteration: 20 loss: 0.45383343838627543 grad: 0.22276600125039386
iteration: 30 loss: 0.34211869338684087 grad: 0.10731344428037208
iteration: 40 loss: 0.27874573789036233 grad: 0.05578034715200855
iteration: 0 loss: 195.3209450127214 grad: 649.899662062772
iteration: 10 loss: 0.6850004847019591 grad: 0.5511579908812466
iteration: 20 loss: 0.4185353960293386 grad: 0.15719045790958047
iteration: 30 loss: 0.3142074501301977 grad: 0.06645296466241407
iteration: 0 loss: 124.89554221177944 grad: 619.3548613559007
iteration: 10 loss: 0.7767699555841484 grad: 0.38053872858008125
iteration: 20 loss: 0.48498489211463247 grad: 0.0527728171290064
iteration: 30 loss: 0.3661053369876093 grad: -0.01722603464971409
iteration: 40 loss: 0.2980809589887279 grad: -0.04170939854079404
iteration: 0 loss: 219.01239773928782 grad: 718.427355512172
iteration: 10 loss: 0.6032397976373018 grad: 0.35221709421872915
iteration: 20 loss: 0.3632614383097296 grad: 0.09434464801932857
iteration: 30 loss: 0.2732488495533041 grad: 0.03243981994611751
iteration: 0 loss: 203.25667590239007 grad: 679.9149729536734
iteration: 10 loss: 0.6503336783085119 grad: 0.3837650345040462
iteration: 20 loss: 0.3988848222412674 grad: 0.09901637115548517
iteration: 30 loss: 0.3008873352229619 grad: 0.037492949140287994
iteration: 0 loss: 140.0373058131702 grad: 557.7590915903513
iteration: 10 loss: 0.778669084531099 grad: 0.5296827038057936
iteration: 20 loss: 0.4905278432129721 grad: 0.1734895534381534
iteration: 30 loss: 0.3705120716764544 grad: 0.07801829463050314
iteration: 40 loss: 0.30162905674544827 grad: 0.036899499850218065
iteration: 0 loss: 98.96424970734105 grad: 538.795867432468
iteration: 10 loss: 0.9013183858758732 grad: 0.4104316256042384
iteration: 20 loss: 0.5666627896622561 grad: 0.13747870848670388
iteration: 30 loss: 0.4251682837116691 grad: 0.058115487600588406
iteration: 40 loss: 0.343831164376752 grad: 0.022897348003337557
iteration: 0 loss: 120.60559952473812 grad: 672.7522483626365
iteration: 10 loss: 0.6969975184337355 grad: 0.20757753648604496
iteration: 20 loss: 0.443835560370148 grad: -0.025735962638539937
iteration: 30 loss: 0.3387376856761131 grad: -0.06172039901098067
iteration: 40 loss: 0.2779026696213226 grad: -0.07010184236805343
iteration: 0 loss: 67.0093430294141 grad: 516.0910311236178
iteration: 10 loss: 0.9956103300391987 grad: 0.2442483202029348
iteration: 20 loss: 0.6196084656034642 grad: 0.032871018351143685
iteration: 30 loss: 0.46071525966445653 grad: -0.016467534621559345
iteration: 40 loss: 0.369961115854494 grad: -0.03433749945009347
iteration: 50 loss: 0.3104634818287609 grad: -0.04173945886524931
iteration: 0 loss: 152.78105746559066 grad: 522.7660729436897
iteration: 10 loss: 0.6440585689617199 grad: 0.9111503276759736
iteration: 20 loss: 0.39800936593655933 grad: 0.2924579215768397
iteration: 30 loss: 0.30205712552602065 grad: 0.14106046600089273
iteration: 0 loss: 106.93103585131286 grad: 536.4610843519968
iteration: 10 loss: 0.8302696180487394 grad: 0.5525863042699952
iteration: 20 loss: 0.5185798818856711 grad: 0.1419980060570662
iteration: 30 loss: 0.3896691769787966 grad: 0.044608891148288324
iteration: 40 loss: 0.31593278568322025 grad: 0.00642897898945269
iteration: 0 loss: 225.36742419577405 grad: 693.7968833237983
iteration: 10 loss: 0.643494569901453 grad: 0.5547199837279373
iteration: 20 loss: 0.3825832303452692 grad: 0.14656415146936103
iteration: 30 loss: 0.28478286916792556 grad: 0.055013299776281874
iteration: 0 loss: 152.68468974562538 grad: 588.6334266540639
iteration: 10 loss: 0.7334519201446958 grad: 0.8537706728479834
iteration: 20 loss: 0.45822515287595295 grad: 0.28440538017616834
iteration: 30 loss: 0.3468661764337329 grad: 0.13589956731472883
iteration: 40 loss: 0.2832010945171119 grad: 0.07199968156023329
iteration: 0 loss: 188.99881566154224 grad: 689.7559842726084
iteration: 10 loss: 0.6705916370304046 grad: 0.38331423532262376
iteration: 20 loss: 0.4015429626097929 grad: 0.06414893684761823
iteration: 30 loss: 0.2993769297350313 grad: 0.0008568911388214147
iteration: 0 loss: 225.24413416583045 grad: 690.6075273261956
iteration: 10 loss: 0.6592821529478566 grad: 0.4211939166963662
iteration: 20 loss: 0.40353964003778936 grad: 0.10642782099055766
iteration: 30 loss: 0.30337384817539714 grad: 0.034935833168400314
iteration: 0 loss: 222.9616725268357 grad: 743.7788220225261
iteration: 10 loss: 0.6435812155200025 grad: 0.37133726653082455
iteration: 20 loss: 0.3784860083664667 grad: 0.08236535455232354
iteration: 30 loss: 0.2807176091583559 grad: 0.022746415951297186
iteration: 0 loss: 103.84912674135661 grad: 581.8830830595534
iteration: 10 loss: 0.8083516060943855 grad: 0.2662321076268211
iteration: 20 loss: 0.5110172105319593 grad: 0.05240133550265515
iteration: 30 loss: 0.38635624116829026 grad: -0.001832498962973269
iteration: 40 loss: 0.31452513696355955 grad: -0.023500691702876778
iteration: 0 loss: 115.42554080611059 grad: 590.3776829838946
iteration: 10 loss: 0.8128046671335224 grad: 0.38364600376806735
iteration: 20 loss: 0.5119358250321966 grad: 0.08498549485056113
iteration: 30 loss: 0.38610158414237294 grad: 0.015196320449597421
iteration: 40 loss: 0.31378208523991896 grad: -0.011894875971608281
iteration: 0 loss: 132.93481105071075 grad: 667.0008503481558
iteration: 10 loss: 0.697458691167495 grad: 0.1985531637489497
iteration: 20 loss: 0.4354159948199429 grad: -0.02692225756575818
iteration: 30 loss: 0.32957282366392476 grad: -0.06075381559618703
iteration: 40 loss: 0.26918991511163104 grad: -0.06808372068315836
iteration: 0 loss: 141.90353033972735 grad: 643.5952254644059
iteration: 10 loss: 0.6885613113952125 grad: 0.20871973657993564
iteration: 20 loss: 0.43161405584082374 grad: -0.005480344986955968
iteration: 30 loss: 0.32763912252481864 grad: -0.042640015741342144
iteration: 40 loss: 0.2681669837445291 grad: -0.05397212097781714
iteration: 0 loss: 279.45213898868917 grad: 751.2175774315824
iteration: 10 loss: 0.5838769512774061 grad: 0.6051827956488978
iteration: 20 loss: 0.33288174764716544 grad: 0.17433025635123905
iteration: 30 loss: 0.2456999951634564 grad: 0.08185708713485693
iteration: 0 loss: 108.07363181070014 grad: 630.265561830766
iteration: 10 loss: 0.8167973307403869 grad: 0.3562308275531061
iteration: 20 loss: 0.5103828008384219 grad: 0.03233011369334525
iteration: 30 loss: 0.3842582389546475 grad: -0.03186413565175451
iteration: 40 loss: 0.31220632969689177 grad: -0.05325913972551072
iteration: 0 loss: 204.37194581957704 grad: 717.8633979856295
iteration: 10 loss: 0.6016516792258865 grad: 0.6084748282825048
iteration: 20 loss: 0.35466221069038345 grad: 0.17751776388756735
iteration: 30 loss: 0.26537540258215647 grad: 0.07211445364932746
iteration: 0 loss: 147.58128367783831 grad: 623.9806929876481
iteration: 10 loss: 0.7544325705377394 grad: 0.2549983975973271
iteration: 20 loss: 0.46956522471753653 grad: 0.0708594732426821
iteration: 30 loss: 0.35327724204869976 grad: 0.024685060042404748
iteration: 40 loss: 0.2871486751034602 grad: 0.004304420468773324
iteration: 0 loss: 75.31059048890756 grad: 587.9014886671691
iteration: 10 loss: 0.9211318139226602 grad: -0.02697290243805176
iteration: 20 loss: 0.5843012580635332 grad: -0.09697096964311198
iteration: 30 loss: 0.4393222113483055 grad: -0.103581288720896
iteration: 40 loss: 0.3554631308858337 grad: -0.10069804385102099
iteration: 0 loss: 225.3456106988422 grad: 703.7839755697823
iteration: 10 loss: 0.6609641134638712 grad: 0.6156176074304329
iteration: 20 loss: 0.39637285288435203 grad: 0.19082294638529468
iteration: 30 loss: 0.29601447389499796 grad: 0.09103087875224608
iteration: 0 loss: 112.66272711304687 grad: 585.1366188715962
iteration: 10 loss: 0.8105763075756675 grad: 0.6173393407035641
iteration: 20 loss: 0.5050258037239143 grad: 0.15748182419765364
iteration: 30 loss: 0.3806150539226523 grad: 0.04815764977739917
iteration: 40 loss: 0.30943985685627756 grad: 0.004849758352162212
iteration: 0 loss: 241.7845948286229 grad: 715.9782809056252
iteration: 10 loss: 0.629646306134005 grad: 0.6401360689698583
iteration: 20 loss: 0.36780618494694034 grad: 0.17901312607067305
iteration: 30 loss: 0.27292570336182753 grad: 0.08119202838547429
iteration: 0 loss: 101.09846300423992 grad: 553.7333710877047
iteration: 10 loss: 0.8336522605043435 grad: 0.607888324875854
iteration: 20 loss: 0.5207731493706391 grad: 0.16570056619578205
iteration: 30 loss: 0.3920691975338493 grad: 0.052980193786396235
iteration: 40 loss: 0.31841299961202324 grad: 0.007260565260280241
iteration: 0 loss: 207.43763957620888 grad: 681.6362320316214
iteration: 10 loss: 0.6285608829320236 grad: 0.36780169450805567
iteration: 20 loss: 0.3876320663367468 grad: 0.10112109760602983
iteration: 30 loss: 0.2931708871470123 grad: 0.03349198369751211
iteration: 0 loss: 103.78329573981843 grad: 548.227863531128
iteration: 10 loss: 0.8457110988049614 grad: 0.5728739280089555
iteration: 20 loss: 0.5350985094094292 grad: 0.17373005361205016
iteration: 30 loss: 0.4044886509895646 grad: 0.0627550690139731
iteration: 40 loss: 0.3290705826043863 grad: 0.01623888032226544
iteration: 0 loss: 150.3172579070363 grad: 676.3441049913117
iteration: 10 loss: 0.7166187249613358 grad: 0.17024833007615603
iteration: 20 loss: 0.44678938629534637 grad: -0.05010176995378951
iteration: 30 loss: 0.33715835704465874 grad: -0.07633644083182778
iteration: 40 loss: 0.27471027929550473 grad: -0.07856313203284956
iteration: 0 loss: 108.37994017958748 grad: 597.9668875232087
iteration: 10 loss: 0.7931222392212063 grad: 0.3929685462143
iteration: 20 loss: 0.4981041005850619 grad: 0.05251907228686393
iteration: 30 loss: 0.3758362655679031 grad: -0.02153692921996736
iteration: 40 loss: 0.3057122763587937 grad: -0.047028435186912444
iteration: 0 loss: 117.45139086044843 grad: 626.9449223488139
iteration: 10 loss: 0.7442479133481935 grad: 0.12754715656797383
iteration: 20 loss: 0.4650121281788829 grad: -0.03589906748819132
iteration: 30 loss: 0.35098016399412946 grad: -0.06737847201868702
iteration: 40 loss: 0.28579650597517187 grad: -0.0752318168100257
iteration: 0 loss: 173.8248638269048 grad: 624.5470333510867
iteration: 10 loss: 0.7094864322900982 grad: 0.6191821731073157
iteration: 20 loss: 0.43975895169693435 grad: 0.1895241002458145
iteration: 30 loss: 0.33154788942523405 grad: 0.08405920398512139
iteration: 40 loss: 0.2700746976338864 grad: 0.03980918016499791
iteration: 0 loss: 82.50934260706622 grad: 529.2702311775816
iteration: 10 loss: 0.9149922674875846 grad: 0.4153571498151777
iteration: 20 loss: 0.5782602250934691 grad: 0.11684090010316295
iteration: 30 loss: 0.4341046093895784 grad: 0.033156441081105464
iteration: 40 loss: 0.35090076341144927 grad: -0.0010882172869479028
iteration: 0 loss: 203.73435895869804 grad: 647.7460582507747
iteration: 10 loss: 0.6702075582239889 grad: 0.439861433970138
iteration: 20 loss: 0.4069721466867949 grad: 0.12468551590336316
iteration: 30 loss: 0.30522097409464716 grad: 0.04941651441955634
iteration: 0 loss: 180.91973062901226 grad: 656.5498808490906
iteration: 10 loss: 0.6777687577489566 grad: 0.4437344945858416
iteration: 20 loss: 0.41302821074168605 grad: 0.09421379349803116
iteration: 30 loss: 0.31036821531291603 grad: 0.019644030803197984
iteration: 0 loss: 8730.339941385577 grad: 2705.7897111329744
iteration: 10 loss: 0.0007547464655627581 grad: -0.05550775170756669
iteration: 0 loss: 6937.629622214514 grad: 2542.846129065197
iteration: 0 loss: 7687.458379720841 grad: 2570.868318067254
iteration: 10 loss: 0.0007348600311987949 grad: -0.08890074014599565
iteration: 0 loss: 9605.022433669701 grad: 2818.34312131338
iteration: 10 loss: 0.0009188353221609511 grad: 0.008633402474137079
iteration: 0 loss: 5936.854778456405 grad: 2452.1145391505916
iteration: 0 loss: 4050.1079697564537 grad: 2191.0192455193683
iteration: 0 loss: 6406.046301036581 grad: 2529.254252695579
iteration: 0 loss: 3421.1706719486665 grad: 2071.276728293339
iteration: 0 loss: 9481.054017854303 grad: 2650.9817297659624
iteration: 10 loss: 0.0007114306618777019 grad: 0.008471419595474835
iteration: 0 loss: 6267.099650764114 grad: 2467.869583186879
iteration: 0 loss: 3921.6497021482123 grad: 2256.9943332951034
iteration: 0 loss: 7942.508567912258 grad: 2652.0975187979557
iteration: 10 loss: 0.0010505306682634082 grad: 0.01300684391186022
iteration: 0 loss: 5415.854142682415 grad: 2242.7156176843314
iteration: 0 loss: 5853.537044424925 grad: 2505.410953009301
iteration: 0 loss: 7669.527504178064 grad: 2548.0154411600024
iteration: 10 loss: 0.0005663510938492519 grad: 0.00014112508071218452
iteration: 0 loss: 11139.298563002447 grad: 2882.555267031635
iteration: 10 loss: 0.013047244381265376 grad: 1.7094162543427518
iteration: 0 loss: 6599.309445534703 grad: 2475.3960921011476
iteration: 10 loss: 0.000780959782952612 grad: 0.009877987868480398
iteration: 0 loss: 4683.283024700693 grad: 2306.75434231693
iteration: 0 loss: 11112.862385941231 grad: 2886.308032880817
iteration: 10 loss: 0.0029607001025314357 grad: 1.2356267334829385
iteration: 0 loss: 5476.597243594427 grad: 2339.0221007716677
iteration: 0 loss: 7914.724548356584 grad: 2648.7105009958645
iteration: 0 loss: 9573.540472980183 grad: 2751.003273469799
iteration: 10 loss: 0.0009305822578343478 grad: 1.2584015317535178
iteration: 0 loss: 4946.331727898646 grad: 2220.405944949681
iteration: 0 loss: 5999.121651818757 grad: 2448.0127376120918
iteration: 0 loss: 6156.500077929654 grad: 2563.442334023589
iteration: 0 loss: 5270.535878442146 grad: 2473.1717657830995
iteration: 10 loss: 0.0010914978047367185 grad: -0.006911811436816432
iteration: 0 loss: 11993.043146438187 grad: 2902.2867764696653
iteration: 10 loss: 0.011587510388132861 grad: 1.4333935303576593
iteration: 0 loss: 11725.997079919007 grad: 2810.6657858501267
iteration: 10 loss: 1.041343095334014 grad: 4.367549358463604
iteration: 0 loss: 6784.935090253291 grad: 2526.589967245287
iteration: 0 loss: 5072.76386684877 grad: 2328.6154408204484
iteration: 10 loss: 0.0006469147946600887 grad: -0.2595522677286754
iteration: 0 loss: inf grad: 3066.355885521585
iteration: 0 loss: 6762.066326237154 grad: 2669.785552896663
iteration: 10 loss: 0.0007770503646249629 grad: 0.0010570347160692452
iteration: 0 loss: 6516.295265947096 grad: 2521.7216222327097
iteration: 0 loss: 9067.073901720061 grad: 2616.8598044736173
iteration: 10 loss: 0.04202613936980593 grad: 0.37875435451741035
iteration: 0 loss: 8898.32573156956 grad: 2701.919108956163
iteration: 10 loss: 0.0009409064461413601 grad: 0.0973564952007625
iteration: 0 loss: 7479.1273017134745 grad: 2627.56157658799
iteration: 10 loss: 0.049802561828983016 grad: 1.1216013343013291
iteration: 0 loss: 7045.120169939714 grad: 2518.6304545194744
iteration: 10 loss: 0.0007897381067529998 grad: -0.09565521340938306
iteration: 0 loss: 11209.441198007948 grad: 2908.8094253382405
iteration: 10 loss: 0.03422729165653105 grad: 2.8160917112569335
iteration: 0 loss: 5274.297981569569 grad: 2399.23049855471
iteration: 0 loss: 5849.993045760441 grad: 2443.075194506593
iteration: 0 loss: 7400.766633609906 grad: 2559.5109418689553
iteration: 0 loss: 3818.9741665058277 grad: 2222.938277027589
iteration: 0 loss: 15593.41825781126 grad: 3161.6480859290787
iteration: 0 loss: 6160.867076364415 grad: 2354.386889391901
iteration: 0 loss: 8489.968416018535 grad: 2830.5656575013522
iteration: 10 loss: 0.0009798195797801864 grad: -0.006717280268339519
iteration: 0 loss: 8711.06173955029 grad: 2614.0361199036606
iteration: 0 loss: 6659.627461601984 grad: 2301.083270138425
iteration: 10 loss: 0.0007175091384190389 grad: -0.031155550672779194
iteration: 0 loss: 5473.366919363268 grad: 2486.317720112068
iteration: 0 loss: 9583.237780815885 grad: 2743.515248754904
iteration: 10 loss: 0.0008585323256821456 grad: -0.015638655509065263
iteration: 0 loss: 9944.412246870495 grad: 2757.40104508728
iteration: 10 loss: 0.0016412209065906195 grad: 1.1526818743017353
iteration: 0 loss: 11621.609468625427 grad: 2844.2718999629524
iteration: 10 loss: 1.1004306916994127 grad: -1.2878324591275956
iteration: 0 loss: 6299.02195244361 grad: 2556.470366592054
iteration: 10 loss: 0.0008845806891754778 grad: 0.01235467982825969
iteration: 0 loss: 10551.183738317348 grad: 2917.2408569084487
iteration: 10 loss: 0.0010275345676663246 grad: 0.7480062107647756
iteration: 0 loss: 7735.95475999713 grad: 2611.3284752648333
iteration: 0 loss: 5225.070345185757 grad: 2363.3802230573256
iteration: 0 loss: 6973.38246256916 grad: 2542.021651821827
iteration: 10 loss: 0.0008146393692269074 grad: 0.00942259952811133
iteration: 0 loss: 5253.73430816459 grad: 2468.045427543746
iteration: 0 loss: 7598.351190734885 grad: 2609.666196312514
iteration: 0 loss: 11264.673090310296 grad: 2962.050783466879
iteration: 10 loss: 0.0007712585846258497 grad: 0.18497019065452985
iteration: 0 loss: 6109.76740646497 grad: 2509.0498429204813
iteration: 0 loss: 9909.633493708387 grad: 2773.0754242870535
iteration: 10 loss: 0.04660310967566064 grad: 2.51071083312746
iteration: 0 loss: 9906.484774788572 grad: 2924.6156012552856
iteration: 10 loss: 0.0010376734995092688 grad: -0.06201214200784573
iteration: 0 loss: 5198.808403939573 grad: 2173.7985930859277
iteration: 0 loss: 5448.371594324272 grad: 2385.4653662760775
iteration: 10 loss: 0.0007928254728374833 grad: 0.005447397269483695
iteration: 0 loss: 6437.413745481154 grad: 2584.3463386975154
iteration: 0 loss: 6698.651826537918 grad: 2526.0007544550213
iteration: 0 loss: 5033.778653535195 grad: 2274.633550117015
iteration: 0 loss: 9087.151531226495 grad: 2782.4251890537507
iteration: 10 loss: 0.000985061237853105 grad: 1.0198853793283855
iteration: 0 loss: 7394.489181085942 grad: 2550.0988725166962
iteration: 10 loss: 0.0009234294147146019 grad: -0.012299374457775488
iteration: 0 loss: 7408.386878129569 grad: 2710.2917715961757
iteration: 0 loss: 8854.429913122089 grad: 2761.810172233372
iteration: 10 loss: 0.0007229546463349834 grad: 0.03311172388700248
iteration: 0 loss: 13357.248733295983 grad: 3076.595632758753
iteration: 10 loss: 0.0887600018653426 grad: -0.5641655531422058
iteration: 0 loss: 5656.226793463576 grad: 2377.098800195297
iteration: 0 loss: 9393.885343506046 grad: 2747.4651917163687
iteration: 10 loss: 0.0010348231364994056 grad: 0.4694084112848859
iteration: 0 loss: 5891.532445441429 grad: 2471.3525920598845
iteration: 10 loss: 0.0009055136033566669 grad: 0.011177705196295386
iteration: 0 loss: 4679.538572642647 grad: 2294.657285472093
iteration: 0 loss: 5886.6958125561805 grad: 2318.762355037706
iteration: 0 loss: 4886.406910796521 grad: 2301.478979344607
iteration: 0 loss: 7089.157243644571 grad: 2672.5861802750455
iteration: 0 loss: 9903.103095658515 grad: 2710.793622609782
iteration: 10 loss: 0.0009391893135299059 grad: -0.010649618521130881
iteration: 0 loss: 11262.332235102916 grad: 2937.4012772677534
iteration: 10 loss: 0.0005487139027205889 grad: 1.256281935589701
iteration: 0 loss: 10378.694860820302 grad: 2805.716168501758
iteration: 10 loss: 0.07665635453436566 grad: 0.7684528048045838
iteration: 0 loss: 8415.754143171716 grad: 2695.5552178978064
iteration: 0 loss: 10650.255347469705 grad: 2806.044170395229
iteration: 10 loss: 0.07727940489977217 grad: -0.42738215360696186
iteration: 0 loss: 10503.479735663863 grad: 2810.306697103857
iteration: 10 loss: 0.12739684525612657 grad: 1.2819048235889376
iteration: 0 loss: 7933.740641203371 grad: 2443.2618224175676
iteration: 10 loss: 0.002267898089485243 grad: -0.014518129595635275
iteration: 0 loss: 12496.384054974375 grad: 2887.2325224087076
iteration: 10 loss: 0.004864332895852964 grad: 2.9360371016098274
iteration: 0 loss: 8594.198659472091 grad: 2586.8829464685473
iteration: 10 loss: 0.0007912846247729084 grad: 0.01210039883905593
iteration: 0 loss: 4649.897988063051 grad: 2211.8421248462537
iteration: 0 loss: 4056.761642052702 grad: 2103.0778212530904
iteration: 0 loss: 5586.063916759779 grad: 2306.0638936920654
iteration: 0 loss: 7391.356411197442 grad: 2649.3094389017956
iteration: 0 loss: 7065.2605043345775 grad: 2486.6736552800694
iteration: 0 loss: 5816.636870791543 grad: 2410.223386669689
iteration: 0 loss: 4996.3012109630545 grad: 2332.6515431518956
iteration: 0 loss: 10188.752187842241 grad: 2926.97015920024
iteration: 10 loss: 0.0010324683561603624 grad: -0.49734244801615485
iteration: 0 loss: 8273.76068522823 grad: 2657.444186597526
iteration: 10 loss: 0.001155056203556755 grad: 0.6611250231187283
iteration: 0 loss: 7573.27430864551 grad: 2564.987470135554
iteration: 10 loss: 0.0007937298426192931 grad: 0.0007395019988830772
iteration: 0 loss: 6855.203767848902 grad: 2542.4251140056545
iteration: 0 loss: 4983.162219971626 grad: 2193.1001347829947
iteration: 0 loss: 8083.259738363192 grad: 2626.3614348217284
iteration: 0 loss: 6033.023004541651 grad: 2429.153772892806
iteration: 0 loss: 6239.637004489723 grad: 2555.4872913089166
iteration: 10 loss: 0.0007717274473345077 grad: 0.007041427840387026
iteration: 0 loss: 13668.35637220628 grad: 3147.273990453461
iteration: 0 loss: 4579.726094147801 grad: 2305.539013297324
iteration: 0 loss: 4540.902857982606 grad: 2222.1263879066482
iteration: 0 loss: 5384.360613243166 grad: 2336.2213939886924
iteration: 0 loss: 6806.191583500061 grad: 2604.508077830308
iteration: 0 loss: 8166.386571994441 grad: 2672.015778241
iteration: 10 loss: 0.0007953447215682404 grad: 0.3806456031324615
iteration: 0 loss: 9342.563424013082 grad: 2850.631246289421
iteration: 10 loss: 0.0007748566369462588 grad: 0.6192492775411712
iteration: 0 loss: 11571.024254786938 grad: 2797.921893504605
iteration: 10 loss: 0.47754942427325825 grad: 6.367272879325871
iteration: 0 loss: 9111.804169596915 grad: 2690.6450204884786
iteration: 10 loss: 0.10644889443276705 grad: 0.8866058082962379
iteration: 0 loss: 7075.095484073688 grad: 2558.3624756120985
iteration: 0 loss: 11682.64219636459 grad: 2885.411743004517
iteration: 10 loss: 0.13912058448874054 grad: 1.5079290078505254
iteration: 0 loss: 8814.202265247084 grad: 2695.0425391371787
iteration: 0 loss: 5753.567219005469 grad: 2527.810148164276
iteration: 10 loss: 0.0009363592067860405 grad: -0.19271491487215361
iteration: 0 loss: 6631.907413229765 grad: 2538.918001404052
iteration: 0 loss: 6000.522006710097 grad: 2488.4801081169203
iteration: 10 loss: 0.0006823480176866393 grad: 0.02076774300382969
iteration: 0 loss: 10534.37335619388 grad: 2889.701899668305
iteration: 10 loss: 0.0012333692337216978 grad: 0.27748994415204253
iteration: 0 loss: 5558.983125808427 grad: 2545.6706494134073
iteration: 10 loss: 0.0008062606701225212 grad: 0.01646683975549517
iteration: 0 loss: 6786.149022923149 grad: 2428.8124680189812
iteration: 0 loss: 3811.7614828487394 grad: 2168.2052116234045
iteration: 0 loss: 7947.290100901044 grad: 2769.5816677466464
iteration: 0 loss: 7671.5526956472495 grad: 2691.6737504404778
iteration: 10 loss: 0.0007248259736860001 grad: 0.006060724978923193
iteration: 0 loss: 7629.260453095673 grad: 2655.599789867292
iteration: 10 loss: 0.0009036572896134617 grad: -0.009358104394262974
iteration: 0 loss: 10850.529801528766 grad: 2758.1327865536714
iteration: 10 loss: 0.3541828712305604 grad: 2.1643317789160132
iteration: 0 loss: 6214.824283207438 grad: 2496.152613300717
iteration: 0 loss: 7240.278194139991 grad: 2603.6119036186446
iteration: 10 loss: 0.0008483425296038728 grad: 0.01822129228310709
iteration: 0 loss: 11025.305418274644 grad: 2897.291213951946
iteration: 10 loss: 0.0008357736382591115 grad: 0.9661337029060278
iteration: 0 loss: 6990.919384368296 grad: 2472.088752463682
iteration: 0 loss: 9429.01749290467 grad: 2646.191458042217
iteration: 10 loss: 0.0011361967587039214 grad: -0.8580887217386071
iteration: 0 loss: 6097.114988044621 grad: 2528.850482331501
iteration: 0 loss: 10322.620802654143 grad: 2913.619131937151
iteration: 10 loss: 0.0009190574081995609 grad: 0.017898858218040423
iteration: 0 loss: 8823.334384465099 grad: 2759.6364909727126
iteration: 0 loss: 5767.1910082980385 grad: 2293.4815553974067
iteration: 0 loss: 4501.155640419088 grad: 2218.7709968366353
iteration: 0 loss: 7361.776210982638 grad: 2736.9181136599373
iteration: 0 loss: 3759.039543494989 grad: 2127.6042900742787
iteration: 0 loss: 5062.049299139855 grad: 2154.905203448057
iteration: 0 loss: 4454.413674029811 grad: 2207.3160022122975
iteration: 0 loss: 10636.849837361406 grad: 2810.8863284837507
iteration: 10 loss: 0.0009058431288312105 grad: 0.36304098947532687
iteration: 0 loss: 6446.403845194745 grad: 2406.548084583035
iteration: 10 loss: 0.0009194661767899313 grad: 0.021423696726709685
iteration: 0 loss: 11221.30385529847 grad: 2800.5442009610206
iteration: 10 loss: 0.6838664611039514 grad: 2.2828473594711056
iteration: 0 loss: 10433.3167424996 grad: 2801.9797763228635
iteration: 10 loss: 0.0008293547368587249 grad: 1.6004731238236853
iteration: 0 loss: 12540.37775981548 grad: 3006.0110226556217
iteration: 10 loss: 0.23956715941048143 grad: 3.2999884990915223
iteration: 0 loss: 5717.3506181794 grad: 2382.5333383052925
iteration: 0 loss: 5758.923935460262 grad: 2415.610869912878
iteration: 0 loss: 7487.250216923026 grad: 2708.870562918125
iteration: 10 loss: 0.0006668210196287625 grad: 0.6820922952101824
iteration: 0 loss: 7653.305212195529 grad: 2622.047758465629
iteration: 0 loss: 14278.081553812213 grad: 3036.4974596364746
iteration: 0 loss: 6400.88071227425 grad: 2572.080254087252
iteration: 0 loss: 11074.197792760435 grad: 2907.486604097218
iteration: 10 loss: 0.018844512283315205 grad: 2.5845370810731687
iteration: 0 loss: 7235.561840175854 grad: 2550.827535691349
iteration: 0 loss: 4913.128305567323 grad: 2400.942682227552
iteration: 0 loss: 10330.836444928285 grad: 2849.947152106958
iteration: 10 loss: 0.02488555242201652 grad: 1.0223394664845595
iteration: 0 loss: 5231.221873579194 grad: 2389.3307746105143
iteration: 0 loss: 12515.660732973047 grad: 2903.066198469647
iteration: 10 loss: 0.7184545908504251 grad: 7.265169424946452
iteration: 0 loss: 4621.795928594252 grad: 2274.265500680401
iteration: 0 loss: 9789.593359865625 grad: 2763.102778365807
iteration: 10 loss: 0.0008401275104419751 grad: 0.5903669638640018
iteration: 0 loss: 5065.865970774844 grad: 2256.289147151299
iteration: 0 loss: 7633.8725106556085 grad: 2748.712043976365
iteration: 10 loss: 0.0008749264459104531 grad: 0.015189700837784237
iteration: 0 loss: 5526.647711481851 grad: 2453.721500007699
iteration: 0 loss: 7473.137783163324 grad: 2552.8700765075882
iteration: 10 loss: 0.0009372757221254604 grad: 0.004975972960427135
iteration: 0 loss: 7185.737962126362 grad: 2547.7546831223613
iteration: 0 loss: 4095.5331902568173 grad: 2173.960391816613
iteration: 0 loss: 8992.602661516798 grad: 2634.9417173061424
iteration: 10 loss: 0.0014955068393398753 grad: 0.45661552522178966
iteration: 0 loss: 8797.50862096831 grad: 2668.7634990384786
iteration: 10 loss: 0.0010990019734229215 grad: 0.022824405738223752
iteration: 0 loss: inf grad: 3976.6782612846882
iteration: 0 loss: inf grad: 3735.7399679929626
iteration: 0 loss: inf grad: 3776.519149935288
iteration: 0 loss: inf grad: 4140.92554487268
iteration: 0 loss: inf grad: 3603.436764679279
iteration: 0 loss: inf grad: 3214.1736062239133
iteration: 0 loss: inf grad: 3714.5532204145325
iteration: 0 loss: inf grad: 3048.4190710489192
iteration: 0 loss: inf grad: 3896.022175188221
iteration: 0 loss: inf grad: 3626.5270636770456
iteration: 0 loss: inf grad: 3319.6622068899887
iteration: 0 loss: inf grad: 3899.0431929037077
iteration: 0 loss: inf grad: 3302.685641433023
iteration: 0 loss: inf grad: 3685.4636750338495
iteration: 0 loss: inf grad: 3740.2950694697342
iteration: 0 loss: inf grad: 4235.744130157052
iteration: 0 loss: inf grad: 3635.2682259953394
iteration: 0 loss: inf grad: 3392.62406849338
iteration: 0 loss: inf grad: 4243.704443757093
iteration: 0 loss: inf grad: 3437.3581771321287
iteration: 0 loss: inf grad: 3893.071013715619
iteration: 0 loss: inf grad: 4046.3782096855243
iteration: 0 loss: inf grad: 3262.542922579702
iteration: 0 loss: inf grad: 3592.6888160790186
iteration: 0 loss: inf grad: 3768.59700429018
iteration: 0 loss: inf grad: 3636.0277022393498
iteration: 0 loss: inf grad: 4266.1866425610515
iteration: 0 loss: inf grad: 4129.805732047304
iteration: 0 loss: inf grad: 3719.8577466651086
iteration: 0 loss: inf grad: 3423.167159531237
iteration: 0 loss: inf grad: 4510.3947568335225
iteration: 0 loss: inf grad: 3928.751612072947
iteration: 0 loss: inf grad: 3705.373970806638
iteration: 0 loss: inf grad: 3849.9079960055974
iteration: 0 loss: inf grad: 3970.31552934776
iteration: 0 loss: inf grad: 3857.1448342279186
iteration: 0 loss: inf grad: 3700.67340055605
iteration: 0 loss: inf grad: 4269.1978857363665
iteration: 0 loss: inf grad: 3526.9989365826586
iteration: 0 loss: inf grad: 3591.3460913732365
iteration: 0 loss: inf grad: 3760.3050096364477
iteration: 0 loss: inf grad: 3267.6468298356403
iteration: 0 loss: inf grad: 4647.7098615099285
iteration: 0 loss: inf grad: 3466.4812006703846
iteration: 0 loss: inf grad: 4156.616514901614
iteration: 0 loss: inf grad: 3844.493525709744
iteration: 0 loss: inf grad: 3381.1083528267286
iteration: 0 loss: inf grad: 3649.777892996739
iteration: 0 loss: inf grad: 4033.238212416664
iteration: 0 loss: inf grad: 4053.748759170061
iteration: 0 loss: inf grad: 4178.976635491501
iteration: 0 loss: inf grad: 3764.967051750218
iteration: 0 loss: inf grad: 4288.817594084643
iteration: 0 loss: inf grad: 3836.2590285192264
iteration: 0 loss: inf grad: 3469.2099086883477
iteration: 0 loss: inf grad: 3741.206215842653
iteration: 0 loss: inf grad: 3626.75679778737
iteration: 0 loss: inf grad: 3832.604655519246
iteration: 0 loss: inf grad: 4352.8934879871895
iteration: 0 loss: inf grad: 3691.608633866658
iteration: 0 loss: inf grad: 4075.3224934250306
iteration: 0 loss: inf grad: 4298.002750177268
iteration: 0 loss: inf grad: 3198.0611990122734
iteration: 0 loss: inf grad: 3502.852107927387
iteration: 0 loss: inf grad: 3795.956191592551
iteration: 0 loss: inf grad: 3713.6780362616664
iteration: 0 loss: inf grad: 3340.8213074928926
iteration: 0 loss: inf grad: 4087.189685584132
iteration: 0 loss: inf grad: 3744.995939433805
iteration: 0 loss: inf grad: 3983.145959932845
iteration: 0 loss: inf grad: 4061.756471873769
iteration: 0 loss: inf grad: 4522.0031343842165
iteration: 0 loss: inf grad: 3495.234354947143
iteration: 0 loss: inf grad: 4038.1533083389313
iteration: 0 loss: inf grad: 3637.5135987002477
iteration: 0 loss: inf grad: 3368.8744748980425
iteration: 0 loss: inf grad: 3401.366788003233
iteration: 0 loss: inf grad: 3383.8370644455586
iteration: 0 loss: inf grad: 3928.2684524385386
iteration: 0 loss: inf grad: 3983.9087649441503
iteration: 0 loss: inf grad: 4321.150584752953
iteration: 0 loss: inf grad: 4120.524743320071
iteration: 0 loss: inf grad: 3963.117990304121
iteration: 0 loss: inf grad: 4127.594576488993
iteration: 0 loss: inf grad: 4135.212203511719
iteration: 0 loss: inf grad: 3589.8086863126164
iteration: 0 loss: inf grad: 4245.057780621462
iteration: 0 loss: inf grad: 3802.6540497641913
iteration: 0 loss: inf grad: 3257.5949292018076
iteration: 0 loss: inf grad: 3093.392419914808
iteration: 0 loss: inf grad: 3392.1154042223307
iteration: 0 loss: inf grad: 3894.5835868247714
iteration: 0 loss: inf grad: 3659.2621341985046
iteration: 0 loss: inf grad: 3539.9290092537794
iteration: 0 loss: inf grad: 3426.1643084998163
iteration: 0 loss: inf grad: 4300.635772875427
iteration: 0 loss: inf grad: 3902.9961741685647
iteration: 0 loss: inf grad: 3771.7681169208836
iteration: 0 loss: inf grad: 3734.6314923657064
iteration: 0 loss: inf grad: 3220.188025727981
iteration: 0 loss: inf grad: 3860.2299359012877
iteration: 0 loss: inf grad: 3570.0153909624332
iteration: 0 loss: inf grad: 3754.957741568216
iteration: 0 loss: inf grad: 4623.434109257794
iteration: 0 loss: inf grad: 3389.5631686042643
iteration: 0 loss: inf grad: 3259.3819200473836
iteration: 0 loss: inf grad: 3437.2938244978973
iteration: 0 loss: inf grad: 3824.146965830888
iteration: 0 loss: inf grad: 3926.689328964211
iteration: 0 loss: inf grad: 4190.774909997926
iteration: 0 loss: inf grad: 4112.441593891473
iteration: 0 loss: inf grad: 3955.7683539265663
iteration: 0 loss: inf grad: 3755.562001084193
iteration: 0 loss: inf grad: 4236.2843534035965
iteration: 0 loss: inf grad: 3960.2935513095977
iteration: 0 loss: inf grad: 3715.217879061066
iteration: 0 loss: inf grad: 3724.314043501597
iteration: 0 loss: inf grad: 3658.1661477292655
iteration: 0 loss: inf grad: 4245.796755012099
iteration: 0 loss: inf grad: 3742.3119567891845
iteration: 0 loss: inf grad: 3568.7038294479607
iteration: 0 loss: inf grad: 3188.4174799677135
iteration: 0 loss: inf grad: 4070.7148232337486
iteration: 0 loss: inf grad: 3956.0776148269633
iteration: 0 loss: inf grad: 3906.256432520744
iteration: 0 loss: inf grad: 4057.3027467313905
iteration: 0 loss: inf grad: 3669.1854000827307
iteration: 0 loss: inf grad: 3827.151715478355
iteration: 0 loss: inf grad: 4257.649658189292
iteration: 0 loss: inf grad: 3640.9090195492154
iteration: 0 loss: inf grad: 3892.701551727169
iteration: 0 loss: inf grad: 3714.9006571342134
iteration: 0 loss: inf grad: 4280.497193497555
iteration: 0 loss: inf grad: 4056.5569664774357
iteration: 0 loss: inf grad: 3363.643227482489
iteration: 0 loss: inf grad: 3266.3959280118997
iteration: 0 loss: inf grad: 4023.677361809245
iteration: 0 loss: inf grad: 3133.2934238912435
iteration: 0 loss: inf grad: 3168.404775074166
iteration: 0 loss: inf grad: 3245.638636128155
iteration: 0 loss: inf grad: 4131.265897115798
iteration: 0 loss: inf grad: 3538.8202957740477
iteration: 0 loss: inf grad: 4110.053364487715
iteration: 0 loss: inf grad: 4118.6435189568665
iteration: 0 loss: inf grad: 4415.889638654277
iteration: 0 loss: inf grad: 3502.4519767059414
iteration: 0 loss: inf grad: 3544.5988045411605
iteration: 0 loss: inf grad: 3981.7381094400894
iteration: 0 loss: inf grad: 3855.505499278885
iteration: 0 loss: inf grad: 4462.393770100354
iteration: 0 loss: inf grad: 3775.860355108495
iteration: 0 loss: inf grad: 4267.555813529894
iteration: 0 loss: inf grad: 3751.7254302588544
iteration: 0 loss: inf grad: 3539.952194367247
iteration: 0 loss: inf grad: 4193.828184777857
iteration: 0 loss: inf grad: 3514.625558629844
iteration: 0 loss: inf grad: 4266.38938362247
iteration: 0 loss: inf grad: 3337.2705090606923
iteration: 0 loss: inf grad: 4066.3887073231767
iteration: 0 loss: inf grad: 3314.2905405218603
iteration: 0 loss: inf grad: 4039.9853754709534
iteration: 0 loss: inf grad: 3602.509428197259
iteration: 0 loss: inf grad: 3751.7324151944313
iteration: 0 loss: inf grad: 3743.4488852426093
iteration: 0 loss: inf grad: 3192.5333539502412
iteration: 0 loss: inf grad: 3874.5686973275388
iteration: 0 loss: inf grad: 3922.282085649601
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:57: RuntimeWarning: invalid value encountered in double_scalars
  diff = np.absolute(f_history[-1]-f_history[-2])
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:57: RuntimeWarning: invalid value encountered in double_scalars
  diff = np.absolute(f_history[-1]-f_history[-2])
iteration: 0 loss: 1849.520816241215 grad: 1435.202899006627
iteration: 0 loss: 1467.2656563544735 grad: 1346.097613330076
iteration: 0 loss: 1593.1744711555782 grad: 1361.9274249834175
iteration: 0 loss: 2090.166774021667 grad: 1494.5578605505184
iteration: 10 loss: 0.01902543936078225 grad: 0.18104227177448506
iteration: 0 loss: 1256.218266130435 grad: 1298.2657222283156
iteration: 0 loss: 852.3349120624288 grad: 1156.5221004646062
iteration: 0 loss: 1346.5432126027601 grad: 1339.8216208962663
iteration: 0 loss: 687.2800703386135 grad: 1094.2605470635958
iteration: 0 loss: 2050.643552367723 grad: 1404.7566237077467
iteration: 0 loss: 1322.325852527203 grad: 1305.769931801205
iteration: 0 loss: 776.9352890670524 grad: 1193.170839881378
iteration: 0 loss: 1638.1650714191069 grad: 1405.869971434811
iteration: 0 loss: 1167.1232149127284 grad: 1189.348845454418
iteration: 0 loss: 1204.990920918511 grad: 1326.4566637844787
iteration: 0 loss: 1634.6944921535098 grad: 1348.1496294350632
iteration: 0 loss: 2331.553194390696 grad: 1529.6772565477481
iteration: 10 loss: 0.023409841025899037 grad: 0.1689219733149297
iteration: 0 loss: 1389.8959167853154 grad: 1311.061306726716
iteration: 0 loss: 971.333715058658 grad: 1221.5232847896643
iteration: 0 loss: 2461.802342276583 grad: 1532.4568807533728
iteration: 10 loss: 0.06743292665024372 grad: 0.16426064778673816
iteration: 0 loss: 1143.8348460662876 grad: 1237.234153345632
iteration: 0 loss: 1667.6034457939022 grad: 1403.5244274051754
iteration: 0 loss: 1998.0896911382792 grad: 1458.2015106756485
iteration: 10 loss: 0.030166190954994156 grad: -0.25061116306834863
iteration: 0 loss: 1098.763234490637 grad: 1176.286470512443
iteration: 0 loss: 1180.4640189785541 grad: 1295.709038341352
iteration: 10 loss: 0.020346914523873817 grad: 0.33426393298542156
iteration: 0 loss: 1278.118346774469 grad: 1358.7353797812261
iteration: 0 loss: 1131.8092335039787 grad: 1310.3974158033034
iteration: 10 loss: 0.020064240460421635 grad: 0.07501658119700236
iteration: 0 loss: 2648.194744920287 grad: 1538.6710723387355
iteration: 10 loss: 0.024660972845925822 grad: 0.8693376829910044
iteration: 0 loss: 2537.503495502466 grad: 1491.3328781204123
iteration: 10 loss: 0.22825323738255643 grad: 0.6683164581599275
iteration: 0 loss: 1365.9661572934738 grad: 1340.8099786362109
iteration: 0 loss: 1067.325308106958 grad: 1233.225943065829
iteration: 10 loss: 0.0182601341561663 grad: 0.017410065411035514
iteration: 0 loss: 3013.9999410294095 grad: 1626.3713451257465
iteration: 10 loss: 0.03306832277171039 grad: -0.12521983752533167
iteration: 0 loss: 1416.2188408443346 grad: 1415.4569473853517
iteration: 0 loss: 1362.6172871361819 grad: 1335.123043011964
iteration: 0 loss: 1931.1887413014456 grad: 1387.257663820747
iteration: 10 loss: 0.019157138821769462 grad: 0.17641061917642328
iteration: 0 loss: 1896.047617374332 grad: 1432.2178119112866
iteration: 10 loss: 0.020192002676181306 grad: 0.14901262278199098
iteration: 0 loss: 1565.1130096016445 grad: 1391.3963339482893
iteration: 10 loss: 0.02384738057993342 grad: 0.1705121923768438
iteration: 0 loss: 1489.2923612657853 grad: 1333.0396105070304
iteration: 0 loss: 2348.2664734012365 grad: 1541.8790518799203
iteration: 10 loss: 0.02641313548916845 grad: 0.5208092851388537
iteration: 0 loss: 1119.4900590729778 grad: 1271.2626298815926
iteration: 0 loss: 1199.0774527179228 grad: 1293.0723221082526
iteration: 0 loss: 1655.2415324162998 grad: 1357.089533274157
iteration: 0 loss: 755.5276601231998 grad: 1177.1264422678578
iteration: 0 loss: 3403.628229369353 grad: 1678.2369405810493
iteration: 10 loss: 0.16737663351523224 grad: 1.0260786948323548
iteration: 0 loss: 1346.0305311533834 grad: 1248.1399399083284
iteration: 0 loss: 1749.5870434642559 grad: 1498.6016795659395
iteration: 0 loss: 1947.8109982850567 grad: 1386.750542534626
iteration: 0 loss: 1502.3617570407923 grad: 1217.1897033633559
iteration: 0 loss: 1137.7301319796823 grad: 1316.8950665033942
iteration: 0 loss: 2049.4272374750362 grad: 1455.0123236394875
iteration: 10 loss: 0.021883449925966983 grad: 0.44939597059001074
iteration: 0 loss: 2165.98316304672 grad: 1462.4346754392736
iteration: 10 loss: 0.024609167624227914 grad: 0.13441735040381303
iteration: 0 loss: 2471.235613174468 grad: 1509.244837061673
iteration: 10 loss: 0.12402196038667833 grad: 0.6641817443387165
iteration: 0 loss: 1289.7603333939644 grad: 1356.5711654114602
iteration: 0 loss: 2228.419136776228 grad: 1547.712979760754
iteration: 10 loss: 0.021016690296660687 grad: 0.01684597378753929
iteration: 0 loss: 1657.350462284775 grad: 1386.1381236756351
iteration: 0 loss: 1057.1538892228746 grad: 1249.7499538854377
iteration: 0 loss: 1432.702742856989 grad: 1348.4761522585063
iteration: 0 loss: 1030.6233167002483 grad: 1307.1810826047292
iteration: 0 loss: 1615.251872729999 grad: 1382.8934739652777
iteration: 0 loss: 2472.4373384677647 grad: 1571.6404462632308
iteration: 10 loss: 0.020152027475308965 grad: 0.04292547233922743
iteration: 0 loss: 1281.6753643554382 grad: 1328.6255643636885
iteration: 0 loss: 2106.63827478852 grad: 1469.1382596807052
iteration: 10 loss: 0.024554387455282267 grad: 0.4597365547161094
iteration: 0 loss: 2087.3060057271455 grad: 1550.7476084092725
iteration: 10 loss: 0.01996793061698554 grad: 0.03555780356882006
iteration: 0 loss: 1061.8386244936758 grad: 1147.3658338779824
iteration: 0 loss: 1079.0943209339277 grad: 1263.1235316064437
iteration: 0 loss: 1362.2980710544666 grad: 1369.4233436279842
iteration: 0 loss: 1410.3073474394494 grad: 1337.3165667714452
iteration: 0 loss: 1082.2106978717711 grad: 1203.9660568553631
iteration: 0 loss: 1934.8509826121044 grad: 1474.3519825349438
iteration: 10 loss: 0.021794031709511975 grad: 0.17850207023016776
iteration: 0 loss: 1658.138131515809 grad: 1352.0722871656765
iteration: 10 loss: 0.02006659550930966 grad: 0.18810712723512035
iteration: 0 loss: 1547.0706898395097 grad: 1437.0818509871112
iteration: 10 loss: 0.021848775853868574 grad: 0.15817297225479246
iteration: 0 loss: 1855.8303223934834 grad: 1464.3361285654046
iteration: 0 loss: 2924.0090369535105 grad: 1632.8195808628284
iteration: 10 loss: 0.051110681697371714 grad: -0.13310777093556125
iteration: 0 loss: 1210.1258650641225 grad: 1258.3963315462454
iteration: 0 loss: 1958.5530589143173 grad: 1457.0769019921938
iteration: 10 loss: 0.020568591059021524 grad: 0.19943622475031397
iteration: 0 loss: 1257.727856659683 grad: 1309.264563782022
iteration: 0 loss: 982.4526936581586 grad: 1215.527455623163
iteration: 0 loss: 1282.1690960941776 grad: 1227.412514902209
iteration: 0 loss: 1001.9719559997861 grad: 1218.5881508182883
iteration: 0 loss: 1481.0508937595766 grad: 1416.702713749639
iteration: 0 loss: 2159.8202273782167 grad: 1436.3598347873985
iteration: 10 loss: 0.019733643558125055 grad: 0.13578718064356654
iteration: 0 loss: 2397.808821071839 grad: 1558.6788588653335
iteration: 10 loss: 0.018696429967266424 grad: 0.22932857094424033
iteration: 0 loss: 2241.152463380332 grad: 1486.1856801068388
iteration: 10 loss: 0.023028424557361923 grad: -0.3200225424036643
iteration: 0 loss: 1779.5903480031034 grad: 1430.1972602692672
iteration: 0 loss: 2313.321696365805 grad: 1486.6120582682825
iteration: 10 loss: 0.020574962874392935 grad: 0.12051683688802485
iteration: 0 loss: 2273.2263379266747 grad: 1491.0958185135207
iteration: 10 loss: 0.02967335869536989 grad: 0.4261546899732685
iteration: 0 loss: 1802.6370281579293 grad: 1295.647013764603
iteration: 0 loss: 2828.3761113525793 grad: 1531.739813650784
iteration: 10 loss: 0.021667503530228383 grad: -0.30613050264504
iteration: 0 loss: 1922.7339709520434 grad: 1370.8750164984924
iteration: 10 loss: 0.020430138989468105 grad: 0.10032042814157482
iteration: 0 loss: 939.3886966502654 grad: 1172.207202165309
iteration: 0 loss: 852.5730402563026 grad: 1113.4625406060604
iteration: 0 loss: 1212.7985563891643 grad: 1220.1998019660725
iteration: 0 loss: 1585.0545393238738 grad: 1403.9333695330738
iteration: 0 loss: 1479.709905976448 grad: 1316.8204744360262
iteration: 0 loss: 1187.3792493173148 grad: 1275.6939957916998
iteration: 0 loss: 1037.0924480896574 grad: 1235.91774961773
iteration: 0 loss: 2183.963426414438 grad: 1553.6317034496283
iteration: 10 loss: 0.019516307580157776 grad: 0.09321625428166117
iteration: 0 loss: 1789.522698240447 grad: 1407.0612203515143
iteration: 10 loss: 0.021370921338530025 grad: -0.09144567398550654
iteration: 0 loss: 1700.181005312431 grad: 1358.311443951532
iteration: 10 loss: 0.019198132706978042 grad: 0.0840085479002205
iteration: 0 loss: 1430.517785046993 grad: 1346.9439305354385
iteration: 0 loss: 1104.3910994077319 grad: 1160.1748836755064
iteration: 0 loss: 1759.6353197553835 grad: 1391.9388954089138
iteration: 0 loss: 1264.0352258778157 grad: 1286.4738509181757
iteration: 0 loss: 1246.0484240834244 grad: 1351.9686137973017
iteration: 0 loss: 2913.6203131593415 grad: 1669.17530633001
iteration: 10 loss: 0.03821726788605288 grad: 0.4164215596129111
iteration: 0 loss: 939.6455853238548 grad: 1221.4169468564362
iteration: 0 loss: 911.6076139045825 grad: 1174.6916833309785
iteration: 0 loss: 1117.6873243115647 grad: 1238.0740474448057
iteration: 0 loss: 1385.419233465515 grad: 1378.5429697813415
iteration: 0 loss: 1762.6877094510207 grad: 1415.156819898742
iteration: 10 loss: 0.018164411599255716 grad: 0.11552536427875705
iteration: 0 loss: 1963.509539230402 grad: 1510.8398472421038
iteration: 10 loss: 0.01959949145583034 grad: 0.22231716644583735
iteration: 0 loss: 2393.417490601481 grad: 1484.1511574095366
iteration: 10 loss: 0.027276663147362837 grad: -0.05039666145307076
iteration: 0 loss: 1870.679435173977 grad: 1426.8470933183257
iteration: 10 loss: 0.028248350409822622 grad: 0.8063188901093976
iteration: 0 loss: 1486.5979893461672 grad: 1355.5048945954159
iteration: 0 loss: 2501.630468906755 grad: 1529.4846140948994
iteration: 10 loss: 0.02034468901084355 grad: 0.053992587093202996
iteration: 0 loss: 1920.3229644907747 grad: 1427.8773560622442
iteration: 0 loss: 1125.0619795282544 grad: 1339.0944785494687
iteration: 0 loss: 1429.486648966266 grad: 1344.2194685369898
iteration: 0 loss: 1262.9860357434136 grad: 1318.9281860381013
iteration: 0 loss: 2208.6424189921995 grad: 1532.9648472970134
iteration: 10 loss: 0.024198365552530795 grad: 0.46485456658871965
iteration: 0 loss: 1130.6741055931207 grad: 1349.2773715544392
iteration: 0 loss: 1442.7152705893943 grad: 1285.0785470453673
iteration: 0 loss: 768.8815363936105 grad: 1148.3429784438156
iteration: 0 loss: 1723.603513421469 grad: 1468.7030320672666
iteration: 0 loss: 1625.402028829319 grad: 1427.1334462986501
iteration: 0 loss: 1622.1914677769719 grad: 1408.1363348969815
iteration: 10 loss: 0.01919726530832122 grad: 0.12194942263722454
iteration: 0 loss: 2438.156567488579 grad: 1462.6478157573306
iteration: 10 loss: 0.019416910903105003 grad: 0.05375462421073892
iteration: 0 loss: 1276.9820064699918 grad: 1322.6309945392018
iteration: 0 loss: 1517.1137194917071 grad: 1379.3940774153866
iteration: 0 loss: 2465.325067495911 grad: 1537.0200889935693
iteration: 10 loss: 0.02017421962822978 grad: 0.055995794840361116
iteration: 0 loss: 1484.5264133514233 grad: 1311.3282827092935
iteration: 0 loss: 2047.0522347940541 grad: 1402.9451023375373
iteration: 10 loss: 0.019042621552878038 grad: 0.10985843596132651
iteration: 0 loss: 1269.3269747430722 grad: 1338.5206701600905
iteration: 0 loss: 2231.4770813164755 grad: 1544.0264183640547
iteration: 10 loss: 0.0203627763145381 grad: 0.1792251114651579
iteration: 0 loss: 1934.5765492848368 grad: 1463.4332062643111
iteration: 0 loss: 1238.1885911515164 grad: 1212.7881359291562
iteration: 0 loss: 943.4332267381509 grad: 1174.5573462393181
iteration: 0 loss: 1503.9742806692295 grad: 1449.5938558965988
iteration: 0 loss: 745.7344177880674 grad: 1125.6352113097735
iteration: 0 loss: 1124.1953480968348 grad: 1139.2366530781194
iteration: 0 loss: 940.9169312502275 grad: 1167.2521787568332
iteration: 0 loss: 2298.3869936203237 grad: 1491.3834965247427
iteration: 10 loss: 0.0201142609042687 grad: 0.08763136681449564
iteration: 0 loss: 1396.2753355265004 grad: 1276.3673440952966
iteration: 0 loss: 2333.5217428757587 grad: 1484.0068944039665
iteration: 10 loss: 0.15331598940577928 grad: 0.5405233007182996
iteration: 0 loss: 2270.0953492160147 grad: 1487.590047120094
iteration: 10 loss: 0.02001045387375846 grad: 0.0975442905300258
iteration: 0 loss: 2669.04384583016 grad: 1594.172226915336
iteration: 10 loss: 0.030298559171735014 grad: 0.626464302490964
iteration: 0 loss: 1170.940787162446 grad: 1260.6066554824156
iteration: 0 loss: 1209.5602146591239 grad: 1278.3124253369274
iteration: 0 loss: 1557.735998834685 grad: 1437.0813040602707
iteration: 0 loss: 1599.7783222165097 grad: 1390.4812777108086
iteration: 0 loss: 3091.8019618744947 grad: 1610.745190851334
iteration: 10 loss: 0.02608747611454667 grad: 1.2594255140528299
iteration: 0 loss: 1310.6632167490327 grad: 1361.7669953400018
iteration: 0 loss: 2350.4201452533957 grad: 1542.8624850922251
iteration: 10 loss: 0.023447864122731102 grad: 0.07720572468747106
iteration: 0 loss: 1534.7665291961625 grad: 1350.9249118122666
iteration: 0 loss: 971.4246881068549 grad: 1273.6674410492724
iteration: 0 loss: 2254.9839070506305 grad: 1513.3216445704984
iteration: 10 loss: 0.021493965462220578 grad: 0.10105909255587021
iteration: 0 loss: 1088.2666919714568 grad: 1266.3641022658994
iteration: 0 loss: 2675.48789543777 grad: 1538.250706234345
iteration: 10 loss: 0.2772704810016545 grad: 1.529558352946087
iteration: 0 loss: 954.7945707654676 grad: 1203.4462215440558
iteration: 0 loss: 2117.203223111043 grad: 1466.2259862826395
iteration: 0 loss: 1052.9563598334287 grad: 1192.8689163434485
iteration: 0 loss: 1594.1006614123994 grad: 1455.8070732294357
iteration: 10 loss: 0.020149443883962125 grad: 0.13160456039486015
iteration: 0 loss: 1144.1525064351981 grad: 1297.3420649769032
iteration: 0 loss: 1520.5620505116372 grad: 1352.2243975512963
iteration: 10 loss: 0.020049423226737417 grad: 0.1371579051353073
iteration: 0 loss: 1554.9757425838113 grad: 1349.2842776319135
iteration: 0 loss: 837.4982114204755 grad: 1150.4762156625643
iteration: 0 loss: 1964.4062391455116 grad: 1396.6709868231192
iteration: 0 loss: 1866.8142960288287 grad: 1413.666732156888
iteration: 10 loss: 0.02188818105456779 grad: 5.902217979882225e-05
iteration: 0 loss: 5835.031192596955 grad: 2283.448278341126
iteration: 10 loss: 0.0023626193049660123 grad: 0.021264695417828425
iteration: 0 loss: 4688.468515011267 grad: 2144.891923223091
iteration: 0 loss: 5141.185212465106 grad: 2167.309304254937
iteration: 0 loss: 6521.154267610687 grad: 2379.128591431104
iteration: 10 loss: 0.001745201741207645 grad: -0.058269181577671934
iteration: 0 loss: 3991.3948566836075 grad: 2069.0825114068657
iteration: 0 loss: 2693.9920150776966 grad: 1843.7735463133138
iteration: 10 loss: 0.0018173845402832906 grad: -0.010873353728952435
iteration: 0 loss: 4306.883571227024 grad: 2135.4038746696306
iteration: 0 loss: 2291.821670478533 grad: 1748.7168417937787
iteration: 0 loss: 6434.259481587074 grad: 2234.925172578704
iteration: 0 loss: 4202.738330649928 grad: 2079.221618739148
iteration: 0 loss: 2603.1637231576024 grad: 1903.7073647406273
iteration: 0 loss: 5326.085775817447 grad: 2238.668939880459
iteration: 10 loss: 0.0020120905484707855 grad: -0.27127504630553084
iteration: 0 loss: 3634.1248548535737 grad: 1893.7463632189676
iteration: 0 loss: 3884.916291300947 grad: 2114.114619132535
iteration: 0 loss: 5147.176451081695 grad: 2147.7039649697635
iteration: 0 loss: 7493.564746121511 grad: 2431.4287040142162
iteration: 10 loss: 0.005568069188897921 grad: 0.7723759611388789
iteration: 0 loss: 4464.292461917159 grad: 2088.9541144041173
iteration: 0 loss: 3131.274614423543 grad: 1950.0275931877497
iteration: 0 loss: 7570.948457614688 grad: 2437.7561222741488
iteration: 10 loss: 0.002498427731387148 grad: 0.6981793258815829
iteration: 0 loss: 3657.2190025203836 grad: 1971.8434267683504
iteration: 0 loss: 5303.411481008402 grad: 2231.897416699866
iteration: 0 loss: 6423.8326261230695 grad: 2321.962030457334
iteration: 10 loss: 0.0027352327081395992 grad: -0.2880825099278147
iteration: 0 loss: 3309.973948068904 grad: 1872.3475279233942
iteration: 0 loss: 3974.095796039332 grad: 2063.7460586043185
iteration: 0 loss: 4122.716567843127 grad: 2161.3091143285933
iteration: 0 loss: 3568.4786770042424 grad: 2086.9038879766654
iteration: 0 loss: 8166.15108616752 grad: 2451.1002842375083
iteration: 10 loss: 0.005724180735838177 grad: 0.26412003289573416
iteration: 0 loss: 7919.032445057118 grad: 2371.1148709657787
iteration: 10 loss: 1.1408684786385328 grad: 1.8121921789375772
iteration: 0 loss: 4508.878112294861 grad: 2130.963427145456
iteration: 0 loss: 3414.8978067940498 grad: 1964.6637873023526
iteration: 0 loss: 9337.390856032082 grad: 2587.523675388513
iteration: 10 loss: 0.03807916130939371 grad: 2.9644176668552342
iteration: 0 loss: 4539.757407189057 grad: 2252.957416616659
iteration: 0 loss: 4370.492749011152 grad: 2127.4228389489454
iteration: 0 loss: 6116.769256718893 grad: 2207.7772843774756
iteration: 10 loss: 0.012139948295291768 grad: 0.5604091295082991
iteration: 0 loss: 5986.609609567563 grad: 2277.282940916779
iteration: 10 loss: 0.0017959098463920368 grad: -0.008563196529641347
iteration: 0 loss: 5062.130607275077 grad: 2215.58069910436
iteration: 10 loss: 0.012633235420005141 grad: 0.9845954394741553
iteration: 0 loss: 4732.902414362246 grad: 2123.0933664453005
iteration: 10 loss: 0.0018176478016274896 grad: 0.2762244510634858
iteration: 0 loss: 7539.4721280501 grad: 2454.0366152483944
iteration: 10 loss: 0.016513810903151436 grad: 0.006488262500129516
iteration: 0 loss: 3553.882910582171 grad: 2023.1363399910547
iteration: 0 loss: 3920.076191430875 grad: 2058.767466284153
iteration: 0 loss: 5031.665974392999 grad: 2158.139689578584
iteration: 0 loss: 2512.514870669079 grad: 1871.9855052362789
iteration: 0 loss: 10559.048030187449 grad: 2668.917958612503
iteration: 10 loss: 1.1402558202310253 grad: 2.127069220905798
iteration: 0 loss: 4176.853183379182 grad: 1985.783182193955
iteration: 0 loss: 5685.510353701172 grad: 2385.0414184403708
iteration: 10 loss: 0.0018297072290972044 grad: -0.014377797881488314
iteration: 0 loss: 5926.223433525055 grad: 2207.9776011197027
iteration: 0 loss: 4536.179707512884 grad: 1940.7661476480062
iteration: 0 loss: 3668.8384343227863 grad: 2093.6622664600736
iteration: 0 loss: 6447.810824552314 grad: 2313.0486140777234
iteration: 10 loss: 0.0020311765296024864 grad: 0.5909734595881888
iteration: 0 loss: 6731.785313332571 grad: 2326.040175855829
iteration: 10 loss: 0.0027354576900093393 grad: 0.7192538882501709
iteration: 0 loss: 7789.19777269207 grad: 2399.3984230396854
iteration: 10 loss: 0.5721862021375934 grad: 3.480241414384948
iteration: 0 loss: 4190.926198092219 grad: 2159.7346059715296
iteration: 0 loss: 7081.298435635169 grad: 2460.72949364769
iteration: 10 loss: 0.0018700724070764738 grad: 0.2952777306962879
iteration: 0 loss: 5177.955977699458 grad: 2203.698485144832
iteration: 0 loss: 3478.0130439827817 grad: 1992.5802406121124
iteration: 0 loss: 4640.193841019975 grad: 2145.5762134474357
iteration: 10 loss: 0.002288220230680467 grad: 0.0929948691186041
iteration: 0 loss: 3456.8345855892335 grad: 2079.9839960094187
iteration: 0 loss: 5132.337252293707 grad: 2200.558280816963
iteration: 10 loss: 0.0017291266875955362 grad: -0.3287160048840675
iteration: 0 loss: 7632.919661917296 grad: 2500.081473097965
iteration: 10 loss: 0.002105390594806522 grad: 1.0350628339068977
iteration: 0 loss: 4105.696426493264 grad: 2114.4625589032357
iteration: 0 loss: 6697.565443699047 grad: 2338.650245368087
iteration: 10 loss: 0.005394098331717859 grad: 0.5634505875835438
iteration: 0 loss: 6659.679452978205 grad: 2465.218706042726
iteration: 10 loss: 0.0018690030357736925 grad: 0.05305886263630965
iteration: 0 loss: 3462.078018413222 grad: 1832.2047623046537
iteration: 0 loss: 3617.6917968804646 grad: 2010.0048929847442
iteration: 0 loss: 4356.426252838738 grad: 2180.7913005167884
iteration: 0 loss: 4498.013041071709 grad: 2129.8451879656586
iteration: 0 loss: 3380.5311442636535 grad: 1921.6224509488618
iteration: 0 loss: 6130.084087537251 grad: 2345.830719874522
iteration: 0 loss: 5004.759255089086 grad: 2150.8968963847074
iteration: 0 loss: 4955.853434864428 grad: 2286.4816346340476
iteration: 10 loss: 0.002074378180127083 grad: 0.021801954405367627
iteration: 0 loss: 5975.452870184081 grad: 2333.7590598252027
iteration: 10 loss: 0.0019598194617587565 grad: -0.18088899592029306
iteration: 0 loss: 9075.293631868331 grad: 2595.633727280161
iteration: 10 loss: 0.15197930378516586 grad: 0.4280284162604079
iteration: 0 loss: 3800.747677051078 grad: 2005.2096687280223
iteration: 0 loss: 6284.9227017678095 grad: 2318.152393830631
iteration: 0 loss: 3985.4776551363434 grad: 2085.627665302017
iteration: 0 loss: 3130.1662030896478 grad: 1934.9772373803748
iteration: 0 loss: 3986.8188725952064 grad: 1953.8218760921159
iteration: 0 loss: 3262.5383780756715 grad: 1937.8072573813736
iteration: 0 loss: 4776.594913319902 grad: 2256.5341550787143
iteration: 0 loss: 6721.35980123334 grad: 2284.730080287358
iteration: 10 loss: 0.0030247518747769805 grad: -0.9587558634670332
iteration: 0 loss: 7594.831959239501 grad: 2479.1618735635834
iteration: 10 loss: 0.001847536195947958 grad: 0.02678356666297159
iteration: 0 loss: 7025.347690911233 grad: 2365.7559340403013
iteration: 10 loss: 0.017347472889179535 grad: 1.6423711337829259
iteration: 0 loss: 5689.075061287002 grad: 2274.6882209044825
iteration: 0 loss: 7190.3967405555395 grad: 2364.6832225694247
iteration: 10 loss: 0.017726001972650094 grad: -0.10716157129554968
iteration: 0 loss: 7130.171729729061 grad: 2371.520176944236
iteration: 10 loss: 0.13441269332146144 grad: 1.4132601242808316
iteration: 0 loss: 5425.556932190791 grad: 2064.043246089437
iteration: 10 loss: 0.022995072513532996 grad: 0.23772172139974715
iteration: 0 loss: 8529.807561906984 grad: 2436.1916169732376
iteration: 10 loss: 0.006530854104891081 grad: 2.1343498897793647
iteration: 0 loss: 5827.777647994079 grad: 2182.0024844462896
iteration: 10 loss: 0.0016548892678904601 grad: 0.010005645531255103
iteration: 0 loss: 3113.150822750069 grad: 1868.891972529706
iteration: 0 loss: 2719.709061316082 grad: 1774.0949153127885
iteration: 0 loss: 3739.412927990883 grad: 1945.586490388229
iteration: 0 loss: 4979.423888475165 grad: 2235.071027640036
iteration: 0 loss: 4737.167069760196 grad: 2096.5214503530233
iteration: 10 loss: 0.0017138054086403413 grad: 0.03390631012349407
iteration: 0 loss: 3883.1137895384745 grad: 2031.5696762079226
iteration: 0 loss: 3334.0621235743706 grad: 1966.3806013388512
iteration: 0 loss: 6879.6285755684685 grad: 2470.20449833146
iteration: 10 loss: 0.00205716641291722 grad: -0.022769317509559187
iteration: 0 loss: 5586.32688980856 grad: 2240.255336765223
iteration: 10 loss: 0.004846147962697697 grad: 0.5437620194913364
iteration: 0 loss: 5159.604226057354 grad: 2164.4511756278744
iteration: 10 loss: 0.0019852884949862278 grad: 0.012977714757255024
iteration: 0 loss: 4573.861976793544 grad: 2142.113803625034
iteration: 10 loss: 0.0017619613023601812 grad: 0.06250148639050176
iteration: 0 loss: 3349.9826358187333 grad: 1846.4922105255516
iteration: 0 loss: 5449.864472271437 grad: 2214.458361587559
iteration: 0 loss: 4087.4209328871325 grad: 2048.6176962101126
iteration: 0 loss: 4159.088276288908 grad: 2153.1373369403086
iteration: 0 loss: 9246.713721951268 grad: 2655.822463348355
iteration: 10 loss: 0.13138272078041072 grad: -0.7438235033902816
iteration: 0 loss: 3076.705100357835 grad: 1944.4526205528864
iteration: 10 loss: 0.0014261834988825615 grad: 0.025185134185191926
iteration: 0 loss: 3044.160143345429 grad: 1872.0553327518971
iteration: 0 loss: 3576.109918522248 grad: 1969.8261457297403
iteration: 0 loss: 4561.256579403686 grad: 2197.4109129242343
iteration: 0 loss: 5529.798267426744 grad: 2251.444945072844
iteration: 10 loss: 0.0019178332440787926 grad: -0.001908463914560999
iteration: 0 loss: 6309.4552023175775 grad: 2405.987333349195
iteration: 10 loss: 0.0018923230858159843 grad: 0.29467238794503303
iteration: 0 loss: 7759.078164713718 grad: 2360.6677619147003
iteration: 10 loss: 0.23381557522754354 grad: 0.3990094786344923
iteration: 0 loss: 6090.6975436750445 grad: 2270.7227479423086
iteration: 10 loss: 0.04974355608944527 grad: 3.0105185192085084
iteration: 0 loss: 4760.878229742233 grad: 2159.2138305331828
iteration: 10 loss: 0.0019111874386329543 grad: -0.1439681063546687
iteration: 0 loss: 7861.70940459333 grad: 2432.411368463455
iteration: 10 loss: 0.05687874170623466 grad: 1.163557615798743
iteration: 0 loss: 5982.770565946409 grad: 2273.2691554505636
iteration: 0 loss: 3820.70185544609 grad: 2133.3856414832035
iteration: 10 loss: 0.002005954888194207 grad: 0.007986254826885364
iteration: 0 loss: 4434.567577105391 grad: 2139.8305709402134
iteration: 0 loss: 4057.295307486647 grad: 2099.9474012067058
iteration: 0 loss: 7116.358818623071 grad: 2437.976985266796
iteration: 10 loss: 0.13550564747261393 grad: 0.7341455034109546
iteration: 0 loss: 3706.1684623684737 grad: 2146.8264018847503
iteration: 10 loss: 0.0021057869215093724 grad: 0.004612835137956902
iteration: 0 loss: 4532.759238009417 grad: 2047.4915811642404
iteration: 0 loss: 2535.248438146539 grad: 1830.324585548286
iteration: 0 loss: 5347.819745943703 grad: 2337.8002604360436
iteration: 0 loss: 5160.655838868509 grad: 2271.3763365506434
iteration: 0 loss: 5142.968887885755 grad: 2239.2146663079116
iteration: 10 loss: 0.002013509943605062 grad: 0.0010099308667240944
iteration: 0 loss: 7404.541280755042 grad: 2326.2525316864544
iteration: 10 loss: 0.12627766440379096 grad: 2.006944422523234
iteration: 0 loss: 4150.364298117891 grad: 2104.859832886829
iteration: 0 loss: 4842.544899513031 grad: 2194.1842544235396
iteration: 0 loss: 7512.023221358425 grad: 2445.2585045124215
iteration: 10 loss: 0.002284892593954944 grad: 0.034076540367982244
iteration: 0 loss: 4722.049170173709 grad: 2086.405359836872
iteration: 0 loss: 6370.25653596792 grad: 2233.613601757444
iteration: 10 loss: 0.0019166470420631495 grad: 0.7645226489182995
iteration: 0 loss: 4091.726801757826 grad: 2133.599319678486
iteration: 0 loss: 6979.4092073254 grad: 2456.9812038043356
iteration: 0 loss: 5987.34709505564 grad: 2329.1163568378306
iteration: 0 loss: 3842.1183643824206 grad: 1934.9595625962813
iteration: 0 loss: 3001.474916396961 grad: 1873.0035998856229
iteration: 0 loss: 4915.221790862954 grad: 2309.64058264271
iteration: 0 loss: 2509.3421678563554 grad: 1798.4835481698667
iteration: 0 loss: 3484.367218339931 grad: 1817.4666628868151
iteration: 0 loss: 2995.3840785851776 grad: 1862.5651147623241
iteration: 0 loss: 7168.863782035065 grad: 2372.6349492320824
iteration: 10 loss: 0.0019244457203471525 grad: 0.015775280769629342
iteration: 0 loss: 4361.836855343526 grad: 2028.8686961278565
iteration: 0 loss: 7495.081892663683 grad: 2360.5090611517
iteration: 10 loss: 0.36566509982036555 grad: 1.667702463356544
iteration: 0 loss: 7068.864209267983 grad: 2366.1172845416927
iteration: 10 loss: 0.0022132543116723273 grad: -0.21866166283500252
iteration: 0 loss: 8453.388134238501 grad: 2536.031236381844
iteration: 10 loss: 0.09654795624903106 grad: 1.3795686570830497
iteration: 0 loss: 3836.399629278938 grad: 2007.9726159215784
iteration: 0 loss: 3889.4861831562816 grad: 2037.0239707590574
iteration: 0 loss: 5033.316704386662 grad: 2283.8862647006326
iteration: 0 loss: 5137.817518309105 grad: 2213.710727821996
iteration: 0 loss: 9673.054044435152 grad: 2563.572312977774
iteration: 10 loss: 0.31717453939067625 grad: 4.090057845786207
iteration: 0 loss: 4292.232133369198 grad: 2170.016432669175
iteration: 0 loss: 7439.310378995651 grad: 2452.538264468996
iteration: 10 loss: 0.002183593332037245 grad: -0.12636097096968776
iteration: 0 loss: 4887.07662134695 grad: 2153.4449179338962
iteration: 0 loss: 3283.743415832713 grad: 2027.3783735231275
iteration: 0 loss: 6993.527262195377 grad: 2407.112083496564
iteration: 10 loss: 0.001932623640062626 grad: 1.2681214976668105
iteration: 0 loss: 3501.4096444228812 grad: 2014.8326502146047
iteration: 0 loss: 8446.587891284671 grad: 2448.1039662462626
iteration: 10 loss: 0.6576256203420185 grad: 4.475759320709753
iteration: 0 loss: 3088.4102462318024 grad: 1916.18753579351
iteration: 0 loss: 6601.728585795248 grad: 2330.888345499492
iteration: 10 loss: 0.041482875126265834 grad: 0.5371186002437057
iteration: 0 loss: 3384.730811812067 grad: 1902.589402096238
iteration: 0 loss: 5088.7403767403775 grad: 2316.796033065815
iteration: 0 loss: 3685.96298805866 grad: 2064.8446445443637
iteration: 0 loss: 4989.748705841801 grad: 2153.9087790831477
iteration: 10 loss: 0.0021748154567004267 grad: -0.14800664222411145
iteration: 0 loss: 4829.036217141609 grad: 2148.1948028454144
iteration: 0 loss: 2725.9740007470873 grad: 1831.544513658681
iteration: 0 loss: 6088.145569464951 grad: 2223.781282059665
iteration: 10 loss: 0.014330216478397646 grad: 0.4187541872137932
iteration: 0 loss: 5889.053623340979 grad: 2250.8629859350667
iteration: 10 loss: 0.002023508296009492 grad: 0.34147137617818246
iteration: 0 loss: inf grad: 3725.996992376413
iteration: 0 loss: inf grad: 3499.884252826833
iteration: 0 loss: inf grad: 3537.425993453753
iteration: 0 loss: inf grad: 3879.8074394582327
iteration: 0 loss: inf grad: 3377.3193438312605
iteration: 0 loss: inf grad: 3013.043493692011
iteration: 0 loss: inf grad: 3483.586546742945
iteration: 0 loss: 7096.703734057088 grad: 2849.7109192726166
iteration: 0 loss: inf grad: 3651.513711800785
iteration: 0 loss: inf grad: 3397.260170775737
iteration: 0 loss: 8192.263960906897 grad: 3107.27175296896
iteration: 0 loss: inf grad: 3650.524287706099
iteration: 0 loss: inf grad: 3092.8678780346318
iteration: 0 loss: inf grad: 3451.593457597524
iteration: 0 loss: inf grad: 3503.925477461526
iteration: 0 loss: inf grad: 3965.787196556642
iteration: 0 loss: inf grad: 3409.123149269387
iteration: 0 loss: inf grad: 3173.948875303567
iteration: 0 loss: inf grad: 3975.6715718175465
iteration: 0 loss: 11237.435349309277 grad: 3217.4364291649545
iteration: 0 loss: inf grad: 3644.635857025257
iteration: 0 loss: inf grad: 3790.6663123307026
iteration: 0 loss: inf grad: 3060.543019105002
iteration: 0 loss: inf grad: 3366.0412036437856
iteration: 0 loss: inf grad: 3529.3752084620182
iteration: 0 loss: inf grad: 3406.484623457255
iteration: 0 loss: inf grad: 3995.1279287416014
iteration: 0 loss: inf grad: 3868.269516874436
iteration: 0 loss: inf grad: 3480.1186056294664
iteration: 0 loss: inf grad: 3205.522491852303
iteration: 10 loss: 0.00024395945100960407 grad: 0.039328906935362715
iteration: 0 loss: inf grad: 4217.818024493507
iteration: 0 loss: inf grad: 3677.233548030285
iteration: 0 loss: inf grad: 3472.406941090724
iteration: 0 loss: inf grad: 3609.158292844957
iteration: 0 loss: inf grad: 3717.1063593360295
iteration: 0 loss: inf grad: 3616.403993447073
iteration: 0 loss: inf grad: 3462.0356147749444
iteration: 0 loss: inf grad: 4001.1769309999054
iteration: 0 loss: inf grad: 3303.468898145674
iteration: 0 loss: inf grad: 3358.5462382616065
iteration: 0 loss: inf grad: 3524.1762782934697
iteration: 0 loss: inf grad: 3057.8012095635204
iteration: 0 loss: inf grad: 4352.329978310154
iteration: 0 loss: inf grad: 3242.9452586929824
iteration: 0 loss: inf grad: 3893.3574359970917
iteration: 0 loss: inf grad: 3599.2092929188966
iteration: 0 loss: inf grad: 3170.2762895956603
iteration: 0 loss: inf grad: 3418.1904227864784
iteration: 10 loss: 0.00010760607917539097 grad: 0.002666484682765523
iteration: 0 loss: inf grad: 3777.3817531357286
iteration: 0 loss: inf grad: 3795.3387604280692
iteration: 0 loss: inf grad: 3915.169552644527
iteration: 0 loss: inf grad: 3521.9107695404336
iteration: 0 loss: inf grad: 4019.2373682839775
iteration: 0 loss: inf grad: 3597.8931730616805
iteration: 0 loss: 10758.42379380809 grad: 3246.9368413305483
iteration: 0 loss: inf grad: 3500.3773901952827
iteration: 0 loss: 10809.210247197943 grad: 3392.4754907278952
iteration: 0 loss: inf grad: 3589.7829887210555
iteration: 0 loss: inf grad: 4074.0448963403833
iteration: 0 loss: inf grad: 3455.0693085951675
iteration: 0 loss: inf grad: 3816.624597687901
iteration: 0 loss: inf grad: 4021.739541467435
iteration: 0 loss: inf grad: 2992.2766583364473
iteration: 0 loss: inf grad: 3285.3602395010184
iteration: 0 loss: inf grad: 3556.644013242254
iteration: 0 loss: inf grad: 3474.267117499693
iteration: 0 loss: inf grad: 3134.2425699619216
iteration: 0 loss: inf grad: 3827.3389106786485
iteration: 0 loss: inf grad: 3510.294251767991
iteration: 0 loss: inf grad: 3731.064186325441
iteration: 0 loss: inf grad: 3802.127825034365
iteration: 0 loss: inf grad: 4234.156389004775
iteration: 0 loss: inf grad: 3266.811332243093
iteration: 0 loss: inf grad: 3780.185811248102
iteration: 0 loss: inf grad: 3403.8887032308385
iteration: 10 loss: 0.0002733140480069613 grad: 0.0033875029793944914
iteration: 0 loss: inf grad: 3158.0633570513774
iteration: 0 loss: inf grad: 3192.2611806980285
iteration: 0 loss: inf grad: 3166.9465843383978
iteration: 10 loss: 0.00016017803467217493 grad: -0.002301218007378444
iteration: 0 loss: inf grad: 3676.022804831021
iteration: 0 loss: inf grad: 3728.8945870016823
iteration: 0 loss: inf grad: 4041.201945506231
iteration: 0 loss: inf grad: 3856.580404083771
iteration: 0 loss: inf grad: 3713.0109891334637
iteration: 0 loss: inf grad: 3860.927099808978
iteration: 0 loss: inf grad: 3868.6139031143393
iteration: 0 loss: inf grad: 3365.3882136517714
iteration: 0 loss: inf grad: 3975.8847897598275
iteration: 0 loss: inf grad: 3561.960795734128
iteration: 0 loss: inf grad: 3052.036400507689
iteration: 0 loss: inf grad: 2900.297624097936
iteration: 0 loss: inf grad: 3177.9683736041743
iteration: 0 loss: inf grad: 3643.4747136623864
iteration: 0 loss: inf grad: 3419.827212917789
iteration: 0 loss: inf grad: 3316.6652916445764
iteration: 0 loss: inf grad: 3211.776909311635
iteration: 0 loss: inf grad: 4025.833251124009
iteration: 0 loss: inf grad: 3652.9425120994138
iteration: 0 loss: inf grad: 3529.899812849506
iteration: 0 loss: inf grad: 3499.5395697305003
iteration: 0 loss: inf grad: 3019.5215448207514
iteration: 0 loss: inf grad: 3612.1135759386475
iteration: 0 loss: inf grad: 3346.348646726486
iteration: 0 loss: inf grad: 3517.0160987499758
iteration: 0 loss: inf grad: 4327.3482892774355
iteration: 0 loss: inf grad: 3172.3075542356587
iteration: 0 loss: inf grad: 3055.9898463929158
iteration: 0 loss: inf grad: 3215.9260626604537
iteration: 0 loss: inf grad: 3584.990941054469
iteration: 0 loss: inf grad: 3675.382981869647
iteration: 0 loss: inf grad: 3923.143339325648
iteration: 0 loss: inf grad: 3853.3483056028786
iteration: 0 loss: inf grad: 3701.0353694056685
iteration: 0 loss: inf grad: 3519.6337013703305
iteration: 0 loss: inf grad: 3970.681842051071
iteration: 0 loss: inf grad: 3703.9527323713855
iteration: 0 loss: inf grad: 3477.8848039184145
iteration: 0 loss: inf grad: 3492.06703816792
iteration: 0 loss: inf grad: 3425.786420880944
iteration: 0 loss: inf grad: 3974.2793263351914
iteration: 0 loss: inf grad: 3504.2916044185167
iteration: 10 loss: 0.00020125191076658666 grad: 0.004465522708220935
iteration: 0 loss: inf grad: 3342.4535118038893
iteration: 0 loss: inf grad: 2985.682650581895
iteration: 0 loss: inf grad: 3806.347173336131
iteration: 0 loss: inf grad: 3704.0159427768244
iteration: 0 loss: inf grad: 3656.901455665186
iteration: 0 loss: inf grad: 3797.9644732128986
iteration: 0 loss: inf grad: 3433.331562792002
iteration: 0 loss: inf grad: 3583.268469694486
iteration: 0 loss: inf grad: 3983.812631398438
iteration: 0 loss: inf grad: 3402.890986925554
iteration: 0 loss: inf grad: 3645.490997889202
iteration: 0 loss: inf grad: 3481.86433788816
iteration: 0 loss: inf grad: 4010.582969177536
iteration: 0 loss: inf grad: 3799.6898117658284
iteration: 0 loss: inf grad: 3154.9005072928508
iteration: 10 loss: 0.00019514282096431336 grad: 0.0012097535309729675
iteration: 0 loss: inf grad: 3062.2303550195584
iteration: 0 loss: inf grad: 3765.127605280102
iteration: 0 loss: inf grad: 2936.253514883127
iteration: 0 loss: inf grad: 2970.3317024731896
iteration: 0 loss: inf grad: 3036.5481443318968
iteration: 0 loss: inf grad: 3867.079398280986
iteration: 0 loss: inf grad: 3314.138990568636
iteration: 0 loss: inf grad: 3855.6252278587326
iteration: 0 loss: inf grad: 3860.8478569742833
iteration: 0 loss: inf grad: 4134.628559692746
iteration: 0 loss: inf grad: 3279.135415458986
iteration: 0 loss: inf grad: 3319.1587094065017
iteration: 0 loss: inf grad: 3729.6951087988496
iteration: 0 loss: inf grad: 3611.1261281389707
iteration: 0 loss: inf grad: 4178.405537875003
iteration: 0 loss: inf grad: 3537.220262652306
iteration: 0 loss: inf grad: 3997.9205483165924
iteration: 0 loss: inf grad: 3512.9330732703907
iteration: 0 loss: inf grad: 3313.2482968301233
iteration: 0 loss: inf grad: 3924.0317706933633
iteration: 0 loss: inf grad: 3293.829209795289
iteration: 0 loss: inf grad: 3995.467137509883
iteration: 0 loss: inf grad: 3132.1369511262365
iteration: 0 loss: inf grad: 3801.178013537095
iteration: 0 loss: inf grad: 3104.8733621516867
iteration: 0 loss: inf grad: 3782.086632012445
iteration: 0 loss: inf grad: 3373.680416072004
iteration: 0 loss: inf grad: 3510.838082449756
iteration: 0 loss: inf grad: 3507.776054126387
iteration: 0 loss: inf grad: 2992.3550431699387
iteration: 0 loss: inf grad: 3625.583497421986
iteration: 0 loss: inf grad: 3671.5907914314575
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:57: RuntimeWarning: invalid value encountered in double_scalars
  diff = np.absolute(f_history[-1]-f_history[-2])
iteration: 0 loss: 275.01358241204923 grad: 752.9404099703297
iteration: 10 loss: 0.36212742952209903 grad: 0.7099223427363466
iteration: 20 loss: 0.20804651293920787 grad: 0.2720615242282921
iteration: 0 loss: 202.29670097009082 grad: 703.0615379645966
iteration: 10 loss: 0.4251527255481216 grad: 0.3818718788940577
iteration: 20 loss: 0.2590181577447352 grad: 0.14748083345139346
iteration: 0 loss: 211.0072028994221 grad: 712.1570014856561
iteration: 10 loss: 0.4144569595662853 grad: 0.4233013048072297
iteration: 20 loss: 0.25155787988809275 grad: 0.15952614944452195
iteration: 0 loss: 316.97627071260007 grad: 783.9177288651088
iteration: 10 loss: 0.3594686583786494 grad: 0.6530338518636916
iteration: 20 loss: 0.21146954256164463 grad: 0.22340595291827695
iteration: 0 loss: 190.1468239067264 grad: 678.8337410368547
iteration: 10 loss: 0.4307761454605167 grad: 0.35056187417862716
iteration: 20 loss: 0.27124386612228013 grad: 0.12044402432619883
iteration: 30 loss: 0.20690414776733881 grad: 0.061351842524924274
iteration: 0 loss: 133.8818155566124 grad: 600.3878434025498
iteration: 10 loss: 0.4666386387111329 grad: 0.5630216637527478
iteration: 20 loss: 0.29128140604015323 grad: 0.22577010954005663
iteration: 30 loss: 0.22170036930666273 grad: 0.11998978500458478
iteration: 0 loss: 192.5586725412281 grad: 698.2834456847488
iteration: 10 loss: 0.4038698872281285 grad: 0.3965973038467989
iteration: 20 loss: 0.2553256879775125 grad: 0.18249329052719304
iteration: 0 loss: 96.89182858065413 grad: 568.9075920096313
iteration: 10 loss: 0.5919583438398232 grad: 0.5225778751737169
iteration: 20 loss: 0.37616665099002683 grad: 0.23186231936481289
iteration: 30 loss: 0.28547583155366074 grad: 0.13018908263636778
iteration: 0 loss: 322.74931011274026 grad: 736.0361759526959
iteration: 10 loss: 0.40614410580383264 grad: 0.6280601455730545
iteration: 20 loss: 0.2416553809185147 grad: 0.20281242307003
iteration: 0 loss: 184.4208210250513 grad: 682.5464018780523
iteration: 10 loss: 0.4197753747900854 grad: 0.3359044817135041
iteration: 20 loss: 0.26484243717748707 grad: 0.12088895565563111
iteration: 0 loss: 105.20218244327242 grad: 620.3795904791875
iteration: 10 loss: 0.5107007515868066 grad: 0.4423793670372173
iteration: 20 loss: 0.3297410018458322 grad: 0.14316939143989943
iteration: 30 loss: 0.2530077806293114 grad: 0.06133685254477591
iteration: 0 loss: 213.66174581326197 grad: 736.1942178040846
iteration: 10 loss: 0.40721514099591744 grad: 0.3666700537917435
iteration: 20 loss: 0.2474259688277181 grad: 0.11547714283760471
iteration: 0 loss: 184.4068859134043 grad: 619.5888077687874
iteration: 10 loss: 0.43601043342732737 grad: 0.4378943429988508
iteration: 20 loss: 0.2754796412322817 grad: 0.22234604696013183
iteration: 30 loss: 0.21016686530740777 grad: 0.1468601564067275
iteration: 0 loss: 165.11918810649306 grad: 692.573473094479
iteration: 10 loss: 0.41874847083032696 grad: 0.24146975523862962
iteration: 20 loss: 0.26354949065014144 grad: 0.06534745681562557
iteration: 0 loss: 236.5910135057271 grad: 705.8032215793953
iteration: 10 loss: 0.38688671898125904 grad: 0.41603342947450234
iteration: 20 loss: 0.23871011756422045 grad: 0.13665665018801954
iteration: 0 loss: 306.5709279048075 grad: 802.3377739752382
iteration: 10 loss: 0.385499286625914 grad: 0.5858180438638972
iteration: 20 loss: 0.21839528870557182 grad: 0.1890702794799512
iteration: 0 loss: 198.05850887206873 grad: 685.1803800470497
iteration: 10 loss: 0.4073183917716134 grad: 0.34551144437355474
iteration: 20 loss: 0.2524465522477509 grad: 0.11298908672981742
iteration: 0 loss: 143.61780654730202 grad: 635.1020665605645
iteration: 10 loss: 0.4693652929259274 grad: 0.49662902139921017
iteration: 20 loss: 0.29818148959425755 grad: 0.2143464632605841
iteration: 30 loss: 0.22811293556134676 grad: 0.1259176084121241
iteration: 0 loss: 376.7793702116907 grad: 804.962071908061
iteration: 10 loss: 0.39299367832047044 grad: 0.6878378714319637
iteration: 20 loss: 0.21634664695970554 grad: 0.26038897371171954
iteration: 0 loss: 164.33246680138993 grad: 644.336796449402
iteration: 10 loss: 0.46111425005470996 grad: 0.34659635947982265
iteration: 20 loss: 0.2889893092248961 grad: 0.1697130225446885
iteration: 30 loss: 0.2197254259629269 grad: 0.10343452947779477
iteration: 0 loss: 228.7202522328699 grad: 733.5335956524345
iteration: 10 loss: 0.38984045392424543 grad: 0.2669448858213598
iteration: 20 loss: 0.23837892444821177 grad: 0.10493880164770093
iteration: 0 loss: 255.69789532868592 grad: 763.8527154303448
iteration: 10 loss: 0.39670225697219313 grad: 0.6773732964154398
iteration: 20 loss: 0.22867091168758302 grad: 0.24058729306104923
iteration: 0 loss: 189.455016978228 grad: 612.2415767255695
iteration: 10 loss: 0.4199132635201915 grad: 0.5421511126450238
iteration: 20 loss: 0.26457981995140883 grad: 0.21497869159251337
iteration: 0 loss: 134.28502318206077 grad: 677.2371842624133
iteration: 10 loss: 0.47618598902028514 grad: 0.28247501881650516
iteration: 20 loss: 0.2958412633361976 grad: 0.06235717786227726
iteration: 30 loss: 0.22366398063819692 grad: 0.013313679989747643
iteration: 0 loss: 180.1832829399295 grad: 709.1777863193338
iteration: 10 loss: 0.37627424032151874 grad: 0.4204959584702679
iteration: 20 loss: 0.2339988000135649 grad: 0.15237087303406283
iteration: 0 loss: 180.15904651652917 grad: 683.6439207353228
iteration: 10 loss: 0.4402015507123658 grad: 0.476570113617707
iteration: 20 loss: 0.26927301119634317 grad: 0.18561469127197974
iteration: 30 loss: 0.20362218750456537 grad: 0.10346800508523965
iteration: 0 loss: 414.1557600929295 grad: 808.4793911469897
iteration: 10 loss: 0.36981117217791587 grad: 0.841452332526341
iteration: 20 loss: 0.20220052660641324 grad: 0.3238866514312294
iteration: 0 loss: 377.05926120644057 grad: 781.4458765671504
iteration: 10 loss: 0.45405129248286935 grad: 0.6473831539521109
iteration: 20 loss: 0.23644877821210353 grad: 0.24643491117657307
iteration: 0 loss: 173.18819918294741 grad: 701.9099608640884
iteration: 10 loss: 0.41659626539140765 grad: 0.18817806579516766
iteration: 20 loss: 0.2598339628033996 grad: 0.041549762351954844
iteration: 0 loss: 162.4877375492124 grad: 643.1910733478999
iteration: 10 loss: 0.45196780229087546 grad: 0.4156534751444473
iteration: 20 loss: 0.28193903556116723 grad: 0.15352248209530395
iteration: 30 loss: 0.21442536789279323 grad: 0.07874982112698282
iteration: 0 loss: 460.7120711617106 grad: 855.2447667872266
iteration: 10 loss: 0.34781293929385854 grad: 0.6456033916568283
iteration: 20 loss: 0.18725924686573722 grad: 0.2077876383276925
iteration: 0 loss: 196.2482149626136 grad: 741.0715422973956
iteration: 10 loss: 0.40145697725049767 grad: -0.03442511549236313
iteration: 20 loss: 0.25101483862280555 grad: -0.02755263449091725
iteration: 0 loss: 192.8521311511784 grad: 697.1320751833478
iteration: 10 loss: 0.4418248688094867 grad: 0.38456107569683545
iteration: 20 loss: 0.2716867110715281 grad: 0.13691404855562211
iteration: 30 loss: 0.20526291721530884 grad: 0.0716262320249336
iteration: 0 loss: 273.26881624452477 grad: 725.6389252534029
iteration: 10 loss: 0.401941947965001 grad: 0.5933979679735478
iteration: 20 loss: 0.2405832924543409 grad: 0.22923053560529077
iteration: 0 loss: 273.0514275525977 grad: 750.5929637887409
iteration: 10 loss: 0.4024409055031025 grad: 0.6091343260103813
iteration: 20 loss: 0.237338763487225 grad: 0.2259240440947468
iteration: 0 loss: 219.70359411006874 grad: 729.4182827067751
iteration: 10 loss: 0.4202163901048731 grad: 0.6970064958734482
iteration: 20 loss: 0.2510943670859805 grad: 0.26849044037190395
iteration: 0 loss: 223.39610949861898 grad: 697.8831659959017
iteration: 10 loss: 0.38285621201785863 grad: 0.6579283504924995
iteration: 20 loss: 0.23321558261911626 grad: 0.23604304276640847
iteration: 0 loss: 300.4734428598691 grad: 809.0735623663797
iteration: 10 loss: 0.35267192034420575 grad: 0.6184381786286006
iteration: 20 loss: 0.20186649963100287 grad: 0.23005101139268858
iteration: 0 loss: 174.29963773128674 grad: 662.912104632392
iteration: 10 loss: 0.40867055501853017 grad: 0.5051776206973664
iteration: 20 loss: 0.25642361409104936 grad: 0.20257271092346762
iteration: 0 loss: 167.22824789890868 grad: 676.1212027180129
iteration: 10 loss: 0.43370291538717126 grad: 0.304983278360483
iteration: 20 loss: 0.27393905711621547 grad: 0.10429671483181575
iteration: 30 loss: 0.2091317113222844 grad: 0.049696735652043564
iteration: 0 loss: 284.44239161400657 grad: 710.4802905056858
iteration: 10 loss: 0.40189636950592766 grad: 0.7750706180817124
iteration: 20 loss: 0.2379472812400226 grad: 0.29099710905936615
iteration: 0 loss: 104.39288067074581 grad: 610.890387434787
iteration: 10 loss: 0.5431131886461199 grad: 0.3779590865789883
iteration: 20 loss: 0.34454473032887606 grad: 0.13664583569676783
iteration: 30 loss: 0.261901511979886 grad: 0.0626033499280326
iteration: 0 loss: 495.6944480409817 grad: 883.1356216065094
iteration: 10 loss: 0.34857614316123625 grad: 0.5798014274527964
iteration: 20 loss: 0.1822987236494307 grad: 0.1941123580764118
iteration: 0 loss: 224.3269460105325 grad: 650.6914106816619
iteration: 10 loss: 0.41460283095222455 grad: 0.5845628182542557
iteration: 20 loss: 0.2521793558662466 grad: 0.24313211434817428
iteration: 0 loss: 224.84082245174721 grad: 786.2300615709792
iteration: 10 loss: 0.3861844891673007 grad: 0.28289396989006693
iteration: 20 loss: 0.23361666129742595 grad: 0.07461658422939774
iteration: 0 loss: 323.31626135562186 grad: 725.1712075674676
iteration: 10 loss: 0.3644143179087471 grad: 0.9078960064121412
iteration: 20 loss: 0.21389467629300765 grad: 0.35875576041446783
iteration: 0 loss: 259.2054411272897 grad: 631.9707104497222
iteration: 10 loss: 0.41203941432187013 grad: 0.7083087445832139
iteration: 20 loss: 0.24979659211484515 grad: 0.28582087265201156
iteration: 0 loss: 166.20920694327924 grad: 686.7821735880252
iteration: 10 loss: 0.4373409000812568 grad: 0.43384818859816693
iteration: 20 loss: 0.2724704493053981 grad: 0.1441569960057289
iteration: 30 loss: 0.2072823756657685 grad: 0.06798665176586505
iteration: 0 loss: 295.24574372823804 grad: 763.281144598528
iteration: 10 loss: 0.38760132068463543 grad: 0.7360931334602634
iteration: 20 loss: 0.22503791801765063 grad: 0.29325461786848306
iteration: 0 loss: 327.2193781146787 grad: 767.2724240194539
iteration: 10 loss: 0.4012233264822333 grad: 0.8389584043145978
iteration: 20 loss: 0.22844104467739354 grad: 0.29264783097928493
iteration: 0 loss: 352.1945923622201 grad: 792.0889552851011
iteration: 10 loss: 0.40533127923878387 grad: 0.7349752243254133
iteration: 20 loss: 0.2259598719397631 grad: 0.20836662052580024
iteration: 0 loss: 176.21206643698787 grad: 710.4329237170414
iteration: 10 loss: 0.41361470225332264 grad: 0.49349212584966456
iteration: 20 loss: 0.254775558741709 grad: 0.16315059084381653
iteration: 0 loss: 302.88847972418034 grad: 811.5607111514548
iteration: 10 loss: 0.3831819168942523 grad: 0.4207836010471967
iteration: 20 loss: 0.2213231103059198 grad: 0.1377193742911154
iteration: 0 loss: 255.29635289290226 grad: 725.8036488446164
iteration: 10 loss: 0.3952665061950194 grad: 0.5976295253278988
iteration: 20 loss: 0.23816157091790624 grad: 0.21921353389386394
iteration: 0 loss: 139.44107843043932 grad: 652.2976179167961
iteration: 10 loss: 0.4481215153155889 grad: -0.12585367474498188
iteration: 20 loss: 0.290688547545704 grad: -0.06631228549111184
iteration: 30 loss: 0.22396600448047882 grad: -0.05093790417636232
iteration: 0 loss: 195.14583346637963 grad: 704.8214181599437
iteration: 10 loss: 0.4174219492493979 grad: 0.5443236156592137
iteration: 20 loss: 0.25620123235528974 grad: 0.21081327233259284
iteration: 0 loss: 120.20499069959297 grad: 682.0254754134118
iteration: 10 loss: 0.47849950044135103 grad: -0.14229372434002285
iteration: 20 loss: 0.3078238623749547 grad: -0.09983329897397665
iteration: 30 loss: 0.2360487622901763 grad: -0.08277968227448465
iteration: 0 loss: 228.52230388732121 grad: 724.0084371411401
iteration: 10 loss: 0.38388119741218607 grad: 0.4162031446296703
iteration: 20 loss: 0.2343320622890546 grad: 0.14253869754958734
iteration: 0 loss: 368.8305289656596 grad: 825.04185725956
iteration: 10 loss: 0.354387564027506 grad: 0.8629717066115533
iteration: 20 loss: 0.19872872481242998 grad: 0.34633927804082576
iteration: 0 loss: 176.10865791819924 grad: 694.3912048652147
iteration: 10 loss: 0.42669757732793107 grad: 0.02560408756343928
iteration: 20 loss: 0.2684945424202438 grad: -0.012320890752231903
iteration: 0 loss: 286.6646550154209 grad: 770.0820381007054
iteration: 10 loss: 0.34087966411938064 grad: 0.8534669136683517
iteration: 20 loss: 0.1946964015911693 grad: 0.29852183543458954
iteration: 0 loss: 273.72547564650034 grad: 813.057131075386
iteration: 10 loss: 0.3696007031223251 grad: 0.1732304603209031
iteration: 20 loss: 0.21801392673370917 grad: 0.047109785003716244
iteration: 0 loss: 150.02235215672863 grad: 595.2636499631185
iteration: 10 loss: 0.46281702516874 grad: 0.30114331290408586
iteration: 20 loss: 0.2930022480676681 grad: 0.15229511478302793
iteration: 30 loss: 0.22343612967217102 grad: 0.09747545767312307
iteration: 0 loss: 127.76244765543991 grad: 658.0227573454873
iteration: 10 loss: 0.4842976284820576 grad: 0.2807229218668832
iteration: 20 loss: 0.31021463357071544 grad: 0.08674063363207396
iteration: 30 loss: 0.2378664180521148 grad: 0.03166492938536515
iteration: 0 loss: 190.40721690717916 grad: 717.7445833270252
iteration: 10 loss: 0.40559992226810043 grad: 0.10130209519784508
iteration: 20 loss: 0.2534786077895577 grad: 0.07004984935172395
iteration: 0 loss: 201.6762496834437 grad: 699.9655390965427
iteration: 10 loss: 0.4029819107112086 grad: 0.44759865855114855
iteration: 20 loss: 0.2506726454631379 grad: 0.16832991402557934
iteration: 0 loss: 172.50650390237487 grad: 625.8914657588346
iteration: 10 loss: 0.4318726329635286 grad: 0.42963141132742805
iteration: 20 loss: 0.2759959066430714 grad: 0.1733805954660699
iteration: 30 loss: 0.2119651288764792 grad: 0.09583312390501308
iteration: 0 loss: 270.2948216923104 grad: 773.0464097528829
iteration: 10 loss: 0.38771759863497707 grad: 0.6106950589107376
iteration: 20 loss: 0.22791139798490886 grad: 0.21426004596089315
iteration: 0 loss: 283.52650819870126 grad: 705.4102873727877
iteration: 10 loss: 0.40417924005049083 grad: 0.7443864883808494
iteration: 20 loss: 0.24001800456566838 grad: 0.278001016706072
iteration: 0 loss: 217.00572387676922 grad: 753.7132946378943
iteration: 10 loss: 0.3850132022248131 grad: 0.3646872722088518
iteration: 20 loss: 0.23409869830844732 grad: 0.13890609517945743
iteration: 0 loss: 241.64212995699862 grad: 768.0917047175785
iteration: 10 loss: 0.40076154849496315 grad: 0.406573820409574
iteration: 20 loss: 0.24023679620503655 grad: 0.13965290279549225
iteration: 0 loss: 437.1825064780111 grad: 858.869598443844
iteration: 10 loss: 0.38788500996104797 grad: 0.8551965320657885
iteration: 20 loss: 0.19855912009949284 grad: 0.24864894440810448
iteration: 0 loss: 189.83708205663004 grad: 655.4476085406794
iteration: 10 loss: 0.4441806401442095 grad: 0.5313588775820044
iteration: 20 loss: 0.2770417327043436 grad: 0.21227334651698748
iteration: 30 loss: 0.21065350137202835 grad: 0.12363009585816105
iteration: 0 loss: 267.74860765029416 grad: 763.4920613600411
iteration: 10 loss: 0.38941205582523253 grad: 0.575486556092961
iteration: 20 loss: 0.22949472308399974 grad: 0.22081358495550188
iteration: 0 loss: 193.7332918073112 grad: 685.2380901029838
iteration: 10 loss: 0.4296767619649803 grad: 0.35224434487604905
iteration: 20 loss: 0.26347874243624986 grad: 0.1174833988668145
iteration: 30 loss: 0.19894638802774847 grad: 0.059013419435407145
iteration: 0 loss: 150.01732913026453 grad: 633.1721051703248
iteration: 10 loss: 0.43000394947218656 grad: 0.44007669418333295
iteration: 20 loss: 0.2710717647917342 grad: 0.19021559719137293
iteration: 30 loss: 0.20736033919704575 grad: 0.11271886783223187
iteration: 0 loss: 210.551095543503 grad: 639.5777635198799
iteration: 10 loss: 0.40743714780323487 grad: 0.6705457186320005
iteration: 20 loss: 0.2512387527436989 grad: 0.27811859019382584
iteration: 0 loss: 136.79373221156823 grad: 635.3355906583912
iteration: 10 loss: 0.47850760372197826 grad: 0.35404822388558066
iteration: 20 loss: 0.3022226420778018 grad: 0.16292325184609274
iteration: 30 loss: 0.23050920568121827 grad: 0.09751900129673488
iteration: 0 loss: 200.9601760618544 grad: 742.3428944854357
iteration: 10 loss: 0.3947629969652553 grad: 0.22642295293024994
iteration: 20 loss: 0.24457689344920006 grad: 0.07139464146963638
iteration: 0 loss: 340.8483054989056 grad: 751.996176472159
iteration: 10 loss: 0.37986064977650874 grad: 0.6269686138534227
iteration: 20 loss: 0.2194113262379357 grad: 0.24257626581818073
iteration: 0 loss: 331.28250725198944 grad: 819.1837631239958
iteration: 10 loss: 0.3417858114892376 grad: 0.665059581519791
iteration: 20 loss: 0.1946718128906055 grad: 0.23633185377529486
iteration: 0 loss: 332.6003225144773 grad: 780.2217562591225
iteration: 10 loss: 0.375989297283013 grad: 0.8082508402532241
iteration: 20 loss: 0.21788255911542365 grad: 0.2792891015077923
iteration: 0 loss: 244.862247536427 grad: 748.0500379234693
iteration: 10 loss: 0.4041179274283257 grad: 0.46314970889168583
iteration: 20 loss: 0.2457932181043196 grad: 0.15811410837083706
iteration: 0 loss: 349.392351383175 grad: 779.1735048488451
iteration: 10 loss: 0.3599233342534221 grad: 0.5844419279869535
iteration: 20 loss: 0.2095194555462182 grad: 0.2515647748039593
iteration: 0 loss: 326.60391248896354 grad: 783.2674001736876
iteration: 10 loss: 0.38534905234849826 grad: 0.5866720882355092
iteration: 20 loss: 0.22059172035129782 grad: 0.2314874264513449
iteration: 0 loss: 317.02080828690146 grad: 676.9489614022109
iteration: 10 loss: 0.399753454539016 grad: 0.6103918743120753
iteration: 20 loss: 0.23871819707677455 grad: 0.26887715601960527
iteration: 0 loss: 468.4140710790971 grad: 802.6768886281354
iteration: 10 loss: 0.38013822910381184 grad: 0.7051214670713881
iteration: 20 loss: 0.20862635941474847 grad: 0.28830306436245545
iteration: 0 loss: 328.8678435082216 grad: 716.6124949050801
iteration: 10 loss: 0.3868689912052022 grad: 0.7565417517882543
iteration: 20 loss: 0.2241313576281424 grad: 0.29763966229430733
iteration: 0 loss: 128.34459141071883 grad: 609.972731333434
iteration: 10 loss: 0.4964000040486512 grad: 0.3500559037381988
iteration: 20 loss: 0.316444840479066 grad: 0.1569947722469518
iteration: 30 loss: 0.24190563743856977 grad: 0.090785111222133
iteration: 0 loss: 132.05447953070407 grad: 576.5883077232788
iteration: 10 loss: 0.5046710581130107 grad: 0.3437036702322328
iteration: 20 loss: 0.3211042563900099 grad: 0.16790780893656074
iteration: 30 loss: 0.2447478557212228 grad: 0.10408442881287705
iteration: 0 loss: 191.4022199969618 grad: 635.275233672528
iteration: 10 loss: 0.41781161728282645 grad: 0.35717611844827546
iteration: 20 loss: 0.26402656365669164 grad: 0.17210182173824723
iteration: 0 loss: 230.54717351796927 grad: 735.240200373205
iteration: 10 loss: 0.40978227571130527 grad: 0.04095441807968698
iteration: 20 loss: 0.25254306295143825 grad: -0.004270530009148191
iteration: 0 loss: 207.68127452327536 grad: 688.2449714883435
iteration: 10 loss: 0.40620904110379674 grad: 0.5537032744796268
iteration: 20 loss: 0.24991412943423216 grad: 0.2120818882444041
iteration: 0 loss: 163.5582321628832 grad: 665.3429094574146
iteration: 10 loss: 0.4454007590808363 grad: 0.22341144409794714
iteration: 20 loss: 0.28025823169032676 grad: 0.07477592250098648
iteration: 30 loss: 0.2136482154537589 grad: 0.033402080464166706
iteration: 0 loss: 144.2246102090098 grad: 644.1747219706933
iteration: 10 loss: 0.45545246375803056 grad: 0.09757510783070678
iteration: 20 loss: 0.2932229211232466 grad: 0.0335237878957924
iteration: 30 loss: 0.22514391492983454 grad: 0.013609821494680763
iteration: 0 loss: 307.2342532784474 grad: 817.1097967970557
iteration: 10 loss: 0.3425770350914635 grad: 0.3969656691604394
iteration: 20 loss: 0.1999472039142347 grad: 0.16095922225863568
iteration: 0 loss: 271.51244122090486 grad: 737.8507946341208
iteration: 10 loss: 0.3961006390629171 grad: 0.7308870482355453
iteration: 20 loss: 0.2334286980539301 grad: 0.27985116098833396
iteration: 0 loss: 288.4936938755911 grad: 710.8803763342685
iteration: 10 loss: 0.3869530954962928 grad: 0.5563441627377005
iteration: 20 loss: 0.23157335708691046 grad: 0.22434347365660665
iteration: 0 loss: 203.7609940943564 grad: 705.2492928301313
iteration: 10 loss: 0.3975658433364515 grad: 0.2844304900530993
iteration: 20 loss: 0.2480037237007557 grad: 0.10495991800562296
iteration: 0 loss: 196.091267533599 grad: 603.393370433816
iteration: 10 loss: 0.4338433807333786 grad: 0.43992630771306573
iteration: 20 loss: 0.273954321303129 grad: 0.20043312479732966
iteration: 30 loss: 0.20920223989254405 grad: 0.1235078701599094
iteration: 0 loss: 259.3637835004945 grad: 728.4433332448398
iteration: 10 loss: 0.3703349354281355 grad: 0.4138092511435686
iteration: 20 loss: 0.22302939746689596 grad: 0.15268837803096125
iteration: 0 loss: 175.43736255969162 grad: 671.617294783804
iteration: 10 loss: 0.43012824138101924 grad: 0.12720853229248952
iteration: 20 loss: 0.2689785678490311 grad: 0.039505193444495636
iteration: 30 loss: 0.2048586240472022 grad: 0.019701217887190536
iteration: 0 loss: 153.03256718359154 grad: 706.7982119977792
iteration: 10 loss: 0.4565217641074097 grad: 0.35757818854620604
iteration: 20 loss: 0.282157925304587 grad: 0.12682583595338914
iteration: 30 loss: 0.21374273663447887 grad: 0.062314784854838316
iteration: 0 loss: 390.4955754066857 grad: 878.5149993934505
iteration: 10 loss: 0.35775791808175034 grad: 0.5431364194199194
iteration: 20 loss: 0.194220678354296 grad: 0.13966941616205777
iteration: 0 loss: 138.66828401694585 grad: 636.2180742523374
iteration: 10 loss: 0.4638437508980463 grad: 0.34402257026607075
iteration: 20 loss: 0.2956229744709576 grad: 0.13126859594520396
iteration: 30 loss: 0.22587323859037992 grad: 0.06745441914734068
iteration: 0 loss: 122.25128595681657 grad: 611.2428348135395
iteration: 10 loss: 0.49249869401170476 grad: 0.3347162688205304
iteration: 20 loss: 0.31607237328642607 grad: 0.12877957082410563
iteration: 30 loss: 0.2419446957803137 grad: 0.06534821099599453
iteration: 0 loss: 154.62804322036664 grad: 645.5339439282994
iteration: 10 loss: 0.45439033585014177 grad: 0.13175243358602373
iteration: 20 loss: 0.2871632103461624 grad: 0.06350275057627282
iteration: 30 loss: 0.21900540917299208 grad: 0.03751482995549757
iteration: 0 loss: 174.7029608539174 grad: 720.4396679195424
iteration: 10 loss: 0.4237451359591763 grad: 0.2324584999287126
iteration: 20 loss: 0.26239506650067185 grad: 0.08134718891118861
iteration: 0 loss: 264.6073871973377 grad: 739.9307185570974
iteration: 10 loss: 0.35977048262578715 grad: 0.5003595806905372
iteration: 20 loss: 0.21239262503265557 grad: 0.1918066184410243
iteration: 0 loss: 265.03294804184384 grad: 792.6454727411768
iteration: 10 loss: 0.39555862510513345 grad: 0.3041391855888107
iteration: 20 loss: 0.23637952385400818 grad: 0.10501707056644993
iteration: 0 loss: 298.0504121192505 grad: 779.6755137925252
iteration: 10 loss: 0.39153997288667597 grad: 0.7751992431044425
iteration: 20 loss: 0.21520834083772075 grad: 0.2557092032197913
iteration: 0 loss: 230.2781478469609 grad: 747.6866030123663
iteration: 10 loss: 0.4139829509719096 grad: 0.7169281286825069
iteration: 20 loss: 0.24048427644761847 grad: 0.245512367658119
iteration: 0 loss: 210.6595980980542 grad: 709.1773831849598
iteration: 10 loss: 0.39567072233082645 grad: 0.49329809286504733
iteration: 20 loss: 0.24332087905473931 grad: 0.174646659512273
iteration: 0 loss: 366.7843576817577 grad: 803.791202136924
iteration: 10 loss: 0.37718402044199384 grad: 0.5693191380018576
iteration: 20 loss: 0.20991440000166636 grad: 0.17681098145832394
iteration: 0 loss: 287.46356549417976 grad: 747.2868985986736
iteration: 10 loss: 0.39587691893013305 grad: 0.5846585338063295
iteration: 20 loss: 0.23286044508809442 grad: 0.23951210543664297
iteration: 0 loss: 130.0774917428939 grad: 700.3047766299296
iteration: 10 loss: 0.4516954360161352 grad: 0.11065651043803328
iteration: 20 loss: 0.287016115532176 grad: 0.010871268880118742
iteration: 30 loss: 0.21970890082064382 grad: -0.014715540456970658
iteration: 0 loss: 215.93120067075307 grad: 702.0539514550912
iteration: 10 loss: 0.38960425765237067 grad: 0.2655790223599336
iteration: 20 loss: 0.2426345691019378 grad: 0.10302608549205855
iteration: 0 loss: 181.4724647292919 grad: 690.4999289539584
iteration: 10 loss: 0.43080621698515065 grad: 0.5261877621929911
iteration: 20 loss: 0.2648854765075157 grad: 0.19601312345216
iteration: 0 loss: 286.06973759802355 grad: 805.1097517000819
iteration: 10 loss: 0.3848984782360441 grad: 0.5925523048719052
iteration: 20 loss: 0.21556686769541888 grad: 0.18647294774063644
iteration: 0 loss: 155.7466336650994 grad: 705.0874680094544
iteration: 10 loss: 0.43831501532407524 grad: 0.2892513210006945
iteration: 20 loss: 0.27756542930935457 grad: 0.08766792263464004
iteration: 30 loss: 0.21263173259004278 grad: 0.03603879681034663
iteration: 0 loss: 221.7973583597244 grad: 670.8764044532248
iteration: 10 loss: 0.42883395874932 grad: 0.5612412335929027
iteration: 20 loss: 0.2668147585976509 grad: 0.2122311375199248
iteration: 30 loss: 0.20231301492805703 grad: 0.1191019479422373
iteration: 0 loss: 113.08320102413994 grad: 597.3425659230214
iteration: 10 loss: 0.5284555566052156 grad: 0.6013036707677256
iteration: 20 loss: 0.3340851556691136 grad: 0.21609845302797093
iteration: 30 loss: 0.2544075027571208 grad: 0.10440105569991648
iteration: 0 loss: 274.7144605097772 grad: 770.3607570576139
iteration: 10 loss: 0.3605325686330616 grad: 0.7661342048646047
iteration: 20 loss: 0.2167654094579451 grad: 0.26306215449606923
iteration: 0 loss: 231.46474932768226 grad: 746.6180690947558
iteration: 10 loss: 0.4098674314973323 grad: 0.33905536810196946
iteration: 20 loss: 0.24708655115533806 grad: 0.11999511895632196
iteration: 0 loss: 230.97488268551444 grad: 735.5575752475447
iteration: 10 loss: 0.40780620262318157 grad: 0.4936574397238621
iteration: 20 loss: 0.24611402443422942 grad: 0.1872742858678228
iteration: 0 loss: 404.31421666580974 grad: 765.3710529180635
iteration: 10 loss: 0.3834140383139377 grad: 0.5634975998246992
iteration: 20 loss: 0.22214272112581634 grad: 0.1989131983416655
iteration: 0 loss: 171.91090004229335 grad: 692.2152412578425
iteration: 10 loss: 0.42228599379797577 grad: 0.3323487031775014
iteration: 20 loss: 0.2626007334449076 grad: 0.12188723673170947
iteration: 0 loss: 223.19758507481674 grad: 721.3825558492821
iteration: 10 loss: 0.36746187956776505 grad: 0.5782229616684402
iteration: 20 loss: 0.22613832440046364 grad: 0.21818737850515862
iteration: 0 loss: 408.4434248070915 grad: 807.8292609381734
iteration: 10 loss: 0.3688390466456688 grad: 0.7225171320967027
iteration: 20 loss: 0.20872674979132586 grad: 0.2738863491517012
iteration: 0 loss: 217.34402020077604 grad: 684.2034205522198
iteration: 10 loss: 0.4168847793868984 grad: 0.5807949299772291
iteration: 20 loss: 0.25197734890794393 grad: 0.23070025566748859
iteration: 0 loss: 297.2931001632921 grad: 733.998916573385
iteration: 10 loss: 0.3859049786934437 grad: 0.6991768179591962
iteration: 20 loss: 0.2284234476751821 grad: 0.281765742414736
iteration: 0 loss: 185.38244980155866 grad: 700.1458843411035
iteration: 10 loss: 0.4368532330069932 grad: 0.4134353736360221
iteration: 20 loss: 0.2689499875886296 grad: 0.12152603526037146
iteration: 30 loss: 0.20386487982101575 grad: 0.05370944577465416
iteration: 0 loss: 327.534041171161 grad: 809.9395131980584
iteration: 10 loss: 0.35740750013428624 grad: 0.4233968278956547
iteration: 20 loss: 0.2058441202045122 grad: 0.16135893164085863
iteration: 0 loss: 302.685340992645 grad: 767.7460581746468
iteration: 10 loss: 0.3712789463844191 grad: 0.41242392800257144
iteration: 20 loss: 0.22091192370796556 grad: 0.16505667952104486
iteration: 0 loss: 203.33731543264258 grad: 631.8048963225639
iteration: 10 loss: 0.43924120127361344 grad: 0.39080012796504
iteration: 20 loss: 0.2746816814553974 grad: 0.15562686927450387
iteration: 30 loss: 0.20845031358393581 grad: 0.08707060548968704
iteration: 0 loss: 142.58824144374756 grad: 610.2517461940483
iteration: 10 loss: 0.495303024195974 grad: 0.28365258994330034
iteration: 20 loss: 0.3154378366681902 grad: 0.12504027202580073
iteration: 30 loss: 0.24087375507381445 grad: 0.07575089436841133
iteration: 0 loss: 188.6168281923985 grad: 759.6056966797653
iteration: 10 loss: 0.3790629522298497 grad: 0.3205154853830897
iteration: 20 loss: 0.23425069173150737 grad: 0.09080019406640866
iteration: 0 loss: 96.93492515034747 grad: 584.4480734084646
iteration: 10 loss: 0.5645224903433204 grad: 0.15489369290665272
iteration: 20 loss: 0.36061344475481083 grad: 0.05264553197242701
iteration: 30 loss: 0.27421270764452027 grad: 0.02209324669739595
iteration: 0 loss: 200.0986191714085 grad: 591.9101939210123
iteration: 10 loss: 0.379186966746519 grad: 0.6281521874472971
iteration: 20 loss: 0.23205058336754353 grad: 0.25925212267815756
iteration: 0 loss: 151.52672061016528 grad: 606.7676088858227
iteration: 10 loss: 0.4665001672058431 grad: 0.5096533596630399
iteration: 20 loss: 0.29423402240172436 grad: 0.19183569463531483
iteration: 30 loss: 0.22415129828484417 grad: 0.1031630461917746
iteration: 0 loss: 339.0389468673922 grad: 782.5623363272608
iteration: 10 loss: 0.37835506808683433 grad: 0.6489270228641743
iteration: 20 loss: 0.21994898864415352 grad: 0.2303784565538266
iteration: 0 loss: 218.60417963010158 grad: 664.8107905463061
iteration: 10 loss: 0.4109684499270853 grad: 0.6541329113739558
iteration: 20 loss: 0.25261274771508563 grad: 0.27294413345340446
iteration: 0 loss: 297.88776363927525 grad: 777.9617249929859
iteration: 10 loss: 0.3787614755231268 grad: 0.55845268977966
iteration: 20 loss: 0.2206104525563536 grad: 0.17066241951748606
iteration: 0 loss: 337.2488109641515 grad: 779.0450657423099
iteration: 10 loss: 0.3767482267128137 grad: 0.5790018049748812
iteration: 20 loss: 0.2219560474537006 grad: 0.22627188815163385
iteration: 0 loss: 357.0572411409732 grad: 838.6171446836277
iteration: 10 loss: 0.3587021262494752 grad: 0.48782661910963254
iteration: 20 loss: 0.20137111373068448 grad: 0.18841895993767427
iteration: 0 loss: 156.44927023967128 grad: 657.7191410923458
iteration: 10 loss: 0.44780061882168165 grad: 0.19595280642961782
iteration: 20 loss: 0.28129316217192274 grad: 0.06455027752012443
iteration: 30 loss: 0.21419319900393352 grad: 0.031628188822425594
iteration: 0 loss: 171.10987835027407 grad: 666.6551624130061
iteration: 10 loss: 0.4376108075794863 grad: 0.3473363127386667
iteration: 20 loss: 0.27623704157105583 grad: 0.12534492201876613
iteration: 30 loss: 0.21079298548118625 grad: 0.06589492641015005
iteration: 0 loss: 205.82675047513237 grad: 753.0095160413063
iteration: 10 loss: 0.3938289292808887 grad: 0.36687797852120424
iteration: 20 loss: 0.23867675291835025 grad: 0.10067075315733741
iteration: 0 loss: 215.88718578839027 grad: 726.8973711773472
iteration: 10 loss: 0.38983527323762246 grad: 0.3909869115750518
iteration: 20 loss: 0.23722248204219135 grad: 0.12275849705830601
iteration: 0 loss: 434.8748687319217 grad: 847.0690819082241
iteration: 10 loss: 0.35478842350758816 grad: 0.6689541440125708
iteration: 20 loss: 0.1897989835570942 grad: 0.2666431420414305
iteration: 0 loss: 164.88904274582953 grad: 711.2149548449083
iteration: 10 loss: 0.45190612261103524 grad: 0.4053439293540611
iteration: 20 loss: 0.2826429037170535 grad: 0.1344348029171475
iteration: 30 loss: 0.2147268752675948 grad: 0.06533885118032479
iteration: 0 loss: 320.1903516208942 grad: 810.6160266144054
iteration: 10 loss: 0.3728530934655613 grad: 0.8580745624753141
iteration: 20 loss: 0.2057681368527979 grad: 0.3155607883499323
iteration: 0 loss: 221.04416128647455 grad: 705.2171688540003
iteration: 10 loss: 0.42204491710287007 grad: 0.3041342081928661
iteration: 20 loss: 0.26047006377592136 grad: 0.11062203695906066
iteration: 0 loss: 115.63185312297 grad: 665.3497088295837
iteration: 10 loss: 0.4883473287133132 grad: -0.0919586584896615
iteration: 20 loss: 0.3160536988230392 grad: -0.062077251764717026
iteration: 30 loss: 0.2430101167326277 grad: -0.05244882671095065
iteration: 0 loss: 339.3039655688339 grad: 794.0377093389258
iteration: 10 loss: 0.3742152336461169 grad: 0.6334599173164976
iteration: 20 loss: 0.2178340828278868 grad: 0.23163813179707116
iteration: 0 loss: 162.5991684923761 grad: 660.5289517515089
iteration: 10 loss: 0.45892200674081157 grad: 0.6259964719473743
iteration: 20 loss: 0.2838082850829861 grad: 0.22766795675641177
iteration: 30 loss: 0.21524555011579982 grad: 0.12309817868125328
iteration: 0 loss: 377.09049203008726 grad: 807.33344778858
iteration: 10 loss: 0.3696818545658144 grad: 0.7296747259927698
iteration: 20 loss: 0.2060735608657441 grad: 0.27170988237393867
iteration: 0 loss: 145.03870296784729 grad: 626.4909913056692
iteration: 10 loss: 0.4667667051219475 grad: 0.49700111597711966
iteration: 20 loss: 0.29178805054818274 grad: 0.19330110725443722
iteration: 30 loss: 0.22202277183722882 grad: 0.10378564219593592
iteration: 0 loss: 312.4852114491884 grad: 769.1596727504431
iteration: 10 loss: 0.35847457364401303 grad: 0.5397648384846503
iteration: 20 loss: 0.21277619956477298 grad: 0.22166370916329645
iteration: 0 loss: 153.80317546907347 grad: 621.6327014196772
iteration: 10 loss: 0.46283881023257645 grad: 0.45116967126872687
iteration: 20 loss: 0.2936600886513671 grad: 0.1962365849616617
iteration: 30 loss: 0.22450582340372255 grad: 0.1150773970069973
iteration: 0 loss: 224.40902161146053 grad: 762.746251953839
iteration: 10 loss: 0.4065747178966293 grad: 0.2789979626553372
iteration: 20 loss: 0.24904778554371826 grad: 0.05618949293132745
iteration: 0 loss: 159.41636896078796 grad: 675.9183004634415
iteration: 10 loss: 0.4391722112901665 grad: 0.5247921812896137
iteration: 20 loss: 0.27487651611045294 grad: 0.1786066547731702
iteration: 30 loss: 0.20940328910430914 grad: 0.08761875962425889
iteration: 0 loss: 183.19600890642388 grad: 708.5598033553916
iteration: 10 loss: 0.4200596236223646 grad: 0.3462917429455789
iteration: 20 loss: 0.2564654348952073 grad: 0.11098686765021967
iteration: 0 loss: 251.67092547263795 grad: 705.605396197171
iteration: 10 loss: 0.39833115169974254 grad: 0.5995151127399921
iteration: 20 loss: 0.2436805157830432 grad: 0.23765247591260505
iteration: 0 loss: 119.5428709831573 grad: 597.8871529865316
iteration: 10 loss: 0.4913155640541928 grad: 0.3347292949019631
iteration: 20 loss: 0.3192190318790877 grad: 0.15686499187811545
iteration: 30 loss: 0.2455678701216212 grad: 0.09404526767102353
iteration: 0 loss: 299.2806217120291 grad: 730.8728256658179
iteration: 10 loss: 0.3814062830283133 grad: 0.49650614444390073
iteration: 20 loss: 0.22735279052143265 grad: 0.18733301046055625
iteration: 0 loss: 271.2120403293847 grad: 741.3014690840778
iteration: 10 loss: 0.3932815270368751 grad: 0.5578150613819565
iteration: 20 loss: 0.23226612502821808 grad: 0.2005344890625983
iteration: 0 loss: 8009.435403359315 grad: 2624.29487133818
iteration: 10 loss: 0.0009549984745469621 grad: 0.007866682288899467
iteration: 0 loss: 6433.9984543268165 grad: 2463.576575611844
iteration: 10 loss: 0.0011378881489773366 grad: 0.008756069429355163
iteration: 0 loss: 7138.643106953529 grad: 2492.695278437497
iteration: 10 loss: 0.000843392132082954 grad: 0.017010109338285576
iteration: 0 loss: 8967.463255632487 grad: 2729.08052450883
iteration: 10 loss: 0.001043243950169364 grad: 0.01849196579659027
iteration: 0 loss: 5528.258429047231 grad: 2377.6028826445104
iteration: 0 loss: 3683.4268777892476 grad: 2115.309025986615
iteration: 10 loss: 0.001030074675906111 grad: -0.10948569008661983
iteration: 0 loss: 5935.62709257732 grad: 2451.3989940688516
iteration: 0 loss: 3166.3094545233525 grad: 2006.5789207892649
iteration: 0 loss: 8810.138443198195 grad: 2569.7123914004706
iteration: 10 loss: 0.0011087280330354008 grad: 0.011212760601921149
iteration: 0 loss: 5802.005660752875 grad: 2389.6063341406816
iteration: 0 loss: 3607.820061820122 grad: 2185.821928765998
iteration: 0 loss: 7330.691769777916 grad: 2568.934366996541
iteration: 10 loss: 0.0009261738120006736 grad: 0.0008615546551692319
iteration: 0 loss: 5016.723418908047 grad: 2175.990909823406
iteration: 0 loss: 5426.663574568419 grad: 2424.8318177106357
iteration: 0 loss: 7120.519065536471 grad: 2465.3736427845715
iteration: 0 loss: 10302.248567662127 grad: 2793.472280808158
iteration: 10 loss: 0.002411657130737281 grad: 1.1398555954670637
iteration: 0 loss: 6063.040432783677 grad: 2394.7360745073815
iteration: 0 loss: 4288.379287659141 grad: 2237.0377322711543
iteration: 0 loss: 10413.694430928788 grad: 2799.7719941437213
iteration: 10 loss: 0.019225242551659572 grad: 0.9396610762078254
iteration: 0 loss: 5077.935573078557 grad: 2264.734981370966
iteration: 0 loss: 7358.110762101162 grad: 2568.246475332537
iteration: 0 loss: 8881.509550738532 grad: 2666.0983288588595
iteration: 10 loss: 0.0017571687521095473 grad: -0.9628150243454663
iteration: 0 loss: 4553.258048169496 grad: 2151.526059503497
iteration: 0 loss: 5539.501223881202 grad: 2369.920992882947
iteration: 0 loss: 5715.623928327023 grad: 2485.6442455746837
iteration: 0 loss: 4898.949542739947 grad: 2398.097831905253
iteration: 10 loss: 0.0010892755923454058 grad: 0.009829799780531749
iteration: 0 loss: 11141.593570831365 grad: 2811.4524177621265
iteration: 10 loss: 0.051805851462466475 grad: 0.31383247169920914
iteration: 0 loss: 10875.309109781838 grad: 2721.5231149580823
iteration: 10 loss: 1.2340515602369455 grad: 1.8218148095265496
iteration: 0 loss: 6288.241321082809 grad: 2449.8134389162487
iteration: 0 loss: 4689.86580998712 grad: 2255.8982566377026
iteration: 0 loss: 12785.770965771864 grad: 2970.191815268163
iteration: 10 loss: 0.1943312153109053 grad: 1.3858173360982586
iteration: 0 loss: 6267.933002323427 grad: 2588.249015225839
iteration: 0 loss: 6057.602785310192 grad: 2444.502808128605
iteration: 0 loss: 8448.080764211656 grad: 2534.2802562768097
iteration: 10 loss: 0.0009123058129758151 grad: -0.4379551832075407
iteration: 0 loss: 8257.430423979085 grad: 2619.0794013017576
iteration: 10 loss: 0.0008943996786415069 grad: -0.07767692521476392
iteration: 0 loss: 6905.555728857763 grad: 2541.7348738630626
iteration: 10 loss: 0.007499223393758505 grad: 1.0960603039856593
iteration: 0 loss: 6524.032199533171 grad: 2438.7034285587783
iteration: 0 loss: 10399.312489322672 grad: 2816.2742165681375
iteration: 10 loss: 0.0013527805647093126 grad: 3.6452780959205455
iteration: 0 loss: 4914.204895386888 grad: 2325.690985192656
iteration: 10 loss: 0.0008613053770122034 grad: 0.009760136727430837
iteration: 0 loss: 5424.594662588734 grad: 2362.917114803345
iteration: 0 loss: 6874.899657669152 grad: 2481.932086388556
iteration: 10 loss: 0.0008130206393120302 grad: -0.3175819620315938
iteration: 0 loss: 3489.391653166435 grad: 2158.3530204708013
iteration: 0 loss: 14505.874914065422 grad: 3065.07974066885
iteration: 10 loss: 1.6923030974100006 grad: 3.724802452762895
iteration: 0 loss: 5692.9939545872385 grad: 2281.905631457169
iteration: 10 loss: 0.0007817685506200756 grad: -0.0014427483054001
iteration: 0 loss: 7848.2353311160905 grad: 2738.095484300283
iteration: 10 loss: 0.0008512581262039021 grad: 0.005668017609026651
iteration: 0 loss: 8112.322290387462 grad: 2534.2192103933735
iteration: 10 loss: 0.001063292336766608 grad: 0.01841847166013679
iteration: 0 loss: 6174.496781244114 grad: 2228.523390400347
iteration: 0 loss: 5076.835954291826 grad: 2406.469215829947
iteration: 0 loss: 8893.505965594279 grad: 2659.5971415637077
iteration: 10 loss: 0.0008666353929914873 grad: -0.09282187068636878
iteration: 0 loss: 9240.225239504975 grad: 2671.163080711667
iteration: 10 loss: 0.16536929867288563 grad: 1.2037001152215663
iteration: 0 loss: 10762.928246902527 grad: 2758.9771956866084
iteration: 10 loss: 0.5839241533328525 grad: 3.532270377458007
iteration: 0 loss: 5786.862158777607 grad: 2480.6321244049177
iteration: 10 loss: 0.001149066379928792 grad: 0.010205860547186509
iteration: 0 loss: 9763.536548100097 grad: 2825.7852029607266
iteration: 10 loss: 0.0010234945088582622 grad: 0.6987558615206773
iteration: 0 loss: 7163.809735743686 grad: 2533.515350954146
iteration: 0 loss: 4825.111848681284 grad: 2288.620171874382
iteration: 0 loss: 6445.595507064723 grad: 2465.3570151300673
iteration: 0 loss: 4836.626562869894 grad: 2391.4547626405156
iteration: 0 loss: 7055.564788223273 grad: 2526.9484911402424
iteration: 0 loss: 10480.548768877603 grad: 2870.3309736449073
iteration: 10 loss: 0.001185099284736101 grad: -0.012375593218866783
iteration: 0 loss: 5683.120972414164 grad: 2431.7015853296216
iteration: 0 loss: 9211.013942299545 grad: 2685.313736557433
iteration: 10 loss: 0.022894329117579302 grad: 0.7282458525561795
iteration: 0 loss: 9184.435764101238 grad: 2832.8678249990076
iteration: 10 loss: 0.0009880602738121524 grad: 0.008911148308509241
iteration: 0 loss: 4775.277404243437 grad: 2103.8535047143523
iteration: 0 loss: 5036.734201928091 grad: 2308.1804210855503
iteration: 0 loss: 5987.576376399081 grad: 2503.9494345653843
iteration: 0 loss: 6211.336796536666 grad: 2448.467926461243
iteration: 0 loss: 4665.512013847735 grad: 2205.939345949363
iteration: 0 loss: 8418.812039068054 grad: 2695.0380488164674
iteration: 10 loss: 0.0011450139189202507 grad: 0.022525972815687405
iteration: 0 loss: 6843.687811967523 grad: 2469.054312038497
iteration: 10 loss: 0.0011046429466329175 grad: 0.004899497102932745
iteration: 0 loss: 6863.069154309601 grad: 2627.230172450878
iteration: 0 loss: 8242.925758874318 grad: 2676.8900490979167
iteration: 10 loss: 0.0012400335055479611 grad: 0.010817622754658616
iteration: 0 loss: 12446.397496400681 grad: 2982.5258612852635
iteration: 10 loss: 0.12719996940789066 grad: 1.1647287797126702
iteration: 0 loss: 5184.662246228094 grad: 2301.9289503641203
iteration: 0 loss: 8702.991591652411 grad: 2663.144186742351
iteration: 10 loss: 0.0008962565625552088 grad: -0.011138995531442378
iteration: 0 loss: 5475.605592486524 grad: 2393.050708123212
iteration: 10 loss: 0.000974151524133049 grad: 0.014592081389597538
iteration: 0 loss: 4323.932429213572 grad: 2221.1875067378014
iteration: 0 loss: 5533.898550079493 grad: 2244.5985690081297
iteration: 0 loss: 4495.056553394524 grad: 2225.827839654916
iteration: 0 loss: 6579.503859363843 grad: 2589.817123540265
iteration: 0 loss: 9140.157390357192 grad: 2626.885805937987
iteration: 10 loss: 0.001312509845857593 grad: 0.025173354987450804
iteration: 0 loss: 10425.12570642525 grad: 2849.2312561346143
iteration: 10 loss: 0.000942727689768865 grad: 0.8654662604844846
iteration: 0 loss: 9645.519703445148 grad: 2717.1145259321665
iteration: 10 loss: 0.03915655389242933 grad: 1.9991872727589144
iteration: 0 loss: 7808.658360746261 grad: 2612.876223021218
iteration: 0 loss: 9856.946765055314 grad: 2718.097064888549
iteration: 10 loss: 0.021911281789388424 grad: 1.2513620136767298
iteration: 0 loss: 9778.71359153474 grad: 2725.621220160251
iteration: 10 loss: 0.12938526095108085 grad: 1.3237962801970784
iteration: 0 loss: 7432.751495167924 grad: 2369.8880012598165
iteration: 0 loss: 11663.693552336992 grad: 2795.962107728989
iteration: 10 loss: 0.22291024127678777 grad: 1.563628332247903
iteration: 0 loss: 7982.146552277303 grad: 2505.9354504483194
iteration: 10 loss: 0.0008045006609542972 grad: 0.014510814513824601
iteration: 0 loss: 4285.38020712587 grad: 2146.1034955768146
iteration: 0 loss: 3767.6691992196093 grad: 2039.7997348283484
iteration: 0 loss: 5147.638381465028 grad: 2232.092938290042
iteration: 0 loss: 6878.928039047922 grad: 2566.1181722116453
iteration: 0 loss: 6579.885810616481 grad: 2409.063970462844
iteration: 0 loss: 5398.56497090967 grad: 2333.191531027512
iteration: 0 loss: 4665.5133176495265 grad: 2262.2001282783062
iteration: 0 loss: 9476.401670544694 grad: 2837.456531599524
iteration: 10 loss: 0.02115601155674085 grad: -0.6142879485012712
iteration: 0 loss: 7641.4769934056385 grad: 2573.918542714482
iteration: 10 loss: 0.0012650867230893875 grad: -0.699565137909632
iteration: 0 loss: 7027.758142082206 grad: 2483.844813522879
iteration: 10 loss: 0.001040716324827041 grad: -0.18867077174028238
iteration: 0 loss: 6338.721106481736 grad: 2460.69860500078
iteration: 0 loss: 4644.8432166123785 grad: 2126.672567301145
iteration: 0 loss: 7461.593825694019 grad: 2545.2358932252455
iteration: 0 loss: 5596.242679195051 grad: 2354.567562355047
iteration: 0 loss: 5746.0244803978485 grad: 2474.7575246054826
iteration: 10 loss: 0.0010004173170521178 grad: 0.06141638094550258
iteration: 0 loss: 12693.80883384125 grad: 3047.9891723193905
iteration: 10 loss: 0.11386167429207655 grad: -0.5351198915861807
iteration: 0 loss: 4256.354500387985 grad: 2231.7504975888974
iteration: 0 loss: 4193.432127771617 grad: 2150.067448960279
iteration: 0 loss: 4969.906759299395 grad: 2265.9603633216757
iteration: 0 loss: 6290.741950449679 grad: 2522.322828775112
iteration: 0 loss: 7612.717893474997 grad: 2586.874938017586
iteration: 10 loss: 0.0008543852282772688 grad: 0.014538855912793136
iteration: 0 loss: 8685.261050646635 grad: 2763.481514610533
iteration: 10 loss: 0.02290478585266762 grad: -1.1800797061408053
iteration: 0 loss: 10718.303559891096 grad: 2711.295079459601
iteration: 10 loss: 0.11145825833293864 grad: 4.357648630243597
iteration: 0 loss: 8432.190827557743 grad: 2609.210409818613
iteration: 10 loss: 0.01699120480043348 grad: 1.1986835181391626
iteration: 0 loss: 6526.271378933645 grad: 2479.6550013956403
iteration: 0 loss: 10830.825896019736 grad: 2795.036287602642
iteration: 10 loss: 0.10853235098528041 grad: 1.6622869545849956
iteration: 0 loss: 8192.096303683304 grad: 2609.111122684638
iteration: 0 loss: 5335.208683379619 grad: 2451.254853305587
iteration: 10 loss: 0.000961889078925279 grad: -0.2362320321979492
iteration: 0 loss: 6134.575687325591 grad: 2456.606512398146
iteration: 10 loss: 0.0008778851082421501 grad: -0.0011875305961361717
iteration: 0 loss: 5615.96888050625 grad: 2412.7781176113986
iteration: 10 loss: 0.0009247596648161892 grad: 0.02383647846495495
iteration: 0 loss: 9824.408435217163 grad: 2800.319518973332
iteration: 10 loss: 0.14162742118060123 grad: 3.1481380099530774
iteration: 0 loss: 5163.179876696336 grad: 2467.986358512391
iteration: 0 loss: 6240.487031452232 grad: 2352.6708087830625
iteration: 0 loss: 3527.1778484688302 grad: 2100.3718709564373
iteration: 0 loss: 7379.849338079975 grad: 2681.9397446092776
iteration: 0 loss: 7157.368852600881 grad: 2610.528550613418
iteration: 10 loss: 0.001011700905192728 grad: -0.6334378722101845
iteration: 0 loss: 7108.358390038658 grad: 2576.200034574555
iteration: 10 loss: 0.0010386172242843631 grad: 0.03419031782391808
iteration: 0 loss: 10080.856837834137 grad: 2674.672919015443
iteration: 10 loss: 0.11179313307961407 grad: 2.2145488979297454
iteration: 0 loss: 5779.999527611746 grad: 2420.224942007139
iteration: 0 loss: 6670.009335741397 grad: 2525.6633690952453
iteration: 0 loss: 10211.238812992697 grad: 2807.168918900964
iteration: 10 loss: 0.0010104329280280085 grad: -0.2500486346082305
iteration: 0 loss: 6493.393704378872 grad: 2396.7191667299276
iteration: 10 loss: 0.0011186707759572362 grad: -0.01108232580220286
iteration: 0 loss: 8781.567142379341 grad: 2566.2976001052994
iteration: 10 loss: 0.0006736544521779499 grad: 0.8836614175029935
iteration: 0 loss: 5651.571079283551 grad: 2449.717799575038
iteration: 0 loss: 9610.469978744179 grad: 2823.188164174311
iteration: 0 loss: 8211.678807593382 grad: 2672.6916101165534
iteration: 10 loss: 0.000936271137272177 grad: 0.015373564915774768
iteration: 0 loss: 5292.335096681845 grad: 2219.4659108500446
iteration: 0 loss: 4164.129275216838 grad: 2152.805384782863
iteration: 0 loss: 6815.781519918309 grad: 2651.096672663585
iteration: 0 loss: 3483.344620754292 grad: 2065.689683970503
iteration: 0 loss: 4675.673820460003 grad: 2088.2324899420546
iteration: 0 loss: 4092.190401167534 grad: 2138.1697178818345
iteration: 0 loss: 9871.161763668048 grad: 2724.2454005133186
iteration: 10 loss: 0.0011765652135099199 grad: 0.03223330252177199
iteration: 0 loss: 5989.457137781423 grad: 2333.8995684119227
iteration: 10 loss: 0.0008980670867127959 grad: 2.146422201322693e-05
iteration: 0 loss: 10402.146359233746 grad: 2713.285628559256
iteration: 10 loss: 0.5759681043564342 grad: 1.3903162652207033
iteration: 0 loss: 9667.82542198522 grad: 2714.8499882774076
iteration: 10 loss: 0.0008693537344118919 grad: -0.05858994378951292
iteration: 0 loss: 11639.396331902997 grad: 2912.4134631592024
iteration: 10 loss: 0.19425377765649252 grad: 1.9735516222899878
iteration: 0 loss: 5268.352295653719 grad: 2307.6216831735037
iteration: 0 loss: 5360.589949469155 grad: 2337.20215711972
iteration: 0 loss: 6949.448086482343 grad: 2625.1451069133846
iteration: 0 loss: 7082.900971648508 grad: 2542.1430827657996
iteration: 0 loss: 13268.174286275062 grad: 2943.3991820962237
iteration: 10 loss: 0.572691560595889 grad: 5.3995503973522805
iteration: 0 loss: 5936.875671932423 grad: 2489.6417359153616
iteration: 0 loss: 10288.62577854195 grad: 2817.2472050338356
iteration: 10 loss: 0.1094325646911536 grad: 2.0188354286801755
iteration: 0 loss: 6681.539651544844 grad: 2471.684283952813
iteration: 0 loss: 4547.8914240273125 grad: 2328.6558102589056
iteration: 0 loss: 9585.35088345499 grad: 2763.601878924208
iteration: 10 loss: 0.0009633258411089297 grad: 0.22065837100354127
iteration: 0 loss: 4830.231209238842 grad: 2318.484599256477
iteration: 0 loss: 11626.258606273941 grad: 2809.4759198117154
iteration: 10 loss: 0.8831656718526077 grad: 1.5571279521156542
iteration: 0 loss: 4319.492990751919 grad: 2204.3236409098286
iteration: 0 loss: 9064.530608566887 grad: 2675.6953998974836
iteration: 10 loss: 0.001045561750123108 grad: 0.582570393500563
iteration: 0 loss: 4654.560394900347 grad: 2184.9542090805007
iteration: 0 loss: 7048.953705478882 grad: 2662.1831355834815
iteration: 0 loss: 5133.501122199767 grad: 2374.3035320568933
iteration: 0 loss: 6933.311593889214 grad: 2472.9315007780333
iteration: 10 loss: 0.0008887158341663467 grad: -0.005636941102889188
iteration: 0 loss: 6600.917104835087 grad: 2469.2706164112024
iteration: 0 loss: 3803.8278599425685 grad: 2105.9345589719933
iteration: 0 loss: 8389.641195509337 grad: 2556.1374456238163
iteration: 10 loss: 0.00085504659562668 grad: -0.006058079027766926
iteration: 0 loss: 8180.353931242394 grad: 2588.142682388275
iteration: 10 loss: 0.0010664257014551285 grad: 0.01565370137311244
iteration: 0 loss: inf grad: 3894.757309027797
iteration: 0 loss: inf grad: 3661.836518069115
iteration: 0 loss: inf grad: 3701.2756752508285
iteration: 0 loss: inf grad: 4055.8210716406943
iteration: 0 loss: inf grad: 3531.265303721228
iteration: 0 loss: inf grad: 3142.2225940502076
iteration: 10 loss: 0.00018406605390323833 grad: 0.006101253666445719
iteration: 0 loss: inf grad: 3641.090358820433
iteration: 0 loss: inf grad: 2980.681982133142
iteration: 0 loss: inf grad: 3817.577297685636
iteration: 0 loss: inf grad: 3547.9312868454867
iteration: 0 loss: inf grad: 3249.6169068511513
iteration: 0 loss: inf grad: 3813.909258457994
iteration: 0 loss: inf grad: 3230.4440981971693
iteration: 0 loss: inf grad: 3605.509997942977
iteration: 0 loss: inf grad: 3662.1112184608096
iteration: 0 loss: inf grad: 4146.873061152333
iteration: 0 loss: inf grad: 3559.81349220867
iteration: 0 loss: inf grad: 3320.3221621382795
iteration: 0 loss: inf grad: 4156.100357160847
iteration: 0 loss: inf grad: 3364.0412633828064
iteration: 0 loss: inf grad: 3810.6226302247633
iteration: 0 loss: inf grad: 3957.8188669913898
iteration: 0 loss: inf grad: 3194.1223787996805
iteration: 0 loss: inf grad: 3519.713962167357
iteration: 0 loss: inf grad: 3688.7967876275757
iteration: 0 loss: inf grad: 3560.482877512058
iteration: 0 loss: inf grad: 4176.865140488949
iteration: 0 loss: inf grad: 4045.6183682560895
iteration: 0 loss: inf grad: 3637.8621162359277
iteration: 0 loss: inf grad: 3350.535722848208
iteration: 0 loss: inf grad: 4412.604740414166
iteration: 0 loss: inf grad: 3845.6446595979637
iteration: 0 loss: inf grad: 3628.454667626218
iteration: 0 loss: inf grad: 3765.029349986073
iteration: 0 loss: inf grad: 3887.3722414633976
iteration: 0 loss: inf grad: 3779.492289312484
iteration: 0 loss: inf grad: 3623.2039252902578
iteration: 0 loss: inf grad: 4181.448513801997
iteration: 0 loss: inf grad: 3451.35249173714
iteration: 0 loss: inf grad: 3515.517423747475
iteration: 0 loss: inf grad: 3683.601107376535
iteration: 0 loss: inf grad: 3199.7632372666885
iteration: 0 loss: inf grad: 4550.323929072592
iteration: 0 loss: inf grad: 3384.588206506055
iteration: 0 loss: inf grad: 4069.030067506944
iteration: 0 loss: inf grad: 3766.6696227484804
iteration: 0 loss: inf grad: 3311.0461393057994
iteration: 0 loss: inf grad: 3574.592005979892
iteration: 0 loss: inf grad: 3943.738053504684
iteration: 0 loss: inf grad: 3966.0222460120967
iteration: 0 loss: inf grad: 4094.6760521346396
iteration: 0 loss: inf grad: 3682.1738350189903
iteration: 0 loss: inf grad: 4198.165031021699
iteration: 0 loss: inf grad: 3757.354439175454
iteration: 0 loss: inf grad: 3398.6488074838044
iteration: 0 loss: inf grad: 3661.64933633359
iteration: 0 loss: 12048.457935835066 grad: 3550.2654056757347
iteration: 0 loss: inf grad: 3754.9111592474082
iteration: 0 loss: inf grad: 4259.554586690081
iteration: 0 loss: inf grad: 3612.2511860829136
iteration: 0 loss: inf grad: 3989.052237276549
iteration: 0 loss: inf grad: 4203.278870911145
iteration: 0 loss: inf grad: 3130.631783314191
iteration: 0 loss: inf grad: 3434.2172873542736
iteration: 0 loss: inf grad: 3716.776172057011
iteration: 0 loss: inf grad: 3637.0739413734846
iteration: 0 loss: inf grad: 3273.155481736563
iteration: 0 loss: inf grad: 4003.6596442016835
iteration: 0 loss: inf grad: 3667.780565510903
iteration: 0 loss: inf grad: 3900.400918516004
iteration: 0 loss: inf grad: 3973.1277935252083
iteration: 0 loss: inf grad: 4428.215568316385
iteration: 0 loss: inf grad: 3418.4036654217566
iteration: 0 loss: inf grad: 3953.1559102577567
iteration: 0 loss: inf grad: 3556.8465688875385
iteration: 0 loss: inf grad: 3301.3656329641262
iteration: 0 loss: inf grad: 3337.2512149863946
iteration: 0 loss: inf grad: 3309.5796717971957
iteration: 0 loss: inf grad: 3850.677044133528
iteration: 0 loss: inf grad: 3900.1222069132
iteration: 0 loss: inf grad: 4229.797360390521
iteration: 0 loss: inf grad: 4035.598259844618
iteration: 0 loss: inf grad: 3882.75894023477
iteration: 0 loss: inf grad: 4037.5339272746633
iteration: 0 loss: inf grad: 4048.049642949488
iteration: 0 loss: inf grad: 3520.0151386604325
iteration: 0 loss: inf grad: 4154.21821901632
iteration: 0 loss: inf grad: 3721.5120611917832
iteration: 0 loss: inf grad: 3191.2804324840536
iteration: 0 loss: inf grad: 3032.7251771161673
iteration: 0 loss: inf grad: 3319.427200104633
iteration: 0 loss: inf grad: 3810.3836878257252
iteration: 0 loss: inf grad: 3578.6046819038615
iteration: 0 loss: inf grad: 3465.7532506420857
iteration: 0 loss: inf grad: 3356.491299964099
iteration: 0 loss: inf grad: 4209.369106112964
iteration: 0 loss: inf grad: 3824.451792897765
iteration: 0 loss: inf grad: 3688.0727151203787
iteration: 0 loss: inf grad: 3656.968740546567
iteration: 0 loss: inf grad: 3159.6556526037616
iteration: 0 loss: inf grad: 3779.235839505955
iteration: 0 loss: inf grad: 3492.29510898403
iteration: 0 loss: inf grad: 3678.201008673669
iteration: 0 loss: inf grad: 4525.646485496318
iteration: 0 loss: inf grad: 3313.531740530026
iteration: 0 loss: inf grad: 3195.165030333119
iteration: 0 loss: inf grad: 3359.6418378828675
iteration: 0 loss: inf grad: 3744.8800327802214
iteration: 0 loss: inf grad: 3846.2038269699024
iteration: 0 loss: inf grad: 4099.864247573712
iteration: 0 loss: inf grad: 4025.6738632029583
iteration: 0 loss: inf grad: 3873.5164049742025
iteration: 0 loss: inf grad: 3682.2554605869273
iteration: 0 loss: inf grad: 4149.792428152012
iteration: 0 loss: inf grad: 3874.787830383439
iteration: 0 loss: inf grad: 3637.5339392560063
iteration: 0 loss: inf grad: 3653.1104037377027
iteration: 0 loss: inf grad: 3582.6965054558505
iteration: 0 loss: inf grad: 4156.349339578306
iteration: 0 loss: inf grad: 3666.285708249051
iteration: 0 loss: inf grad: 3494.86341988431
iteration: 0 loss: inf grad: 3123.709805338348
iteration: 0 loss: inf grad: 3980.9644830739603
iteration: 0 loss: inf grad: 3874.1028468326817
iteration: 0 loss: inf grad: 3825.402229286645
iteration: 0 loss: inf grad: 3972.238997913266
iteration: 0 loss: inf grad: 3588.6775286899606
iteration: 0 loss: inf grad: 3749.724437352793
iteration: 0 loss: inf grad: 4166.764224513772
iteration: 0 loss: inf grad: 3561.3267209418245
iteration: 0 loss: inf grad: 3811.03893308552
iteration: 0 loss: inf grad: 3636.477700671063
iteration: 0 loss: inf grad: 4191.8552326150875
iteration: 0 loss: inf grad: 3969.7672519027424
iteration: 0 loss: inf grad: 3301.7947716087224
iteration: 0 loss: inf grad: 3199.462925244511
iteration: 0 loss: inf grad: 3937.4582257701577
iteration: 0 loss: inf grad: 3073.1143445916814
iteration: 0 loss: inf grad: 3104.031412571274
iteration: 0 loss: inf grad: 3178.338279627229
iteration: 0 loss: inf grad: 4046.3438810736616
iteration: 0 loss: inf grad: 3465.995982754776
iteration: 0 loss: inf grad: 4027.1716775641207
iteration: 0 loss: inf grad: 4032.476133516322
iteration: 0 loss: inf grad: 4322.058422271125
iteration: 0 loss: inf grad: 3434.314446226126
iteration: 0 loss: inf grad: 3469.824751118772
iteration: 0 loss: inf grad: 3900.6558300797806
iteration: 0 loss: inf grad: 3771.62119833374
iteration: 0 loss: inf grad: 4367.581191154468
iteration: 0 loss: inf grad: 3701.299415427245
iteration: 0 loss: inf grad: 4182.3826886062225
iteration: 0 loss: inf grad: 3673.658248902827
iteration: 0 loss: inf grad: 3460.6425569455455
iteration: 0 loss: inf grad: 4103.617279538274
iteration: 0 loss: inf grad: 3440.9516983889207
iteration: 0 loss: inf grad: 4176.248568254998
iteration: 0 loss: inf grad: 3268.192412474369
iteration: 0 loss: inf grad: 3978.215132579022
iteration: 0 loss: inf grad: 3247.069276685964
iteration: 0 loss: inf grad: 3951.5950581245424
iteration: 0 loss: inf grad: 3527.9116774828944
iteration: 0 loss: inf grad: 3672.0852459513358
iteration: 0 loss: inf grad: 3667.3217659947713
iteration: 0 loss: inf grad: 3127.9481054692956
iteration: 0 loss: inf grad: 3791.0676290096962
iteration: 0 loss: inf grad: 3837.9903329988647
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:57: RuntimeWarning: invalid value encountered in double_scalars
  diff = np.absolute(f_history[-1]-f_history[-2])
iteration: 0 loss: 1100.2655972851394 grad: 1179.0492391919984
iteration: 10 loss: 0.059510832326023694 grad: 0.37822417728855134
iteration: 0 loss: 850.7129292372606 grad: 1105.164893869971
iteration: 10 loss: 0.05346891425307893 grad: 0.18900884158436645
iteration: 0 loss: 919.7128952688898 grad: 1117.4009772422223
iteration: 10 loss: 0.04821070567082974 grad: 0.26543839464524976
iteration: 0 loss: 1234.477787611408 grad: 1228.0894429156378
iteration: 10 loss: 0.05003289036870807 grad: 0.28822589291156486
iteration: 0 loss: 744.4413550891882 grad: 1066.48483726913
iteration: 10 loss: 0.04911753973151049 grad: 0.18941614615365288
iteration: 0 loss: 504.3774873242073 grad: 947.4097159735056
iteration: 10 loss: 0.05324027215035378 grad: 0.06843293212285637
iteration: 0 loss: 785.3203546830414 grad: 1099.6934759924277
iteration: 10 loss: 0.04534918917190887 grad: 0.21701420667746957
iteration: 0 loss: 391.04594287723904 grad: 898.2787912046614
iteration: 0 loss: 1225.3377724765728 grad: 1153.542935512668
iteration: 10 loss: 0.04685687899869994 grad: 0.27903525245863403
iteration: 0 loss: 763.6588039128791 grad: 1071.352919408301
iteration: 10 loss: 0.04573616112280764 grad: 0.18955833639732222
iteration: 0 loss: 447.63545834390874 grad: 979.6936142884299
iteration: 10 loss: 0.05018773857442615 grad: 0.1757749086730701
iteration: 0 loss: 938.067967630181 grad: 1154.612086417491
iteration: 10 loss: 0.05259799689553223 grad: 0.38469323844345116
iteration: 0 loss: 691.9242339946969 grad: 975.9780956744141
iteration: 10 loss: 0.04772418843309755 grad: 0.1597914787705432
iteration: 0 loss: 690.0008165813001 grad: 1087.9400307208416
iteration: 10 loss: 0.05063513488791862 grad: 0.2244818500617112
iteration: 0 loss: 957.3512320752138 grad: 1106.1805210149753
iteration: 10 loss: 0.044582149074730376 grad: 0.23577403482733908
iteration: 0 loss: 1350.337369402167 grad: 1256.7509184969174
iteration: 10 loss: 0.05905736004834497 grad: 0.4370504120403408
iteration: 0 loss: 816.5384783535228 grad: 1076.8974180578998
iteration: 10 loss: 0.05073665539351747 grad: 0.2247893304548968
iteration: 0 loss: 571.8952945799697 grad: 1000.9846423453708
iteration: 10 loss: 0.050762823291899455 grad: 0.12825078106548143
iteration: 0 loss: 1470.3205686915012 grad: 1260.2608537515189
iteration: 10 loss: 0.06120043962668702 grad: 0.3779943615485199
iteration: 0 loss: 664.333280580267 grad: 1013.708538572625
iteration: 10 loss: 0.054814061400975334 grad: 0.1263838026137809
iteration: 0 loss: 966.2926317910883 grad: 1151.616878163774
iteration: 10 loss: 0.04904276121504319 grad: 0.1447967088562292
iteration: 0 loss: 1148.7285411710134 grad: 1199.0864957452475
iteration: 10 loss: 0.058615071494634984 grad: 0.4291851733005267
iteration: 0 loss: 647.542672070381 grad: 964.1817716336956
iteration: 0 loss: 656.2646962422158 grad: 1063.5702976532125
iteration: 10 loss: 0.05504671969802902 grad: 0.290532929086627
iteration: 0 loss: 739.1747581541578 grad: 1113.5160637244808
iteration: 10 loss: 0.05026287081107677 grad: 0.19905798717190185
iteration: 0 loss: 666.3649616504349 grad: 1074.8820401767207
iteration: 10 loss: 0.05682516826411963 grad: 0.14908838656165066
iteration: 0 loss: 1588.2491566274618 grad: 1265.9917608681112
iteration: 10 loss: 0.05829422763748549 grad: -0.030629337690121564
iteration: 0 loss: 1491.3241549370352 grad: 1224.5894904610673
iteration: 10 loss: 0.12167723381968575 grad: 0.9845114696716364
iteration: 0 loss: 769.4507361425566 grad: 1100.3161013591766
iteration: 10 loss: 0.05052366527161212 grad: 0.24937384166324128
iteration: 0 loss: 624.4552761666332 grad: 1012.5071299234977
iteration: 10 loss: 0.050845652523342076 grad: 0.12860063983578096
iteration: 0 loss: 1784.2433689671168 grad: 1336.9933122234995
iteration: 10 loss: 0.06064578205405269 grad: 0.17726570437028968
iteration: 0 loss: 824.6417067918308 grad: 1162.187860034241
iteration: 10 loss: 0.04817433744822417 grad: 0.16589262015580228
iteration: 0 loss: 792.8891971569933 grad: 1095.6828948530256
iteration: 10 loss: 0.051077756593837294 grad: 0.1725238552241904
iteration: 0 loss: 1121.4561998979311 grad: 1139.5539353848137
iteration: 10 loss: 0.049674187882787504 grad: 0.31403852865607607
iteration: 0 loss: 1109.7635666245649 grad: 1176.961538216467
iteration: 10 loss: 0.051873907877796395 grad: 0.31728641208539665
iteration: 0 loss: 907.032723746666 grad: 1142.702513502787
iteration: 10 loss: 0.06359434506025123 grad: -0.012781378212732536
iteration: 0 loss: 875.9134273230175 grad: 1094.637621670016
iteration: 10 loss: 0.051724040382726925 grad: 0.26182432682455864
iteration: 0 loss: 1348.7101099719462 grad: 1266.241125772472
iteration: 10 loss: 0.05224595721434029 grad: 0.5161429848833416
iteration: 0 loss: 651.318841420119 grad: 1040.817648688577
iteration: 10 loss: 0.051543409148011546 grad: 0.12342432364404135
iteration: 0 loss: 700.6990892690802 grad: 1060.9641641371988
iteration: 10 loss: 0.04934299110655874 grad: 0.21220839817269907
iteration: 0 loss: 1002.140065740798 grad: 1113.2143608187926
iteration: 10 loss: 0.05127119133645796 grad: 0.302527912414411
iteration: 0 loss: 429.43989412422854 grad: 963.7042970546956
iteration: 10 loss: 0.05371163812676307 grad: 0.1199896112873588
iteration: 0 loss: 2009.2772656259085 grad: 1380.315123154005
iteration: 10 loss: 0.07286618561887405 grad: 0.8097703262779807
iteration: 0 loss: 817.5957240852368 grad: 1023.771208670226
iteration: 10 loss: 0.05287504697182819 grad: 0.16486839580591978
iteration: 0 loss: 1006.4232459082169 grad: 1232.0852545180003
iteration: 10 loss: 0.05020467899058448 grad: 0.2728797325965177
iteration: 0 loss: 1184.4390777662743 grad: 1139.0926625017878
iteration: 10 loss: 0.05277744872099985 grad: 0.3559386267014615
iteration: 0 loss: 907.4819808613513 grad: 997.6161108311919
iteration: 10 loss: 0.05099027392729609 grad: 0.20690253732210773
iteration: 0 loss: 661.6969117687576 grad: 1079.8437640058455
iteration: 10 loss: 0.05082785998390384 grad: 0.18492587954205542
iteration: 0 loss: 1199.521276689653 grad: 1196.5348710570584
iteration: 10 loss: 0.053236404299571986 grad: 0.2954902346252951
iteration: 0 loss: 1281.310624630172 grad: 1201.3800828484846
iteration: 10 loss: 0.059885480949477904 grad: 0.4867508804675661
iteration: 0 loss: 1444.1059909346413 grad: 1239.9382086577825
iteration: 10 loss: 0.09490229162044918 grad: 1.086407867609451
iteration: 0 loss: 747.6114083773659 grad: 1114.5417604716984
iteration: 10 loss: 0.053288862012612466 grad: 0.32840991958331417
iteration: 0 loss: 1296.440802148443 grad: 1270.620550063584
iteration: 10 loss: 0.056557737804475684 grad: 0.23042564550001912
iteration: 0 loss: 987.180473163004 grad: 1137.12100268228
iteration: 10 loss: 0.05166514851820962 grad: 0.24271550801977185
iteration: 0 loss: 601.2557090744634 grad: 1025.170252667818
iteration: 0 loss: 830.6744571126903 grad: 1107.0041558655425
iteration: 10 loss: 0.05369030397626127 grad: 0.3639936320432634
iteration: 0 loss: 576.5757731644394 grad: 1072.2549257566193
iteration: 10 loss: 0.047739241055585975 grad: 0.2033755225562169
iteration: 0 loss: 940.4230503105174 grad: 1134.8961273236569
iteration: 10 loss: 0.050996756146063985 grad: 0.34573201125824593
iteration: 0 loss: 1462.2724388985393 grad: 1291.9919049828065
iteration: 10 loss: 0.052411951662427535 grad: 0.24629101569306935
iteration: 0 loss: 746.0163167567388 grad: 1091.5034212622447
iteration: 10 loss: 0.04864657653260193 grad: 0.12419208563907819
iteration: 0 loss: 1227.3591892896156 grad: 1206.9413527095087
iteration: 10 loss: 0.06347164279742917 grad: 0.6341034208791387
iteration: 0 loss: 1208.531871331091 grad: 1274.0486801376455
iteration: 10 loss: 0.05042098015937055 grad: 0.2677752199669161
iteration: 0 loss: 613.9101496130978 grad: 941.7638237552659
iteration: 10 loss: 0.047897562913435235 grad: 0.2456817335696348
iteration: 0 loss: 606.6628894249988 grad: 1035.3940187594321
iteration: 10 loss: 0.04753747997132119 grad: 0.2728974842717467
iteration: 0 loss: 796.5451045920846 grad: 1126.3700724380358
iteration: 10 loss: 0.050305750184634235 grad: 0.09634005170649768
iteration: 0 loss: 828.9105785822551 grad: 1099.0873742151618
iteration: 10 loss: 0.0471063134249231 grad: 0.15916735513920754
iteration: 0 loss: 642.5262233632546 grad: 987.9829271366192
iteration: 0 loss: 1132.150350028466 grad: 1212.5652043977063
iteration: 10 loss: 0.055490296791090674 grad: 0.4238022767602235
iteration: 0 loss: 992.397384298543 grad: 1107.849981155262
iteration: 10 loss: 0.052953171680366555 grad: 0.3399560380021982
iteration: 0 loss: 902.5926771795636 grad: 1180.7933621574305
iteration: 10 loss: 0.05553347665673672 grad: 0.19403380835618544
iteration: 0 loss: 1072.2241579093773 grad: 1204.0368303669713
iteration: 10 loss: 0.050355167338487015 grad: 0.2659569220271067
iteration: 0 loss: 1734.1094020889234 grad: 1342.35901405331
iteration: 10 loss: 0.07478109263617873 grad: 1.667516966307788
iteration: 0 loss: 710.5762101112886 grad: 1032.5041393567308
iteration: 10 loss: 0.05070324933066414 grad: 0.19476848131413024
iteration: 0 loss: 1139.6272268087291 grad: 1196.1300544195642
iteration: 10 loss: 0.05383810377911273 grad: 0.27241639524177846
iteration: 0 loss: 740.9594398187143 grad: 1074.9727604929733
iteration: 10 loss: 0.054736075036172144 grad: 0.0717001180331049
iteration: 0 loss: 586.1165402595464 grad: 997.5132542078226
iteration: 10 loss: 0.046209778298361925 grad: 0.10480703548088544
iteration: 0 loss: 762.8640784583852 grad: 1006.9643886171697
iteration: 10 loss: 0.047915584706722504 grad: 0.3145382255764705
iteration: 0 loss: 573.3373308503132 grad: 998.9609937145958
iteration: 10 loss: 0.05155839804931681 grad: 0.13257085549272085
iteration: 0 loss: 857.583591233095 grad: 1163.8020262066057
iteration: 10 loss: 0.04960267385930903 grad: 0.21093422198889097
iteration: 0 loss: 1293.3545157560209 grad: 1179.8898453003715
iteration: 10 loss: 0.05624584531919523 grad: 0.2930656562123088
iteration: 0 loss: 1406.22625920253 grad: 1280.3588119891826
iteration: 10 loss: 0.05003823151227235 grad: 0.10111790510323199
iteration: 0 loss: 1325.2320720765927 grad: 1221.3787574059634
iteration: 10 loss: 0.059468085713226836 grad: 0.8414162949809489
iteration: 0 loss: 1034.5284096838952 grad: 1173.6867597237153
iteration: 10 loss: 0.05155822355134825 grad: 0.26300695824214665
iteration: 0 loss: 1370.0023696204605 grad: 1221.9679450877002
iteration: 10 loss: 0.05322423684265232 grad: 0.2860477731932537
iteration: 0 loss: 1340.7234782733385 grad: 1226.932429541642
iteration: 10 loss: 0.06157687845717936 grad: 0.5299168881228136
iteration: 0 loss: 1095.0304455319856 grad: 1063.1795436204175
iteration: 10 loss: 0.049086118524759186 grad: 0.16112780357739465
iteration: 0 loss: 1707.4016920548527 grad: 1257.7900050409785
iteration: 10 loss: 0.0563996421855832 grad: 0.014395613312203556
iteration: 0 loss: 1152.9101070974941 grad: 1125.231885880162
iteration: 10 loss: 0.05376686999906352 grad: 0.2892769752530624
iteration: 0 loss: 546.4118178345204 grad: 962.2139943861821
iteration: 10 loss: 0.05066113284242006 grad: 0.13578806629233653
iteration: 0 loss: 504.813085931091 grad: 911.4581368501633
iteration: 10 loss: 0.05108686370774442 grad: 0.05535812882406278
iteration: 0 loss: 708.3767254961225 grad: 999.7785621546341
iteration: 10 loss: 0.046899892744311895 grad: 0.07224090523110135
iteration: 0 loss: 931.2350040882735 grad: 1153.874913383956
iteration: 10 loss: 0.04638823505295229 grad: 0.16410105600855318
iteration: 0 loss: 854.6697509747178 grad: 1081.9970257576224
iteration: 10 loss: 0.04508134567136453 grad: 0.3258823286956404
iteration: 0 loss: 675.9744068482488 grad: 1044.1810492726363
iteration: 10 loss: 0.04940762904284797 grad: 0.25485154522765174
iteration: 0 loss: 589.9497642013798 grad: 1012.7368345057184
iteration: 10 loss: 0.04660003076490036 grad: 0.13995268685089834
iteration: 0 loss: 1280.2521540139996 grad: 1276.904562063712
iteration: 10 loss: 0.052613277272030246 grad: 0.22250427884845136
iteration: 0 loss: 1057.7602421001752 grad: 1156.7638600895448
iteration: 10 loss: 0.05468456015164107 grad: 0.2829824847591078
iteration: 0 loss: 1024.905272907994 grad: 1116.2907084924173
iteration: 10 loss: 0.05150000392049234 grad: 0.19312247663469262
iteration: 0 loss: 834.4916377671684 grad: 1105.64540540225
iteration: 10 loss: 0.0513913022706137 grad: 0.21722991665250416
iteration: 0 loss: 662.5844405805121 grad: 949.5770835580093
iteration: 0 loss: 1029.6499098661586 grad: 1143.205959015741
iteration: 10 loss: 0.04754821274433529 grad: 0.2622585980662575
iteration: 0 loss: 740.6138155287844 grad: 1056.0107135993612
iteration: 10 loss: 0.05174335853272086 grad: 0.1080758955950902
iteration: 0 loss: 710.9711845380169 grad: 1111.0392567423792
iteration: 10 loss: 0.05822447131372015 grad: 0.24730915479260626
iteration: 0 loss: 1700.775772567966 grad: 1372.9431121477155
iteration: 10 loss: 0.06451541607715296 grad: 0.2373657003784459
iteration: 0 loss: 553.416977359282 grad: 1001.7816714407847
iteration: 10 loss: 0.0447934291933052 grad: 0.14214339051405783
iteration: 0 loss: 523.042526588665 grad: 963.9486813409152
iteration: 0 loss: 649.0832923463521 grad: 1016.5553622696336
iteration: 10 loss: 0.04869054457429394 grad: 0.1578565018819376
iteration: 0 loss: 792.4144976668063 grad: 1132.2103403509761
iteration: 10 loss: 0.05203068683245378 grad: 0.2708772099420937
iteration: 0 loss: 1039.3270227609612 grad: 1161.4858707131984
iteration: 10 loss: 0.04971788178326361 grad: 0.2344208396112986
iteration: 0 loss: 1141.3268192240391 grad: 1242.3048021240045
iteration: 10 loss: 0.052019250877823324 grad: 0.3109887128557878
iteration: 0 loss: 1370.546938185423 grad: 1220.3674619148933
iteration: 10 loss: 0.07104639001871957 grad: -0.23940369491386176
iteration: 0 loss: 1069.5991692147668 grad: 1171.7457517010687
iteration: 10 loss: 0.062251764447534115 grad: 0.24055208917491322
iteration: 0 loss: 864.8235517997632 grad: 1113.870692677331
iteration: 10 loss: 0.05192990475453288 grad: 0.22356058934725903
iteration: 0 loss: 1472.4260049394995 grad: 1256.5482970974335
iteration: 10 loss: 0.05431659388639012 grad: 0.285995428305596
iteration: 0 loss: 1134.6834642753138 grad: 1172.3320379277893
iteration: 10 loss: 0.0526486226587043 grad: 0.22377272483196198
iteration: 0 loss: 626.8383190517457 grad: 1098.2248503756086
iteration: 10 loss: 0.05037252303273734 grad: 0.28226784021111007
iteration: 0 loss: 843.5550019606308 grad: 1103.0121333966526
iteration: 10 loss: 0.04943666138172954 grad: 0.2144185342120788
iteration: 0 loss: 741.255273775076 grad: 1082.61211438261
iteration: 10 loss: 0.05568691560719427 grad: 0.30890534564538424
iteration: 0 loss: 1277.5523581095085 grad: 1259.636436083267
iteration: 10 loss: 0.0655341530787568 grad: 0.28669268920064145
iteration: 0 loss: 649.6422555210285 grad: 1106.466603529898
iteration: 10 loss: 0.0508450926258361 grad: 0.2111006276244814
iteration: 0 loss: 844.7499156192775 grad: 1053.9468725559207
iteration: 10 loss: 0.0484334597008869 grad: 0.3029669297507304
iteration: 0 loss: 438.0404731415896 grad: 941.5577856872004
iteration: 10 loss: 0.05674530705470137 grad: 0.14783869685457746
iteration: 0 loss: 1019.4792964968681 grad: 1206.9576257344484
iteration: 10 loss: 0.050090734444397225 grad: 0.36714599246320817
iteration: 0 loss: 950.1834074368797 grad: 1171.792073517634
iteration: 10 loss: 0.05666568455523124 grad: 0.2064502865820947
iteration: 0 loss: 952.7887726343208 grad: 1156.233780931656
iteration: 10 loss: 0.05327722983334256 grad: 0.22055116424817994
iteration: 0 loss: 1470.1864469325667 grad: 1201.189354186702
iteration: 10 loss: 0.04865532668257153 grad: 0.1914316522257583
iteration: 0 loss: 736.4713152358186 grad: 1086.6612971145087
iteration: 10 loss: 0.0490607187130081 grad: 0.2664615162257795
iteration: 0 loss: 879.8878385470417 grad: 1132.3099676155277
iteration: 10 loss: 0.0505449514560248 grad: 0.2459452147490727
iteration: 0 loss: 1488.7443401637493 grad: 1264.3350541198697
iteration: 10 loss: 0.05329845514279441 grad: 0.2533078947716677
iteration: 0 loss: 879.02076563398 grad: 1077.0472977437157
iteration: 10 loss: 0.05621010933861147 grad: 0.26233892879714243
iteration: 0 loss: 1202.3760013211806 grad: 1152.3253676177046
iteration: 10 loss: 0.050370233488303544 grad: 0.25230929203406555
iteration: 0 loss: 741.2496270173074 grad: 1100.420967200574
iteration: 10 loss: 0.056673714684372746 grad: 0.21736938253202812
iteration: 0 loss: 1310.0494773488638 grad: 1268.419311262191
iteration: 10 loss: 0.054195700512288815 grad: 0.32812079165009156
iteration: 0 loss: 1153.6754820053375 grad: 1202.8278891655436
iteration: 10 loss: 0.05047570053648087 grad: 0.18065917958913455
iteration: 0 loss: 730.2036351886082 grad: 994.0742049997552
iteration: 10 loss: 0.04633852201004629 grad: 0.14648180796605795
iteration: 0 loss: 550.7463949951415 grad: 962.9501902726905
iteration: 0 loss: 859.3101302431501 grad: 1190.9901093804356
iteration: 10 loss: 0.04889987264389426 grad: 0.21658161789737196
iteration: 0 loss: 429.0967237767922 grad: 922.8113860840233
iteration: 10 loss: 0.050463421205759834 grad: 0.18464674867015907
iteration: 0 loss: 682.303363423563 grad: 933.2633493188116
iteration: 10 loss: 0.048352912984806935 grad: 0.17880310421112516
iteration: 0 loss: 548.7295330986198 grad: 956.8042420599534
iteration: 10 loss: 0.05133270230428686 grad: 0.22807910892649522
iteration: 0 loss: 1352.7385237603673 grad: 1225.3798009779657
iteration: 10 loss: 0.05079661684794701 grad: 0.3206698767312984
iteration: 0 loss: 830.1977501010526 grad: 1044.5824434497883
iteration: 10 loss: 0.053120379759506745 grad: 0.25140529278327783
iteration: 0 loss: 1341.9989487731007 grad: 1219.180537024218
iteration: 10 loss: 0.05567052347462529 grad: 0.7695484318516211
iteration: 0 loss: 1333.8920749033516 grad: 1221.06775549143
iteration: 10 loss: 0.050256562390462604 grad: 0.34376455514284193
iteration: 0 loss: 1554.3916098767331 grad: 1310.95565377821
iteration: 10 loss: 0.05497673035444512 grad: 0.31413442278127046
iteration: 0 loss: 675.0150472154855 grad: 1035.878118732388
iteration: 10 loss: 0.052336893047388665 grad: 0.18374617235102883
iteration: 0 loss: 699.5659300178046 grad: 1048.102655519665
iteration: 10 loss: 0.04894323874528213 grad: 0.19862739316084368
iteration: 0 loss: 902.0720729705583 grad: 1179.4688721938612
iteration: 10 loss: 0.051723644036842 grad: 0.2786932806398397
iteration: 0 loss: 926.7181741591234 grad: 1141.5228664261267
iteration: 10 loss: 0.05077134828752605 grad: 0.2118709157571128
iteration: 0 loss: 1814.5252964417625 grad: 1324.9310442472968
iteration: 10 loss: 0.06277613813422663 grad: 0.6756781543077155
iteration: 0 loss: 750.4484053756822 grad: 1117.6003910469997
iteration: 10 loss: 0.050829694454230114 grad: 0.31431557683092864
iteration: 0 loss: 1362.8705399420733 grad: 1267.0167817343176
iteration: 10 loss: 0.06031659695641296 grad: 0.3004705366726471
iteration: 0 loss: 900.9314531935838 grad: 1109.0694161807442
iteration: 10 loss: 0.04515796687701368 grad: 0.2036950131092746
iteration: 0 loss: 547.8866359058015 grad: 1043.9017345124048
iteration: 10 loss: 0.04899159724201044 grad: 0.13494552708405763
iteration: 0 loss: 1332.0755368415523 grad: 1243.6405708692769
iteration: 10 loss: 0.052699785883305594 grad: 0.29523964144767356
iteration: 0 loss: 633.5248033797135 grad: 1038.9879458352598
iteration: 10 loss: 0.056489653939603486 grad: 0.3145062757384721
iteration: 0 loss: 1563.0460665024796 grad: 1264.2444463158322
iteration: 10 loss: 0.0843818428924847 grad: 1.115926027138391
iteration: 0 loss: 563.4103901381234 grad: 986.249548746448
iteration: 10 loss: 0.05384164004267818 grad: 0.168516787407311
iteration: 0 loss: 1255.2368826162199 grad: 1205.2442056208567
iteration: 10 loss: 0.04803417733769906 grad: 0.26899398454567647
iteration: 0 loss: 615.2110059005619 grad: 978.1095647820869
iteration: 10 loss: 0.05013883485001067 grad: 0.1660527934318281
iteration: 0 loss: 928.5642724283224 grad: 1196.0174070611652
iteration: 10 loss: 0.052496703811894724 grad: 0.17510841238986882
iteration: 0 loss: 657.1055049751925 grad: 1063.0457223411145
iteration: 10 loss: 0.049488015263209076 grad: 0.2705490094242131
iteration: 0 loss: 859.0300275163913 grad: 1111.2032412625188
iteration: 10 loss: 0.05246990771014349 grad: 0.2706562462829449
iteration: 0 loss: 931.9423121515207 grad: 1107.3661324766415
iteration: 10 loss: 0.04925030235542429 grad: 0.22543617715089362
iteration: 0 loss: 481.1419918571509 grad: 942.484427461591
iteration: 0 loss: 1156.6991678696804 grad: 1147.1160804577935
iteration: 10 loss: 0.04897452587796248 grad: 0.28360211672562097
iteration: 0 loss: 1089.921479592972 grad: 1161.9227836951825
iteration: 10 loss: 0.05537918131548478 grad: 0.1983436526187079
iteration: 0 loss: 5385.723387491798 grad: 2198.409951456407
iteration: 10 loss: 0.002381250743796541 grad: 0.015068360192640294
iteration: 0 loss: 4280.699559511244 grad: 2061.2659870248035
iteration: 0 loss: 4731.483573742245 grad: 2089.013435816614
iteration: 0 loss: 5951.423443480787 grad: 2290.5204084577554
iteration: 10 loss: 0.002206177590944042 grad: 0.020731261643375035
iteration: 0 loss: 3652.6494576487385 grad: 1990.6818174849427
iteration: 0 loss: 2477.6689841625894 grad: 1777.2324813801313
iteration: 0 loss: 3957.2683479374386 grad: 2054.371705573215
iteration: 0 loss: 2098.551533348786 grad: 1682.1229601667924
iteration: 0 loss: 5910.833568194639 grad: 2153.0327254021754
iteration: 10 loss: 0.0017827710094438357 grad: 0.007299807810032192
iteration: 0 loss: 3831.2520692685143 grad: 2001.3180079730778
iteration: 0 loss: 2383.333512036646 grad: 1831.4942745391747
iteration: 0 loss: 4878.407628115216 grad: 2154.596302697974
iteration: 10 loss: 0.013012198614887893 grad: -0.2869322552673657
iteration: 0 loss: 3320.47253121681 grad: 1822.8924019244919
iteration: 0 loss: 3582.2731675268496 grad: 2034.5638112777222
iteration: 0 loss: 4743.692155405694 grad: 2066.356423288974
iteration: 0 loss: 6855.610347782437 grad: 2341.7107337154184
iteration: 10 loss: 0.00367155498216479 grad: 1.3905707715361728
iteration: 0 loss: 4088.526325368011 grad: 2009.4027124426932
iteration: 0 loss: 2877.1058121401134 grad: 1871.9050271566052
iteration: 0 loss: 6946.406786691488 grad: 2346.5586576471846
iteration: 10 loss: 0.006616343872420574 grad: 0.9858977537497466
iteration: 0 loss: 3372.5107690433547 grad: 1897.9862056275567
iteration: 0 loss: 4875.134736879409 grad: 2149.6889796274363
iteration: 0 loss: 5887.573059026344 grad: 2233.6926566843085
iteration: 10 loss: 0.00296622117722026 grad: 0.724997571447452
iteration: 0 loss: 3081.144822416993 grad: 1802.4628320672828
iteration: 0 loss: 3643.208729632803 grad: 1985.6059664536597
iteration: 0 loss: 3781.6022558469963 grad: 2080.2539046614065
iteration: 0 loss: 3261.807487914093 grad: 2011.5738398404073
iteration: 10 loss: 0.0022927219423846427 grad: 0.021434825851237144
iteration: 0 loss: 7458.36578851341 grad: 2355.453541308252
iteration: 10 loss: 0.0031012375151145866 grad: -1.1715767252660307
iteration: 0 loss: 7257.400040974584 grad: 2283.764258373566
iteration: 10 loss: 0.820987359804514 grad: -0.9432650256311745
iteration: 0 loss: 4162.113556093205 grad: 2054.3854688282336
iteration: 0 loss: 3132.1250395407887 grad: 1892.225912474285
iteration: 0 loss: 8570.562099208786 grad: 2490.106110383692
iteration: 10 loss: 0.12416169763574461 grad: 1.0275805188444298
iteration: 0 loss: 4166.86436916536 grad: 2168.2895808439102
iteration: 0 loss: 4003.8074945786575 grad: 2048.114389419558
iteration: 0 loss: 5595.948333163286 grad: 2124.8144914673535
iteration: 10 loss: 0.002240107415921309 grad: -0.030699887401285188
iteration: 0 loss: 5501.975564644283 grad: 2193.8508135220136
iteration: 10 loss: 0.002498740747052414 grad: 0.010133853159793748
iteration: 0 loss: 4600.323293107781 grad: 2130.7968563150685
iteration: 10 loss: 0.03144655075051229 grad: 0.9201821537618907
iteration: 0 loss: 4332.917253094186 grad: 2042.6503745474627
iteration: 0 loss: 6903.848346610222 grad: 2361.8168170740514
iteration: 10 loss: 0.009522542141811837 grad: 1.0535172495537988
iteration: 0 loss: 3254.3575556575856 grad: 1947.574206092013
iteration: 0 loss: 3574.7599507741475 grad: 1983.9256747997013
iteration: 0 loss: 4626.366303601308 grad: 2078.512717574765
iteration: 10 loss: 0.0019335946377726611 grad: 0.010056712964602157
iteration: 0 loss: 2307.5710642493837 grad: 1802.7376633641097
iteration: 0 loss: 9707.777798695191 grad: 2569.8439423320324
iteration: 10 loss: 0.9108351844757139 grad: 2.998355701424458
iteration: 0 loss: 3851.7170085658786 grad: 1911.749267989612
iteration: 0 loss: 5210.182840861132 grad: 2297.572217098825
iteration: 10 loss: 0.0024931652799502694 grad: -0.015163385126348001
iteration: 0 loss: 5442.181854494924 grad: 2123.5482671310033
iteration: 0 loss: 4171.140235639599 grad: 1868.1580389664653
iteration: 0 loss: 3385.3871323960634 grad: 2016.6469619639756
iteration: 0 loss: 5923.497209408704 grad: 2227.969004413675
iteration: 10 loss: 0.0055176660984712226 grad: 0.9902298123642278
iteration: 0 loss: 6158.769101493745 grad: 2239.46748431828
iteration: 10 loss: 0.0028281133709242567 grad: 0.734384763873316
iteration: 0 loss: 7196.436455411935 grad: 2311.029833367408
iteration: 10 loss: 0.508561427716896 grad: 1.4181330564181551
iteration: 0 loss: 3823.5449255430376 grad: 2079.5585835612965
iteration: 0 loss: 6519.23827563641 grad: 2372.209310137836
iteration: 10 loss: 0.002366126806009561 grad: 0.07053186303753337
iteration: 0 loss: 4792.095631341227 grad: 2122.523009434487
iteration: 0 loss: 3200.136371085493 grad: 1917.464495631281
iteration: 0 loss: 4256.919617446417 grad: 2065.017882012826
iteration: 0 loss: 3173.1214289707323 grad: 2004.0447596237382
iteration: 0 loss: 4708.818056182307 grad: 2120.524753678785
iteration: 10 loss: 0.001707721197321503 grad: 0.02737418661931646
iteration: 0 loss: 7000.864401429639 grad: 2404.8206253898447
iteration: 10 loss: 0.0023378732267619025 grad: -0.0007283097102654401
iteration: 0 loss: 3774.7358890373216 grad: 2037.5308654965343
iteration: 0 loss: 6139.007755075241 grad: 2252.075516298685
iteration: 10 loss: 0.018448137055119416 grad: 1.0880833893862
iteration: 0 loss: 6117.1993648699945 grad: 2376.0455431661812
iteration: 10 loss: 0.0023322613137266176 grad: 0.003028647394001374
iteration: 0 loss: 3159.835131048602 grad: 1764.0409553408294
iteration: 0 loss: 3319.4235267904573 grad: 1936.616958998965
iteration: 0 loss: 3979.5306454478127 grad: 2098.0337048338533
iteration: 0 loss: 4118.789538743452 grad: 2052.080570351054
iteration: 0 loss: 3101.858519070344 grad: 1846.1615970251605
iteration: 0 loss: 5625.999022770077 grad: 2261.315315973332
iteration: 10 loss: 0.00262850407241124 grad: -0.03138813822799369
iteration: 0 loss: 4583.1476323373245 grad: 2072.2569311707693
iteration: 10 loss: 0.0022739129013974557 grad: -0.28349968856984814
iteration: 0 loss: 4532.758718861532 grad: 2202.7263519280905
iteration: 10 loss: 0.0021464889829406852 grad: 0.02712639604346194
iteration: 0 loss: 5463.419499457197 grad: 2243.702089157395
iteration: 10 loss: 0.0024586612165545707 grad: -0.06075833974128505
iteration: 0 loss: 8283.137731908884 grad: 2499.0806276534054
iteration: 10 loss: 0.09965681050380226 grad: 0.04110077964768366
iteration: 0 loss: 3467.9055749312774 grad: 1927.6259383989693
iteration: 0 loss: 5797.673435701096 grad: 2233.7302807195492
iteration: 10 loss: 0.001980400842677971 grad: -0.07393165329945821
iteration: 0 loss: 3645.908365702279 grad: 2007.2674948688627
iteration: 0 loss: 2884.519974242871 grad: 1862.6540062644585
iteration: 0 loss: 3653.950989500876 grad: 1881.343435427269
iteration: 0 loss: 2996.0939133577454 grad: 1867.564117500662
iteration: 0 loss: 4372.364909860392 grad: 2170.729886324456
iteration: 0 loss: 6142.496372797395 grad: 2201.8928481675935
iteration: 10 loss: 0.0027390575613191522 grad: -0.0025850974286804676
iteration: 0 loss: 6961.959565030007 grad: 2386.4294753994755
iteration: 10 loss: 0.0024377657801696014 grad: 0.2115471573499073
iteration: 0 loss: 6442.563594355191 grad: 2277.9171720634813
iteration: 10 loss: 0.0024197153688874096 grad: 0.22532022524379428
iteration: 0 loss: 5176.571211794048 grad: 2189.893323378903
iteration: 0 loss: 6622.683412096323 grad: 2278.4009840818408
iteration: 10 loss: 0.002749123887042515 grad: 1.7041568685494308
iteration: 0 loss: 6533.055664223377 grad: 2284.683280472393
iteration: 10 loss: 0.04572608931498094 grad: 1.2164058593157843
iteration: 0 loss: 4990.998125658533 grad: 1985.0436716165154
iteration: 10 loss: 0.02169979553913105 grad: 0.3425614877008911
iteration: 0 loss: 7835.035890784789 grad: 2344.3145054016654
iteration: 10 loss: 0.004444237102606249 grad: 1.3076892316731366
iteration: 0 loss: 5367.85683919751 grad: 2101.413377796756
iteration: 0 loss: 2862.6075100348844 grad: 1799.4655508048677
iteration: 0 loss: 2498.6884703634396 grad: 1708.642429106254
iteration: 0 loss: 3488.437516623968 grad: 1873.8386911062848
iteration: 0 loss: 4564.9968776417945 grad: 2150.5884544715304
iteration: 0 loss: 4353.851116983976 grad: 2019.4460944216457
iteration: 0 loss: 3551.1192228592463 grad: 1955.7360452502303
iteration: 0 loss: 3074.353328271599 grad: 1893.7037819031839
iteration: 0 loss: 6296.982576285432 grad: 2378.3105832931988
iteration: 10 loss: 0.006886180586827157 grad: -0.590718337108156
iteration: 0 loss: 5100.001584134345 grad: 2158.166690225591
iteration: 10 loss: 0.002216921715774912 grad: -0.408371532054521
iteration: 0 loss: 4754.458590309665 grad: 2085.217917602434
iteration: 0 loss: 4241.5294839143035 grad: 2066.372663686187
iteration: 0 loss: 3101.243394102268 grad: 1780.107396850266
iteration: 0 loss: 5070.569949236821 grad: 2131.668024538686
iteration: 0 loss: 3706.536656405803 grad: 1971.5689410553362
iteration: 0 loss: 3805.522649137587 grad: 2073.408891916197
iteration: 0 loss: 8478.113836223689 grad: 2557.1155936607875
iteration: 10 loss: 0.15164210240816994 grad: -1.8601475399526088
iteration: 0 loss: 2787.6323123053353 grad: 1870.1685486411393
iteration: 0 loss: 2772.464626595252 grad: 1803.1247894191006
iteration: 0 loss: 3304.1575487324353 grad: 1900.5719615153555
iteration: 0 loss: 4181.264278388971 grad: 2115.662318125215
iteration: 0 loss: 5068.4699948053685 grad: 2168.808383739224
iteration: 0 loss: 5764.485376148796 grad: 2316.4182607688576
iteration: 10 loss: 0.002269450094900094 grad: 0.034531534377832136
iteration: 0 loss: 7108.030089588766 grad: 2272.4602009564474
iteration: 10 loss: 0.166182083423006 grad: 2.2191891841364226
iteration: 0 loss: 5599.914252525809 grad: 2186.7714434099225
iteration: 10 loss: 0.0344016423385421 grad: 1.5234686722065802
iteration: 0 loss: 4354.549379975983 grad: 2078.2396562523973
iteration: 10 loss: 0.0021349610244876453 grad: 0.05059463418432245
iteration: 0 loss: 7247.8208652766825 grad: 2344.385692547713
iteration: 10 loss: 0.003381112126093781 grad: 0.8878332281098161
iteration: 0 loss: 5483.821416410135 grad: 2188.742209768813
iteration: 0 loss: 3491.6187704125755 grad: 2053.098266443677
iteration: 10 loss: 0.002269420116631822 grad: 0.008190018705936013
iteration: 0 loss: 4117.159256242489 grad: 2061.255273481824
iteration: 0 loss: 3705.8176017354435 grad: 2023.5936416852142
iteration: 0 loss: 6497.487765083918 grad: 2346.451153172382
iteration: 10 loss: 0.18163322419885927 grad: -2.1468414103632787
iteration: 0 loss: 3390.359021391971 grad: 2068.0378571141455
iteration: 0 loss: 4194.977153453382 grad: 1972.3434431039327
iteration: 0 loss: 2291.436846946791 grad: 1762.1982002055674
iteration: 0 loss: 4932.160621421504 grad: 2248.2402207926402
iteration: 0 loss: 4756.165127852303 grad: 2188.5245960739344
iteration: 10 loss: 0.0022277706474679608 grad: -0.00129647834848095
iteration: 0 loss: 4714.068082182166 grad: 2155.57564667957
iteration: 10 loss: 0.002405027122056874 grad: -0.10663874699448134
iteration: 0 loss: 6773.136600037652 grad: 2240.7499508255473
iteration: 10 loss: 0.028180752681113187 grad: 1.5347593823681995
iteration: 0 loss: 3830.82794865529 grad: 2026.5468706094853
iteration: 0 loss: 4478.963041092659 grad: 2114.280524570198
iteration: 10 loss: 0.002317628275539557 grad: 0.025172514669401475
iteration: 0 loss: 6900.826914934721 grad: 2352.9822535251637
iteration: 10 loss: 0.002448185029524294 grad: -0.03537453675036147
iteration: 0 loss: 4297.1340225167305 grad: 2008.039640673499
iteration: 0 loss: 5885.278535025804 grad: 2150.7221527044385
iteration: 10 loss: 0.0022824271317486737 grad: -0.003697095160112915
iteration: 0 loss: 3741.8200026578097 grad: 2052.075773518747
iteration: 0 loss: 6393.311850653015 grad: 2366.086527492478
iteration: 0 loss: 5508.049210914298 grad: 2241.441398793724
iteration: 0 loss: 3548.1489465403647 grad: 1861.3177292668133
iteration: 0 loss: 2772.7518682283994 grad: 1800.990682177668
iteration: 0 loss: 4500.611513360936 grad: 2221.122717380752
iteration: 0 loss: 2289.032971888279 grad: 1730.212804770333
iteration: 0 loss: 3260.2962732407946 grad: 1751.1724380148555
iteration: 0 loss: 2756.548749500903 grad: 1792.810991932882
iteration: 0 loss: 6576.748208069115 grad: 2283.25952250892
iteration: 10 loss: 0.0023859071648489175 grad: 0.10750964112820444
iteration: 0 loss: 3991.4416801391067 grad: 1957.432599881459
iteration: 0 loss: 6899.836331965616 grad: 2276.5942310286046
iteration: 10 loss: 0.49193685013398697 grad: 1.2686168612101845
iteration: 0 loss: 6478.210178042482 grad: 2277.5718908756417
iteration: 10 loss: 0.002604907821726308 grad: 0.3055572111417835
iteration: 0 loss: 7764.247500973809 grad: 2442.620387465491
iteration: 10 loss: 0.08782811514604626 grad: -0.14812835026787796
iteration: 0 loss: 3490.9609813836446 grad: 1931.1356789561903
iteration: 0 loss: 3531.53243182465 grad: 1957.5542043254845
iteration: 0 loss: 4602.207153941887 grad: 2200.9442875018617
iteration: 10 loss: 0.0020347726306433537 grad: 0.025765604036926744
iteration: 0 loss: 4718.643416533497 grad: 2131.0310003705017
iteration: 0 loss: 8875.620750388283 grad: 2467.924158420561
iteration: 10 loss: 0.3984403431029271 grad: 1.7730290653063254
iteration: 0 loss: 3923.162493966179 grad: 2087.491214387911
iteration: 0 loss: 6845.589928935082 grad: 2360.3286720370884
iteration: 10 loss: 0.0025535477814793758 grad: 1.459314288068934
iteration: 0 loss: 4459.600531450373 grad: 2070.6897589417413
iteration: 0 loss: 2997.1599677109716 grad: 1953.649652449004
iteration: 0 loss: 6431.798968040786 grad: 2317.3386517721883
iteration: 10 loss: 0.0020632197940192946 grad: 0.8291813558432989
iteration: 0 loss: 3209.0647778437556 grad: 1942.0689825255865
iteration: 0 loss: 7746.5929173835 grad: 2357.3288180637496
iteration: 10 loss: 0.6554869896618502 grad: 3.3820454880474062
iteration: 0 loss: 2818.0175882845815 grad: 1846.7220059297626
iteration: 0 loss: 6079.517944560744 grad: 2243.632739636278
iteration: 10 loss: 0.002276939062126489 grad: 0.5231872511845194
iteration: 0 loss: 3090.9800732195367 grad: 1829.823577028873
iteration: 0 loss: 4666.093684581781 grad: 2230.0436426784645
iteration: 10 loss: 0.0024068086707054385 grad: 0.4723533299484583
iteration: 0 loss: 3387.4433699027995 grad: 1989.5439372908547
iteration: 0 loss: 4585.045356739393 grad: 2071.4632742511913
iteration: 10 loss: 0.002506563425413333 grad: 0.07041725970839834
iteration: 0 loss: 4453.932456596132 grad: 2069.2457690659
iteration: 0 loss: 2536.678847696054 grad: 1765.207547302887
iteration: 0 loss: 5582.890083872549 grad: 2139.5249451897303
iteration: 10 loss: 0.04677365600814069 grad: 0.38345751428302727
iteration: 0 loss: 5425.609476384255 grad: 2166.491667392085
iteration: 10 loss: 0.00248508716520684 grad: 0.025444423604131253
iteration: 0 loss: inf grad: 3471.1267625434493
iteration: 0 loss: inf grad: 3256.311831005529
iteration: 0 loss: inf grad: 3295.4783059269967
iteration: 0 loss: inf grad: 3615.4041817924126
iteration: 0 loss: inf grad: 3143.431608461845
iteration: 0 loss: 7001.707810545057 grad: 2807.4059367289537
iteration: 10 loss: 0.003273158035219901 grad: -0.6818067604903755
iteration: 0 loss: inf grad: 3241.9925460483355
iteration: 0 loss: 6025.673614274446 grad: 2652.80682311036
iteration: 0 loss: inf grad: 3397.950472538509
iteration: 0 loss: inf grad: 3160.754297887841
iteration: 0 loss: 6894.260123450682 grad: 2894.9107957857723
iteration: 0 loss: inf grad: 3402.1556315390835
iteration: 0 loss: 9375.664804876085 grad: 2874.3951733588983
iteration: 0 loss: inf grad: 3214.114755031498
iteration: 10 loss: 0.0002878575046038763 grad: 0.001766613727341706
iteration: 0 loss: inf grad: 3261.700828782311
iteration: 0 loss: inf grad: 3696.7365742158263
iteration: 0 loss: inf grad: 3173.0940575629197
iteration: 0 loss: 8230.07366074686 grad: 2960.034241050567
iteration: 0 loss: inf grad: 3702.2908076909043
iteration: 0 loss: 9599.192727669013 grad: 2993.9322359928833
iteration: 0 loss: inf grad: 3392.2604879514693
iteration: 0 loss: inf grad: 3527.0693539790736
iteration: 0 loss: inf grad: 2848.965738185597
iteration: 0 loss: inf grad: 3134.3579673943887
iteration: 0 loss: 10833.478027389403 grad: 3290.7406018937554
iteration: 0 loss: inf grad: 3169.1928388350466
iteration: 10 loss: 0.0003218440846963362 grad: 0.01130340339037621
iteration: 0 loss: inf grad: 3720.7928140495133
iteration: 0 loss: inf grad: 3602.995378460396
iteration: 0 loss: 11959.269802310197 grad: 3240.816321132274
iteration: 10 loss: 0.0002527281493795189 grad: 0.005883458217244186
iteration: 0 loss: 8854.424330708613 grad: 2983.702833362067
iteration: 0 loss: inf grad: 3929.4132045703636
iteration: 0 loss: 11816.63131305794 grad: 3421.0324596205105
iteration: 0 loss: 11390.459744914955 grad: 3235.412770717031
iteration: 0 loss: inf grad: 3357.4957949054415
iteration: 0 loss: inf grad: 3460.456497031398
iteration: 0 loss: inf grad: 3362.6950599336838
iteration: 0 loss: inf grad: 3226.2994364840206
iteration: 0 loss: inf grad: 3725.1615201132013
iteration: 0 loss: 9260.107166602704 grad: 3071.519521304439
iteration: 0 loss: 10258.15356300499 grad: 3127.2913065828125
iteration: 0 loss: inf grad: 3280.888319868355
iteration: 0 loss: 6692.5723023853725 grad: 2846.458471305357
iteration: 0 loss: inf grad: 4054.3925398604756
iteration: 0 loss: inf grad: 3019.133440042741
iteration: 0 loss: inf grad: 3624.0320453814115
iteration: 0 loss: inf grad: 3353.543319212915
iteration: 0 loss: inf grad: 2947.0259710390974
iteration: 10 loss: 0.0002631802662191066 grad: -0.07548247755783691
iteration: 0 loss: 9636.966371545454 grad: 3182.221441755231
iteration: 10 loss: 0.0002650023221080615 grad: 0.003578116312371737
iteration: 0 loss: inf grad: 3515.663384368345
iteration: 0 loss: inf grad: 3536.3792164174265
iteration: 0 loss: inf grad: 3647.0027644814604
iteration: 0 loss: 10962.790961408418 grad: 3281.7219811528653
iteration: 0 loss: inf grad: 3742.217367249154
iteration: 0 loss: inf grad: 3345.2349194769163
iteration: 0 loss: 9145.633794330906 grad: 3026.058775039363
iteration: 0 loss: inf grad: 3261.00936849177
iteration: 0 loss: 9290.194705241842 grad: 3160.608202321246
iteration: 0 loss: inf grad: 3342.5794426616967
iteration: 0 loss: inf grad: 3796.545148196166
iteration: 0 loss: 10764.389437022763 grad: 3217.0002245074497
iteration: 0 loss: inf grad: 3551.755419443666
iteration: 0 loss: inf grad: 3745.275361168112
iteration: 0 loss: inf grad: 2781.8556361546075
iteration: 0 loss: 9556.06765729422 grad: 3048.9945553075577
iteration: 10 loss: 0.0002529006530742415 grad: -0.0030308547589067256
iteration: 0 loss: 11310.352539526513 grad: 3311.6099564785272
iteration: 0 loss: inf grad: 3240.3958552927365
iteration: 0 loss: 8718.926628393618 grad: 2918.2360510850194
iteration: 0 loss: inf grad: 3567.1282987617083
iteration: 0 loss: inf grad: 3267.951760084788
iteration: 0 loss: inf grad: 3475.9571920914022
iteration: 0 loss: inf grad: 3543.771392479009
iteration: 0 loss: inf grad: 3943.429278644005
iteration: 0 loss: inf grad: 3044.7024090593477
iteration: 0 loss: inf grad: 3518.9043312381355
iteration: 0 loss: 10140.510329570441 grad: 3168.6794235022253
iteration: 10 loss: 0.00028666640653020954 grad: 0.004933218577264363
iteration: 0 loss: 8162.726791528843 grad: 2941.1146423760874
iteration: 0 loss: inf grad: 2971.0078336071742
iteration: 0 loss: 8601.078932635846 grad: 2946.632498847611
iteration: 0 loss: 12467.545232478711 grad: 3425.5391550047743
iteration: 0 loss: inf grad: 3474.4041164127257
iteration: 0 loss: inf grad: 3765.3233893344373
iteration: 0 loss: inf grad: 3594.9960156523557
iteration: 0 loss: inf grad: 3454.486503682885
iteration: 0 loss: inf grad: 3597.4511357548317
iteration: 0 loss: inf grad: 3603.6297080640966
iteration: 0 loss: inf grad: 3132.3763225579546
iteration: 0 loss: inf grad: 3697.1615943892343
iteration: 0 loss: inf grad: 3313.66839421859
iteration: 0 loss: inf grad: 2838.5896035022447
iteration: 0 loss: 7082.698236762689 grad: 2702.563738651931
iteration: 0 loss: 9705.456793915037 grad: 2955.9424169869694
iteration: 0 loss: inf grad: 3392.3438065189407
iteration: 0 loss: inf grad: 3185.124609481891
iteration: 0 loss: inf grad: 3087.2086628608413
iteration: 0 loss: 8778.108805950098 grad: 2991.363941566264
iteration: 0 loss: inf grad: 3751.823001050192
iteration: 0 loss: inf grad: 3403.2215528756274
iteration: 0 loss: inf grad: 3287.0365773634376
iteration: 0 loss: 12045.942836376827 grad: 3258.6197844198714
iteration: 0 loss: 8548.923482822878 grad: 2809.231664531527
iteration: 0 loss: inf grad: 3365.737943035194
iteration: 0 loss: 10633.303035097966 grad: 3115.2833193390406
iteration: 0 loss: inf grad: 3272.024812681845
iteration: 10 loss: 0.0003098866145592183 grad: 0.19478482286173734
iteration: 0 loss: inf grad: 4029.5748533399938
iteration: 0 loss: inf grad: 2953.5171107878496
iteration: 0 loss: inf grad: 2845.2249968159326
iteration: 0 loss: 9365.83935840173 grad: 2996.3939219999993
iteration: 0 loss: inf grad: 3337.3393006391752
iteration: 0 loss: inf grad: 3423.1525788764047
iteration: 0 loss: inf grad: 3652.702131034046
iteration: 0 loss: inf grad: 3585.625352901547
iteration: 0 loss: inf grad: 3448.135238459428
iteration: 0 loss: 12367.637430634333 grad: 3276.997363003791
iteration: 0 loss: inf grad: 3696.974611006025
iteration: 0 loss: inf grad: 3448.747318616431
iteration: 0 loss: inf grad: 3237.8882495741
iteration: 10 loss: 0.00033107348611917007 grad: -0.018323728594296738
iteration: 0 loss: inf grad: 3249.0484352453573
iteration: 0 loss: inf grad: 3190.70852138229
iteration: 10 loss: 0.0002639661304949021 grad: 0.29591865592372313
iteration: 0 loss: inf grad: 3703.913307884178
iteration: 0 loss: inf grad: 3263.892083783482
iteration: 10 loss: 0.0002490931153509089 grad: 0.004494488755733009
iteration: 0 loss: inf grad: 3112.439628843422
iteration: 0 loss: inf grad: 2784.02299688884
iteration: 0 loss: inf grad: 3548.6588953306996
iteration: 0 loss: inf grad: 3451.496980097155
iteration: 0 loss: inf grad: 3401.484074315275
iteration: 0 loss: inf grad: 3534.881855000289
iteration: 0 loss: inf grad: 3200.7344485059466
iteration: 0 loss: inf grad: 3337.314586430506
iteration: 0 loss: inf grad: 3712.8615317124245
iteration: 0 loss: inf grad: 3168.430864517433
iteration: 0 loss: inf grad: 3395.5448944260984
iteration: 0 loss: inf grad: 3240.8984017015387
iteration: 0 loss: inf grad: 3734.1273850466278
iteration: 0 loss: inf grad: 3539.269226765456
iteration: 0 loss: inf grad: 2934.8040930289762
iteration: 0 loss: 7902.534165181353 grad: 2847.6433934774914
iteration: 0 loss: inf grad: 3510.200349223719
iteration: 0 loss: 6675.536650370221 grad: 2728.2599811682644
iteration: 0 loss: inf grad: 2760.7704087965194
iteration: 0 loss: inf grad: 2827.8240355354374
iteration: 0 loss: inf grad: 3602.9685779444485
iteration: 0 loss: inf grad: 3087.734738914946
iteration: 0 loss: inf grad: 3589.4179782656397
iteration: 0 loss: inf grad: 3589.3059019740767
iteration: 0 loss: inf grad: 3854.314015346952
iteration: 0 loss: 9997.568284742732 grad: 3049.921371710875
iteration: 0 loss: 10122.773343864166 grad: 3090.4469006064214
iteration: 0 loss: inf grad: 3471.435755826077
iteration: 0 loss: inf grad: 3360.2335224680537
iteration: 0 loss: inf grad: 3889.994976707085
iteration: 0 loss: 11302.235472586653 grad: 3294.4428106641476
iteration: 10 loss: 0.0002864836477039551 grad: 0.005625603496481097
iteration: 0 loss: inf grad: 3726.73696435871
iteration: 0 loss: 12690.935962231872 grad: 3270.634025783015
iteration: 10 loss: 0.00026089411535808307 grad: 0.005581885280001961
iteration: 0 loss: 8760.637758824872 grad: 3082.9864698621177
iteration: 0 loss: inf grad: 3655.9587615826977
iteration: 0 loss: 9096.612807281574 grad: 3062.3937349100174
iteration: 0 loss: inf grad: 3720.0280459030146
iteration: 0 loss: 8053.361785462286 grad: 2910.622104094804
iteration: 0 loss: inf grad: 3541.096505149681
iteration: 0 loss: 8819.575464493313 grad: 2887.8203773654122
iteration: 0 loss: inf grad: 3521.79636306544
iteration: 0 loss: inf grad: 3139.4323009362474
iteration: 0 loss: inf grad: 3270.981031037093
iteration: 0 loss: inf grad: 3262.129346004613
iteration: 0 loss: 7217.533074751729 grad: 2787.2494795857738
iteration: 0 loss: inf grad: 3379.6091266619514
iteration: 0 loss: inf grad: 3421.2484526308135
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:57: RuntimeWarning: invalid value encountered in double_scalars
  diff = np.absolute(f_history[-1]-f_history[-2])
iteration: 0 loss: 76.69930487684678 grad: 494.06113353330113
iteration: 10 loss: 2.0447914022782823 grad: 0.0042954646962646385
iteration: 20 loss: 1.2394562833641098 grad: -0.3773914786778687
iteration: 30 loss: 0.9080040061137631 grad: -0.3699771835634278
iteration: 40 loss: 0.7209882346397453 grad: -0.33281064176407404
iteration: 50 loss: 0.59957646946011 grad: -0.29720456057538774
iteration: 60 loss: 0.5139720457680762 grad: -0.2669847423255152
iteration: 70 loss: 0.45019414974202926 grad: -0.24181948799158032
iteration: 0 loss: 54.72110747039183 grad: 458.256081401991
iteration: 10 loss: 2.5551791568257367 grad: -0.07713290326827774
iteration: 20 loss: 1.4836436591630942 grad: -0.36991862797936104
iteration: 30 loss: 1.0575351530224824 grad: -0.3465238212496407
iteration: 40 loss: 0.8247907297838786 grad: -0.3057176547472696
iteration: 50 loss: 0.6772703824636582 grad: -0.2700240593552568
iteration: 60 loss: 0.5751188633177496 grad: -0.24085812554690125
iteration: 70 loss: 0.5000748748464254 grad: -0.21708811896994884
iteration: 80 loss: 0.4425534581669126 grad: -0.19750186178700815
iteration: 0 loss: 52.61988667958369 grad: 465.2503453303365
iteration: 10 loss: 2.4486993032824516 grad: -0.36734308619831635
iteration: 20 loss: 1.440897827610694 grad: -0.4636811125443554
iteration: 30 loss: 1.033993082024484 grad: -0.39991103839164027
iteration: 40 loss: 0.8097652704967996 grad: -0.3423051465178739
iteration: 50 loss: 0.6668058752734396 grad: -0.297569094501941
iteration: 60 loss: 0.5673946435299513 grad: -0.262817250531335
iteration: 70 loss: 0.4941309288688816 grad: -0.23528104743592043
iteration: 80 loss: 0.4378337254612978 grad: -0.21299597951879723
iteration: 0 loss: 80.01509760360118 grad: 514.4776759319568
iteration: 10 loss: 2.147616507272811 grad: -0.04829058763475352
iteration: 20 loss: 1.2889140130618713 grad: -0.3104888401927488
iteration: 30 loss: 0.9388787915437949 grad: -0.3036590642242558
iteration: 40 loss: 0.7432278531931099 grad: -0.27451614288517134
iteration: 50 loss: 0.616981554636368 grad: -0.24649139728716454
iteration: 60 loss: 0.5283232275111335 grad: -0.22254311839873225
iteration: 70 loss: 0.4624490673922992 grad: -0.20246702772496536
iteration: 0 loss: 59.226628592883735 grad: 443.22625633781314
iteration: 10 loss: 2.5246920484885944 grad: -0.03598837494056773
iteration: 20 loss: 1.4748855437832058 grad: -0.35542112519207414
iteration: 30 loss: 1.053817349128291 grad: -0.3433365232660207
iteration: 40 loss: 0.822906318825062 grad: -0.3065373149968425
iteration: 50 loss: 0.6762121043047128 grad: -0.2724110213731748
iteration: 60 loss: 0.5744845741764948 grad: -0.24387280401625622
iteration: 70 loss: 0.49967815021029144 grad: -0.22032608355006045
iteration: 80 loss: 0.44229848331162264 grad: -0.2007762823534921
iteration: 0 loss: 55.135272397117205 grad: 388.2311239011742
iteration: 10 loss: 2.4631024670436754 grad: 0.411167252992867
iteration: 20 loss: 1.4403238771493223 grad: -0.2212025407988269
iteration: 30 loss: 1.0309446019048416 grad: -0.2757780834514455
iteration: 40 loss: 0.8060590693419892 grad: -0.26502679878895374
iteration: 50 loss: 0.662967249991611 grad: -0.24399897667781364
iteration: 60 loss: 0.563613885537796 grad: -0.22305354105378922
iteration: 70 loss: 0.49048128133960167 grad: -0.20433340889923385
iteration: 80 loss: 0.43434105010675755 grad: -0.18805802031122995
iteration: 0 loss: 54.356684529336526 grad: 454.7286822538954
iteration: 10 loss: 2.5306068917701108 grad: 0.3148932995707444
iteration: 20 loss: 1.4748483740227956 grad: -0.2055729968941507
iteration: 30 loss: 1.0529416955310724 grad: -0.25736915296141916
iteration: 40 loss: 0.8219468113334187 grad: -0.24943486609144894
iteration: 50 loss: 0.6753217908347903 grad: -0.2311015080304324
iteration: 60 loss: 0.5736907356543849 grad: -0.21226225954929612
iteration: 70 loss: 0.49897695658695407 grad: -0.1951555884911852
iteration: 80 loss: 0.4416788493219263 grad: -0.1801305245108505
iteration: 0 loss: 42.268913298908956 grad: 367.68434655227884
iteration: 10 loss: 2.808700351162894 grad: 0.6629735514130843
iteration: 20 loss: 1.5911609611363764 grad: -0.09193810149156215
iteration: 30 loss: 1.1204054358500302 grad: -0.19150868881577823
iteration: 40 loss: 0.8674302325464189 grad: -0.20335945487326693
iteration: 50 loss: 0.7087782817593093 grad: -0.1958148802333037
iteration: 60 loss: 0.5997389080329909 grad: -0.18377387966312736
iteration: 70 loss: 0.5200821100266197 grad: -0.17134509270031137
iteration: 80 loss: 0.45929008099177404 grad: -0.15973451845745212
iteration: 0 loss: 86.5309646106113 grad: 481.88383393017216
iteration: 10 loss: 2.0791173364228928 grad: -0.04990413506790133
iteration: 20 loss: 1.2435201020770033 grad: -0.29280127192123706
iteration: 30 loss: 0.9067854496116442 grad: -0.28795211726970904
iteration: 40 loss: 0.7192414055138712 grad: -0.2618101130494283
iteration: 50 loss: 0.5983135610096558 grad: -0.23619721990931206
iteration: 60 loss: 0.5133490410128367 grad: -0.21408820426488745
iteration: 70 loss: 0.45015493266882084 grad: -0.1954220540391615
iteration: 0 loss: 52.35856070620862 grad: 444.77923082049807
iteration: 10 loss: 2.542934905627672 grad: 0.21812066374385491
iteration: 20 loss: 1.483338050336401 grad: -0.27266449390573316
iteration: 30 loss: 1.05934651346144 grad: -0.3032254238239354
iteration: 40 loss: 0.8270391047050328 grad: -0.2830746290870836
iteration: 50 loss: 0.6795216259748403 grad: -0.2571066990823295
iteration: 60 loss: 0.5772482145360879 grad: -0.233148873001796
iteration: 70 loss: 0.5020518733522707 grad: -0.2124193588264173
iteration: 80 loss: 0.44437906276628136 grad: -0.1947199969192247
iteration: 0 loss: 44.24179831976728 grad: 402.5142602817398
iteration: 10 loss: 2.68313320765095 grad: 0.019923683282738032
iteration: 20 loss: 1.5372338937651298 grad: -0.37590285252739664
iteration: 30 loss: 1.0873078326590901 grad: -0.3660914305781406
iteration: 40 loss: 0.8437966456555676 grad: -0.3266132438704975
iteration: 50 loss: 0.6904521034307023 grad: -0.28973242087791456
iteration: 60 loss: 0.5847809316525275 grad: -0.258895549576184
iteration: 70 loss: 0.5074428145786772 grad: -0.2334883879126452
iteration: 80 loss: 0.44834132377904595 grad: -0.2124292889375522
iteration: 0 loss: 55.35258904620981 grad: 482.2973378635068
iteration: 10 loss: 2.405318888823579 grad: -0.5502802402623401
iteration: 20 loss: 1.4159405075628952 grad: -0.5912789592819967
iteration: 30 loss: 1.0168486028447163 grad: -0.48898353363400343
iteration: 40 loss: 0.796842740798845 grad: -0.40979238608182655
iteration: 50 loss: 0.6565049390936276 grad: -0.351498895433677
iteration: 60 loss: 0.5588678008460358 grad: -0.3075097701427111
iteration: 70 loss: 0.4868777532804167 grad: -0.2733063685013464
iteration: 80 loss: 0.4315355473021224 grad: -0.24599910178010773
iteration: 0 loss: 58.42284163922216 grad: 401.50727751654676
iteration: 10 loss: 2.5523177939487804 grad: 0.907023225565353
iteration: 20 loss: 1.4789008643491053 grad: 0.05235785062174916
iteration: 30 loss: 1.0538409932725121 grad: -0.09638431013821616
iteration: 40 loss: 0.8219584996896132 grad: -0.13431464230206885
iteration: 50 loss: 0.6750472676767045 grad: -0.14246405497311926
iteration: 60 loss: 0.5733311280333013 grad: -0.14074372351570083
iteration: 70 loss: 0.49860754470699414 grad: -0.13554615398556358
iteration: 80 loss: 0.4413287081162542 grad: -0.12924669877799108
iteration: 0 loss: 50.92769954438299 grad: 451.42823852347243
iteration: 10 loss: 2.555992604036584 grad: -0.3330544247322573
iteration: 20 loss: 1.4890916030504244 grad: -0.49046139800079164
iteration: 30 loss: 1.0608240262317437 grad: -0.42507881970999495
iteration: 40 loss: 0.8265196884131828 grad: -0.36317132348073705
iteration: 50 loss: 0.6780241055110888 grad: -0.31483493991562295
iteration: 60 loss: 0.5752580545087601 grad: -0.27730934490756576
iteration: 70 loss: 0.49981765174437565 grad: -0.2476380289236007
iteration: 80 loss: 0.4420352493547067 grad: -0.22368454593555076
iteration: 0 loss: 63.12135945551898 grad: 461.8647119709444
iteration: 10 loss: 2.3744628932420877 grad: 0.16597791011564433
iteration: 20 loss: 1.4074631481747184 grad: -0.23683959814324484
iteration: 30 loss: 1.0146274297702886 grad: -0.2596013984639213
iteration: 40 loss: 0.7971335825266744 grad: -0.24315247534448467
iteration: 50 loss: 0.6579571375390184 grad: -0.22217271249402595
iteration: 60 loss: 0.5608917687409716 grad: -0.20270162406821185
iteration: 70 loss: 0.4891850227079637 grad: -0.18571965327846618
iteration: 80 loss: 0.43397402526435064 grad: -0.17110529000515157
iteration: 0 loss: 64.65460274748028 grad: 527.0719887477446
iteration: 10 loss: 2.1245802311179265 grad: -0.48914291397948306
iteration: 20 loss: 1.264524093942989 grad: -0.5521482416840571
iteration: 30 loss: 0.9195607499687785 grad: -0.4621852533612182
iteration: 40 loss: 0.7277293670951456 grad: -0.3907417573337039
iteration: 50 loss: 0.6042217593229989 grad: -0.3374048713945913
iteration: 60 loss: 0.5175743605377647 grad: -0.29673373172663536
iteration: 70 loss: 0.45322131594631093 grad: -0.2648487870079437
iteration: 0 loss: 59.94251608295128 grad: 448.27708901837866
iteration: 10 loss: 2.4406003932624363 grad: -0.13788308731069226
iteration: 20 loss: 1.4369336629524696 grad: -0.37990941394297634
iteration: 30 loss: 1.0308840897909333 grad: -0.3533853888344988
iteration: 40 loss: 0.8070019421298557 grad: -0.3119963220750937
iteration: 50 loss: 0.6642603478768995 grad: -0.2759464387608297
iteration: 60 loss: 0.5650196588240678 grad: -0.246445903946484
iteration: 70 loss: 0.49190143180095963 grad: -0.22235356899248482
iteration: 80 loss: 0.43573250391504686 grad: -0.2024638296893727
iteration: 0 loss: 48.5157885209884 grad: 411.93246631058565
iteration: 10 loss: 2.6304716897116123 grad: 0.4461805006594912
iteration: 20 loss: 1.5135352209845223 grad: -0.2001683511353552
iteration: 30 loss: 1.0728714238475732 grad: -0.2593944625018238
iteration: 40 loss: 0.8336721119657837 grad: -0.2511634128589534
iteration: 50 loss: 0.6827657922764377 grad: -0.2318764327832859
iteration: 60 loss: 0.5786437042006582 grad: -0.21225687864265547
iteration: 70 loss: 0.5023687156582871 grad: -0.19459673105913106
iteration: 80 loss: 0.44403789595063164 grad: -0.17919379165038096
iteration: 0 loss: 91.32432711791638 grad: 529.6325173310138
iteration: 10 loss: 2.0469540007252363 grad: 0.29355680355086666
iteration: 20 loss: 1.2236019521304693 grad: -0.19288848306391493
iteration: 30 loss: 0.8951498065281488 grad: -0.23900797655755382
iteration: 40 loss: 0.7116172995947636 grad: -0.2332803041639637
iteration: 50 loss: 0.5928633240451249 grad: -0.21793987462599995
iteration: 60 loss: 0.5091965638794136 grad: -0.20168460502906874
iteration: 70 loss: 0.44683698348828993 grad: -0.18663826504109932
iteration: 0 loss: 51.35406727373972 grad: 418.1660621515082
iteration: 10 loss: 2.656915731272172 grad: 0.48579386163277544
iteration: 20 loss: 1.5293684104350922 grad: -0.16797241325869205
iteration: 30 loss: 1.0847392612458862 grad: -0.23848233454968154
iteration: 40 loss: 0.8433263348248478 grad: -0.2363200877843285
iteration: 50 loss: 0.6909537552170893 grad: -0.2206108761369693
iteration: 60 loss: 0.585771319397364 grad: -0.20328860928075315
iteration: 70 loss: 0.5086873807326397 grad: -0.18720258806043033
iteration: 80 loss: 0.4497164160976271 grad: -0.1729332417515536
iteration: 0 loss: 56.215225414865515 grad: 479.4307074320592
iteration: 10 loss: 2.427532791651877 grad: -0.1514068790454119
iteration: 20 loss: 1.4281674001907627 grad: -0.36814543813196027
iteration: 30 loss: 1.02473676498196 grad: -0.3403499607799086
iteration: 40 loss: 0.8025418446744549 grad: -0.3002132918472531
iteration: 50 loss: 0.6609190797104351 grad: -0.2656280917054108
iteration: 60 loss: 0.5624497610249984 grad: -0.23741372228344
iteration: 70 loss: 0.4898828632140893 grad: -0.214390316828943
iteration: 80 loss: 0.43412032498318615 grad: -0.19538158394765073
iteration: 0 loss: 58.33858944195795 grad: 500.8412788430187
iteration: 10 loss: 2.3431688065946386 grad: -0.564395374233969
iteration: 20 loss: 1.3764656472546284 grad: -0.5770306800347806
iteration: 30 loss: 0.9897646667420552 grad: -0.47538446841265686
iteration: 40 loss: 0.7768660426179601 grad: -0.3980142184920934
iteration: 50 loss: 0.6410005693198234 grad: -0.341388621170542
iteration: 60 loss: 0.5463833395895729 grad: -0.29876839903632846
iteration: 70 loss: 0.47654293565194 grad: -0.26566738087999553
iteration: 0 loss: 69.51591878346943 grad: 396.7771950803172
iteration: 10 loss: 2.3920884615632447 grad: 0.5264297082451418
iteration: 20 loss: 1.4119220423649494 grad: -0.14702516015753248
iteration: 30 loss: 1.015552388999698 grad: -0.2251840369711627
iteration: 40 loss: 0.7965098665520352 grad: -0.227820432714475
iteration: 50 loss: 0.656562861361222 grad: -0.2150698077426042
iteration: 60 loss: 0.5590972298971544 grad: -0.19963940790306284
iteration: 70 loss: 0.4871845033261376 grad: -0.18481314290821693
iteration: 80 loss: 0.43187580161991884 grad: -0.17141264019042987
iteration: 0 loss: 41.098993120649624 grad: 441.50474140449
iteration: 10 loss: 2.6895486567254094 grad: -0.5027294139531859
iteration: 20 loss: 1.5374084850780656 grad: -0.5981560582946537
iteration: 30 loss: 1.0857012947008529 grad: -0.4984469087820564
iteration: 40 loss: 0.841570796455678 grad: -0.4176094436012092
iteration: 50 loss: 0.6880209511386138 grad: -0.35758475729221983
iteration: 60 loss: 0.5823146753211238 grad: -0.31221849959306575
iteration: 70 loss: 0.5050162662930068 grad: -0.2769645578186204
iteration: 80 loss: 0.44598755974981363 grad: -0.2488558368243745
iteration: 0 loss: 55.71346013683644 grad: 462.7193924494578
iteration: 10 loss: 2.4685191579891663 grad: -0.3364770251637771
iteration: 20 loss: 1.4529049054768537 grad: -0.48058738750145436
iteration: 30 loss: 1.0406861499509845 grad: -0.4123862105246361
iteration: 40 loss: 0.8135602940784256 grad: -0.35086405176776264
iteration: 50 loss: 0.6689319228964284 grad: -0.30357657055249865
iteration: 60 loss: 0.5685021447970112 grad: -0.2671392636110308
iteration: 70 loss: 0.49458773145819246 grad: -0.23844713559460057
iteration: 80 loss: 0.43786026175571763 grad: -0.21534107613086736
iteration: 0 loss: 61.27569804649887 grad: 444.9806476529921
iteration: 10 loss: 2.4477869811585564 grad: 0.22702740571440283
iteration: 20 loss: 1.4271438255836157 grad: -0.2536852906053284
iteration: 30 loss: 1.0210160928227197 grad: -0.2876427372514839
iteration: 40 loss: 0.798361664931318 grad: -0.27136923658071865
iteration: 50 loss: 0.6568011692104185 grad: -0.24836377826190048
iteration: 60 loss: 0.5585377961023451 grad: -0.22656484920908532
iteration: 70 loss: 0.486209301850398 grad: -0.20741714269076406
iteration: 80 loss: 0.4306807857649553 grad: -0.19089633022022812
iteration: 0 loss: 100.28845074002228 grad: 531.9642682423097
iteration: 10 loss: 2.0578190610795817 grad: 0.45802074833270506
iteration: 20 loss: 1.2182401158321972 grad: -0.12742448483668134
iteration: 30 loss: 0.8871434565105367 grad: -0.19986665427791356
iteration: 40 loss: 0.7034965137448704 grad: -0.20558598971397318
iteration: 50 loss: 0.585225533183885 grad: -0.19669397085313844
iteration: 60 loss: 0.5021618147091597 grad: -0.18460995505530914
iteration: 70 loss: 0.4403890656836996 grad: -0.1724871514459935
iteration: 0 loss: 89.76225408998202 grad: 511.62593338389223
iteration: 10 loss: 2.107415672259735 grad: 0.27697101529599955
iteration: 20 loss: 1.2291456455144394 grad: -0.2742892333004937
iteration: 30 loss: 0.8909674342030485 grad: -0.29533282489556284
iteration: 40 loss: 0.7048357348042125 grad: -0.2740434187403613
iteration: 50 loss: 0.5855057479780457 grad: -0.24929730171076134
iteration: 60 loss: 0.5019452957799744 grad: -0.22694672054322626
iteration: 70 loss: 0.4399299116408987 grad: -0.20769586225202685
iteration: 0 loss: 48.481450749957524 grad: 460.7140633627604
iteration: 10 loss: 2.527417063105366 grad: -0.696131171194035
iteration: 20 loss: 1.4777548903278082 grad: -0.6355297388750696
iteration: 30 loss: 1.0547465143085075 grad: -0.5105137773732233
iteration: 40 loss: 0.8228401085129917 grad: -0.42211064186690156
iteration: 50 loss: 0.6756421751831361 grad: -0.3592148838239536
iteration: 60 loss: 0.5736528041038169 grad: -0.3126158395819407
iteration: 70 loss: 0.4987098524682303 grad: -0.2768001264715131
iteration: 80 loss: 0.4412620559994387 grad: -0.2484340348380847
iteration: 0 loss: 55.27047349989942 grad: 418.9236084801206
iteration: 10 loss: 2.5459014476834287 grad: 0.14638380005915436
iteration: 20 loss: 1.476701102608827 grad: -0.29748281717077796
iteration: 30 loss: 1.0515095202259876 grad: -0.3096725141492893
iteration: 40 loss: 0.8194163737482747 grad: -0.2827964889304261
iteration: 50 loss: 0.6724230459395503 grad: -0.25403337022712497
iteration: 60 loss: 0.5707114254739115 grad: -0.22885476460065818
iteration: 70 loss: 0.4960394941358986 grad: -0.20761434375177307
iteration: 80 loss: 0.43883586504464556 grad: -0.18974929199158821
iteration: 0 loss: 108.96047817529073 grad: 563.7941622623439
iteration: 10 loss: 1.846512782549408 grad: -0.1096238077010044
iteration: 20 loss: 1.1129809901766003 grad: -0.3720994755242688
iteration: 30 loss: 0.8221800253607977 grad: -0.3425179925322021
iteration: 40 loss: 0.6586142051775657 grad: -0.30172870333426427
iteration: 50 loss: 0.5520379223303693 grad: -0.26715846918530284
iteration: 60 loss: 0.4764760019535805 grad: -0.2391687935573345
iteration: 70 loss: 0.4198444885041681 grad: -0.21640309092280446
iteration: 0 loss: 51.45464069773755 grad: 484.8528823621815
iteration: 10 loss: 2.6117316289742947 grad: -0.13082659318202955
iteration: 20 loss: 1.5150959285915633 grad: -0.3564950108341345
iteration: 30 loss: 1.0777801702969223 grad: -0.3333314978983427
iteration: 40 loss: 0.8392351286598486 grad: -0.2952327870215175
iteration: 50 loss: 0.6882782246304321 grad: -0.2616955932716967
iteration: 60 loss: 0.5838957410236286 grad: -0.2341031343166237
iteration: 70 loss: 0.5073063350285917 grad: -0.21149060406379408
iteration: 80 loss: 0.4486614408666686 grad: -0.1927756505438832
iteration: 0 loss: 56.23153845352586 grad: 454.1174665769528
iteration: 10 loss: 2.4943185024148575 grad: -0.06464028290563242
iteration: 20 loss: 1.455852560301661 grad: -0.35931936375213414
iteration: 30 loss: 1.0409441390491916 grad: -0.3466817183918153
iteration: 40 loss: 0.8134481649683287 grad: -0.3100447359443276
iteration: 50 loss: 0.6688562336178372 grad: -0.2759822786956224
iteration: 60 loss: 0.5685291077756616 grad: -0.24740242232876636
iteration: 70 loss: 0.494711442047529 grad: -0.22375355411311837
iteration: 80 loss: 0.438061390380178 grad: -0.20407256186784145
iteration: 0 loss: 70.9598381156562 grad: 475.0336582487826
iteration: 10 loss: 2.2443180735449046 grad: -0.07362340824797821
iteration: 20 loss: 1.3297422636415335 grad: -0.3393660358552099
iteration: 30 loss: 0.9621391659071296 grad: -0.32359864113101966
iteration: 40 loss: 0.7585925710404808 grad: -0.2890228483353584
iteration: 50 loss: 0.6280631588725473 grad: -0.25765017820308134
iteration: 60 loss: 0.5367938775854616 grad: -0.23149280225080496
iteration: 70 loss: 0.469196135559452 grad: -0.20987048124819177
iteration: 0 loss: 71.0605963625239 grad: 492.1563639302597
iteration: 10 loss: 2.1683578443281393 grad: -0.22895549888646466
iteration: 20 loss: 1.3053934715209257 grad: -0.42964548641754186
iteration: 30 loss: 0.9527786130647242 grad: -0.38790418735032817
iteration: 40 loss: 0.7551283415794139 grad: -0.33914993737197374
iteration: 50 loss: 0.6273181079180755 grad: -0.29860859062336886
iteration: 60 loss: 0.5374209917459997 grad: -0.2659940699893102
iteration: 70 loss: 0.4705500019670746 grad: -0.23957533864516983
iteration: 0 loss: 60.01554207883382 grad: 477.1573845710234
iteration: 10 loss: 2.436999076552266 grad: -0.14133876776625215
iteration: 20 loss: 1.424841977352447 grad: -0.44568664554969495
iteration: 30 loss: 1.0206604387322344 grad: -0.4088976941177278
iteration: 40 loss: 0.79868761413689 grad: -0.3572318845353721
iteration: 50 loss: 0.6573927354647822 grad: -0.3135746669247145
iteration: 60 loss: 0.5592289051977456 grad: -0.2784583137137421
iteration: 70 loss: 0.4869267243685416 grad: -0.2501085726622376
iteration: 80 loss: 0.4313906649487597 grad: -0.22690079397703478
iteration: 0 loss: 65.52366074005559 grad: 456.47257297150304
iteration: 10 loss: 2.3749174431395246 grad: 0.009222175466542328
iteration: 20 loss: 1.4021758794476062 grad: -0.35277264814707576
iteration: 30 loss: 1.0086516368452811 grad: -0.3472160001957756
iteration: 40 loss: 0.7911085937355644 grad: -0.3127295647183737
iteration: 50 loss: 0.65210115971513 grad: -0.27940678894061827
iteration: 60 loss: 0.5552834437792792 grad: -0.2510365104103918
iteration: 70 loss: 0.4838462306328934 grad: -0.22737649594608073
iteration: 80 loss: 0.4289023045893456 grad: -0.20759008406399568
iteration: 0 loss: 63.19006609888716 grad: 531.452889517916
iteration: 10 loss: 2.1108026792245584 grad: -0.4829277179703665
iteration: 20 loss: 1.2663437755667142 grad: -0.5481833856743104
iteration: 30 loss: 0.9230074349218905 grad: -0.45780303741082123
iteration: 40 loss: 0.7311914593286598 grad: -0.3864019405542931
iteration: 50 loss: 0.6074175155389722 grad: -0.33330622921059444
iteration: 60 loss: 0.5204735831407974 grad: -0.2929364607158038
iteration: 70 loss: 0.4558493757376709 grad: -0.26135302613478206
iteration: 0 loss: 58.530102260222975 grad: 430.41909256577173
iteration: 10 loss: 2.411103919353288 grad: 0.13417833326828413
iteration: 20 loss: 1.4189785333664986 grad: -0.28120841495750737
iteration: 30 loss: 1.018452256986579 grad: -0.2994380986931152
iteration: 40 loss: 0.7976560253678557 grad: -0.2775618963045577
iteration: 50 loss: 0.6568560724198588 grad: -0.2519454051855282
iteration: 60 loss: 0.5589362385437859 grad: -0.22871930651238426
iteration: 70 loss: 0.4867676383128128 grad: -0.20870750391468762
iteration: 80 loss: 0.4313102939361422 grad: -0.19162598537062894
iteration: 0 loss: 50.74843124809352 grad: 441.5522391986666
iteration: 10 loss: 2.5530791159700046 grad: -0.040477124743128624
iteration: 20 loss: 1.4844073893145833 grad: -0.37942184506548926
iteration: 30 loss: 1.0574280117405743 grad: -0.3620506414611096
iteration: 40 loss: 0.8240423104956046 grad: -0.3209646975163979
iteration: 50 loss: 0.6761514839603561 grad: -0.2838831930304878
iteration: 60 loss: 0.5737970625297583 grad: -0.2532649407696315
iteration: 70 loss: 0.49864840506630903 grad: -0.2282015160041579
iteration: 80 loss: 0.4410802351154747 grad: -0.207509241107446
iteration: 0 loss: 88.06305190173998 grad: 464.9426520581223
iteration: 10 loss: 2.0942976875943384 grad: 0.5194857026199062
iteration: 20 loss: 1.2584904952264795 grad: -0.1281146592618952
iteration: 30 loss: 0.9184321921629817 grad: -0.20696586894895747
iteration: 40 loss: 0.7280251714478667 grad: -0.21340317865091565
iteration: 50 loss: 0.6049512544523995 grad: -0.20401201959871007
iteration: 60 loss: 0.518399712247215 grad: -0.19117117247469714
iteration: 70 loss: 0.4540190289604983 grad: -0.17829787418067455
iteration: 0 loss: 42.24275502900321 grad: 395.99030183715803
iteration: 10 loss: 2.7111584358125254 grad: 0.1882613241187211
iteration: 20 loss: 1.5540537990016425 grad: -0.32921505114569505
iteration: 30 loss: 1.0992903642005663 grad: -0.3424969663367377
iteration: 40 loss: 0.8530313492725657 grad: -0.31111584378456436
iteration: 50 loss: 0.6979266883668888 grad: -0.2781215292984882
iteration: 60 loss: 0.5910382541892019 grad: -0.2495370319533193
iteration: 70 loss: 0.5128114776394299 grad: -0.225603234098171
iteration: 80 loss: 0.453034495716896 grad: -0.20558959087865958
iteration: 0 loss: 100.58793489615124 grad: 582.0259898309005
iteration: 10 loss: 1.8885615978686225 grad: -0.5741057113399838
iteration: 20 loss: 1.1260690940758153 grad: -0.40054355545285025
iteration: 30 loss: 0.8253856898005655 grad: -0.34378075285843346
iteration: 40 loss: 0.6577467584421274 grad: -0.29721228949657286
iteration: 50 loss: 0.5493184063152512 grad: -0.26132439280275577
iteration: 60 loss: 0.47288697647168604 grad: -0.23324880899928985
iteration: 70 loss: 0.4158645308324677 grad: -0.21077394307486905
iteration: 0 loss: 70.8185637084624 grad: 424.55430539928307
iteration: 10 loss: 2.343676624203495 grad: 0.6158994999775368
iteration: 20 loss: 1.3777126079395947 grad: -0.09838612094423575
iteration: 30 loss: 0.9906685527006299 grad: -0.19380857796749562
iteration: 40 loss: 0.7772949563286602 grad: -0.20554167052657954
iteration: 50 loss: 0.641065770565216 grad: -0.19828502305643536
iteration: 60 loss: 0.5461961759115597 grad: -0.18647236913817974
iteration: 70 loss: 0.47618523061109125 grad: -0.17417456534912626
iteration: 0 loss: 51.323070632426486 grad: 516.3124179020678
iteration: 10 loss: 2.3982389788744842 grad: -0.7836639807745347
iteration: 20 loss: 1.416171491426212 grad: -0.6293030079788329
iteration: 30 loss: 1.017928715966677 grad: -0.5010792781916601
iteration: 40 loss: 0.7979812844193309 grad: -0.41427246238080667
iteration: 50 loss: 0.6575522875957011 grad: -0.35301729834972706
iteration: 60 loss: 0.5598019845624962 grad: -0.3076921727846121
iteration: 70 loss: 0.4877068549241647 grad: -0.27283281843245966
iteration: 80 loss: 0.4322737455704555 grad: -0.24518970491282072
iteration: 0 loss: 91.79324834937118 grad: 473.3215957750609
iteration: 10 loss: 2.1176333619594683 grad: 0.766057250473986
iteration: 20 loss: 1.2689038736569025 grad: -0.01135249427287951
iteration: 30 loss: 0.9257595296702344 grad: -0.13741023373069544
iteration: 40 loss: 0.7339521764805051 grad: -0.1655293274305446
iteration: 50 loss: 0.6100394643692032 grad: -0.1682338414430592
iteration: 60 loss: 0.5229082467277212 grad: -0.16298937173670602
iteration: 70 loss: 0.4580924481061546 grad: -0.15527574310980846
iteration: 0 loss: 84.76890401264072 grad: 409.23661516287297
iteration: 10 loss: 2.3016819107922193 grad: 0.7826006001654326
iteration: 20 loss: 1.364398192899113 grad: -0.05232626704126462
iteration: 30 loss: 0.98681724178852 grad: -0.16509838437349303
iteration: 40 loss: 0.7773302846072841 grad: -0.18369321703612135
iteration: 50 loss: 0.642910932418338 grad: -0.18037990918258406
iteration: 60 loss: 0.5489351457958533 grad: -0.17123032702908475
iteration: 70 loss: 0.4793678771169322 grad: -0.16088536338017648
iteration: 0 loss: 53.78526417958474 grad: 447.4216612912651
iteration: 10 loss: 2.5243390887026593 grad: -0.06952630019243643
iteration: 20 loss: 1.4730824314819158 grad: -0.40522722691610835
iteration: 30 loss: 1.051325115463731 grad: -0.3805354295277213
iteration: 40 loss: 0.8202214891055271 grad: -0.3352790772082569
iteration: 50 loss: 0.6735417600159301 grad: -0.29557095383823084
iteration: 60 loss: 0.5719100154297352 grad: -0.26314997389542316
iteration: 70 loss: 0.4972279459700737 grad: -0.2367710177083751
iteration: 80 loss: 0.439978632992279 grad: -0.21507494633512833
iteration: 0 loss: 72.61101255629752 grad: 501.19114031072513
iteration: 10 loss: 2.198750663764382 grad: -0.06960646764402942
iteration: 20 loss: 1.30305017284536 grad: -0.35721722712183956
iteration: 30 loss: 0.9439295495993644 grad: -0.3410576060415915
iteration: 40 loss: 0.7448594481104671 grad: -0.3047133649700005
iteration: 50 loss: 0.6170636224874628 grad: -0.27160380286159463
iteration: 60 loss: 0.5276312501308976 grad: -0.24397129000817303
iteration: 70 loss: 0.4613515239597374 grad: -0.22112875131502174
iteration: 0 loss: 82.78704382165932 grad: 503.75916237706105
iteration: 10 loss: 2.106246233397583 grad: 0.045634965177218864
iteration: 20 loss: 1.2560988890895715 grad: -0.293255798523092
iteration: 30 loss: 0.9151672002000685 grad: -0.3010946439492087
iteration: 40 loss: 0.7253677922730772 grad: -0.27711937962110644
iteration: 50 loss: 0.6030067868561507 grad: -0.2512508444070038
iteration: 60 loss: 0.5170564815847353 grad: -0.22825309726965318
iteration: 70 loss: 0.45314947068271233 grad: -0.20857494530131873
iteration: 0 loss: 81.74644140053066 grad: 521.0419740327634
iteration: 10 loss: 2.0988209433836262 grad: -0.30146866513077447
iteration: 20 loss: 1.240762491671665 grad: -0.4472833051655293
iteration: 30 loss: 0.9007440246548206 grad: -0.3891052592232184
iteration: 40 loss: 0.7125175455984473 grad: -0.33543193498710233
iteration: 50 loss: 0.5915792727305829 grad: -0.2933522155854321
iteration: 60 loss: 0.5068190400907042 grad: -0.2603907380965529
iteration: 70 loss: 0.4438979695131076 grad: -0.23408292205452882
iteration: 0 loss: 52.60775893031748 grad: 463.9034831621514
iteration: 10 loss: 2.483638198661414 grad: -0.28208221793266264
iteration: 20 loss: 1.4473579342317284 grad: -0.46338409206670217
iteration: 30 loss: 1.0334064886723437 grad: -0.40540186652098653
iteration: 40 loss: 0.8066587654429068 grad: -0.3482763915978093
iteration: 50 loss: 0.6627062521924514 grad: -0.30314047877636874
iteration: 60 loss: 0.5629252500952631 grad: -0.26784361412321855
iteration: 70 loss: 0.4895738476353389 grad: -0.239785825233344
iteration: 80 loss: 0.4333235213998705 grad: -0.21704049295685576
iteration: 0 loss: 66.5311079391738 grad: 532.6203724338529
iteration: 10 loss: 2.0635037851014784 grad: -0.4885371459014356
iteration: 20 loss: 1.2435823803590955 grad: -0.5000254370061955
iteration: 30 loss: 0.9098841247700462 grad: -0.4190171746923108
iteration: 40 loss: 0.7225359662319203 grad: -0.3558459652122065
iteration: 50 loss: 0.6011588263795189 grad: -0.30857134479735904
iteration: 60 loss: 0.5156451327777838 grad: -0.27235293950826056
iteration: 70 loss: 0.45194494604253455 grad: -0.2438257316657459
iteration: 0 loss: 70.11085632816358 grad: 474.10398265166543
iteration: 10 loss: 2.352555374449325 grad: 0.1802985470950184
iteration: 20 loss: 1.3907168526261144 grad: -0.2778691946351014
iteration: 30 loss: 1.0017012840559223 grad: -0.30127181719950935
iteration: 40 loss: 0.7865146760156098 grad: -0.27975380611474415
iteration: 50 loss: 0.6488885654262876 grad: -0.2538275149210197
iteration: 60 loss: 0.552947356768858 grad: -0.23025380585705352
iteration: 70 loss: 0.4820991837698536 grad: -0.2099562647638205
iteration: 0 loss: 43.4025524084441 grad: 424.2479045936922
iteration: 10 loss: 2.7209627177736695 grad: -0.10886842505708771
iteration: 20 loss: 1.565838361387457 grad: -0.3744966654022336
iteration: 30 loss: 1.1081594389141338 grad: -0.3474444560423381
iteration: 40 loss: 0.8599168613905849 grad: -0.30509755338841554
iteration: 50 loss: 0.7034970955728155 grad: -0.26863543313880106
iteration: 60 loss: 0.5956951009237937 grad: -0.2390722257438111
iteration: 70 loss: 0.5168039239628839 grad: -0.21510078183714107
iteration: 80 loss: 0.4565249352702019 grad: -0.1954219556268021
iteration: 0 loss: 55.19002770151689 grad: 460.835017540937
iteration: 10 loss: 2.500855129690825 grad: -0.20831996773269898
iteration: 20 loss: 1.457070185028401 grad: -0.44312289394979193
iteration: 30 loss: 1.0408306439967832 grad: -0.3968378428483478
iteration: 40 loss: 0.8128933753750971 grad: -0.34387938056196055
iteration: 50 loss: 0.6681483487262251 grad: -0.30064994986719673
iteration: 60 loss: 0.5677778142885059 grad: -0.2663659505688536
iteration: 70 loss: 0.4939621284479058 grad: -0.2389001090541866
iteration: 80 loss: 0.4373332922327377 grad: -0.21652376437957055
iteration: 0 loss: 37.27904401534972 grad: 444.70576123199135
iteration: 10 loss: 2.7943334425471718 grad: -0.5311042150267041
iteration: 20 loss: 1.591032615668155 grad: -0.5529375160873234
iteration: 30 loss: 1.1203142429129995 grad: -0.4548789358037636
iteration: 40 loss: 0.8667671933396212 grad: -0.38002427065955713
iteration: 50 loss: 0.707700448610715 grad: -0.32530337795784725
iteration: 60 loss: 0.5984042157023478 grad: -0.284193607049698
iteration: 70 loss: 0.5185969570515723 grad: -0.2523291442892771
iteration: 80 loss: 0.4577222530288054 grad: -0.22694879618151664
iteration: 0 loss: 60.52665358855972 grad: 473.8997764819898
iteration: 10 loss: 2.4611625777033623 grad: 0.014213884783201475
iteration: 20 loss: 1.4400155433515713 grad: -0.32405759851809457
iteration: 30 loss: 1.0313874161288368 grad: -0.3212145879489049
iteration: 40 loss: 0.8069367767577453 grad: -0.28988889787118133
iteration: 50 loss: 0.6640806848068596 grad: -0.25928213911533454
iteration: 60 loss: 0.5648481601913524 grad: -0.23314911100405472
iteration: 70 loss: 0.4917705158365855 grad: -0.21132810897804855
iteration: 80 loss: 0.435647017527365 grad: -0.19306587327941005
iteration: 0 loss: 83.03652072323816 grad: 541.7268785299752
iteration: 10 loss: 2.073221155836522 grad: 0.05066489722449964
iteration: 20 loss: 1.2551360774187246 grad: -0.2684150926530709
iteration: 30 loss: 0.9193686444554756 grad: -0.2792365002859817
iteration: 40 loss: 0.7305053762083844 grad: -0.2590047283021281
iteration: 50 loss: 0.6080594668726256 grad: -0.23614062556962045
iteration: 60 loss: 0.5217559964593502 grad: -0.2154233238782063
iteration: 70 loss: 0.457445911682953 grad: -0.1974836943045385
iteration: 0 loss: 50.11274569935916 grad: 453.2328215196336
iteration: 10 loss: 2.62582753768873 grad: -0.07615300823645285
iteration: 20 loss: 1.5214821423158933 grad: -0.3563345964021939
iteration: 30 loss: 1.0811442284613226 grad: -0.3372546133517377
iteration: 40 loss: 0.8411881947508846 grad: -0.29888589281459055
iteration: 50 loss: 0.6894790600769573 grad: -0.26462154157754636
iteration: 60 loss: 0.5846569677162485 grad: -0.2363800067355632
iteration: 70 loss: 0.5077931801340376 grad: -0.21325563937954417
iteration: 80 loss: 0.44896846958884207 grad: -0.19414654360792116
iteration: 0 loss: 69.2903878217369 grad: 504.26497447607363
iteration: 10 loss: 2.0611633667808746 grad: -0.2755866865405263
iteration: 20 loss: 1.2475566812170407 grad: -0.4603220674723437
iteration: 30 loss: 0.9127664544566456 grad: -0.40375253044911374
iteration: 40 loss: 0.7244410386355494 grad: -0.34858003648263186
iteration: 50 loss: 0.6024166323661782 grad: -0.3048189455542075
iteration: 60 loss: 0.5164772121548925 grad: -0.2704107601380593
iteration: 70 loss: 0.4524897371686 grad: -0.2429130365130612
iteration: 0 loss: 54.860484525133955 grad: 534.5546764983974
iteration: 10 loss: 2.269105921371399 grad: -0.634427549427481
iteration: 20 loss: 1.357963470228986 grad: -0.5254742535743778
iteration: 30 loss: 0.9850631614146385 grad: -0.4249444898271333
iteration: 40 loss: 0.7771896569094927 grad: -0.355167320342777
iteration: 50 loss: 0.6434829498258213 grad: -0.3051452106007152
iteration: 60 loss: 0.5498581300209314 grad: -0.2677006714614356
iteration: 70 loss: 0.48046999055474193 grad: -0.2386382769848141
iteration: 0 loss: 48.62294030254241 grad: 384.997559566845
iteration: 10 loss: 2.7089946100780833 grad: 0.46805061843613704
iteration: 20 loss: 1.5478977543372656 grad: -0.07092903440140336
iteration: 30 loss: 1.0935719328585947 grad: -0.14547342123337276
iteration: 40 loss: 0.8482404269659376 grad: -0.15554547514014377
iteration: 50 loss: 0.6939557644712461 grad: -0.15080369167090638
iteration: 60 loss: 0.5877229416705632 grad: -0.14237608022531123
iteration: 70 loss: 0.510013160926147 grad: -0.1334321916328585
iteration: 80 loss: 0.4506466403217928 grad: -0.12495059942231523
iteration: 0 loss: 40.6278916736643 grad: 428.40571197495535
iteration: 10 loss: 2.7866542883772456 grad: -0.34139766508871316
iteration: 20 loss: 1.587078840315867 grad: -0.48012712891814646
iteration: 30 loss: 1.1183831228080303 grad: -0.4133584360812512
iteration: 40 loss: 0.865796748054229 grad: -0.3520659239588378
iteration: 50 loss: 0.7072371028984477 grad: -0.3047026106108735
iteration: 60 loss: 0.5982310979194484 grad: -0.26813222213983373
iteration: 70 loss: 0.518599631799506 grad: -0.23931227470696648
iteration: 80 loss: 0.45783563323609333 grad: -0.21609714846235123
iteration: 0 loss: 50.41304821299123 grad: 469.9962786449877
iteration: 10 loss: 2.583057140507084 grad: 0.23954388253391107
iteration: 20 loss: 1.493531344440395 grad: -0.19423503002949516
iteration: 30 loss: 1.0617940145206872 grad: -0.23659224527613662
iteration: 40 loss: 0.826695594168913 grad: -0.2287246877572657
iteration: 50 loss: 0.6780148253416717 grad: -0.21211449108031877
iteration: 60 loss: 0.5752331822696678 grad: -0.1951582376213763
iteration: 70 loss: 0.4998259663876855 grad: -0.17976251364514934
iteration: 80 loss: 0.442087587599169 grad: -0.16622058671188095
iteration: 0 loss: 55.43293611050856 grad: 457.2624027837424
iteration: 10 loss: 2.5132996663693037 grad: 0.05272607872930707
iteration: 20 loss: 1.4697803459522965 grad: -0.31027044728446884
iteration: 30 loss: 1.0506569653042872 grad: -0.31311463472574264
iteration: 40 loss: 0.8206473975746618 grad: -0.28367070014935813
iteration: 50 loss: 0.6744609310187777 grad: -0.25394183776668616
iteration: 60 loss: 0.5730557273144025 grad: -0.22833848601733353
iteration: 70 loss: 0.4984711015358698 grad: -0.20689649059113807
iteration: 80 loss: 0.4412530983890148 grad: -0.18893426688570608
iteration: 0 loss: 59.23423570726891 grad: 405.13738957433907
iteration: 10 loss: 2.594798259839246 grad: 0.5014117972370852
iteration: 20 loss: 1.5126036032372874 grad: -0.19168931883651763
iteration: 30 loss: 1.0785400945849852 grad: -0.25801249497584733
iteration: 40 loss: 0.840910198202602 grad: -0.25178438310504625
iteration: 50 loss: 0.6901997833610949 grad: -0.23317240914700754
iteration: 60 loss: 0.585836144474425 grad: -0.21376668063982093
iteration: 70 loss: 0.5091826986798961 grad: -0.1961368810697319
iteration: 80 loss: 0.45044506150919583 grad: -0.18068726903577098
iteration: 0 loss: 64.31870910156717 grad: 507.1957826493248
iteration: 10 loss: 2.2401937944089862 grad: -0.1323138436726844
iteration: 20 loss: 1.3310442684091042 grad: -0.37696347293865434
iteration: 30 loss: 0.9637785569529197 grad: -0.35396214788193214
iteration: 40 loss: 0.7598800604840686 grad: -0.31418049696850164
iteration: 50 loss: 0.6289662792313803 grad: -0.27887928363081305
iteration: 60 loss: 0.5373826264488922 grad: -0.24971699661178123
iteration: 70 loss: 0.4695431166448378 grad: -0.22575378415944625
iteration: 0 loss: 86.88747097098184 grad: 460.1389841306483
iteration: 10 loss: 2.196647575661039 grad: 0.5807134082071439
iteration: 20 loss: 1.3027341626353435 grad: -0.10757909540154312
iteration: 30 loss: 0.9435681551486926 grad: -0.19305084119797594
iteration: 40 loss: 0.7442602403320548 grad: -0.2022629448081043
iteration: 50 loss: 0.6162681508572363 grad: -0.19458797105376405
iteration: 60 loss: 0.5267011409043996 grad: -0.18298063293476663
iteration: 70 loss: 0.46033573693627905 grad: -0.1710555519403637
iteration: 0 loss: 60.36576625814893 grad: 494.0553220816945
iteration: 10 loss: 2.368686623423152 grad: -0.4180962941908424
iteration: 20 loss: 1.4020247566718353 grad: -0.5268777863667782
iteration: 30 loss: 1.0087464852820383 grad: -0.45041696479860627
iteration: 40 loss: 0.7911043633002506 grad: -0.3830779772661343
iteration: 50 loss: 0.6519926374228155 grad: -0.33136542819798653
iteration: 60 loss: 0.5550987658739772 grad: -0.2914859444814247
iteration: 70 loss: 0.4836097408483097 grad: -0.2600605626228283
iteration: 80 loss: 0.4286313994489319 grad: -0.234741787043575
iteration: 0 loss: 55.79838241858215 grad: 503.7693490184621
iteration: 10 loss: 2.341284340389765 grad: -0.27871411283929304
iteration: 20 loss: 1.3782529116252507 grad: -0.4329432747716383
iteration: 30 loss: 0.9924559028366854 grad: -0.384355183780304
iteration: 40 loss: 0.7797480040037705 grad: -0.33377902298674517
iteration: 50 loss: 0.6438462199876085 grad: -0.29286248496417455
iteration: 60 loss: 0.5491147701767741 grad: -0.26036873015678913
iteration: 70 loss: 0.479136287840199 grad: -0.23424380488341368
iteration: 0 loss: 98.85820547009602 grad: 565.3102491370382
iteration: 10 loss: 1.911277806343068 grad: 0.14804989692587317
iteration: 20 loss: 1.1316306388499386 grad: -0.3194310004551173
iteration: 30 loss: 0.8287961880738625 grad: -0.31991326654805213
iteration: 40 loss: 0.6605140647207008 grad: -0.2909457694830563
iteration: 50 loss: 0.5517339240449556 grad: -0.2622811024654239
iteration: 60 loss: 0.47504665844429506 grad: -0.23759638653060128
iteration: 70 loss: 0.4178168448746543 grad: -0.21680721478184012
iteration: 0 loss: 60.22442261293331 grad: 425.6654504530935
iteration: 10 loss: 2.5711956675705054 grad: 0.336559408399502
iteration: 20 loss: 1.4957173002976147 grad: -0.19834243432708173
iteration: 30 loss: 1.0664509814216052 grad: -0.24783392007738853
iteration: 40 loss: 0.8317484328236705 grad: -0.23983314742686063
iteration: 50 loss: 0.6829418586869245 grad: -0.2221170523019962
iteration: 60 loss: 0.5798931041202774 grad: -0.20403904388868713
iteration: 70 loss: 0.5041919777446641 grad: -0.18766961684733843
iteration: 80 loss: 0.44617033851673943 grad: -0.1733101182481901
iteration: 0 loss: 65.26756068833744 grad: 501.0490764642167
iteration: 10 loss: 2.2619493507868356 grad: -0.4229246514568328
iteration: 20 loss: 1.3447101022232097 grad: -0.5087316729401037
iteration: 30 loss: 0.9728927713149873 grad: -0.43648126693632905
iteration: 40 loss: 0.7665221989103553 grad: -0.3728096243617437
iteration: 50 loss: 0.6341077069573657 grad: -0.3236835394866032
iteration: 60 loss: 0.5415339649844927 grad: -0.2856360320649395
iteration: 70 loss: 0.47300009306416707 grad: -0.25553926512093994
iteration: 0 loss: 60.50824954610434 grad: 447.2650530424935
iteration: 10 loss: 2.4311454443935543 grad: 0.0836559493812903
iteration: 20 loss: 1.421273581003127 grad: -0.30008138953205754
iteration: 30 loss: 1.0176242092842254 grad: -0.3093682108882574
iteration: 40 loss: 0.7960118523408461 grad: -0.2831682976264396
iteration: 50 loss: 0.6550067754964899 grad: -0.255215246245154
iteration: 60 loss: 0.5570827688863408 grad: -0.2306342470580467
iteration: 70 loss: 0.4849813595021921 grad: -0.20979602527720512
iteration: 80 loss: 0.42961499243202445 grad: -0.19219120019682415
iteration: 0 loss: 52.725769826268625 grad: 411.69880221460664
iteration: 10 loss: 2.528903294685361 grad: 0.3485908354747424
iteration: 20 loss: 1.472552757711252 grad: -0.1674509112021435
iteration: 30 loss: 1.0503462754538553 grad: -0.22112404738300143
iteration: 40 loss: 0.8193404931593746 grad: -0.2170606165573764
iteration: 50 loss: 0.6728226727067191 grad: -0.20238385331093056
iteration: 60 loss: 0.5713361694670311 grad: -0.18665208760191102
iteration: 70 loss: 0.49677209071747214 grad: -0.17213286376882272
iteration: 80 loss: 0.4396165764421973 grad: -0.15926557661251103
iteration: 0 loss: 69.7426012169483 grad: 416.44894067538934
iteration: 10 loss: 2.24893752936036 grad: 0.7365723981765664
iteration: 20 loss: 1.3366338776464048 grad: -0.009859022846099606
iteration: 30 loss: 0.968255404451487 grad: -0.12789616978755874
iteration: 40 loss: 0.7636546877939426 grad: -0.15374983058793387
iteration: 50 loss: 0.6322363745374869 grad: -0.15601751973177908
iteration: 60 loss: 0.5402708474989182 grad: -0.15099967315424528
iteration: 70 loss: 0.472132560622082 grad: -0.1437606425969153
iteration: 0 loss: 43.52864096183562 grad: 413.1879398712228
iteration: 10 loss: 2.6842527864080448 grad: 0.3197479767095987
iteration: 20 loss: 1.5355441252284188 grad: -0.23950580645398215
iteration: 30 loss: 1.0858366629764864 grad: -0.27865397005508863
iteration: 40 loss: 0.8426283067517695 grad: -0.26245101048346375
iteration: 50 loss: 0.6895177547960636 grad: -0.239198081027043
iteration: 60 loss: 0.5840203312042253 grad: -0.21732457833223673
iteration: 70 loss: 0.5068129618525332 grad: -0.1982634118704275
iteration: 80 loss: 0.4478119856037004 grad: -0.18193144339457726
iteration: 0 loss: 51.952385187465715 grad: 486.17185247103964
iteration: 10 loss: 2.5002445220522778 grad: -0.3476588197020899
iteration: 20 loss: 1.463841885751332 grad: -0.48569745830069794
iteration: 30 loss: 1.04618819588243 grad: -0.4201134110038409
iteration: 40 loss: 0.8168783323589692 grad: -0.35882967320932835
iteration: 50 loss: 0.6711500096526751 grad: -0.31109710869230733
iteration: 60 loss: 0.5700856297235904 grad: -0.27406699739951745
iteration: 70 loss: 0.495770497031365 grad: -0.2447930388442089
iteration: 80 loss: 0.438773231324624 grad: -0.2211600331719095
iteration: 0 loss: 91.12879865243836 grad: 492.7016385230078
iteration: 10 loss: 2.109817595871722 grad: 0.1992032594780636
iteration: 20 loss: 1.2692858540758194 grad: -0.23661729746221571
iteration: 30 loss: 0.9278212739347758 grad: -0.2668575206311369
iteration: 40 loss: 0.7364893614845972 grad: -0.2530846145029
iteration: 50 loss: 0.6126761642949119 grad: -0.23307201377518919
iteration: 60 loss: 0.5255052513945305 grad: -0.2138393782609802
iteration: 70 loss: 0.46059617764595634 grad: -0.19675834118296875
iteration: 0 loss: 72.5707813018846 grad: 538.7700450621742
iteration: 10 loss: 2.07944211785984 grad: -0.38345344627817135
iteration: 20 loss: 1.251952477470571 grad: -0.48515868046459065
iteration: 30 loss: 0.9150291214625047 grad: -0.41202084189727506
iteration: 40 loss: 0.7263214253524563 grad: -0.3512913921369215
iteration: 50 loss: 0.604291152291146 grad: -0.30532453624004174
iteration: 60 loss: 0.5184196799585731 grad: -0.2699486443993404
iteration: 70 loss: 0.45449802985116217 grad: -0.2420082319171914
iteration: 0 loss: 82.52447516651881 grad: 512.6167961716194
iteration: 10 loss: 2.1438391497020515 grad: 0.14021259967717864
iteration: 20 loss: 1.2751179220715791 grad: -0.24893814299585518
iteration: 30 loss: 0.9275828246747385 grad: -0.26614301720350697
iteration: 40 loss: 0.7343876724421947 grad: -0.24851134573540284
iteration: 50 loss: 0.609991200321102 grad: -0.22718723258677678
iteration: 60 loss: 0.5227028164696425 grad: -0.2075734757717791
iteration: 70 loss: 0.45785878059152973 grad: -0.19049794532972286
iteration: 0 loss: 63.442811754828114 grad: 489.7358291569867
iteration: 10 loss: 2.333762750816525 grad: -0.3655189939882183
iteration: 20 loss: 1.3798767961434764 grad: -0.4156025711418212
iteration: 30 loss: 0.9947873959203704 grad: -0.36144563111284134
iteration: 40 loss: 0.781890083192965 grad: -0.31241809315835567
iteration: 50 loss: 0.6456912978479853 grad: -0.2737153178056294
iteration: 60 loss: 0.5506892335772797 grad: -0.24321952371426436
iteration: 70 loss: 0.4804861584721452 grad: -0.21877948692705343
iteration: 0 loss: 87.97245759107119 grad: 511.8940965434201
iteration: 10 loss: 2.132660221009411 grad: 0.04784551052363375
iteration: 20 loss: 1.2732536227775364 grad: -0.28829620377032694
iteration: 30 loss: 0.9274613344892217 grad: -0.2922982749685488
iteration: 40 loss: 0.7347963018575517 grad: -0.2675466639769174
iteration: 50 loss: 0.6105715838998136 grad: -0.2417981101249382
iteration: 60 loss: 0.5233234030627666 grad: -0.21918958585542014
iteration: 70 loss: 0.4584667947434685 grad: -0.19997084734082685
iteration: 0 loss: 74.04016025632755 grad: 513.639726777766
iteration: 10 loss: 2.2332225131990113 grad: 0.013456271383719326
iteration: 20 loss: 1.3214172705011564 grad: -0.29286790865354917
iteration: 30 loss: 0.956462692959204 grad: -0.2942437878328116
iteration: 40 loss: 0.7545238956134365 grad: -0.2685969140685567
iteration: 50 loss: 0.6250345651855738 grad: -0.2424632470344782
iteration: 60 loss: 0.5344803584616371 grad: -0.21967731721819458
iteration: 70 loss: 0.46739721918964444 grad: -0.20037347409238787
iteration: 0 loss: 94.8715035894019 grad: 441.49048879682874
iteration: 10 loss: 2.2018437695616573 grad: 1.161005800455357
iteration: 20 loss: 1.3135177316680298 grad: 0.18723594108810016
iteration: 30 loss: 0.9546832074903581 grad: -0.010455303155179884
iteration: 40 loss: 0.7549279608614267 grad: -0.07477482468395852
iteration: 50 loss: 0.6263092466442686 grad: -0.09881687080682983
iteration: 60 loss: 0.5361015311989997 grad: -0.10741513375109366
iteration: 70 loss: 0.46913326932099153 grad: -0.10929889266486545
iteration: 0 loss: 121.52175132064858 grad: 526.8971333226737
iteration: 10 loss: 1.9871509221745576 grad: 0.7360194209646118
iteration: 20 loss: 1.1798918098397735 grad: -0.0011480648743792364
iteration: 30 loss: 0.86237805384115 grad: -0.12093412313391692
iteration: 40 loss: 0.6859688664081577 grad: -0.15128952812423271
iteration: 50 loss: 0.5721132786508282 grad: -0.15680759005286593
iteration: 60 loss: 0.49197656523764716 grad: -0.15388988310720014
iteration: 70 loss: 0.4322582883179083 grad: -0.14797928865656845
iteration: 0 loss: 96.56475205962148 grad: 468.1726677484719
iteration: 10 loss: 2.1156824729287522 grad: 0.5169531655035553
iteration: 20 loss: 1.270738503880156 grad: -0.14508397422901226
iteration: 30 loss: 0.9274626629876779 grad: -0.22251258360743215
iteration: 40 loss: 0.7352066698262703 grad: -0.22645990883298128
iteration: 50 loss: 0.6109104944120687 grad: -0.214978778558983
iteration: 60 loss: 0.523488171588476 grad: -0.2005081854087877
iteration: 70 loss: 0.45845533097450575 grad: -0.1863644526415414
iteration: 0 loss: 45.1485947493891 grad: 394.64432097666895
iteration: 10 loss: 2.7128525940938744 grad: 0.2954040733966863
iteration: 20 loss: 1.5529249972318295 grad: -0.2639230551237213
iteration: 30 loss: 1.0981675423332407 grad: -0.29573323299308935
iteration: 40 loss: 0.8521357508857363 grad: -0.27502711430999904
iteration: 50 loss: 0.697232747138969 grad: -0.24893977728266198
iteration: 60 loss: 0.5905002165496956 grad: -0.22516890166036183
iteration: 70 loss: 0.5123918595450463 grad: -0.2047685649626317
iteration: 80 loss: 0.4527055032566833 grad: -0.18745048610327575
iteration: 0 loss: 49.69103278117332 grad: 371.2160510007875
iteration: 10 loss: 2.6619512791160314 grad: 0.6244050575301195
iteration: 20 loss: 1.5260291777213255 grad: -0.08661277638640064
iteration: 30 loss: 1.0805620567286744 grad: -0.1818457609716364
iteration: 40 loss: 0.8393327018178008 grad: -0.19350232576428056
iteration: 50 loss: 0.6873123254770855 grad: -0.1865083062411696
iteration: 60 loss: 0.5824794217628964 grad: -0.17513724164249345
iteration: 70 loss: 0.5057060461919967 grad: -0.16334980889531742
iteration: 80 loss: 0.4470030935161214 grad: -0.15231767144561828
iteration: 0 loss: 61.17384042903429 grad: 412.3947760708165
iteration: 10 loss: 2.552655810611792 grad: 0.7259993436383617
iteration: 20 loss: 1.483988804772583 grad: -0.02486134274690825
iteration: 30 loss: 1.0581708240909145 grad: -0.14166084199103182
iteration: 40 loss: 0.8254608691332909 grad: -0.16481011520391306
iteration: 50 loss: 0.6779240350579543 grad: -0.16471111988631082
iteration: 60 loss: 0.5757449742330375 grad: -0.15784439802766875
iteration: 70 loss: 0.5006723635112642 grad: -0.14918660854378377
iteration: 80 loss: 0.4431239174247656 grad: -0.14043205203714587
iteration: 0 loss: 59.160569505644595 grad: 481.27514252955075
iteration: 10 loss: 2.4708065657751335 grad: -0.011812252875691386
iteration: 20 loss: 1.452040614899488 grad: -0.29006745287989466
iteration: 30 loss: 1.0415969715737778 grad: -0.2905312851642803
iteration: 40 loss: 0.8156083217747685 grad: -0.2647753556298908
iteration: 50 loss: 0.6715830833190921 grad: -0.23857167564010795
iteration: 60 loss: 0.5714516069402552 grad: -0.21574051386759335
iteration: 70 loss: 0.49766579911523545 grad: -0.1964228613628801
iteration: 80 loss: 0.44097129501328297 grad: -0.18009956635507446
iteration: 0 loss: 60.03675352705213 grad: 448.3331188309307
iteration: 10 loss: 2.351639989421983 grad: 0.10310129822314656
iteration: 20 loss: 1.3837223663383817 grad: -0.2541221566668024
iteration: 30 loss: 0.995727746906265 grad: -0.2745817128161717
iteration: 40 loss: 0.7819076799175739 grad: -0.25708829998098615
iteration: 50 loss: 0.6453670887214519 grad: -0.23506828309576244
iteration: 60 loss: 0.550235375550735 grad: -0.21459218368565108
iteration: 70 loss: 0.4799900450393166 grad: -0.1966872572149706
iteration: 0 loss: 50.436157977005514 grad: 433.45192982220965
iteration: 10 loss: 2.5768299414612144 grad: -0.14863095017795624
iteration: 20 loss: 1.4923015715098287 grad: -0.38632374013400345
iteration: 30 loss: 1.0613398221793515 grad: -0.3560236739132081
iteration: 40 loss: 0.8264400976903413 grad: -0.3125615095483151
iteration: 50 loss: 0.6778207849659504 grad: -0.2753859982799336
iteration: 60 loss: 0.5750586794933539 grad: -0.2452550515399078
iteration: 70 loss: 0.49965668426077836 grad: -0.22080313207077906
iteration: 80 loss: 0.4419185963704215 grad: -0.2007093700380382
iteration: 0 loss: 47.304634120179465 grad: 418.95654394639996
iteration: 10 loss: 2.716604714755621 grad: 0.06668910334827055
iteration: 20 loss: 1.5571229443382684 grad: -0.28212538777165563
iteration: 30 loss: 1.101159303628306 grad: -0.28774148411996814
iteration: 40 loss: 0.854466001291588 grad: -0.26239156656136753
iteration: 50 loss: 0.6991713792716506 grad: -0.23609484992992405
iteration: 60 loss: 0.5921818765630384 grad: -0.21318378565696985
iteration: 70 loss: 0.513890728354206 grad: -0.1938491887694335
iteration: 80 loss: 0.45406631228682026 grad: -0.17755717977082477
iteration: 0 loss: 72.51768815854547 grad: 537.9769893702792
iteration: 10 loss: 2.0740458961891735 grad: -0.42964964092129043
iteration: 20 loss: 1.2591026405780625 grad: -0.43966191122788395
iteration: 30 loss: 0.9232976231153522 grad: -0.38264705565747936
iteration: 40 loss: 0.7340198574238457 grad: -0.3313456680258039
iteration: 50 loss: 0.6111541850545605 grad: -0.2907723337716177
iteration: 60 loss: 0.5244875602259278 grad: -0.2587516875453464
iteration: 70 loss: 0.4598737175457989 grad: -0.23305220852152675
iteration: 0 loss: 70.68440038676503 grad: 482.8458142020645
iteration: 10 loss: 2.3056348781077842 grad: 0.11569724154419557
iteration: 20 loss: 1.3621034192416863 grad: -0.2560759871247526
iteration: 30 loss: 0.9829751372581913 grad: -0.2729646221421902
iteration: 40 loss: 0.7732827525463687 grad: -0.2539280889385747
iteration: 50 loss: 0.639019954400696 grad: -0.23127561870547103
iteration: 60 loss: 0.5452933606046909 grad: -0.21057915725654458
iteration: 70 loss: 0.4759850272949651 grad: -0.1926519726306644
iteration: 0 loss: 86.52386121538451 grad: 464.8106604210746
iteration: 10 loss: 2.228310469917439 grad: 0.629188691956509
iteration: 20 loss: 1.3281467853726925 grad: -0.08430938201941811
iteration: 30 loss: 0.963280657943828 grad: -0.18232980251760922
iteration: 40 loss: 0.7601411389282888 grad: -0.19655651867938112
iteration: 50 loss: 0.629480354578432 grad: -0.19118219796485736
iteration: 60 loss: 0.537968524115389 grad: -0.18073956619783116
iteration: 70 loss: 0.4701307977027847 grad: -0.1694456898865775
iteration: 0 loss: 58.81121036686045 grad: 462.13827693395893
iteration: 10 loss: 2.5199165471684535 grad: -0.16337783692149
iteration: 20 loss: 1.474563341993003 grad: -0.4075725808054117
iteration: 30 loss: 1.0535405698361004 grad: -0.3726404674886262
iteration: 40 loss: 0.8225051529329903 grad: -0.32593777108617195
iteration: 50 loss: 0.6757379672869154 grad: -0.2865278379033284
iteration: 60 loss: 0.5739802155177048 grad: -0.2547829172970542
iteration: 70 loss: 0.49916828837500693 grad: -0.22911530335818114
iteration: 80 loss: 0.44179636400506506 grad: -0.2080745759841004
iteration: 0 loss: 68.13320201862349 grad: 390.8794645132899
iteration: 10 loss: 2.456965299898746 grad: 0.9303323347460841
iteration: 20 loss: 1.4448866317963869 grad: -0.004756699969006094
iteration: 30 loss: 1.036871183392453 grad: -0.14866719120707922
iteration: 40 loss: 0.811957929662061 grad: -0.17797268989238185
iteration: 50 loss: 0.6685296380579447 grad: -0.17898873882468266
iteration: 60 loss: 0.5687840040338249 grad: -0.17174215877267554
iteration: 70 loss: 0.4952740022366594 grad: -0.1622520223795901
iteration: 80 loss: 0.4387904018708882 grad: -0.15256551866982465
iteration: 0 loss: 70.92237432539423 grad: 476.9721889597196
iteration: 10 loss: 2.241305449718931 grad: -0.13673319649456248
iteration: 20 loss: 1.3395153633165882 grad: -0.3682691404360824
iteration: 30 loss: 0.9709996372542016 grad: -0.34158788997998263
iteration: 40 loss: 0.7656576676477016 grad: -0.30179681602927344
iteration: 50 loss: 0.6336282427966414 grad: -0.267343454816254
iteration: 60 loss: 0.5412127679158154 grad: -0.23916566355279076
iteration: 70 loss: 0.47274648332652514 grad: -0.21613075927725914
iteration: 0 loss: 48.56339696340633 grad: 438.68091339069116
iteration: 10 loss: 2.651040232067027 grad: 0.2053736872618069
iteration: 20 loss: 1.529542561774261 grad: -0.23900164119564024
iteration: 30 loss: 1.0858218081306072 grad: -0.26823469324366
iteration: 40 loss: 0.844584111495475 grad: -0.251895421424448
iteration: 50 loss: 0.6922057180517682 grad: -0.22984954006251665
iteration: 60 loss: 0.5869649597625539 grad: -0.20921820437669653
iteration: 70 loss: 0.5098091597106262 grad: -0.19121621961818258
iteration: 80 loss: 0.45076611003853695 grad: -0.17575278315538836
iteration: 0 loss: 43.21584545281774 grad: 461.6763307086561
iteration: 10 loss: 2.620003022706855 grad: -0.4982320614739605
iteration: 20 loss: 1.509396672120519 grad: -0.5671013430869674
iteration: 30 loss: 1.070818029635699 grad: -0.47688153493410124
iteration: 40 loss: 0.8325121809237482 grad: -0.402396585125282
iteration: 50 loss: 0.6820462609481927 grad: -0.34640674133036614
iteration: 60 loss: 0.5781620831361248 grad: -0.3037200256337069
iteration: 70 loss: 0.5020241801853857 grad: -0.27032737894965636
iteration: 80 loss: 0.44377617199047975 grad: -0.2435612486382151
iteration: 0 loss: 70.50483457872033 grad: 579.2968819623634
iteration: 10 loss: 1.9590689895791746 grad: -0.560631695179263
iteration: 20 loss: 1.1834337201604201 grad: -0.5292535904095155
iteration: 30 loss: 0.869905308397971 grad: -0.4356629807792964
iteration: 40 loss: 0.6936449866847066 grad: -0.3679898775100089
iteration: 50 loss: 0.5791500871583511 grad: -0.3185307915954452
iteration: 60 loss: 0.49825209092176176 grad: -0.28101503905965297
iteration: 70 loss: 0.4378199127445855 grad: -0.2516083789504833
iteration: 0 loss: 51.894585626300255 grad: 412.70420318334243
iteration: 10 loss: 2.587024613827204 grad: 0.11395149482216772
iteration: 20 loss: 1.5052563645628487 grad: -0.27050724437130796
iteration: 30 loss: 1.073472920471142 grad: -0.28642984511900915
iteration: 40 loss: 0.8373656800778752 grad: -0.264505196401461
iteration: 50 loss: 0.6876409497228699 grad: -0.23943382363273724
iteration: 60 loss: 0.5839380075449299 grad: -0.21692190151305762
iteration: 70 loss: 0.5077446740650967 grad: -0.19764531216013587
iteration: 80 loss: 0.44933833611693524 grad: -0.18126459256077124
iteration: 0 loss: 43.973552207943655 grad: 396.743576410636
iteration: 10 loss: 2.6934979691138157 grad: 0.1966229610879796
iteration: 20 loss: 1.54011121448585 grad: -0.2478532459710472
iteration: 30 loss: 1.0886173922340276 grad: -0.2744110931118996
iteration: 40 loss: 0.8445560902843102 grad: -0.2561763645443027
iteration: 50 loss: 0.6909660255602664 grad: -0.23293146821159216
iteration: 60 loss: 0.5851684169428236 grad: -0.21152603345603793
iteration: 70 loss: 0.507758444673858 grad: -0.1930051135729056
iteration: 80 loss: 0.44861293430552573 grad: -0.17718079201066447
iteration: 0 loss: 46.403224103741856 grad: 420.5438372091997
iteration: 10 loss: 2.6811668527187646 grad: 0.3774774859601271
iteration: 20 loss: 1.5386180386739894 grad: -0.15293634948272106
iteration: 30 loss: 1.0893767624933886 grad: -0.21170344834050958
iteration: 40 loss: 0.8459989345601571 grad: -0.2101370271158391
iteration: 50 loss: 0.6926220566670881 grad: -0.19692334346967763
iteration: 60 loss: 0.5868646336881614 grad: -0.18215012729756655
iteration: 70 loss: 0.5094250428791466 grad: -0.16830778412318814
iteration: 80 loss: 0.450221451522752 grad: -0.1559443116519724
iteration: 0 loss: 50.05524246251587 grad: 469.6203869564282
iteration: 10 loss: 2.5762532253398933 grad: -0.6933331513187626
iteration: 20 loss: 1.4947421307789972 grad: -0.6531167490151144
iteration: 30 loss: 1.0633152670218557 grad: -0.5272555633867412
iteration: 40 loss: 0.8279103217588603 grad: -0.4367069749996506
iteration: 50 loss: 0.6789196848689326 grad: -0.3718357653720232
iteration: 60 loss: 0.5758916157532109 grad: -0.3236056535766004
iteration: 70 loss: 0.5002959922348137 grad: -0.28646728837087504
iteration: 80 loss: 0.4424137605883516 grad: -0.25702434969907717
iteration: 0 loss: 72.20730641968952 grad: 482.97736396781323
iteration: 10 loss: 2.175478808187067 grad: 0.09614676310953366
iteration: 20 loss: 1.308572199524943 grad: -0.25730283886458305
iteration: 30 loss: 0.9528606308374286 grad: -0.27141280289435843
iteration: 40 loss: 0.7536782980264847 grad: -0.2523970015175347
iteration: 50 loss: 0.6251126678639181 grad: -0.23018417556627555
iteration: 60 loss: 0.534848618181472 grad: -0.20991309486829518
iteration: 70 loss: 0.4678141918625029 grad: -0.19232584365809127
iteration: 0 loss: 60.8320901577488 grad: 520.6434409989295
iteration: 10 loss: 2.2068568150974155 grad: -0.5766663540643691
iteration: 20 loss: 1.3199202737738978 grad: -0.5337917618733335
iteration: 30 loss: 0.9588896827074016 grad: -0.44394714305392147
iteration: 40 loss: 0.7575970547393974 grad: -0.3757659443737482
iteration: 50 loss: 0.6279827965390723 grad: -0.3250979661237118
iteration: 60 loss: 0.5371156761065404 grad: -0.28642786701377276
iteration: 70 loss: 0.469696661521084 grad: -0.2560519685618452
iteration: 0 loss: 63.03468792211532 grad: 513.1761353789695
iteration: 10 loss: 2.1494412961255462 grad: -0.5115699129479314
iteration: 20 loss: 1.2751071890291066 grad: -0.5508343109792285
iteration: 30 loss: 0.9243921585502903 grad: -0.4614424957691557
iteration: 40 loss: 0.7298875379924126 grad: -0.3902970289411476
iteration: 50 loss: 0.6049919014493009 grad: -0.3371568775308891
iteration: 60 loss: 0.5175684359537627 grad: -0.2966314467826935
iteration: 70 loss: 0.4527601712029123 grad: -0.2648556391509106
iteration: 0 loss: 55.59586688070588 grad: 489.801846140513
iteration: 10 loss: 2.3338268210001263 grad: -0.4720781002594131
iteration: 20 loss: 1.3682446588621298 grad: -0.5410314939891365
iteration: 30 loss: 0.9826671979160231 grad: -0.45746559466450554
iteration: 40 loss: 0.7706337182091209 grad: -0.38800032753287617
iteration: 50 loss: 0.635444815599261 grad: -0.3354987582722174
iteration: 60 loss: 0.5413693383530203 grad: -0.2952549187952742
iteration: 70 loss: 0.4719721609678593 grad: -0.2636186827483147
iteration: 0 loss: 58.77041915426273 grad: 463.07990378745455
iteration: 10 loss: 2.4352756965816464 grad: -0.04191150021173246
iteration: 20 loss: 1.4245095155461471 grad: -0.3665772385617632
iteration: 30 loss: 1.0197100095479072 grad: -0.35420773570233044
iteration: 40 loss: 0.797448512472122 grad: -0.316281472182648
iteration: 50 loss: 0.6560531299276342 grad: -0.28108243368356794
iteration: 60 loss: 0.5578767606771593 grad: -0.25163561471492163
iteration: 70 loss: 0.4856025107826347 grad: -0.22733338168739303
iteration: 80 loss: 0.4301123772302162 grad: -0.2071531905212945
iteration: 0 loss: 91.4357288082088 grad: 528.9538547043811
iteration: 10 loss: 1.8818627683768185 grad: -0.3017429446779823
iteration: 20 loss: 1.13907991268996 grad: -0.45701478531368556
iteration: 30 loss: 0.8413370605282828 grad: -0.3945413996643187
iteration: 40 loss: 0.6734277322803709 grad: -0.3402676064589761
iteration: 50 loss: 0.5639519290430557 grad: -0.2981289034250154
iteration: 60 loss: 0.4863401052841503 grad: -0.26516713022433247
iteration: 70 loss: 0.42819337497274484 grad: -0.23883537467512228
iteration: 0 loss: 72.23193489347614 grad: 487.80915943866154
iteration: 10 loss: 2.3336307872385658 grad: 0.2721467855487139
iteration: 20 loss: 1.372474970507028 grad: -0.15110234108607123
iteration: 30 loss: 0.9882310599939708 grad: -0.20311768266343944
iteration: 40 loss: 0.7765037216215867 grad: -0.20321819891680598
iteration: 50 loss: 0.6412503346728045 grad: -0.19225046489685882
iteration: 60 loss: 0.5469722857957425 grad: -0.1793093571401204
iteration: 70 loss: 0.47732592856978456 grad: -0.1668456494591549
iteration: 0 loss: 40.16272063201743 grad: 457.29669068019916
iteration: 10 loss: 2.648971563483056 grad: -0.7874897667783719
iteration: 20 loss: 1.5214103712997216 grad: -0.6850949171513219
iteration: 30 loss: 1.0758820839777483 grad: -0.5436292480241631
iteration: 40 loss: 0.8344887524494466 grad: -0.4464118051619747
iteration: 50 loss: 0.6824802972705997 grad: -0.37810029641213716
iteration: 60 loss: 0.5777633040755642 grad: -0.3278827980780227
iteration: 70 loss: 0.5011540514319264 grad: -0.2895040550394287
iteration: 80 loss: 0.4426331496612296 grad: -0.25924188106679713
iteration: 0 loss: 63.061617529880714 grad: 458.86969778560865
iteration: 10 loss: 2.4546514889185262 grad: 0.008418265752855963
iteration: 20 loss: 1.4457172125355762 grad: -0.31310115355653534
iteration: 30 loss: 1.0366875389184893 grad: -0.31077444512664343
iteration: 40 loss: 0.8111270982361599 grad: -0.2808851023740271
iteration: 50 loss: 0.6673624549855297 grad: -0.2515311175437762
iteration: 60 loss: 0.5674528485524206 grad: -0.22640592843760743
iteration: 70 loss: 0.49387236571775167 grad: -0.20539272525808394
iteration: 80 loss: 0.4373697613299638 grad: -0.1877851000394739
iteration: 0 loss: 54.45405418317953 grad: 450.20554888676446
iteration: 10 loss: 2.5094444706598797 grad: 0.14846036559933154
iteration: 20 loss: 1.4602932040020278 grad: -0.3274456666348927
iteration: 30 loss: 1.0419496270119453 grad: -0.3361734089707458
iteration: 40 loss: 0.8130004056600394 grad: -0.30493099377968075
iteration: 50 loss: 0.66772642655947 grad: -0.27278042066267627
iteration: 60 loss: 0.567066401426597 grad: -0.24503934902650892
iteration: 70 loss: 0.493089779792585 grad: -0.22182249959887218
iteration: 80 loss: 0.4363731024981131 grad: -0.20239546327808433
iteration: 0 loss: 62.72950516440919 grad: 530.4868288965167
iteration: 10 loss: 2.238946905819127 grad: -0.6011580321102181
iteration: 20 loss: 1.3285815638692755 grad: -0.6052792029614412
iteration: 30 loss: 0.9622323741329886 grad: -0.49178708226405676
iteration: 40 loss: 0.758985375150091 grad: -0.4099040333521388
iteration: 50 loss: 0.6284983128649732 grad: -0.3509218071772305
iteration: 60 loss: 0.5371982952590109 grad: -0.3068127199580155
iteration: 70 loss: 0.46955190575293027 grad: -0.272660606023694
iteration: 0 loss: 49.685565105336245 grad: 459.72461706499973
iteration: 10 loss: 2.6274841427464874 grad: -0.392772617280377
iteration: 20 loss: 1.514682241938813 grad: -0.5263610394798119
iteration: 30 loss: 1.0741549744540697 grad: -0.4460505537940167
iteration: 40 loss: 0.8347464665875205 grad: -0.3771892585729927
iteration: 50 loss: 0.683628669203906 grad: -0.32499604818557104
iteration: 60 loss: 0.579334484070304 grad: -0.28507631649498266
iteration: 70 loss: 0.5029246204978997 grad: -0.25380177945591575
iteration: 80 loss: 0.4444882217221675 grad: -0.22871482861231743
iteration: 0 loss: 67.78941063111225 grad: 437.3053847083147
iteration: 10 loss: 2.379375540464025 grad: 0.21445969602066814
iteration: 20 loss: 1.4071492103743035 grad: -0.21525745408070757
iteration: 30 loss: 1.0141182003040243 grad: -0.25143621808700667
iteration: 40 loss: 0.7966643424814802 grad: -0.2401153373816743
iteration: 50 loss: 0.65753858242004 grad: -0.22143295878850566
iteration: 60 loss: 0.5605164279294595 grad: -0.20310939109075826
iteration: 70 loss: 0.4888454667144205 grad: -0.18674037159616816
iteration: 80 loss: 0.4336644118662992 grad: -0.17246510601487658
iteration: 0 loss: 45.61046962564883 grad: 385.90577177152966
iteration: 10 loss: 2.651137985624411 grad: 0.363512552448887
iteration: 20 loss: 1.5237203512708055 grad: -0.28320207918468265
iteration: 30 loss: 1.0799664753469436 grad: -0.3208434353338824
iteration: 40 loss: 0.8391506208127297 grad: -0.29837559626526333
iteration: 50 loss: 0.6872189727986221 grad: -0.26965409496121023
iteration: 60 loss: 0.5823822961275998 grad: -0.2434593080528126
iteration: 70 loss: 0.5055797983539544 grad: -0.2210044850635196
iteration: 80 loss: 0.44684375884480687 grad: -0.20197445571427353
iteration: 0 loss: 78.3737748094224 grad: 504.94248983840225
iteration: 10 loss: 2.179123865009718 grad: 0.025802699549861592
iteration: 20 loss: 1.3082042814074322 grad: -0.3416621663511892
iteration: 30 loss: 0.952027453870745 grad: -0.3386973307297261
iteration: 40 loss: 0.7527443870084508 grad: -0.30693284118754105
iteration: 50 loss: 0.624165078489651 grad: -0.2756308579364327
iteration: 60 loss: 0.5339168142075075 grad: -0.24867927416659052
iteration: 70 loss: 0.4669095161721502 grad: -0.2260140025768222
iteration: 0 loss: 61.252058886487 grad: 488.85048210657226
iteration: 10 loss: 2.4136077078603857 grad: -0.22124459097891702
iteration: 20 loss: 1.4190152435162005 grad: -0.40837091714935225
iteration: 30 loss: 1.018387846389363 grad: -0.3683542123417698
iteration: 40 loss: 0.7976819856177247 grad: -0.32137608934277
iteration: 50 loss: 0.6569665502634465 grad: -0.2824426924344302
iteration: 60 loss: 0.5591074198251094 grad: -0.2512481415828898
iteration: 70 loss: 0.4869798190629808 grad: -0.22607012710923374
iteration: 80 loss: 0.431549365006669 grad: -0.2054396077408207
iteration: 0 loss: 59.968624451086065 grad: 479.92079209973673
iteration: 10 loss: 2.4154473854242 grad: 0.04116590634330818
iteration: 20 loss: 1.4156683601172517 grad: -0.3327680293481544
iteration: 30 loss: 1.0152694178049055 grad: -0.3337018381393151
iteration: 40 loss: 0.795093469719797 grad: -0.30203029463652636
iteration: 50 loss: 0.6548226295426502 grad: -0.27043135250435296
iteration: 60 loss: 0.5573054139099027 grad: -0.24329332871270068
iteration: 70 loss: 0.48544001356061595 grad: -0.2205783805124929
iteration: 80 loss: 0.4302136177418354 grad: -0.20154468484022794
iteration: 0 loss: 113.3406812411586 grad: 501.7238791462942
iteration: 10 loss: 2.044271747912758 grad: 0.39768818434435693
iteration: 20 loss: 1.2160202612783244 grad: -0.14604840342311215
iteration: 30 loss: 0.8864894027530515 grad: -0.20553546465409997
iteration: 40 loss: 0.7034127217283722 grad: -0.20673384597158417
iteration: 50 loss: 0.5854451883451475 grad: -0.19603986553550398
iteration: 60 loss: 0.5025701046599186 grad: -0.18319204246773635
iteration: 70 loss: 0.4409227049347299 grad: -0.1707535015248643
iteration: 0 loss: 49.258135010017504 grad: 452.19258008819156
iteration: 10 loss: 2.5384924801259414 grad: -0.20477043320642488
iteration: 20 loss: 1.4751224258161941 grad: -0.44142493928116266
iteration: 30 loss: 1.0510122759220726 grad: -0.4009147511082612
iteration: 40 loss: 0.8193063911605184 grad: -0.3497742777274756
iteration: 50 loss: 0.6724806940155759 grad: -0.3070394069052185
iteration: 60 loss: 0.5708460718134037 grad: -0.2727546621521749
iteration: 70 loss: 0.4962086590050438 grad: -0.2450936804106829
iteration: 80 loss: 0.43901816939433047 grad: -0.22244971511161016
iteration: 0 loss: 67.71029323952479 grad: 473.0416401305709
iteration: 10 loss: 2.220852857605347 grad: -0.20840373596925754
iteration: 20 loss: 1.3261768198644859 grad: -0.4212474649984654
iteration: 30 loss: 0.9622660906583488 grad: -0.386863797278901
iteration: 40 loss: 0.759612701142859 grad: -0.34057996894852843
iteration: 50 loss: 0.6292477777606109 grad: -0.30094244279942156
iteration: 60 loss: 0.5379240031067887 grad: -0.26864631298650316
iteration: 70 loss: 0.47020834442760545 grad: -0.2423026635839599
iteration: 0 loss: 106.30132867707636 grad: 531.3807300790379
iteration: 10 loss: 1.9651688437485808 grad: 0.28619297332694404
iteration: 20 loss: 1.1817093580803801 grad: -0.2146125585888476
iteration: 30 loss: 0.8673046265636334 grad: -0.2539363810016442
iteration: 40 loss: 0.6910741814335779 grad: -0.24381243425548393
iteration: 50 loss: 0.5767716042307505 grad: -0.22577797181568418
iteration: 60 loss: 0.4960796828144422 grad: -0.20779066379320957
iteration: 70 loss: 0.4358341639626815 grad: -0.19156816335184895
iteration: 0 loss: 62.41294557200978 grad: 445.8301307136338
iteration: 10 loss: 2.4711034324177077 grad: 0.3693213869018269
iteration: 20 loss: 1.437120004271616 grad: -0.19750655627429284
iteration: 30 loss: 1.0275157741144043 grad: -0.2535091816090827
iteration: 40 loss: 0.8032528133580854 grad: -0.24706356600008456
iteration: 50 loss: 0.6607417590225437 grad: -0.22952428778174155
iteration: 60 loss: 0.5618420232609754 grad: -0.21119286111333052
iteration: 70 loss: 0.489054706365261 grad: -0.1944410694826165
iteration: 80 loss: 0.4331786530456983 grad: -0.17967637833185524
iteration: 0 loss: 71.70286403066528 grad: 479.0584487120336
iteration: 10 loss: 2.284492741979358 grad: 0.03352957938313196
iteration: 20 loss: 1.3555163197395566 grad: -0.23987173970524336
iteration: 30 loss: 0.9810813373112317 grad: -0.2489634331537845
iteration: 40 loss: 0.7735214419416149 grad: -0.23095455487631275
iteration: 50 loss: 0.6403522029843522 grad: -0.21069884784109635
iteration: 60 loss: 0.547218875750001 grad: -0.19237235354597787
iteration: 70 loss: 0.47823772703734657 grad: -0.17651196371761813
iteration: 0 loss: 56.600643190320994 grad: 456.519829222731
iteration: 10 loss: 2.5343918087188615 grad: -0.12521938230323587
iteration: 20 loss: 1.4727239543046629 grad: -0.43038144757153657
iteration: 30 loss: 1.0502670624906303 grad: -0.3979790748101105
iteration: 40 loss: 0.8193168723840824 grad: -0.34872115706070317
iteration: 50 loss: 0.6728539952032819 grad: -0.3064627257903729
iteration: 60 loss: 0.5714005270080882 grad: -0.2722660184120754
iteration: 70 loss: 0.49685339706816606 grad: -0.24458277234647616
iteration: 80 loss: 0.43970486425140815 grad: -0.221891367154122
iteration: 0 loss: 80.41805583584868 grad: 532.7590151681194
iteration: 10 loss: 2.070701609570733 grad: -0.14256711567067634
iteration: 20 loss: 1.255579160750048 grad: -0.35221221424523574
iteration: 30 loss: 0.9199713534618571 grad: -0.33396804527985313
iteration: 40 loss: 0.7309630583500124 grad: -0.29936258473482236
iteration: 50 loss: 0.6083565867756988 grad: -0.26787455291857026
iteration: 60 loss: 0.5219199762959761 grad: -0.24142800583490015
iteration: 70 loss: 0.4575057978739675 grad: -0.219420392816367
iteration: 0 loss: 78.68991131565161 grad: 502.6320834971681
iteration: 10 loss: 2.230822419567589 grad: 0.050668866817281416
iteration: 20 loss: 1.3383970211898073 grad: -0.2886172720536133
iteration: 30 loss: 0.9724110839797504 grad: -0.2988570558053068
iteration: 40 loss: 0.7679038007983184 grad: -0.2755694612163126
iteration: 50 loss: 0.6361449436285924 grad: -0.2498246010351859
iteration: 60 loss: 0.5437809471508941 grad: -0.22677602037295747
iteration: 70 loss: 0.47527430869608495 grad: -0.20700338557556494
iteration: 0 loss: 65.94972463754605 grad: 409.5571601141593
iteration: 10 loss: 2.525689232606386 grad: 0.6383059596946767
iteration: 20 loss: 1.474110049387739 grad: -0.056451386709597325
iteration: 30 loss: 1.053373835972252 grad: -0.1530731189096598
iteration: 40 loss: 0.8227992800547823 grad: -0.16837595256395102
iteration: 50 loss: 0.6763420815512869 grad: -0.16473803735732728
iteration: 60 loss: 0.5747746032861456 grad: -0.15612631111852954
iteration: 70 loss: 0.5000760144051707 grad: -0.14656838931973973
iteration: 80 loss: 0.44276915102446684 grad: -0.13734726652732912
iteration: 0 loss: 49.98678680455975 grad: 394.79367417529295
iteration: 10 loss: 2.7305098854928307 grad: 0.45632057655717545
iteration: 20 loss: 1.5557170589561957 grad: -0.08282695136201132
iteration: 30 loss: 1.098146660445433 grad: -0.1580844858989383
iteration: 40 loss: 0.8514514595622131 grad: -0.1678202007260806
iteration: 50 loss: 0.6964305803565118 grad: -0.16240897299111973
iteration: 60 loss: 0.5897394144684437 grad: -0.15324766997057518
iteration: 70 loss: 0.511716925332621 grad: -0.14359716404268136
iteration: 80 loss: 0.4521231344041636 grad: -0.13446452515225188
iteration: 0 loss: 49.39929822765796 grad: 497.00380466157594
iteration: 10 loss: 2.524644454404138 grad: -0.5835993439068098
iteration: 20 loss: 1.4723727782138667 grad: -0.5808498712645497
iteration: 30 loss: 1.0509099648557325 grad: -0.48008048723283514
iteration: 40 loss: 0.8201188103193172 grad: -0.4031522249320746
iteration: 50 loss: 0.6736477719427967 grad: -0.3465162966415969
iteration: 60 loss: 0.572145187003116 grad: -0.3036883270987142
iteration: 70 loss: 0.49754089498326887 grad: -0.2703094941937638
iteration: 80 loss: 0.44033702118237916 grad: -0.2436017506599913
iteration: 0 loss: 38.00629291732205 grad: 378.6307252599116
iteration: 10 loss: 2.7964054946637176 grad: 0.1723240560338234
iteration: 20 loss: 1.5838023827324668 grad: -0.20423421570895878
iteration: 30 loss: 1.1134834747711218 grad: -0.22835992753631024
iteration: 40 loss: 0.8609736816637947 grad: -0.21437187749680692
iteration: 50 loss: 0.7028133205656311 grad: -0.1957539882639247
iteration: 60 loss: 0.5942370455264939 grad: -0.17839230597854377
iteration: 70 loss: 0.5149968607751031 grad: -0.16326181988453317
iteration: 80 loss: 0.4545734321756589 grad: -0.15026785730938524
iteration: 0 loss: 76.44490424332402 grad: 383.6195028340528
iteration: 10 loss: 2.162340271095135 grad: 0.7735973804316321
iteration: 20 loss: 1.3043981879949653 grad: -0.0528974699368054
iteration: 30 loss: 0.9508468012183833 grad: -0.16740848361949578
iteration: 40 loss: 0.7523515000535057 grad: -0.1872202302563356
iteration: 50 loss: 0.6240639361839467 grad: -0.1845262362175644
iteration: 60 loss: 0.533938534057432 grad: -0.1756371898949035
iteration: 70 loss: 0.4669871561324147 grad: -0.16535333653323495
iteration: 0 loss: 56.681130036119946 grad: 392.45436949233164
iteration: 10 loss: 2.51277821009448 grad: 0.348858180230904
iteration: 20 loss: 1.4634113732207135 grad: -0.18429900570297997
iteration: 30 loss: 1.0446034253415168 grad: -0.23272726741540725
iteration: 40 loss: 0.8154064005000857 grad: -0.22533751787816053
iteration: 50 loss: 0.6699647296734177 grad: -0.20863602711685075
iteration: 60 loss: 0.5691717230750907 grad: -0.19159534630737984
iteration: 70 loss: 0.4950813466039108 grad: -0.1761800610127143
iteration: 80 loss: 0.43826397896778374 grad: -0.16266978644103397
iteration: 0 loss: 82.34967937380428 grad: 514.153344064043
iteration: 10 loss: 2.0687480419505846 grad: -0.20053415556049237
iteration: 20 loss: 1.241480866200494 grad: -0.3924704684218767
iteration: 30 loss: 0.9054122509509067 grad: -0.355546217951284
iteration: 40 loss: 0.7177360209156673 grad: -0.3122072976558655
iteration: 50 loss: 0.5966308640891756 grad: -0.2761541858670282
iteration: 60 loss: 0.5115424145381191 grad: -0.24708686648059053
iteration: 70 loss: 0.4482771794682776 grad: -0.2234657592957967
iteration: 0 loss: 67.34955982998508 grad: 433.54565917161506
iteration: 10 loss: 2.4452069912998593 grad: 0.5182867701676188
iteration: 20 loss: 1.4337798110569482 grad: -0.19720608478950563
iteration: 30 loss: 1.0280206299343346 grad: -0.2704797323336782
iteration: 40 loss: 0.804716333763281 grad: -0.2660234902617078
iteration: 50 loss: 0.6624269282939911 grad: -0.24746538843007465
iteration: 60 loss: 0.5635180436531134 grad: -0.22752069768120062
iteration: 70 loss: 0.4906454505174803 grad: -0.20916366926405452
iteration: 80 loss: 0.43466236731149943 grad: -0.19295436493639123
iteration: 0 loss: 64.22534472509551 grad: 510.6181351379581
iteration: 10 loss: 2.129567119621201 grad: -0.4769453864944719
iteration: 20 loss: 1.2681319444969135 grad: -0.5252572130815759
iteration: 30 loss: 0.9222898141114125 grad: -0.44542892865370054
iteration: 40 loss: 0.7300251749588411 grad: -0.3797817649806474
iteration: 50 loss: 0.6062615891597406 grad: -0.3298518009923418
iteration: 60 loss: 0.5194382346163283 grad: -0.29131662617832377
iteration: 70 loss: 0.454950805754627 grad: -0.2608442361562489
iteration: 0 loss: 85.09245881119833 grad: 511.5261651647187
iteration: 10 loss: 2.179813311721525 grad: -0.08910898568209621
iteration: 20 loss: 1.2957519069586851 grad: -0.3377579933259118
iteration: 30 loss: 0.9411434354071223 grad: -0.3195542853449605
iteration: 40 loss: 0.7442744434574706 grad: -0.28507158193044163
iteration: 50 loss: 0.6176726747025896 grad: -0.2541660473819297
iteration: 60 loss: 0.5289291345550076 grad: -0.2284794474395349
iteration: 70 loss: 0.46306028208618955 grad: -0.20726501850721785
iteration: 0 loss: 67.20554880925397 grad: 552.5404007761192
iteration: 10 loss: 2.123207901656315 grad: -0.48679460388831375
iteration: 20 loss: 1.2693960358697867 grad: -0.4911322344572679
iteration: 30 loss: 0.9255087755918381 grad: -0.4132984240544657
iteration: 40 loss: 0.7338541951748994 grad: -0.35259117184575406
iteration: 50 loss: 0.6102114682540063 grad: -0.3068817404912
iteration: 60 loss: 0.5233138419288562 grad: -0.271658241651417
iteration: 70 loss: 0.4586745668030661 grad: -0.24377948152502188
iteration: 0 loss: 47.64840387184619 grad: 428.89591836841805
iteration: 10 loss: 2.6668509688263593 grad: 0.05358670587629527
iteration: 20 loss: 1.535648824861379 grad: -0.3030234427423839
iteration: 30 loss: 1.0886143938803137 grad: -0.305592228646965
iteration: 40 loss: 0.8459381923378053 grad: -0.27705626574805825
iteration: 50 loss: 0.6928333163851179 grad: -0.24826877257653945
iteration: 60 loss: 0.587189180503091 grad: -0.2234640199279162
iteration: 70 loss: 0.5097950553866791 grad: -0.2026734685129788
iteration: 80 loss: 0.4506053297796329 grad: -0.1852408493167556
iteration: 0 loss: 51.243606983296225 grad: 434.76153626667497
iteration: 10 loss: 2.6264854141354177 grad: 0.12849774379909273
iteration: 20 loss: 1.5158651554815648 grad: -0.26738623141045104
iteration: 30 loss: 1.076290135627749 grad: -0.2837474073152477
iteration: 40 loss: 0.8372526018621845 grad: -0.2618897986475701
iteration: 50 loss: 0.6862445672993751 grad: -0.2369600028095012
iteration: 60 loss: 0.581941871257682 grad: -0.2146268484562154
iteration: 70 loss: 0.5054695304251337 grad: -0.19553247721915584
iteration: 80 loss: 0.4469470123557668 grad: -0.1793228188634271
iteration: 0 loss: 49.97326473469672 grad: 492.8128345272863
iteration: 10 loss: 2.4170044174657463 grad: -0.431733828975823
iteration: 20 loss: 1.4183955316868213 grad: -0.5107030318774486
iteration: 30 loss: 1.0160622135092787 grad: -0.43692562424796216
iteration: 40 loss: 0.7947399035067313 grad: -0.37261098600867637
iteration: 50 loss: 0.6538300590187149 grad: -0.32307803275271196
iteration: 60 loss: 0.5559546314819024 grad: -0.28474574587078927
iteration: 70 loss: 0.48388969547489646 grad: -0.2544476883925486
iteration: 80 loss: 0.4285566281031689 grad: -0.2299752784304258
iteration: 0 loss: 56.16076052847141 grad: 475.2891657033163
iteration: 10 loss: 2.4244933108147135 grad: -0.40370904848142264
iteration: 20 loss: 1.4225659020994912 grad: -0.4702762016774665
iteration: 30 loss: 1.0195740737812313 grad: -0.4013584077957982
iteration: 40 loss: 0.7979674479472785 grad: -0.3421891102570357
iteration: 50 loss: 0.6568574597096647 grad: -0.2968833526739958
iteration: 60 loss: 0.5588114169759137 grad: -0.26190857224485786
iteration: 70 loss: 0.48659276928339584 grad: -0.2342888281844996
iteration: 80 loss: 0.43111931006718474 grad: -0.21198155489781867
iteration: 0 loss: 86.62913699203946 grad: 558.2491889808671
iteration: 10 loss: 1.948362827734317 grad: -0.07995899262475584
iteration: 20 loss: 1.1591045238338997 grad: -0.3605160509674672
iteration: 30 loss: 0.8472710698463495 grad: -0.3322797811063133
iteration: 40 loss: 0.6739116178096083 grad: -0.2935151726136204
iteration: 50 loss: 0.5620230293011859 grad: -0.26058111803638995
iteration: 60 loss: 0.48328225764502847 grad: -0.2338113703072921
iteration: 70 loss: 0.424614221970263 grad: -0.21195195633637096
iteration: 0 loss: 46.26908695794728 grad: 464.7070147503093
iteration: 10 loss: 2.6698518308193013 grad: -0.4130654394662444
iteration: 20 loss: 1.5353534781362959 grad: -0.5330610069290768
iteration: 30 loss: 1.087805982087709 grad: -0.4524021703709962
iteration: 40 loss: 0.8449694787654649 grad: -0.3828950310165493
iteration: 50 loss: 0.6918183736381086 grad: -0.3301571142758224
iteration: 60 loss: 0.5861749835103183 grad: -0.2897981825671006
iteration: 70 loss: 0.5088024813903758 grad: -0.25816296687502116
iteration: 80 loss: 0.4496435821038176 grad: -0.23277293514507705
iteration: 0 loss: 70.9277764047461 grad: 532.9715114234599
iteration: 10 loss: 2.142410676542361 grad: -0.38808292560775787
iteration: 20 loss: 1.2732502844793834 grad: -0.4874577575014129
iteration: 30 loss: 0.9232048661329331 grad: -0.41971790647061974
iteration: 40 loss: 0.7288707044410164 grad: -0.36013896620993924
iteration: 50 loss: 0.6040317242701679 grad: -0.3140281195197039
iteration: 60 loss: 0.5166348805256679 grad: -0.2781542841602689
iteration: 70 loss: 0.4518457153568637 grad: -0.24964544259727345
iteration: 0 loss: 59.03936488068703 grad: 459.7768247880124
iteration: 10 loss: 2.5156003855785953 grad: 0.11417569084062884
iteration: 20 loss: 1.4727589498216647 grad: -0.24708239461151782
iteration: 30 loss: 1.0547006813023307 grad: -0.2666963767719797
iteration: 40 loss: 0.8250829257676329 grad: -0.24896434835805048
iteration: 50 loss: 0.6789616085543044 grad: -0.2270637139054871
iteration: 60 loss: 0.5774748466222015 grad: -0.20687476426934925
iteration: 70 loss: 0.5027449085361116 grad: -0.18932403646716756
iteration: 80 loss: 0.4453569072973716 grad: -0.17425398225644104
iteration: 0 loss: 37.49802060949411 grad: 433.24203779236075
iteration: 10 loss: 2.8231265352263275 grad: -0.39263063764097583
iteration: 20 loss: 1.6052996947016502 grad: -0.48381983968719816
iteration: 30 loss: 1.1296289710086802 grad: -0.410895702890656
iteration: 40 loss: 0.8736324009606009 grad: -0.34843730431305464
iteration: 50 loss: 0.7131176544485243 grad: -0.30095258226720845
iteration: 60 loss: 0.6028696560254254 grad: -0.26453115903555996
iteration: 70 loss: 0.5223910409495974 grad: -0.23592683830664768
iteration: 80 loss: 0.4610182856810173 grad: -0.2129322193406346
iteration: 0 loss: 80.27232541585239 grad: 521.9158684087032
iteration: 10 loss: 2.1434595756181394 grad: 0.18966754185397638
iteration: 20 loss: 1.2781312564310694 grad: -0.19268173167956212
iteration: 30 loss: 0.9307318448785405 grad: -0.22621218436077203
iteration: 40 loss: 0.7373552591543557 grad: -0.21838831733165592
iteration: 50 loss: 0.6127263509476755 grad: -0.20331054611176702
iteration: 60 loss: 0.5252126307115593 grad: -0.18794630457801825
iteration: 70 loss: 0.46016457550393336 grad: -0.17391933855281105
iteration: 0 loss: 55.16358091546288 grad: 430.3051176431711
iteration: 10 loss: 2.5297683115070204 grad: 0.1061629438552322
iteration: 20 loss: 1.4651417220606247 grad: -0.3159130793929179
iteration: 30 loss: 1.0435195289340051 grad: -0.3237569615388445
iteration: 40 loss: 0.8134399937559713 grad: -0.29471985034971293
iteration: 50 loss: 0.6676873520738643 grad: -0.26448073084380697
iteration: 60 loss: 0.5668028784645106 grad: -0.23817835784790187
iteration: 70 loss: 0.4927165832039629 grad: -0.21603956641850175
iteration: 80 loss: 0.43594714474048407 grad: -0.19743547877180226
iteration: 0 loss: 80.2549107932857 grad: 531.2629513480396
iteration: 10 loss: 2.00376216861762 grad: -0.03012900371132522
iteration: 20 loss: 1.1904279040178898 grad: -0.3073838891914108
iteration: 30 loss: 0.8691176283836367 grad: -0.298650201810047
iteration: 40 loss: 0.6904630559504519 grad: -0.2704359299985126
iteration: 50 loss: 0.5751711434852702 grad: -0.24354294865490406
iteration: 60 loss: 0.4940647033338543 grad: -0.2205379272613049
iteration: 70 loss: 0.43366508406936854 grad: -0.20119685407937166
iteration: 0 loss: 52.15385197354028 grad: 406.4183753616416
iteration: 10 loss: 2.5949554678684517 grad: 0.2783913378432118
iteration: 20 loss: 1.4970153422419075 grad: -0.26703700862896795
iteration: 30 loss: 1.063670136318499 grad: -0.2975085081297132
iteration: 40 loss: 0.8279060718223676 grad: -0.27699333303747925
iteration: 50 loss: 0.6788727728504372 grad: -0.25118870093494217
iteration: 60 loss: 0.5758786773442559 grad: -0.22760779295346234
iteration: 70 loss: 0.50033226688662 grad: -0.20730778322468923
iteration: 80 loss: 0.442497086037375 grad: -0.19002655403664842
iteration: 0 loss: 77.14364786394766 grad: 505.3182811779734
iteration: 10 loss: 2.2037434611947075 grad: -0.09923909561995599
iteration: 20 loss: 1.3151213492451874 grad: -0.31644721119366004
iteration: 30 loss: 0.9545500047402956 grad: -0.3021638657266448
iteration: 40 loss: 0.7538939075568372 grad: -0.27062624411442326
iteration: 50 loss: 0.6248273646105272 grad: -0.24175410852656523
iteration: 60 loss: 0.5344024684668924 grad: -0.2175591547147796
iteration: 70 loss: 0.4673388623186838 grad: -0.19749550527618637
iteration: 0 loss: 50.09985810629232 grad: 402.9650861050174
iteration: 10 loss: 2.67828510652986 grad: 0.5047058613828883
iteration: 20 loss: 1.542415977192625 grad: -0.1655841881943706
iteration: 30 loss: 1.093863207207712 grad: -0.23678951076181046
iteration: 40 loss: 0.8502312979556088 grad: -0.23466686948332968
iteration: 50 loss: 0.6964584391683853 grad: -0.2190201261771615
iteration: 60 loss: 0.5903225459404563 grad: -0.20179591786192463
iteration: 70 loss: 0.5125524294863598 grad: -0.18582067550756753
iteration: 80 loss: 0.45306650100511353 grad: -0.17166117957903954
iteration: 0 loss: 60.32641929063179 grad: 499.9640148332353
iteration: 10 loss: 2.4074278807992084 grad: -0.487030241223916
iteration: 20 loss: 1.4233121668445496 grad: -0.5047009154444502
iteration: 30 loss: 1.0235855648998866 grad: -0.4251036162569728
iteration: 40 loss: 0.8026828963147636 grad: -0.3610265521203314
iteration: 50 loss: 0.6615799006320396 grad: -0.3127100090500753
iteration: 60 loss: 0.5633258757690617 grad: -0.2756243043962505
iteration: 70 loss: 0.4908392135265868 grad: -0.24641518779350152
iteration: 80 loss: 0.4350927997357675 grad: -0.22285756124435818
iteration: 0 loss: 53.58537026512088 grad: 439.1844240989623
iteration: 10 loss: 2.5268537475689383 grad: -0.28479229299522096
iteration: 20 loss: 1.4778261899720995 grad: -0.48427859402867734
iteration: 30 loss: 1.0557685899931462 grad: -0.42647383087828444
iteration: 40 loss: 0.8241761504787477 grad: -0.3670938567731165
iteration: 50 loss: 0.6770536172191648 grad: -0.31973364205026006
iteration: 60 loss: 0.5750499843274652 grad: -0.2825570823134258
iteration: 70 loss: 0.5000589235723965 grad: -0.2529489028675699
iteration: 80 loss: 0.4425515594378091 grad: -0.2289211515465665
iteration: 0 loss: 48.17015979826701 grad: 464.442358019843
iteration: 10 loss: 2.4762016420131245 grad: -0.687441724465989
iteration: 20 loss: 1.4470910845619238 grad: -0.5963439930714354
iteration: 30 loss: 1.0345587885840772 grad: -0.48238147427231076
iteration: 40 loss: 0.8084138267872318 grad: -0.40112998058502974
iteration: 50 loss: 0.6647338461492506 grad: -0.34278406288875696
iteration: 60 loss: 0.5650633364078782 grad: -0.2992689563201643
iteration: 70 loss: 0.4917377464610333 grad: -0.26565803797144394
iteration: 80 loss: 0.43546777544791193 grad: -0.23893471890836399
iteration: 0 loss: 74.54465538534023 grad: 460.5263714852035
iteration: 10 loss: 2.277879695125276 grad: 0.545069190474149
iteration: 20 loss: 1.3465662520645558 grad: -0.13347231617940664
iteration: 30 loss: 0.9727276471942126 grad: -0.2217441328916242
iteration: 40 loss: 0.765819880451049 grad: -0.22876470338387414
iteration: 50 loss: 0.633229244357856 grad: -0.21808443835555308
iteration: 60 loss: 0.5406012151580316 grad: -0.20368490148947585
iteration: 70 loss: 0.47206239593817206 grad: -0.18937301789071612
iteration: 0 loss: 43.16604516317288 grad: 387.4845337190395
iteration: 10 loss: 2.728245816148377 grad: 0.38986317975659607
iteration: 20 loss: 1.5553932716221341 grad: -0.14855183921996978
iteration: 30 loss: 1.0976692712332474 grad: -0.20730992790407118
iteration: 40 loss: 0.8507961874030033 grad: -0.20589350845016033
iteration: 50 loss: 0.6956681437637516 grad: -0.19298125257908078
iteration: 60 loss: 0.5889206054528338 grad: -0.17853566912117022
iteration: 70 loss: 0.5108728772479068 grad: -0.1649989595429826
iteration: 80 loss: 0.4512725110021579 grad: -0.15290632610240484
iteration: 0 loss: 80.32326230883312 grad: 478.14050124555604
iteration: 10 loss: 2.2372980768506583 grad: 0.1900006397221419
iteration: 20 loss: 1.3329862506020205 grad: -0.23470384725175908
iteration: 30 loss: 0.9667421429944212 grad: -0.2641512554449746
iteration: 40 loss: 0.7629874020574128 grad: -0.24947595389533933
iteration: 50 loss: 0.6319753128257368 grad: -0.22889297777294598
iteration: 60 loss: 0.5402280808172939 grad: -0.20936036610215913
iteration: 70 loss: 0.4722153967012756 grad: -0.19215320702525657
iteration: 0 loss: 67.45173286738724 grad: 485.7105712676749
iteration: 10 loss: 2.215181857527031 grad: -0.2428342155151808
iteration: 20 loss: 1.3148133451427384 grad: -0.41615311302480984
iteration: 30 loss: 0.9519751923155788 grad: -0.3741091491095595
iteration: 40 loss: 0.7506371077044439 grad: -0.3266913027584454
iteration: 50 loss: 0.6213800408501232 grad: -0.2875265398952331
iteration: 60 loss: 0.5309529979635541 grad: -0.2561260328457534
iteration: 70 loss: 0.46396565971190284 grad: -0.23074482800415877
iteration: 0 loss: inf grad: 3810.3314448593133
iteration: 0 loss: inf grad: 3575.5798232418297
iteration: 0 loss: inf grad: 3620.491480355109
iteration: 0 loss: inf grad: 3966.02116308408
iteration: 0 loss: inf grad: 3452.436695213006
iteration: 0 loss: inf grad: 3077.133469269137
iteration: 0 loss: inf grad: 3557.038887035873
iteration: 0 loss: inf grad: 2916.429253573872
iteration: 0 loss: inf grad: 3728.7377789763177
iteration: 0 loss: inf grad: 3468.7915479793287
iteration: 0 loss: inf grad: 3173.934105191066
iteration: 0 loss: inf grad: 3729.6556386238176
iteration: 0 loss: inf grad: 3155.922937666178
iteration: 0 loss: inf grad: 3530.189112257698
iteration: 0 loss: inf grad: 3577.4799764537247
iteration: 0 loss: inf grad: 4054.7000852955957
iteration: 0 loss: inf grad: 3478.042633647519
iteration: 0 loss: inf grad: 3242.914741722684
iteration: 0 loss: inf grad: 4063.597742850504
iteration: 0 loss: 11850.483487583058 grad: 3290.8805681293106
iteration: 0 loss: inf grad: 3728.683454321652
iteration: 0 loss: inf grad: 3874.3583432251185
iteration: 0 loss: inf grad: 3124.988485521674
iteration: 0 loss: inf grad: 3441.2532980387596
iteration: 0 loss: inf grad: 3609.90325329066
iteration: 0 loss: inf grad: 3483.492499214002
iteration: 0 loss: inf grad: 4081.7039156385567
iteration: 0 loss: inf grad: 3955.10242642228
iteration: 0 loss: inf grad: 3560.0733805263767
iteration: 0 loss: inf grad: 3276.1109760565346
iteration: 0 loss: inf grad: 4314.379974538134
iteration: 0 loss: inf grad: 3762.2632134392516
iteration: 0 loss: inf grad: 3548.973002769055
iteration: 0 loss: inf grad: 3683.3506270231173
iteration: 0 loss: inf grad: 3799.3153455356714
iteration: 0 loss: inf grad: 3691.1498902852327
iteration: 0 loss: inf grad: 3537.4498181521344
iteration: 0 loss: inf grad: 4089.1399803805216
iteration: 0 loss: inf grad: 3373.0398233813476
iteration: 0 loss: inf grad: 3433.9074226990783
iteration: 0 loss: inf grad: 3604.102301153732
iteration: 0 loss: inf grad: 3127.097036959727
iteration: 0 loss: inf grad: 4448.379876670564
iteration: 0 loss: inf grad: 3312.0066120505635
iteration: 0 loss: inf grad: 3980.249864218864
iteration: 0 loss: inf grad: 3681.2166152345058
iteration: 0 loss: inf grad: 3237.240733944755
iteration: 0 loss: inf grad: 3494.799748954759
iteration: 0 loss: inf grad: 3860.0174696808053
iteration: 0 loss: inf grad: 3879.358701487745
iteration: 0 loss: inf grad: 4001.768982107319
iteration: 0 loss: inf grad: 3600.2985832592913
iteration: 0 loss: inf grad: 4108.979868089715
iteration: 0 loss: inf grad: 3673.605924779283
iteration: 0 loss: inf grad: 3324.940686186166
iteration: 0 loss: inf grad: 3579.530062001611
iteration: 0 loss: 11452.706761621223 grad: 3472.881156862544
iteration: 0 loss: inf grad: 3669.3669810905426
iteration: 0 loss: inf grad: 4165.526849937694
iteration: 0 loss: inf grad: 3532.9778995225083
iteration: 0 loss: inf grad: 3903.5151187679294
iteration: 0 loss: inf grad: 4111.540186168615
iteration: 0 loss: inf grad: 3058.2289686782224
iteration: 0 loss: inf grad: 3356.987757294124
iteration: 0 loss: inf grad: 3634.0867527267046
iteration: 0 loss: inf grad: 3555.6529315613625
iteration: 0 loss: inf grad: 3199.142975427108
iteration: 0 loss: inf grad: 3915.0695440100476
iteration: 0 loss: inf grad: 3582.8998619366766
iteration: 0 loss: inf grad: 3811.9069694134278
iteration: 0 loss: inf grad: 3887.652422946622
iteration: 0 loss: inf grad: 4328.607774279039
iteration: 0 loss: inf grad: 3341.275119624646
iteration: 0 loss: inf grad: 3861.890369120365
iteration: 0 loss: inf grad: 3475.4672211351094
iteration: 0 loss: inf grad: 3224.2632065098687
iteration: 0 loss: inf grad: 3256.626489416577
iteration: 0 loss: inf grad: 3232.955275596809
iteration: 0 loss: inf grad: 3764.230492257587
iteration: 0 loss: inf grad: 3814.6311045314324
iteration: 0 loss: inf grad: 4132.986475928598
iteration: 0 loss: inf grad: 3942.876169636391
iteration: 0 loss: inf grad: 3790.842144342395
iteration: 0 loss: inf grad: 3946.5846976546827
iteration: 0 loss: inf grad: 3954.981923289297
iteration: 0 loss: inf grad: 3438.3145465839025
iteration: 0 loss: inf grad: 4059.479834342711
iteration: 0 loss: inf grad: 3634.477645052204
iteration: 0 loss: inf grad: 3116.9367013450146
iteration: 0 loss: inf grad: 2961.603960120953
iteration: 0 loss: inf grad: 3247.782129773359
iteration: 0 loss: inf grad: 3724.3061459872133
iteration: 0 loss: inf grad: 3498.949335290121
iteration: 0 loss: inf grad: 3389.7118282073475
iteration: 0 loss: inf grad: 3284.266518836579
iteration: 0 loss: inf grad: 4116.728463256696
iteration: 0 loss: inf grad: 3739.3000719476836
iteration: 0 loss: inf grad: 3610.1503807806994
iteration: 0 loss: inf grad: 3576.144961639845
iteration: 0 loss: inf grad: 3086.683919674183
iteration: 0 loss: inf grad: 3696.7081667874972
iteration: 0 loss: inf grad: 3415.6456678250315
iteration: 0 loss: inf grad: 3596.2399776360226
iteration: 0 loss: inf grad: 4426.036354613734
iteration: 0 loss: inf grad: 3245.351052119153
iteration: 0 loss: inf grad: 3121.3397530749417
iteration: 0 loss: inf grad: 3288.776772004209
iteration: 0 loss: inf grad: 3662.0235196550448
iteration: 0 loss: inf grad: 3756.338466986088
iteration: 0 loss: inf grad: 4010.1932636859915
iteration: 0 loss: inf grad: 3935.7511247104985
iteration: 0 loss: inf grad: 3785.578730647922
iteration: 0 loss: inf grad: 3598.333612412139
iteration: 0 loss: inf grad: 4059.618034062522
iteration: 0 loss: inf grad: 3786.068262725362
iteration: 0 loss: inf grad: 3558.3467191454174
iteration: 0 loss: inf grad: 3572.67623943756
iteration: 0 loss: inf grad: 3500.475369446256
iteration: 0 loss: inf grad: 4065.7171220215832
iteration: 0 loss: inf grad: 3583.2839634597062
iteration: 0 loss: inf grad: 3415.680047570408
iteration: 0 loss: inf grad: 3054.268166067238
iteration: 0 loss: inf grad: 3894.25715224523
iteration: 0 loss: inf grad: 3784.3966208144284
iteration: 0 loss: inf grad: 3732.8734421931213
iteration: 0 loss: inf grad: 3885.25698444072
iteration: 0 loss: inf grad: 3509.014223026783
iteration: 0 loss: inf grad: 3666.1098828912836
iteration: 0 loss: inf grad: 4074.421299341392
iteration: 0 loss: inf grad: 3485.2018472823584
iteration: 0 loss: inf grad: 3726.163269658492
iteration: 0 loss: inf grad: 3553.6684877053244
iteration: 0 loss: inf grad: 4098.901752908344
iteration: 0 loss: inf grad: 3882.665467434111
iteration: 0 loss: inf grad: 3227.2240401249555
iteration: 0 loss: inf grad: 3115.9250226423133
iteration: 0 loss: inf grad: 3850.5863965045196
iteration: 0 loss: inf grad: 2999.016137357749
iteration: 0 loss: inf grad: 3029.9206504251624
iteration: 0 loss: inf grad: 3106.344165491034
iteration: 0 loss: inf grad: 3956.45056918271
iteration: 0 loss: inf grad: 3389.718579991445
iteration: 0 loss: inf grad: 3941.125651544323
iteration: 0 loss: inf grad: 3943.820457521747
iteration: 0 loss: inf grad: 4225.102182084869
iteration: 0 loss: inf grad: 3351.804012856999
iteration: 0 loss: inf grad: 3395.4962777447263
iteration: 0 loss: inf grad: 3813.806178727002
iteration: 0 loss: inf grad: 3688.57480194132
iteration: 0 loss: inf grad: 4271.683413534931
iteration: 0 loss: inf grad: 3615.2631623377983
iteration: 0 loss: inf grad: 4087.4094873687973
iteration: 0 loss: inf grad: 3592.93948085322
iteration: 0 loss: inf grad: 3385.026440067062
iteration: 0 loss: inf grad: 4013.823314271161
iteration: 0 loss: inf grad: 3363.7479056072543
iteration: 0 loss: inf grad: 4083.109506724885
iteration: 0 loss: inf grad: 3202.615579149334
iteration: 0 loss: inf grad: 3889.0422062069415
iteration: 0 loss: inf grad: 3177.734578835955
iteration: 0 loss: inf grad: 3863.0245731027762
iteration: 0 loss: inf grad: 3452.4990770273953
iteration: 0 loss: inf grad: 3593.189286539434
iteration: 0 loss: inf grad: 3588.8293986961858
iteration: 0 loss: inf grad: 3061.7573097290615
iteration: 0 loss: inf grad: 3709.722167241762
iteration: 0 loss: inf grad: 3755.1265692454012
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:57: RuntimeWarning: invalid value encountered in double_scalars
  diff = np.absolute(f_history[-1]-f_history[-2])
iteration: 0 loss: 2468.382681332594 grad: 1605.2746999050319
iteration: 10 loss: 0.011965094769865655 grad: 0.07927162508977799
iteration: 0 loss: 1961.754687864455 grad: 1505.1002272458582
iteration: 0 loss: 2151.8036574145544 grad: 1522.6024433142434
iteration: 0 loss: 2782.8504114269494 grad: 1670.4162905304393
iteration: 10 loss: 0.01144253962874328 grad: 0.007110423872721039
iteration: 0 loss: 1682.4197523896996 grad: 1451.9748764013743
iteration: 0 loss: 1146.1637366805496 grad: 1292.4994818973373
iteration: 0 loss: 1806.1121231521313 grad: 1498.1738229559978
iteration: 0 loss: 940.40840533854 grad: 1225.4036423835294
iteration: 0 loss: 2717.5476699481883 grad: 1570.1411192536134
iteration: 0 loss: 1762.334623963051 grad: 1458.1827196353674
iteration: 0 loss: 1074.985497141866 grad: 1335.0949229639882
iteration: 0 loss: 2210.378061597746 grad: 1570.7386133331024
iteration: 10 loss: 0.010608886884504252 grad: 0.0450755649714035
iteration: 0 loss: 1564.5156659519992 grad: 1328.7189399233055
iteration: 0 loss: 1619.8297760563064 grad: 1484.0330760880533
iteration: 0 loss: 2197.5489004813303 grad: 1508.054193142525
iteration: 0 loss: 3142.324998873274 grad: 1709.9375847256708
iteration: 10 loss: 0.01230490757073064 grad: 0.11006599128839295
iteration: 0 loss: 1865.6868319906537 grad: 1464.7267520772089
iteration: 0 loss: 1315.6862033740128 grad: 1365.8861791531358
iteration: 0 loss: 3261.847610070197 grad: 1713.3780512272037
iteration: 10 loss: 0.02194878354269101 grad: 0.1331727596608968
iteration: 0 loss: 1538.1155966066856 grad: 1383.5957618250202
iteration: 0 loss: 2244.068708732542 grad: 1568.831139344178
iteration: 0 loss: 2700.2851201552858 grad: 1631.2289946047824
iteration: 10 loss: 0.013928036411181727 grad: 0.14224722722694624
iteration: 0 loss: 1442.8903159706208 grad: 1314.073357494497
iteration: 0 loss: 1614.1363156890338 grad: 1447.7944336016249
iteration: 0 loss: 1727.857728015176 grad: 1517.7076246519362
iteration: 0 loss: 1501.5540681688558 grad: 1463.8420211398225
iteration: 0 loss: 3518.796261241537 grad: 1721.1394965824882
iteration: 10 loss: 0.015255927160482812 grad: 0.6627793158017594
iteration: 0 loss: 3364.5629251397813 grad: 1664.7321875310913
iteration: 10 loss: 0.20851736492940373 grad: 3.3761412074290202
iteration: 0 loss: 1850.331475038263 grad: 1497.1588862049011
iteration: 0 loss: 1429.2318031696275 grad: 1379.1971737926992
iteration: 10 loss: 0.009872702010563278 grad: 0.005755354577302518
iteration: 0 loss: 4009.2192463826486 grad: 1819.40563725411
iteration: 10 loss: 0.10080801555820719 grad: 1.174243753080502
iteration: 0 loss: 1912.7623760813767 grad: 1582.8450104295562
iteration: 0 loss: 1834.232200334665 grad: 1492.9806765070557
iteration: 0 loss: 2589.268946815045 grad: 1550.857393920976
iteration: 10 loss: 0.013358376706574663 grad: 0.05848670156783203
iteration: 0 loss: 2538.7973658929445 grad: 1601.347904271926
iteration: 10 loss: 0.010885703770988832 grad: -0.001966001330198853
iteration: 0 loss: 2112.1257867442664 grad: 1556.3363159261467
iteration: 0 loss: 2001.9450501777144 grad: 1490.7262367008448
iteration: 10 loss: 0.010198007529569704 grad: 0.04767485020327948
iteration: 0 loss: 3156.2691447554475 grad: 1722.6415565635364
iteration: 10 loss: 0.012920052723281762 grad: -0.20093781811433611
iteration: 0 loss: 1503.3594847216823 grad: 1418.7984580960574
iteration: 0 loss: 1625.9534473450824 grad: 1445.4726271158304
iteration: 0 loss: 2169.8195950893096 grad: 1516.8958446225004
iteration: 0 loss: 1019.4165225411293 grad: 1313.1330662395908
iteration: 0 loss: 4529.5660229559 grad: 1876.6195297047886
iteration: 10 loss: 0.16899924515878825 grad: -0.11922981008803257
iteration: 0 loss: 1792.6776117830113 grad: 1394.1990275487867
iteration: 0 loss: 2377.318794831187 grad: 1677.1537514990919
iteration: 0 loss: 2574.3171522005828 grad: 1550.5824225084054
iteration: 0 loss: 1960.6975010935296 grad: 1359.4858384219274
iteration: 0 loss: 1527.9905061955933 grad: 1470.0682718821597
iteration: 10 loss: 0.010738948213904885 grad: 0.03911006347633544
iteration: 0 loss: 2740.4517986677306 grad: 1627.2518620672506
iteration: 10 loss: 0.011528036314385563 grad: 0.10676417726656921
iteration: 0 loss: 2884.7327648670575 grad: 1634.1916922902487
iteration: 10 loss: 0.013752641413620122 grad: 0.20957242703233708
iteration: 0 loss: 3311.430704736279 grad: 1687.607778811927
iteration: 10 loss: 0.271981706864608 grad: -0.015044690411990108
iteration: 0 loss: 1741.5419889499963 grad: 1516.7385477807234
iteration: 0 loss: 2997.338537238489 grad: 1729.4596756591409
iteration: 10 loss: 0.012664849480312445 grad: 0.03114188696933172
iteration: 0 loss: 2218.3918069810734 grad: 1546.979155712163
iteration: 0 loss: 1434.748245787339 grad: 1399.2516119912723
iteration: 0 loss: 1940.811227649117 grad: 1508.2256883807238
iteration: 0 loss: 1409.0376058174743 grad: 1461.657276758387
iteration: 0 loss: 2163.92618241585 grad: 1545.7205682476738
iteration: 10 loss: 0.01019805617331506 grad: 0.05505423289309823
iteration: 0 loss: 3286.4204645017057 grad: 1756.0426241686891
iteration: 10 loss: 0.01203131805827566 grad: -0.24880063133574773
iteration: 0 loss: 1726.2101743653066 grad: 1485.852576844782
iteration: 0 loss: 2826.5437850884873 grad: 1642.5401575920914
iteration: 10 loss: 0.03131354143194833 grad: 1.0285325246813168
iteration: 0 loss: 2797.3877438063946 grad: 1732.5464718703113
iteration: 10 loss: 0.011225384583336894 grad: 0.03803910533282385
iteration: 0 loss: 1433.1313507349764 grad: 1284.5803466769344
iteration: 0 loss: 1461.5470421110633 grad: 1411.927729400812
iteration: 0 loss: 1825.3717390678628 grad: 1529.9991889280466
iteration: 0 loss: 1897.2041257577987 grad: 1495.3180870593887
iteration: 0 loss: 1438.9081283383414 grad: 1345.9967126181093
iteration: 0 loss: 2590.981030684381 grad: 1649.28332016711
iteration: 10 loss: 0.012013692006638104 grad: 0.057190452805712194
iteration: 0 loss: 2167.851474895286 grad: 1510.405373460825
iteration: 10 loss: 0.010995209619233554 grad: 0.06839137370095935
iteration: 0 loss: 2079.1654772623274 grad: 1606.8390491799382
iteration: 10 loss: 0.012047949921741912 grad: 0.06040294220900805
iteration: 0 loss: 2494.7981513798336 grad: 1639.096602541163
iteration: 0 loss: 3897.9042114079416 grad: 1824.8860078978787
iteration: 10 loss: 0.03613225592959101 grad: -1.2110788912273964
iteration: 0 loss: 1616.581726265773 grad: 1406.045460883321
iteration: 0 loss: 2635.1889074874516 grad: 1628.3230492611624
iteration: 0 loss: 1671.9386407310967 grad: 1463.6087046512343
iteration: 0 loss: 1320.2153780980043 grad: 1357.2560574586996
iteration: 0 loss: 1725.5046735506164 grad: 1370.7992396843242
iteration: 0 loss: 1355.6063180275296 grad: 1362.2149203258962
iteration: 0 loss: 2003.9686089033958 grad: 1584.7523734912058
iteration: 0 loss: 2875.334778697069 grad: 1605.1004209833388
iteration: 10 loss: 0.012262971268765714 grad: -0.49334148211184153
iteration: 0 loss: 3209.4005667355887 grad: 1741.5492574040754
iteration: 10 loss: 0.009689675218065862 grad: -0.5181205482903768
iteration: 0 loss: 3000.766433419741 grad: 1661.5294793044695
iteration: 10 loss: 0.01946823598435995 grad: -0.5511944560534471
iteration: 0 loss: 2381.4437466031723 grad: 1597.3992667037996
iteration: 0 loss: 3075.817975863231 grad: 1663.0189364548926
iteration: 10 loss: 0.01182702148600418 grad: 0.1612961629137664
iteration: 0 loss: 3037.763458890487 grad: 1668.3555318622869
iteration: 10 loss: 0.02138571681809315 grad: 1.2108433667480485
iteration: 0 loss: 2375.9295808426123 grad: 1449.2620921420248
iteration: 0 loss: 3714.867403513617 grad: 1710.7917355832096
iteration: 10 loss: 0.013924785758056467 grad: 0.2687272040041482
iteration: 0 loss: 2536.955300129585 grad: 1531.9951061964957
iteration: 10 loss: 0.01092317379053301 grad: 0.18896469869692192
iteration: 0 loss: 1278.5975790451002 grad: 1311.5902858483844
iteration: 0 loss: 1149.9852831826524 grad: 1245.673978961524
iteration: 0 loss: 1608.713719275073 grad: 1365.28310814315
iteration: 0 loss: 2114.5046654281073 grad: 1569.224431119852
iteration: 0 loss: 1991.351319393784 grad: 1472.967119783424
iteration: 0 loss: 1604.4527769926121 grad: 1425.9646673688662
iteration: 0 loss: 1385.6942011210494 grad: 1381.457672546183
iteration: 0 loss: 2921.2690316287376 grad: 1736.426710785279
iteration: 10 loss: 0.010640796316279606 grad: 0.08537258289925914
iteration: 0 loss: 2385.645862276549 grad: 1574.08345312124
iteration: 10 loss: 0.011406044047775636 grad: -0.1914395929658933
iteration: 0 loss: 2255.1972751427516 grad: 1520.0785120338833
iteration: 0 loss: 1938.3880640077396 grad: 1505.663016907749
iteration: 0 loss: 1456.547591448233 grad: 1296.0471610201007
iteration: 0 loss: 2331.1783063116486 grad: 1555.6670443246107
iteration: 0 loss: 1707.8158954480525 grad: 1437.6114409120078
iteration: 0 loss: 1707.6457129751564 grad: 1513.0644131977497
iteration: 0 loss: 3904.4937611416854 grad: 1865.443337197458
iteration: 10 loss: 0.022671234725170176 grad: -0.25065462383270043
iteration: 0 loss: 1282.7705107239838 grad: 1363.7285549230764
iteration: 0 loss: 1242.4944584077164 grad: 1313.8391069645932
iteration: 0 loss: 1500.3573554187335 grad: 1382.7268003017516
iteration: 0 loss: 1879.6464282076415 grad: 1541.2296858250916
iteration: 0 loss: 2362.9750159107057 grad: 1580.9119412113553
iteration: 0 loss: 2648.9834989286915 grad: 1689.8379142111514
iteration: 10 loss: 0.010690016707733528 grad: 0.06389527469963474
iteration: 0 loss: 3235.7567709606824 grad: 1659.3969943645527
iteration: 10 loss: 0.029646647801696832 grad: 0.6219552923335503
iteration: 0 loss: 2520.43842514205 grad: 1594.9701956625258
iteration: 10 loss: 0.038209238805972164 grad: 0.6459755425407244
iteration: 0 loss: 1991.8550741515005 grad: 1515.5983239840677
iteration: 0 loss: 3351.544653146098 grad: 1711.0728129071356
iteration: 10 loss: 0.012353552265928804 grad: -0.0715446814729283
iteration: 0 loss: 2550.385027627529 grad: 1595.3595263380332
iteration: 0 loss: 1545.359263692859 grad: 1498.1430756417228
iteration: 0 loss: 1908.5859742376692 grad: 1502.4467027768478
iteration: 0 loss: 1698.656119564381 grad: 1474.1408281077747
iteration: 0 loss: 2980.003849796287 grad: 1715.082474579407
iteration: 10 loss: 0.02157659574930387 grad: 2.3511918838500274
iteration: 0 loss: 1544.0579306508102 grad: 1507.8706044366538
iteration: 0 loss: 1928.956753511147 grad: 1436.97986740248
iteration: 0 loss: 1040.9456046686362 grad: 1284.6785294932956
iteration: 0 loss: 2297.4109087215456 grad: 1642.7683498264014
iteration: 0 loss: 2174.4358394138835 grad: 1595.755626634268
iteration: 0 loss: 2177.3434925334655 grad: 1573.7865461984975
iteration: 10 loss: 0.01135586937280922 grad: 0.13227877588621764
iteration: 0 loss: 3215.5885715159325 grad: 1635.6891523981271
iteration: 10 loss: 0.011435287501213183 grad: -0.04677279922325758
iteration: 0 loss: 1719.1050262280326 grad: 1480.3393481545222
iteration: 0 loss: 2027.2107444733113 grad: 1542.614892472248
iteration: 0 loss: 3271.4849764284563 grad: 1717.6271337198714
iteration: 10 loss: 0.010554921952213837 grad: 0.09612887611383696
iteration: 0 loss: 1991.565231099922 grad: 1463.767298485067
iteration: 0 loss: 2726.86143040023 grad: 1569.6535763339316
iteration: 10 loss: 0.010595846670515708 grad: 0.09795400970852769
iteration: 0 loss: 1718.1294139055635 grad: 1498.1574510569258
iteration: 0 loss: 2971.923806828021 grad: 1725.7111129966697
iteration: 0 loss: 2574.326125560923 grad: 1635.8335012515545
iteration: 0 loss: 1655.750905805364 grad: 1356.9041310542902
iteration: 0 loss: 1264.8854368715108 grad: 1313.7479268280588
iteration: 0 loss: 2046.3256748568058 grad: 1622.1014848367954
iteration: 0 loss: 1016.1913908188435 grad: 1259.9305352483982
iteration: 0 loss: 1529.4148249845255 grad: 1272.058751473621
iteration: 0 loss: 1260.5181228573556 grad: 1306.565694957067
iteration: 0 loss: 3053.1327479588217 grad: 1666.3484821017623
iteration: 10 loss: 0.010763812556846957 grad: 0.04592151162795388
iteration: 0 loss: 1866.1344800891418 grad: 1425.0671385318956
iteration: 0 loss: 3137.018280415381 grad: 1660.2708983304233
iteration: 10 loss: 0.19629748064868513 grad: 0.6439627581345089
iteration: 0 loss: 3031.0500021022426 grad: 1662.4062823244078
iteration: 10 loss: 0.011961137283254753 grad: 0.10073107096339114
iteration: 0 loss: 3581.8286999809297 grad: 1782.0973238151362
iteration: 10 loss: 0.014216201733771331 grad: -0.29473823018530837
iteration: 0 loss: 1587.2578477863071 grad: 1409.441812800622
iteration: 0 loss: 1635.6692620976146 grad: 1429.3186900677006
iteration: 0 loss: 2099.527660329721 grad: 1606.3573688603828
iteration: 0 loss: 2154.618316794935 grad: 1551.936747871087
iteration: 0 loss: 4123.782653876275 grad: 1800.9224469726553
iteration: 10 loss: 0.019909218382963445 grad: 1.69858678441251
iteration: 0 loss: 1767.224977133572 grad: 1522.937240276627
iteration: 0 loss: 3150.5124828424955 grad: 1724.5195102620828
iteration: 10 loss: 0.01266217083320953 grad: -0.46083039659577685
iteration: 0 loss: 2066.6675044587505 grad: 1511.747716403021
iteration: 0 loss: 1319.3012465698312 grad: 1422.695477048832
iteration: 0 loss: 3004.423652679662 grad: 1691.2392607124943
iteration: 10 loss: 0.010776980220477773 grad: 0.06533263456907193
iteration: 0 loss: 1483.8405155522566 grad: 1416.14247170399
iteration: 0 loss: 3582.487406349757 grad: 1719.6272735121306
iteration: 10 loss: 0.3604120449639264 grad: 2.0704899862114927
iteration: 0 loss: 1286.7769974503817 grad: 1344.9291502017345
iteration: 0 loss: 2823.2657885880285 grad: 1638.4630798037751
iteration: 0 loss: 1417.8661028250303 grad: 1334.339384897656
iteration: 0 loss: 2144.792464065953 grad: 1628.3858461207908
iteration: 0 loss: 1542.4066008298594 grad: 1451.4471176157767
iteration: 0 loss: 2040.980673906685 grad: 1510.5042272992441
iteration: 10 loss: 0.011736676583082457 grad: 0.08894091314898173
iteration: 0 loss: 2064.9515234065807 grad: 1509.9090995389215
iteration: 0 loss: 1128.8695397249178 grad: 1286.93102620567
iteration: 0 loss: 2606.4051573729726 grad: 1561.785101913452
iteration: 10 loss: 0.010685280489269644 grad: 0.08911657176546345
iteration: 0 loss: 2489.921895895652 grad: 1580.7408083566957
iteration: 10 loss: 0.0123160424693858 grad: 0.10126055951412122
iteration: 0 loss: 2803.8202176607333 grad: 1689.4382607740486
iteration: 0 loss: 2237.3662508464367 grad: 1585.9911696815898
iteration: 0 loss: 2445.5327151543647 grad: 1602.939071617111
iteration: 0 loss: 3156.9616758691323 grad: 1760.0094170643788
iteration: 10 loss: 0.00805607381334994 grad: 0.015349961039382437
iteration: 0 loss: 1913.4743382986892 grad: 1529.663714802715
iteration: 0 loss: 1284.1551128772714 grad: 1362.8683252121507
iteration: 0 loss: 2059.4822653711512 grad: 1578.5461509847155
iteration: 0 loss: 1058.6301371902293 grad: 1289.127953285385
iteration: 0 loss: 3107.8666804598465 grad: 1654.9284190367107
iteration: 0 loss: 1995.7153986269398 grad: 1538.2476899936487
iteration: 0 loss: 1210.9635701212894 grad: 1407.825968387467
iteration: 0 loss: 2519.0829750440744 grad: 1655.8797166990307
iteration: 0 loss: 1768.8187043982937 grad: 1399.012562939686
iteration: 0 loss: 1851.6967893521726 grad: 1564.209145787723
iteration: 0 loss: 2486.313280022212 grad: 1587.184767133608
iteration: 0 loss: 3578.5206835766094 grad: 1799.8830288017277
iteration: 10 loss: 0.010161736136069521 grad: 0.08160273327002844
iteration: 0 loss: 2126.2389268225493 grad: 1543.8948450467249
iteration: 0 loss: 1504.7651896184034 grad: 1439.0050763249262
iteration: 0 loss: 3698.62491177826 grad: 1802.9554594997649
iteration: 10 loss: 0.06435037338749018 grad: 0.11602357912495567
iteration: 0 loss: 1756.1434615372295 grad: 1457.6414966743123
iteration: 0 loss: 2546.690144268731 grad: 1651.8151232534472
iteration: 0 loss: 3056.0031149776546 grad: 1717.226972054036
iteration: 10 loss: 0.035829107657429464 grad: -0.4805525544225712
iteration: 0 loss: 1625.1812747169986 grad: 1384.376041682492
iteration: 0 loss: 1853.564478893043 grad: 1527.0573529999926
iteration: 0 loss: 1967.759302123209 grad: 1600.5625894099771
iteration: 0 loss: 1709.2881772796916 grad: 1544.5499338828577
iteration: 0 loss: 4004.4802119345245 grad: 1814.4772304970977
iteration: 10 loss: 0.010067421057928268 grad: 0.7792675556512838
iteration: 0 loss: 3817.8507517685603 grad: 1753.6841237962362
iteration: 10 loss: 0.2798748962589095 grad: 2.8463583871332356
iteration: 0 loss: 2116.0706738382823 grad: 1577.511429739146
iteration: 0 loss: 1625.1162725495885 grad: 1453.597936250102
iteration: 0 loss: 4536.163474505882 grad: 1915.089031479457
iteration: 10 loss: 0.02051671169928423 grad: 0.8296161438084472
iteration: 0 loss: 2168.8862272314555 grad: 1667.2125988554958
iteration: 0 loss: 2086.587723273626 grad: 1572.1391458056646
iteration: 0 loss: 2946.074614932956 grad: 1632.7063874985733
iteration: 10 loss: 0.008247259875960563 grad: 0.06753995119606605
iteration: 0 loss: 2875.742851614337 grad: 1686.3035727238778
iteration: 10 loss: 0.008346210745830003 grad: -0.14031768562007288
iteration: 0 loss: 2394.8119483248206 grad: 1638.9702730344902
iteration: 0 loss: 2276.5406635759546 grad: 1570.4853467979203
iteration: 10 loss: 0.009357885461544025 grad: 0.05781125603394985
iteration: 0 loss: 3593.8891368725303 grad: 1814.2932936971088
iteration: 10 loss: 0.01808156486566242 grad: 1.6180065561011447
iteration: 0 loss: 1698.7771022094341 grad: 1496.2005790847893
iteration: 0 loss: 1851.5649484543815 grad: 1523.6949437431645
iteration: 0 loss: 2469.7180144656104 grad: 1596.1040839916968
iteration: 0 loss: 1169.8792256038003 grad: 1385.167091339878
iteration: 0 loss: 5126.170517121554 grad: 1974.5429432172066
iteration: 10 loss: 0.2958503101961634 grad: 2.185748928785488
iteration: 0 loss: 2032.6638412671493 grad: 1468.6554827213358
iteration: 0 loss: 2694.7966796257906 grad: 1765.3328659263339
iteration: 0 loss: 2907.8026983232876 grad: 1632.6638026781945
iteration: 0 loss: 2235.8844689245097 grad: 1433.352446568701
iteration: 0 loss: 1744.1020557331037 grad: 1549.906075247512
iteration: 0 loss: 3101.1746712249646 grad: 1712.747333840111
iteration: 10 loss: 0.010010699989586348 grad: 0.698932676816238
iteration: 0 loss: 3267.063236593671 grad: 1721.4847023880861
iteration: 10 loss: 0.02468709887563653 grad: -0.2769610918757077
iteration: 0 loss: 3782.696785427848 grad: 1778.9698020399624
iteration: 10 loss: 0.21452411999119 grad: 0.895727105725299
iteration: 0 loss: 1988.1573631296435 grad: 1597.1234577051105
iteration: 0 loss: 3404.3093145674634 grad: 1820.7872831400111
iteration: 10 loss: 0.008958054459733168 grad: -0.035967959763563095
iteration: 0 loss: 2511.4937624278627 grad: 1631.0308290719577
iteration: 0 loss: 1635.9526864401014 grad: 1472.6530328193571
iteration: 0 loss: 2212.582860520219 grad: 1587.0425146100401
iteration: 10 loss: 0.009100996252858418 grad: 0.02381489352093331
iteration: 0 loss: 1606.5328384145687 grad: 1535.6053822839476
iteration: 0 loss: 2456.3767748137625 grad: 1628.4764692337553
iteration: 0 loss: 3726.215220588951 grad: 1849.9452900609056
iteration: 10 loss: 0.008133301348028577 grad: 0.41960035499233994
iteration: 0 loss: 1973.6024985159063 grad: 1565.6518759837595
iteration: 0 loss: 3218.0699861667354 grad: 1729.708931575336
iteration: 10 loss: 0.017282931511793075 grad: -0.07488400643810528
iteration: 0 loss: 3194.5499486007043 grad: 1825.382693968511
iteration: 0 loss: 1649.5451072615244 grad: 1353.774500370238
iteration: 0 loss: 1691.074023920363 grad: 1487.7802272418426
iteration: 0 loss: 2082.606444475571 grad: 1612.6755404922546
iteration: 0 loss: 2164.0781899421386 grad: 1576.4615211913228
iteration: 0 loss: 1624.4833148944447 grad: 1418.2816929128035
iteration: 0 loss: 2947.412570704958 grad: 1735.4530278864113
iteration: 10 loss: 0.009394395433032927 grad: 0.26335652608919485
iteration: 0 loss: 2457.249999443574 grad: 1592.3122483065565
iteration: 10 loss: 0.008445163548458368 grad: 0.03914902649234956
iteration: 0 loss: 2367.6976048537304 grad: 1691.762134315404
iteration: 0 loss: 2845.482934199422 grad: 1725.3450738538882
iteration: 10 loss: 0.007561801250135018 grad: 0.03930799528094693
iteration: 0 loss: 4408.893621437059 grad: 1921.6488574822056
iteration: 10 loss: 0.05664476996091914 grad: -1.9256921043498862
iteration: 0 loss: 1826.0851546511085 grad: 1482.8722628178293
iteration: 0 loss: 3014.1971237911257 grad: 1714.4204736443583
iteration: 10 loss: 0.008873656843868295 grad: -0.030177354519918757
iteration: 0 loss: 1901.7962498764675 grad: 1542.6140281726177
iteration: 0 loss: 1489.2975233445761 grad: 1430.6610181654614
iteration: 0 loss: 1948.0573369098704 grad: 1445.160550576692
iteration: 0 loss: 1539.0376931586761 grad: 1434.0892437931511
iteration: 0 loss: 2259.937643884173 grad: 1669.3670406942065
iteration: 0 loss: 3258.6148834469536 grad: 1691.121066262499
iteration: 10 loss: 0.009770534924014513 grad: -0.010745314234968384
iteration: 0 loss: 3653.844161445735 grad: 1835.4530370071789
iteration: 10 loss: 0.010493649160542356 grad: -1.0393413630639763
iteration: 0 loss: 3405.3375968109963 grad: 1750.5377819723712
iteration: 10 loss: 0.009216329072924882 grad: -0.07700756978186264
iteration: 0 loss: 2712.4879193167576 grad: 1683.107764041435
iteration: 0 loss: 3500.6232016716353 grad: 1751.558630062078
iteration: 10 loss: 0.008885338841619987 grad: 0.1424969304678238
iteration: 0 loss: 3453.698777056903 grad: 1755.0547293261868
iteration: 10 loss: 0.015104203384411945 grad: 0.9376580015782003
iteration: 0 loss: 2688.8218907155274 grad: 1525.9062919922249
iteration: 0 loss: 4192.687939578889 grad: 1801.9390272557994
iteration: 10 loss: 0.01001077766969419 grad: 0.752175777738682
iteration: 0 loss: 2864.9472263941875 grad: 1614.2756139122912
iteration: 10 loss: 0.008848980088342532 grad: 0.010141783322904057
iteration: 0 loss: 1457.251268575625 grad: 1381.6067322664198
iteration: 0 loss: 1294.5813046022884 grad: 1310.4809633383222
iteration: 0 loss: 1818.0896274420106 grad: 1437.3948654092399
iteration: 0 loss: 2410.2026565554256 grad: 1653.3390485364248
iteration: 0 loss: 2250.3003885933317 grad: 1550.9701064591018
iteration: 10 loss: 0.00783662788572573 grad: 0.08280983057985103
iteration: 0 loss: 1826.7850536574485 grad: 1502.756655400007
iteration: 0 loss: 1586.9526757571343 grad: 1456.0011616524696
iteration: 0 loss: 3312.5542923346834 grad: 1827.9981005265136
iteration: 10 loss: 0.009504181195965926 grad: 0.05549727638834308
iteration: 0 loss: 2689.965807630298 grad: 1657.1754842873831
iteration: 10 loss: 0.009121594209070528 grad: -0.1942963568490984
iteration: 0 loss: 2536.023257032173 grad: 1600.6239813678515
iteration: 10 loss: 0.008804044082353357 grad: 0.04924049781535948
iteration: 0 loss: 2202.2980067576723 grad: 1585.4969782419573
iteration: 0 loss: 1654.4697796315602 grad: 1367.353533831516
iteration: 0 loss: 2609.807033323729 grad: 1638.0547979986566
iteration: 0 loss: 1945.8233940641062 grad: 1514.9208157363823
iteration: 0 loss: 1947.1386195898294 grad: 1593.799126149744
iteration: 0 loss: 4447.756941522165 grad: 1965.648018477732
iteration: 10 loss: 0.048428529753106304 grad: -0.8983806155092222
iteration: 0 loss: 1459.5552778206709 grad: 1439.338353583776
iteration: 0 loss: 1418.7833374870738 grad: 1383.6352689624086
iteration: 0 loss: 1715.8813038530395 grad: 1458.2498250496174
iteration: 0 loss: 2145.698737078177 grad: 1623.99409358784
iteration: 0 loss: 2668.866366290785 grad: 1666.8559148709635
iteration: 0 loss: 3006.8297000509356 grad: 1779.4521062720003
iteration: 0 loss: 3680.9152336481534 grad: 1747.3986461536279
iteration: 10 loss: 0.042980361034014175 grad: -0.1893236557565271
iteration: 0 loss: 2881.6449017370383 grad: 1681.3245888566726
iteration: 10 loss: 0.027865455745169045 grad: -0.5085203942843003
iteration: 0 loss: 2283.0837999700702 grad: 1597.4532928915567
iteration: 0 loss: 3791.1395215048847 grad: 1801.1770265338153
iteration: 10 loss: 0.022787313360657372 grad: -1.0895506296496604
iteration: 0 loss: 2901.033955374512 grad: 1681.0772092695606
iteration: 0 loss: 1768.1192552155721 grad: 1577.1292377420798
iteration: 0 loss: 2153.3625785263253 grad: 1583.0419496283478
iteration: 0 loss: 1926.324687630435 grad: 1552.5642146203709
iteration: 0 loss: 3380.533799898201 grad: 1804.3003506767936
iteration: 10 loss: 0.055936134233144745 grad: 1.4695513311835244
iteration: 0 loss: 1751.750232793523 grad: 1589.5734429886465
iteration: 0 loss: 2203.4432587835845 grad: 1513.8853097474062
iteration: 0 loss: 1187.734436620847 grad: 1353.0755973806022
iteration: 0 loss: 2606.6799636912538 grad: 1729.851963608775
iteration: 0 loss: 2475.4809032367216 grad: 1679.595961858311
iteration: 0 loss: 2478.7783527372053 grad: 1658.1030630875812
iteration: 10 loss: 0.008631779386963568 grad: 0.00926467904776938
iteration: 0 loss: 3617.5376030436983 grad: 1722.3675117621565
iteration: 10 loss: 0.009093766189339063 grad: 0.8926414846438696
iteration: 0 loss: 1974.7927583870296 grad: 1557.3997791138809
iteration: 0 loss: 2307.3557966938915 grad: 1624.5400109508641
iteration: 0 loss: 3684.226973559908 grad: 1808.8598414844532
iteration: 10 loss: 0.007528391227507117 grad: -0.08876348005777665
iteration: 0 loss: 2269.9173129336864 grad: 1542.7468415555036
iteration: 0 loss: 3089.2291335939303 grad: 1652.4801088511076
iteration: 10 loss: 0.008395051378598013 grad: -0.0305833417915845
iteration: 0 loss: 1942.0094226802262 grad: 1576.4241439653783
iteration: 0 loss: 3379.800271984076 grad: 1819.246028655979
iteration: 10 loss: 0.010568410043064929 grad: 0.08217159164082129
iteration: 0 loss: 2916.241577064749 grad: 1723.2212926346253
iteration: 0 loss: 1867.2056769537003 grad: 1428.3574497127179
iteration: 0 loss: 1440.616605007845 grad: 1383.0484351591824
iteration: 0 loss: 2326.558894956444 grad: 1708.790553254198
iteration: 0 loss: 1172.334295457894 grad: 1327.2387123187189
iteration: 0 loss: 1695.2619449996594 grad: 1343.0149011646058
iteration: 0 loss: 1433.2352349518633 grad: 1375.7371692399843
iteration: 0 loss: 3461.8536307357467 grad: 1754.3747171595119
iteration: 10 loss: 0.00821608003951207 grad: 0.030181795271927006
iteration: 0 loss: 2118.7398446469447 grad: 1502.5489945358613
iteration: 0 loss: 3578.244918080882 grad: 1748.705707745417
iteration: 10 loss: 0.22219941106463506 grad: 0.7423118469926087
iteration: 0 loss: 3421.0722283082473 grad: 1750.1981576622866
iteration: 10 loss: 0.008961287083035462 grad: 0.11392763935139694
iteration: 0 loss: 4070.7843530265513 grad: 1877.7805934094704
iteration: 10 loss: 0.029283556603662542 grad: 1.1753662584523252
iteration: 0 loss: 1813.877158842551 grad: 1485.433746985784
iteration: 0 loss: 1863.5303196714847 grad: 1505.5511568639618
iteration: 0 loss: 2399.532738470922 grad: 1692.4828065671945
iteration: 0 loss: 2457.4494595940923 grad: 1637.7043250365004
iteration: 0 loss: 4680.199772498915 grad: 1895.7860986362111
iteration: 10 loss: 0.06069233516675674 grad: 2.353614633106419
iteration: 0 loss: 2011.9235903580557 grad: 1603.6129608518463
iteration: 0 loss: 3583.354103337656 grad: 1816.0732451938202
iteration: 10 loss: 0.013241979735374282 grad: -0.4637064089893614
iteration: 0 loss: 2341.030653294305 grad: 1592.0477719043379
iteration: 0 loss: 1521.0322188730306 grad: 1498.8853116159567
iteration: 0 loss: 3401.561099681132 grad: 1780.5308995952064
iteration: 10 loss: 0.009456198471360205 grad: 0.059076044173952354
iteration: 0 loss: 1664.817424735897 grad: 1489.020215642242
iteration: 0 loss: 4081.7035269570074 grad: 1812.9530539866494
iteration: 10 loss: 0.47013979764067865 grad: 2.355209514699033
iteration: 0 loss: 1473.8172425956275 grad: 1417.6452017584397
iteration: 0 loss: 3198.3182061879866 grad: 1724.3909814574483
iteration: 0 loss: 1621.1031414456265 grad: 1407.0930046795759
iteration: 0 loss: 2434.260012034269 grad: 1714.3858981407884
iteration: 10 loss: 0.008778388742278119 grad: 0.04459233108268688
iteration: 0 loss: 1759.6936511421634 grad: 1528.302698837839
iteration: 0 loss: 2339.838007668279 grad: 1592.886895644815
iteration: 10 loss: 0.008352406736362767 grad: -0.005089391204442503
iteration: 0 loss: 2334.175212205351 grad: 1590.2226254767206
iteration: 0 loss: 1302.9345340083273 grad: 1354.885778622929
iteration: 0 loss: 2972.9077961808744 grad: 1645.0694955450078
iteration: 10 loss: 0.007630329836419233 grad: 0.09384386138482774
iteration: 0 loss: 2842.7787978029573 grad: 1666.0134588900432
iteration: 10 loss: 0.008640652480111881 grad: 0.3685828010208485
iteration: 0 loss: 9961.922949861671 grad: 2876.159826445306
iteration: 10 loss: 0.0008007332269864327 grad: -0.17091778401713123
iteration: 0 loss: 8015.7829981005925 grad: 2703.3349113863937
iteration: 0 loss: 8841.212963809061 grad: 2734.7750502232716
iteration: 10 loss: 0.0005997495622564616 grad: -0.06873656346881757
iteration: 0 loss: 11016.614165337109 grad: 2997.376321708026
iteration: 10 loss: 0.000690293850229037 grad: -0.008502609096586573
iteration: 0 loss: 6857.182643951483 grad: 2610.2959339961585
iteration: 0 loss: 4628.392119915538 grad: 2327.198153887123
iteration: 10 loss: 0.005497947128870609 grad: 0.018660106545788487
iteration: 0 loss: 7335.636279155416 grad: 2689.5154114203096
iteration: 0 loss: 3962.6083038001775 grad: 2202.6777452878973
iteration: 0 loss: 10877.519023472792 grad: 2819.2701706478656
iteration: 10 loss: 0.000629451403669505 grad: 0.5637222625511434
iteration: 0 loss: 7176.796077104809 grad: 2622.0121388267657
iteration: 0 loss: 4521.224223746993 grad: 2400.2783643059583
iteration: 0 loss: 9153.24493510212 grad: 2821.1353040406834
iteration: 10 loss: 0.0007838463934604079 grad: -0.001653420848035391
iteration: 0 loss: 6209.051099865025 grad: 2387.411196616953
iteration: 0 loss: 6681.016051221602 grad: 2663.774018186482
iteration: 0 loss: 8835.844095138476 grad: 2706.3428001720113
iteration: 0 loss: 12768.880147484642 grad: 3063.170287653194
iteration: 0 loss: 7543.648761237096 grad: 2629.9367471701976
iteration: 0 loss: 5359.510824717764 grad: 2454.800403164336
iteration: 0 loss: 12883.398038699042 grad: 3070.9278696111824
iteration: 10 loss: 0.004286969064692544 grad: 2.993326769083188
iteration: 0 loss: 6321.490968745801 grad: 2488.1285617627373
iteration: 0 loss: 9107.964981748328 grad: 2817.320564086175
iteration: 0 loss: 11020.959384844888 grad: 2927.62124197877
iteration: 10 loss: 0.0007299901309042153 grad: -0.06939910276856201
iteration: 0 loss: 5570.5716499689825 grad: 2362.794976473927
iteration: 0 loss: 6931.7223417917985 grad: 2601.882627733503
iteration: 0 loss: 7099.046523607728 grad: 2724.6072903727063
iteration: 0 loss: 6030.196193106391 grad: 2630.9260429359774
iteration: 10 loss: 0.000770644935652275 grad: 0.00026312453699535313
iteration: 0 loss: 13760.451050004624 grad: 3086.936715112639
iteration: 10 loss: 0.01087475029370663 grad: -0.5017593988850022
iteration: 0 loss: 13489.513153226046 grad: 2989.8432481054006
iteration: 0 loss: 7791.270402392679 grad: 2688.5104924010466
iteration: 10 loss: 0.0006398570575666698 grad: 0.024666773197614556
iteration: 0 loss: 5804.439886287316 grad: 2477.453203415542
iteration: 10 loss: 0.028929781820889646 grad: 0.22199444473135704
iteration: 0 loss: inf grad: 3262.1251231438164
iteration: 0 loss: 7779.602724783408 grad: 2841.149940404868
iteration: 0 loss: 7446.670561160355 grad: 2682.025205914888
iteration: 0 loss: 10434.220026629315 grad: 2782.8572546676687
iteration: 10 loss: 0.0006609634649728171 grad: 1.0212826425896155
iteration: 0 loss: 10188.006814339638 grad: 2871.3384581232403
iteration: 10 loss: 0.0006998688093682921 grad: 0.0774087137894954
iteration: 0 loss: 8580.682549253432 grad: 2792.434904798181
iteration: 10 loss: 0.0007290661795361137 grad: 0.05798947675491269
iteration: 0 loss: 8077.636012885837 grad: 2677.4302583479785
iteration: 10 loss: 0.009962754771219228 grad: 0.41311647190355594
iteration: 0 loss: 12901.888111378632 grad: 3091.050397266251
iteration: 10 loss: 0.054210376293419606 grad: 2.6357542085788745
iteration: 0 loss: 6041.993307060423 grad: 2551.36165802059
iteration: 0 loss: 6728.735085986435 grad: 2597.6422219488195
iteration: 0 loss: 8378.609901062217 grad: 2720.4111743780127
iteration: 0 loss: 4354.885694486646 grad: 2361.33268056742
iteration: 0 loss: 17929.50075369228 grad: 3361.990140675969
iteration: 0 loss: 7037.543489143263 grad: 2505.722732627143
iteration: 0 loss: 9758.280798128422 grad: 3009.265740674752
iteration: 10 loss: 0.0006932278497102247 grad: 0.029824323644445982
iteration: 0 loss: 10038.531518818996 grad: 2782.3330399721726
iteration: 10 loss: 0.0006918549161954699 grad: 0.02508918675340127
iteration: 0 loss: 7538.977014038897 grad: 2443.595649892897
iteration: 0 loss: 6270.0864117955525 grad: 2642.8828264833646
iteration: 0 loss: 10997.080460132236 grad: 2916.331072477994
iteration: 10 loss: 0.0006502041228073226 grad: -0.18940662490113158
iteration: 0 loss: 11442.521166751721 grad: 2931.013704192987
iteration: 10 loss: 0.08314609082828445 grad: 1.6768403554909466
iteration: 0 loss: inf grad: 3027.8165231214643
iteration: 0 loss: 7185.313719865398 grad: 2722.704477059104
iteration: 10 loss: 0.0005296783466209573 grad: 0.01107259740343366
iteration: 0 loss: 12105.30931144173 grad: 3101.432603654813
iteration: 10 loss: 0.0006414672137576748 grad: 0.7676179587322197
iteration: 0 loss: 8854.234243683719 grad: 2777.56180386462
iteration: 10 loss: 0.0006126061092469503 grad: -0.1262287869862329
iteration: 0 loss: 5986.02132163567 grad: 2511.3526784847672
iteration: 0 loss: 7964.470238094757 grad: 2705.935912735923
iteration: 0 loss: 6030.026763592156 grad: 2623.184840197302
iteration: 0 loss: 8723.46668038672 grad: 2773.5910611717363
iteration: 0 loss: inf grad: 3151.063794360814
iteration: 0 loss: 7071.788129155421 grad: 2670.276052159398
iteration: 0 loss: 11388.251120448464 grad: 2947.418758031582
iteration: 10 loss: 0.018778667314274407 grad: 3.7262184041838893
iteration: 0 loss: 11373.949268097804 grad: 3106.6823313572027
iteration: 10 loss: 0.0005810518685558981 grad: -0.00410369329549164
iteration: 0 loss: 5913.656110100398 grad: 2309.544233984509
iteration: 0 loss: 6276.256361409053 grad: 2538.1502791895136
iteration: 0 loss: 7416.345136384771 grad: 2744.6675061750084
iteration: 0 loss: 7685.43854286043 grad: 2684.8050117666317
iteration: 0 loss: 5795.014621818774 grad: 2422.836399285261
iteration: 0 loss: 10485.71306695215 grad: 2959.0309571615526
iteration: 10 loss: 0.0008304352734491906 grad: 0.026822278606221703
iteration: 0 loss: 8449.208941294155 grad: 2710.374750182733
iteration: 10 loss: 0.01100993474194018 grad: 0.8431498753271277
iteration: 0 loss: 8497.242181431136 grad: 2883.0112290759207
iteration: 0 loss: 10217.481850011192 grad: 2937.5915216451885
iteration: 10 loss: 0.0007730769258077172 grad: -0.30054688148199277
iteration: 0 loss: 15364.04920018546 grad: 3269.8978239367925
iteration: 0 loss: 6466.414635892915 grad: 2527.7214017060596
iteration: 0 loss: 10788.328135061347 grad: 2917.7358034558056
iteration: 10 loss: 0.0007585732541470365 grad: 0.0511081333710968
iteration: 0 loss: 6743.214021571705 grad: 2626.6220172154262
iteration: 0 loss: 5338.219637811099 grad: 2440.087468239638
iteration: 0 loss: 6917.519750163867 grad: 2464.721776714231
iteration: 0 loss: 5595.765806016766 grad: 2445.895306251351
iteration: 0 loss: 8199.51562236067 grad: 2844.527883587737
iteration: 0 loss: 11363.78811730813 grad: 2884.071623204241
iteration: 10 loss: 0.0005367405829019845 grad: -0.541062330774088
iteration: 0 loss: 12911.31404454403 grad: 3121.0503711777183
iteration: 10 loss: 0.0008048927861223506 grad: -0.03416001596599544
iteration: 0 loss: 11938.410528577113 grad: 2981.735207374671
iteration: 10 loss: 0.036708952676483685 grad: -2.1507662003101413
iteration: 0 loss: 9700.808494210323 grad: 2869.013856302354
iteration: 0 loss: 12210.281606263372 grad: 2984.777901548383
iteration: 10 loss: 0.11842689191954295 grad: 2.8180045834896332
iteration: 0 loss: 12104.070424819809 grad: 2991.2299954743535
iteration: 10 loss: 0.018298758556325498 grad: 1.247729107926299
iteration: 0 loss: 9055.5446337919 grad: 2599.8553038354626
iteration: 10 loss: 0.02272167289894159 grad: 0.6036116162961596
iteration: 0 loss: 14356.92470320656 grad: 3069.254523119859
iteration: 10 loss: 0.22037969625142234 grad: 2.8590453838805026
iteration: 0 loss: 9780.444306960419 grad: 2753.3143953841563
iteration: 0 loss: 5355.192677752901 grad: 2357.8394983809003
iteration: 0 loss: 4694.591179383709 grad: 2240.618986750943
iteration: 0 loss: 6354.323594942239 grad: 2451.032217217101
iteration: 0 loss: 8527.38207537672 grad: 2818.2320932754396
iteration: 0 loss: inf grad: 2642.5946485034056
iteration: 10 loss: 0.0004243001220112836 grad: 0.015403435446095869
iteration: 0 loss: 6667.635080652987 grad: 2561.952701924958
iteration: 0 loss: 5719.032559724738 grad: 2481.7301591611003
iteration: 0 loss: 11714.634439221243 grad: 3112.7264184137175
iteration: 10 loss: 0.006542006459891458 grad: 0.3125073789094347
iteration: 0 loss: 9475.67989438646 grad: 2823.865168797473
iteration: 10 loss: 0.0007516821841074324 grad: 1.211714573801111
iteration: 0 loss: 8637.371389769427 grad: 2728.172705284748
iteration: 10 loss: 0.0006796726216138763 grad: 0.012279546753352698
iteration: 0 loss: 7832.124231427194 grad: 2701.589669916656
iteration: 10 loss: 0.0006110805658284913 grad: 0.11578776504017425
iteration: 0 loss: 5661.169050150301 grad: 2332.058163888344
iteration: 0 loss: 9225.75982357188 grad: 2793.917580714933
iteration: 0 loss: 6974.310634105699 grad: 2582.4989712368038
iteration: 0 loss: 7176.772271870327 grad: 2714.254891980781
iteration: 0 loss: 15704.350895978074 grad: 3344.801457133947
iteration: 0 loss: 5292.670292586587 grad: 2451.706510916356
iteration: 0 loss: 5196.857056621826 grad: 2360.497017020349
iteration: 0 loss: 6129.026655486185 grad: 2483.201585154655
iteration: 0 loss: 7857.224890614684 grad: 2769.884990676019
iteration: 0 loss: 9368.209114590298 grad: 2840.0675824776963
iteration: 10 loss: 0.0004672492650570348 grad: 0.008215675001050192
iteration: 0 loss: 10759.506905743254 grad: 3029.913496986491
iteration: 10 loss: 0.0007591418794948946 grad: 0.006588665658447394
iteration: 0 loss: 13287.995739703287 grad: 2974.744372221283
iteration: 0 loss: 10471.963709322816 grad: 2861.285768168366
iteration: 10 loss: 0.001354095174147832 grad: 2.7886929584165108
iteration: 0 loss: 8122.599991867824 grad: 2720.3712195070975
iteration: 0 loss: 13440.69829008697 grad: 3067.6207565036575
iteration: 10 loss: 0.4340739921582016 grad: -0.7521619793582727
iteration: 0 loss: 10116.934933448489 grad: 2863.9284253237515
iteration: 0 loss: 6627.559763696793 grad: 2687.871376122641
iteration: 10 loss: 0.0006570323391563513 grad: 0.01132451974740499
iteration: 0 loss: 7648.565031456694 grad: 2697.6192979063353
iteration: 0 loss: 6923.214892253058 grad: 2646.1389311311245
iteration: 0 loss: 12176.534454331244 grad: 3073.117661140758
iteration: 10 loss: 0.048120279050305144 grad: 2.3166591016227986
iteration: 0 loss: 6374.952413914173 grad: 2708.3627410800977
iteration: 10 loss: 0.0006186408175959845 grad: 0.01955786874493364
iteration: 0 loss: 7750.426248996265 grad: 2581.209373722321
iteration: 0 loss: 4360.602395432959 grad: 2305.5586657241774
iteration: 0 loss: 9075.359939164937 grad: 2942.624166969419
iteration: 10 loss: 0.0005788225495383482 grad: 0.01907173042596972
iteration: 0 loss: 8833.841751988122 grad: 2861.8003795185687
iteration: 10 loss: 0.0007578865783712404 grad: 0.1347668099409106
iteration: 0 loss: 8836.547077333425 grad: 2825.257907587413
iteration: 0 loss: 12439.40305311656 grad: 2934.0410523825913
iteration: 10 loss: 0.6096825154752217 grad: 2.3200845428556027
iteration: 0 loss: 7137.53407645137 grad: 2653.3474771363917
iteration: 0 loss: 8303.15470519062 grad: 2769.780760944987
iteration: 0 loss: 12687.25344061678 grad: 3081.6657314167396
iteration: 10 loss: 0.0008360551093408668 grad: 1.0370779824251959
iteration: 0 loss: 8054.496147556187 grad: 2633.413459361171
iteration: 10 loss: 0.009680743204874241 grad: -0.17326663189213154
iteration: 0 loss: 10842.486685166457 grad: 2817.781475735259
iteration: 10 loss: 0.03917499892254868 grad: -0.7371874444649016
iteration: 0 loss: 7032.1158477943245 grad: 2689.8147598349387
iteration: 0 loss: 11852.157655498524 grad: 3097.2135508035517
iteration: 0 loss: 10159.344042549135 grad: 2934.084702625724
iteration: 0 loss: 6569.955081073394 grad: 2439.3945913460793
iteration: 0 loss: 5176.192100491655 grad: 2365.322316468055
iteration: 0 loss: 8433.988894719352 grad: 2910.520886617639
iteration: 0 loss: 4403.885127660633 grad: 2268.702746471143
iteration: 0 loss: 5715.039249409023 grad: 2292.33600731107
iteration: 0 loss: 5071.870380840332 grad: 2346.5101347388368
iteration: 0 loss: 12166.04802833874 grad: 2988.5252344302544
iteration: 10 loss: 0.0008986137981992215 grad: 0.9433968066766055
iteration: 0 loss: 7421.240147844592 grad: 2563.6158301941923
iteration: 0 loss: 12836.602923257624 grad: 2977.845548054239
iteration: 0 loss: 12002.970527072315 grad: 2980.896160560581
iteration: 10 loss: 0.001482915691502223 grad: -0.4041181301667825
iteration: 0 loss: 14410.07790114592 grad: 3193.8488919075085
iteration: 0 loss: 6601.26721015327 grad: 2535.191093786343
iteration: 0 loss: 6714.6343755902935 grad: 2568.9422341694517
iteration: 0 loss: 8668.310627445906 grad: 2882.9730747775175
iteration: 10 loss: 0.0007355992221908474 grad: 0.014550284675420406
iteration: 0 loss: 8805.58206883575 grad: 2788.0620532182747
iteration: 0 loss: inf grad: 3230.553999718165
iteration: 0 loss: 7401.614155288705 grad: 2730.2675002577826
iteration: 10 loss: 0.0006098324335074391 grad: -0.0012541254916467485
iteration: 0 loss: 12720.025609424121 grad: 3091.4666266759923
iteration: 10 loss: 0.0007281697180587798 grad: 1.2325227321015968
iteration: 0 loss: 8354.12742650239 grad: 2715.0385666644916
iteration: 10 loss: 0.000593778351850977 grad: 0.013781997702999971
iteration: 0 loss: 5689.957804943195 grad: 2553.620548691532
iteration: 0 loss: 11886.213093449654 grad: 3032.366273477562
iteration: 10 loss: 0.005886213020527397 grad: 0.5876046790966402
iteration: 0 loss: 5998.254338329606 grad: 2540.3817779766837
iteration: 0 loss: 14387.877921911353 grad: 3087.824935752829
iteration: 0 loss: 5349.849875660911 grad: 2419.5689123489624
iteration: 0 loss: 11215.113440830126 grad: 2938.941048367107
iteration: 10 loss: 0.03751177047061818 grad: 0.6420765478098046
iteration: 0 loss: 5802.36709155399 grad: 2396.8952736776305
iteration: 0 loss: 8753.111481085194 grad: 2921.8449024881247
iteration: 10 loss: 0.0007463880915152417 grad: 0.06873327661865193
iteration: 0 loss: 6327.688823291958 grad: 2605.656507655603
iteration: 0 loss: 8653.073632494395 grad: 2716.2965496900997
iteration: 10 loss: 0.0008228482144080441 grad: -1.0144650965029944
iteration: 0 loss: 8232.816078664539 grad: 2709.658207905029
iteration: 0 loss: 4733.258288690305 grad: 2312.164885498717
iteration: 0 loss: 10363.211237960373 grad: 2802.332632881902
iteration: 10 loss: 0.0018315950212788514 grad: 0.4658677806987078
iteration: 0 loss: 10076.40925115334 grad: 2838.4396858754
iteration: 10 loss: 0.0007864961252463135 grad: 0.020854434777793633
iteration: 0 loss: inf grad: 4143.595753734693
iteration: 0 loss: inf grad: 3888.692696422713
iteration: 0 loss: inf grad: 3938.331305202574
iteration: 0 loss: inf grad: 4315.747243446158
iteration: 0 loss: inf grad: 3760.4763983651565
iteration: 0 loss: inf grad: 3350.6467830181427
iteration: 0 loss: inf grad: 3877.856994994648
iteration: 0 loss: inf grad: 3171.485065144064
iteration: 0 loss: inf grad: 4063.2074536290183
iteration: 0 loss: inf grad: 3777.6806416255044
iteration: 0 loss: inf grad: 3460.1896429034114
iteration: 0 loss: inf grad: 4061.3854643224786
iteration: 0 loss: inf grad: 3438.8640037264286
iteration: 0 loss: inf grad: 3838.8062208094584
iteration: 0 loss: inf grad: 3896.5886309590815
iteration: 0 loss: inf grad: 4416.830316340913
iteration: 0 loss: inf grad: 3789.732490730471
iteration: 0 loss: inf grad: 3529.922944961683
iteration: 0 loss: inf grad: 4424.328241375908
iteration: 0 loss: inf grad: 3579.4263265454647
iteration: 0 loss: inf grad: 4058.369057751991
iteration: 0 loss: inf grad: 4216.814555022441
iteration: 0 loss: inf grad: 3402.578332082627
iteration: 0 loss: inf grad: 3750.4044116561795
iteration: 0 loss: inf grad: 3933.8630809620763
iteration: 0 loss: inf grad: 3784.927560700579
iteration: 0 loss: inf grad: 4444.184546292272
iteration: 0 loss: inf grad: 4306.646261639399
iteration: 0 loss: inf grad: 3874.8440957718285
iteration: 0 loss: inf grad: 3562.615480481251
iteration: 0 loss: inf grad: 4696.41683385612
iteration: 0 loss: inf grad: 4093.7193025306533
iteration: 0 loss: inf grad: 3863.3307116759975
iteration: 0 loss: inf grad: 4011.8289169682057
iteration: 0 loss: inf grad: 4135.247235413137
iteration: 0 loss: inf grad: 4024.644699042571
iteration: 0 loss: inf grad: 3855.617908228349
iteration: 0 loss: inf grad: 4455.862566952436
iteration: 0 loss: inf grad: 3677.182349322235
iteration: 0 loss: inf grad: 3740.539798420073
iteration: 0 loss: inf grad: 3919.9277865762497
iteration: 0 loss: inf grad: 3403.550906511518
iteration: 0 loss: inf grad: 4846.988608371614
iteration: 0 loss: inf grad: 3609.317106959101
iteration: 0 loss: inf grad: 4332.116821442979
iteration: 0 loss: inf grad: 4009.64610331287
iteration: 0 loss: inf grad: 3521.9467719179424
iteration: 0 loss: inf grad: 3807.981066205978
iteration: 0 loss: inf grad: 4200.723394733647
iteration: 0 loss: inf grad: 4222.4736460590475
iteration: 0 loss: inf grad: 4358.344376643886
iteration: 0 loss: inf grad: 3919.707868446696
iteration: 0 loss: inf grad: 4468.86709478901
iteration: 0 loss: inf grad: 4002.3987561715458
iteration: 0 loss: inf grad: 3619.1403363866766
iteration: 0 loss: inf grad: 3896.3450529911784
iteration: 0 loss: inf grad: 3784.801960259253
iteration: 0 loss: inf grad: 3999.0373765743057
iteration: 0 loss: inf grad: 4537.790171952503
iteration: 0 loss: inf grad: 3848.29108023132
iteration: 0 loss: inf grad: 4246.328109204149
iteration: 0 loss: inf grad: 4479.5720717768945
iteration: 0 loss: inf grad: 3329.323760640442
iteration: 0 loss: inf grad: 3659.7683726779005
iteration: 0 loss: inf grad: 3957.0313050640907
iteration: 0 loss: inf grad: 3867.882628630549
iteration: 0 loss: inf grad: 3487.13139873174
iteration: 0 loss: inf grad: 4263.365042932375
iteration: 0 loss: inf grad: 3902.751240795901
iteration: 0 loss: inf grad: 4151.77325029157
iteration: 0 loss: inf grad: 4232.711037789563
iteration: 0 loss: inf grad: 4708.069965984676
iteration: 0 loss: inf grad: 3637.8640207479575
iteration: 0 loss: inf grad: 4206.276302746511
iteration: 0 loss: inf grad: 3784.0634887410797
iteration: 0 loss: inf grad: 3514.871869489313
iteration: 0 loss: inf grad: 3546.7662863655996
iteration: 0 loss: inf grad: 3523.99817330998
iteration: 0 loss: inf grad: 4092.5890853021265
iteration: 0 loss: inf grad: 4152.527938769728
iteration: 0 loss: inf grad: 4501.6657126362725
iteration: 0 loss: inf grad: 4297.563841557236
iteration: 0 loss: inf grad: 4130.298723684226
iteration: 0 loss: inf grad: 4301.5408002707245
iteration: 0 loss: inf grad: 4308.494041160459
iteration: 0 loss: inf grad: 3746.1070152936277
iteration: 0 loss: inf grad: 4425.1085574788885
iteration: 0 loss: inf grad: 3965.73677979727
iteration: 0 loss: inf grad: 3395.3265610504745
iteration: 0 loss: inf grad: 3225.99105483633
iteration: 0 loss: inf grad: 3537.2966761732296
iteration: 0 loss: inf grad: 4057.402827792076
iteration: 0 loss: inf grad: 3807.582745579147
iteration: 0 loss: inf grad: 3694.5169336560452
iteration: 0 loss: inf grad: 3577.290799943558
iteration: 0 loss: inf grad: 4485.946069789166
iteration: 0 loss: inf grad: 4071.310430404608
iteration: 0 loss: inf grad: 3928.034511369255
iteration: 0 loss: inf grad: 3895.5426883365244
iteration: 0 loss: inf grad: 3360.4376539558943
iteration: 0 loss: inf grad: 4022.8295348481406
iteration: 0 loss: inf grad: 3717.8083547163815
iteration: 0 loss: inf grad: 3913.0496351641664
iteration: 0 loss: inf grad: 4817.820397055746
iteration: 0 loss: inf grad: 3528.8468542369974
iteration: 0 loss: inf grad: 3398.448175668353
iteration: 0 loss: inf grad: 3576.491904348932
iteration: 0 loss: inf grad: 3991.973792095159
iteration: 0 loss: inf grad: 4090.0254614661035
iteration: 0 loss: inf grad: 4368.450184407192
iteration: 0 loss: inf grad: 4288.549348449424
iteration: 0 loss: inf grad: 4119.830291690052
iteration: 0 loss: inf grad: 3912.9016043253114
iteration: 0 loss: inf grad: 4418.954313368973
iteration: 0 loss: inf grad: 4126.018986434704
iteration: 0 loss: inf grad: 3868.449189402889
iteration: 0 loss: inf grad: 3886.6200393965933
iteration: 0 loss: inf grad: 3813.1240992040493
iteration: 0 loss: inf grad: 4424.691837422582
iteration: 0 loss: inf grad: 3899.8010130892485
iteration: 0 loss: inf grad: 3719.6777867716955
iteration: 0 loss: inf grad: 3326.0935857279233
iteration: 0 loss: inf grad: 4244.348653552324
iteration: 0 loss: inf grad: 4127.026960520613
iteration: 0 loss: inf grad: 4065.738028679827
iteration: 0 loss: inf grad: 4230.48775288847
iteration: 0 loss: inf grad: 3828.6964901458846
iteration: 0 loss: inf grad: 3990.174103806137
iteration: 0 loss: inf grad: 4438.961875710656
iteration: 0 loss: inf grad: 3796.5694221327426
iteration: 0 loss: inf grad: 4056.060734818652
iteration: 0 loss: inf grad: 3867.6198933432315
iteration: 0 loss: inf grad: 4460.2805096952925
iteration: 0 loss: inf grad: 4229.781799295553
iteration: 0 loss: inf grad: 3514.894320813921
iteration: 0 loss: inf grad: 3402.3367865963037
iteration: 0 loss: inf grad: 4193.776216774329
iteration: 0 loss: inf grad: 3265.6177182807314
iteration: 0 loss: inf grad: 3302.003510369279
iteration: 0 loss: inf grad: 3381.484799473653
iteration: 0 loss: inf grad: 4301.840638570923
iteration: 0 loss: inf grad: 3689.5520873969763
iteration: 0 loss: inf grad: 4292.65419718783
iteration: 0 loss: inf grad: 4295.60451611807
iteration: 0 loss: inf grad: 4599.83512914777
iteration: 0 loss: inf grad: 3648.0893047469476
iteration: 0 loss: inf grad: 3695.2583089520654
iteration: 0 loss: inf grad: 4151.925090303453
iteration: 0 loss: inf grad: 4017.1127513072424
iteration: 0 loss: inf grad: 4649.630383301923
iteration: 0 loss: inf grad: 3936.7234416395877
iteration: 0 loss: inf grad: 4452.192922158688
iteration: 0 loss: inf grad: 3911.734731108685
iteration: 0 loss: inf grad: 3685.899727567784
iteration: 0 loss: inf grad: 4369.531529773372
iteration: 0 loss: inf grad: 3664.378621716643
iteration: 0 loss: inf grad: 4445.73220984432
iteration: 0 loss: inf grad: 3484.630271859966
iteration: 0 loss: inf grad: 4236.50061548411
iteration: 0 loss: inf grad: 3461.6521441506725
iteration: 0 loss: inf grad: 4208.73567779764
iteration: 0 loss: inf grad: 3761.418097173989
iteration: 0 loss: inf grad: 3910.597334630891
iteration: 0 loss: inf grad: 3906.7370838803554
iteration: 0 loss: inf grad: 3331.289572639756
iteration: 0 loss: inf grad: 4035.9941525872723
iteration: 0 loss: inf grad: 4085.82304709076
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:57: RuntimeWarning: invalid value encountered in double_scalars
  diff = np.absolute(f_history[-1]-f_history[-2])
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:57: RuntimeWarning: invalid value encountered in double_scalars
  diff = np.absolute(f_history[-1]-f_history[-2])
iteration: 0 loss: 54.062488162386714 grad: 319.3584309927471
iteration: 10 loss: 6.578276741945584 grad: -1.380081179180992
iteration: 20 loss: 3.680720872266482 grad: -1.2915301176215601
iteration: 30 loss: 2.556517771562788 grad: -1.006534308972581
iteration: 40 loss: 1.958846628915167 grad: -0.812041634282918
iteration: 50 loss: 1.5879861968817501 grad: -0.6780606236014071
iteration: 60 loss: 1.3354127729557645 grad: -0.5813393476080342
iteration: 70 loss: 1.1523098277749673 grad: -0.5085463450722789
iteration: 80 loss: 1.013473187788019 grad: -0.4518846284348359
iteration: 90 loss: 0.9045755204856918 grad: -0.40656748046603236
iteration: 100 loss: 0.8168706289554227 grad: -0.3695150199707391
iteration: 110 loss: 0.7447170793674228 grad: -0.338662907938436
iteration: 120 loss: 0.6843128306760181 grad: -0.3125784266696374
iteration: 0 loss: 50.98504592678699 grad: 291.5265286192216
iteration: 10 loss: 6.952269795784146 grad: -0.6233746967988119
iteration: 20 loss: 3.8608525068040596 grad: -0.9429236097354787
iteration: 30 loss: 2.6756427967162297 grad: -0.7853534531769011
iteration: 40 loss: 2.048191276869106 grad: -0.6521495540383568
iteration: 50 loss: 1.6596342185689124 grad: -0.5538765905444636
iteration: 60 loss: 1.3953022360494123 grad: -0.4803817160389982
iteration: 70 loss: 1.2038033218872526 grad: -0.423827712153383
iteration: 80 loss: 1.0586628436639414 grad: -0.3791165569620206
iteration: 90 loss: 0.9448533942344594 grad: -0.34293762414507556
iteration: 100 loss: 0.8532107589102121 grad: -0.3130840874956109
iteration: 110 loss: 0.7778282132093491 grad: -0.288039870869898
iteration: 120 loss: 0.714727089427387 grad: -0.2667332778524225
iteration: 130 loss: 0.6611293283841323 grad: -0.24838705217067725
iteration: 0 loss: 52.973407747808636 grad: 297.1729840384718
iteration: 10 loss: 6.934596030219878 grad: -1.169814511398807
iteration: 20 loss: 3.853537040395085 grad: -1.1201352971546494
iteration: 30 loss: 2.670895933770432 grad: -0.8800171629129366
iteration: 40 loss: 2.044704534519455 grad: -0.7143696975168894
iteration: 50 loss: 1.6568998980588647 grad: -0.5994015062602751
iteration: 60 loss: 1.3930660421886965 grad: -0.515912335049654
iteration: 70 loss: 1.2019198337744372 grad: -0.45277251817028147
iteration: 80 loss: 1.0570413734280093 grad: -0.4034249176055387
iteration: 90 loss: 0.9434337375328968 grad: -0.36382062211168
iteration: 100 loss: 0.8519509987118826 grad: -0.3313418812745299
iteration: 110 loss: 0.7766980468883379 grad: -0.30422684954511514
iteration: 120 loss: 0.7137039329815955 grad: -0.28124834321810177
iteration: 130 loss: 0.6601959208107308 grad: -0.26152619616803713
iteration: 0 loss: 51.08880097042891 grad: 331.78694501937935
iteration: 10 loss: 6.875326972260774 grad: -1.0683683739433802
iteration: 20 loss: 3.813866593365922 grad: -1.066233579260304
iteration: 30 loss: 2.6417104215887446 grad: -0.8490799076644475
iteration: 40 loss: 2.021706341760984 grad: -0.6937909133414337
iteration: 50 loss: 1.6379489362493587 grad: -0.5844150676267186
iteration: 60 loss: 1.3769611886681228 grad: -0.5043319603449397
iteration: 70 loss: 1.187923038477534 grad: -0.4434462398911463
iteration: 80 loss: 1.0446676364989054 grad: -0.39568261261773063
iteration: 90 loss: 0.9323476213548102 grad: -0.35724279889818633
iteration: 100 loss: 0.8419110084160324 grad: -0.3256507451086336
iteration: 110 loss: 0.7675246175522674 grad: -0.2992300875734255
iteration: 120 loss: 0.7052599608718992 grad: -0.27680795614728615
iteration: 130 loss: 0.6523743768822731 grad: -0.2575401862293325
iteration: 0 loss: 53.15387422399278 grad: 281.7226222526391
iteration: 10 loss: 6.877740545225606 grad: -0.7877037876966664
iteration: 20 loss: 3.8248335701693823 grad: -0.9886258759035359
iteration: 30 loss: 2.6533339177213664 grad: -0.8083034408743559
iteration: 40 loss: 2.032601953890662 grad: -0.6673309174374522
iteration: 50 loss: 1.6479183286340808 grad: -0.5653454229299965
iteration: 60 loss: 1.3860590317644066 grad: -0.48969104187338297
iteration: 70 loss: 1.1962536010602725 grad: -0.4317167220746544
iteration: 80 loss: 1.0523341299281128 grad: -0.38599254687408346
iteration: 90 loss: 0.9394402340803523 grad: -0.3490494167907384
iteration: 100 loss: 0.8485056463596679 grad: -0.31859559144667615
iteration: 110 loss: 0.7736845068789907 grad: -0.2930653512766635
iteration: 120 loss: 0.7110377709784437 grad: -0.27135594714831235
iteration: 130 loss: 0.657814179462176 grad: -0.25266963125352665
iteration: 0 loss: 52.1816271563151 grad: 242.50009953863437
iteration: 10 loss: 6.697941791640088 grad: -0.10059366769166446
iteration: 20 loss: 3.752634918607576 grad: -0.8292235177284049
iteration: 30 loss: 2.6109813457870503 grad: -0.741969966693015
iteration: 40 loss: 2.0031519732573626 grad: -0.6320360712591326
iteration: 50 loss: 1.625448203351301 grad: -0.5438604224020538
iteration: 60 loss: 1.3679062672996556 grad: -0.4754508906059165
iteration: 70 loss: 1.1810182321134397 grad: -0.4217090766528697
iteration: 80 loss: 1.0391968662555138 grad: -0.3786529365039949
iteration: 90 loss: 0.9278828970943157 grad: -0.34348969312888894
iteration: 100 loss: 0.838180665991827 grad: -0.31427648819396625
iteration: 110 loss: 0.7643477935616829 grad: -0.28964173531217485
iteration: 120 loss: 0.7025114808627884 grad: -0.2685973709720047
iteration: 0 loss: 51.17211122649558 grad: 288.6482923100042
iteration: 10 loss: 6.907039906387457 grad: -0.0866712137616878
iteration: 20 loss: 3.830307114143166 grad: -0.7416533011678978
iteration: 30 loss: 2.6559306196750185 grad: -0.6808573287586968
iteration: 40 loss: 2.0344470711442724 grad: -0.5881203252488773
iteration: 50 loss: 1.6494517966470168 grad: -0.5105925045173313
iteration: 60 loss: 1.387413854833126 grad: -0.44916488697580975
iteration: 70 loss: 1.1974831332957117 grad: -0.40025906016648977
iteration: 80 loss: 1.0534654592054886 grad: -0.36070544945728106
iteration: 90 loss: 0.9404899225290303 grad: -0.32817160735261564
iteration: 100 loss: 0.8494852378437993 grad: -0.30099066533400937
iteration: 110 loss: 0.7746027550425755 grad: -0.2779647921876546
iteration: 120 loss: 0.711901668769108 grad: -0.25821993450148517
iteration: 130 loss: 0.6586294976896997 grad: -0.24110694112545292
iteration: 0 loss: 53.41269948488446 grad: 229.78183775466897
iteration: 10 loss: 6.962161966464051 grad: 0.6428883928602158
iteration: 20 loss: 3.8690764002815077 grad: -0.48376953090198094
iteration: 30 loss: 2.6870865487690025 grad: -0.5355813811313276
iteration: 40 loss: 2.060613342135308 grad: -0.49110973565517235
iteration: 50 loss: 1.672032110564487 grad: -0.43963133951978683
iteration: 60 loss: 1.4072831932972307 grad: -0.3941861027960417
iteration: 70 loss: 1.2152288793904105 grad: -0.3559354558156572
iteration: 80 loss: 1.069501955448562 grad: -0.32391446895146225
iteration: 90 loss: 0.9551205970681788 grad: -0.29694394732023155
iteration: 100 loss: 0.862939028613999 grad: -0.2740137856428898
iteration: 110 loss: 0.7870566867073979 grad: -0.25432517051006154
iteration: 120 loss: 0.723495469200109 grad: -0.23725917968979665
iteration: 130 loss: 0.6694754044147389 grad: -0.22233669617609253
iteration: 0 loss: 55.57155438323744 grad: 310.5934489336245
iteration: 10 loss: 6.766522433237944 grad: -0.9403689238731046
iteration: 20 loss: 3.7862190749044866 grad: -1.0450420577128372
iteration: 30 loss: 2.6325101073708868 grad: -0.8456185833045315
iteration: 40 loss: 2.0188932851975467 grad: -0.6949449648450745
iteration: 50 loss: 1.637845497591198 grad: -0.5870565752283043
iteration: 60 loss: 1.378137675356191 grad: -0.5074519497670673
iteration: 70 loss: 1.1897353792074599 grad: -0.4466631497329832
iteration: 80 loss: 1.04679619880984 grad: -0.3988408932533458
iteration: 90 loss: 0.934623030770705 grad: -0.36027896522170305
iteration: 100 loss: 0.8442395001443814 grad: -0.32854179829001895
iteration: 110 loss: 0.7698529073031337 grad: -0.30197157837716
iteration: 120 loss: 0.707557481886397 grad: -0.2794039804285136
iteration: 130 loss: 0.6546237751895618 grad: -0.25999860049600654
iteration: 0 loss: 51.16173619382275 grad: 282.1332835656298
iteration: 10 loss: 6.90199339049774 grad: -0.12834459513846874
iteration: 20 loss: 3.8373785582593056 grad: -0.8242246057856779
iteration: 30 loss: 2.6634891374539866 grad: -0.7426120397932581
iteration: 40 loss: 2.0411600247311155 grad: -0.6342719025322373
iteration: 50 loss: 1.6552718296631033 grad: -0.5463328558028846
iteration: 60 loss: 1.392478650220503 grad: -0.4777659140637932
iteration: 70 loss: 1.2019338186244732 grad: -0.42376805597001344
iteration: 80 loss: 1.0574175564641501 grad: -0.3804507707889749
iteration: 90 loss: 0.9440335766480117 grad: -0.3450498138691412
iteration: 100 loss: 0.8526903432647569 grad: -0.3156287926122541
iteration: 110 loss: 0.7775238701046047 grad: -0.29081509031826314
iteration: 120 loss: 0.7145818548564917 grad: -0.26961733221797696
iteration: 130 loss: 0.6611031596261537 grad: -0.2513051954711828
iteration: 0 loss: 53.57336792956839 grad: 253.87591732851521
iteration: 10 loss: 6.781417097847597 grad: -0.5009619232010375
iteration: 20 loss: 3.7888812237115417 grad: -0.9306786316187834
iteration: 30 loss: 2.6337601644539643 grad: -0.7903683996350539
iteration: 40 loss: 2.0198186463593713 grad: -0.6616029316639054
iteration: 50 loss: 1.6386580051567678 grad: -0.5643698450277725
iteration: 60 loss: 1.3788934736382379 grad: -0.490809429876615
iteration: 70 loss: 1.1904527934526745 grad: -0.4338092579087631
iteration: 80 loss: 1.0474825307246647 grad: -0.38853468343536823
iteration: 90 loss: 0.9352819504739603 grad: -0.35177776149380435
iteration: 100 loss: 0.8448733549383344 grad: -0.32137202675861065
iteration: 110 loss: 0.7704635040415517 grad: -0.2958159153533939
iteration: 120 loss: 0.7081463667533089 grad: -0.2740411594373857
iteration: 130 loss: 0.6551923377834431 grad: -0.2552692035887201
iteration: 0 loss: 51.680805101982806 grad: 310.40807303637666
iteration: 10 loss: 6.802383848574292 grad: -1.5732425789708373
iteration: 20 loss: 3.805984666841141 grad: -1.3409317618181689
iteration: 30 loss: 2.642496144349438 grad: -1.024815146274169
iteration: 40 loss: 2.0241223241576503 grad: -0.8194587944624271
iteration: 50 loss: 1.6405431874530387 grad: -0.6807604258083109
iteration: 60 loss: 1.3793796243991727 grad: -0.5817202487322739
iteration: 70 loss: 1.190091106948082 grad: -0.5076997637682199
iteration: 80 loss: 1.046589990917999 grad: -0.4503613025804842
iteration: 90 loss: 0.9340502976044924 grad: -0.40466637343582057
iteration: 100 loss: 0.8434233559987661 grad: -0.3674072299609635
iteration: 110 loss: 0.7688737146221766 grad: -0.3364501734769278
iteration: 120 loss: 0.7064692090564428 grad: -0.3103228824570176
iteration: 130 loss: 0.6534635039333807 grad: -0.2879774863743646
iteration: 0 loss: 50.715873857282574 grad: 252.7770581450142
iteration: 10 loss: 6.908213007260986 grad: 0.8030443206850026
iteration: 20 loss: 3.8240087013871498 grad: -0.4016681381626228
iteration: 30 loss: 2.651244884946374 grad: -0.4798567223813569
iteration: 40 loss: 2.0311023985505843 grad: -0.4482682587188789
iteration: 50 loss: 1.647008812910785 grad: -0.40459981278212764
iteration: 60 loss: 1.3855843887935075 grad: -0.36447808836439644
iteration: 70 loss: 1.196085229700698 grad: -0.33012298596521783
iteration: 80 loss: 1.0523810346357592 grad: -0.3010903920988157
iteration: 90 loss: 0.9396395928482971 grad: -0.27649108244248216
iteration: 100 loss: 0.8488138446411374 grad: -0.25549099052638835
iteration: 110 loss: 0.7740708467619531 grad: -0.23740516012866902
iteration: 120 loss: 0.7114803132858882 grad: -0.221691942384042
iteration: 130 loss: 0.6582970141540417 grad: -0.20792668900361
iteration: 0 loss: 53.01594973417179 grad: 287.5370272999828
iteration: 10 loss: 6.798431114192764 grad: -1.1327674474378402
iteration: 20 loss: 3.793275156558428 grad: -1.1057710057467676
iteration: 30 loss: 2.6331369140109264 grad: -0.8760349726488235
iteration: 40 loss: 2.0173757378880426 grad: -0.7138073786762922
iteration: 50 loss: 1.6355252487380973 grad: -0.6001129648554674
iteration: 60 loss: 1.3755257858052077 grad: -0.5171167419339265
iteration: 70 loss: 1.1870501549453614 grad: -0.4541505168094188
iteration: 80 loss: 1.0441366975656736 grad: -0.40483640991113345
iteration: 90 loss: 0.9320347713843479 grad: -0.3652023822019661
iteration: 100 loss: 0.8417423344636021 grad: -0.33266600246759676
iteration: 110 loss: 0.7674538635828867 grad: -0.30548240164202395
iteration: 120 loss: 0.7052570582348132 grad: -0.28243274670745155
iteration: 0 loss: 51.62050806423777 grad: 296.373368979005
iteration: 10 loss: 6.951077294409308 grad: -0.3595642778758064
iteration: 20 loss: 3.860602584851883 grad: -0.7997489194269158
iteration: 30 loss: 2.6769258600619734 grad: -0.6997291623622413
iteration: 40 loss: 2.050127357401297 grad: -0.5942345153878874
iteration: 50 loss: 1.661818858313299 grad: -0.5114064161059368
iteration: 60 loss: 1.3975550711447382 grad: -0.4474831577767587
iteration: 70 loss: 1.2060412215357081 grad: -0.39732682208808795
iteration: 80 loss: 1.0608471280790004 grad: -0.3571374385061737
iteration: 90 loss: 0.9469668232888828 grad: -0.3242938157229143
iteration: 100 loss: 0.8552468430275026 grad: -0.2969845213605798
iteration: 110 loss: 0.779785965884788 grad: -0.2739344132370404
iteration: 120 loss: 0.7166083357888883 grad: -0.25422597500159116
iteration: 130 loss: 0.6629372792661922 grad: -0.2371846717693194
iteration: 0 loss: 52.08904476517304 grad: 341.3393472709088
iteration: 10 loss: 6.753970260980995 grad: -1.7987897042205354
iteration: 20 loss: 3.773009487519134 grad: -1.4064701451193509
iteration: 30 loss: 2.6180506175309017 grad: -1.0625371901157532
iteration: 40 loss: 2.0047289638700105 grad: -0.8455694831364887
iteration: 50 loss: 1.6244688798779778 grad: -0.7005021915262636
iteration: 60 loss: 1.365649969321649 grad: -0.5974561507235263
iteration: 70 loss: 1.1781057655786589 grad: -0.5206970745929556
iteration: 80 loss: 1.0359532482821123 grad: -0.46137697130514926
iteration: 90 loss: 0.9244875969797746 grad: -0.41418734886293007
iteration: 100 loss: 0.834736408319052 grad: -0.3757643564627373
iteration: 110 loss: 0.7609146274485283 grad: -0.34387790442101895
iteration: 120 loss: 0.699124727423995 grad: -0.3169930376962303
iteration: 0 loss: 52.37919921819424 grad: 287.03939881599604
iteration: 10 loss: 6.8321010764468815 grad: -0.944003081596659
iteration: 20 loss: 3.8041728333010227 grad: -1.02516658811372
iteration: 30 loss: 2.6391599092918443 grad: -0.8279292836722771
iteration: 40 loss: 2.021514041315599 grad: -0.6807641991098546
iteration: 50 loss: 1.638696201500352 grad: -0.5755415358656453
iteration: 60 loss: 1.3781118068355185 grad: -0.4978840552237902
iteration: 70 loss: 1.1892437082802976 grad: -0.4385453879343586
iteration: 80 loss: 1.0460477039437963 grad: -0.3918323487784502
iteration: 90 loss: 0.9337319047263836 grad: -0.35414035920349796
iteration: 100 loss: 0.8432714429042211 grad: -0.3231004907361844
iteration: 110 loss: 0.7688471468901216 grad: -0.2970997124064938
iteration: 120 loss: 0.7065380471016066 grad: -0.2750046303021997
iteration: 130 loss: 0.6536056128713674 grad: -0.25599676795948645
iteration: 0 loss: 50.140164155114874 grad: 258.6642882618853
iteration: 10 loss: 6.788202725949346 grad: 0.17442426641758985
iteration: 20 loss: 3.783711704702951 grad: -0.7130726766877582
iteration: 30 loss: 2.628161526972814 grad: -0.6724735730358022
iteration: 40 loss: 2.0147864453738706 grad: -0.5832942726993237
iteration: 50 loss: 1.6342331900620841 grad: -0.5066790931836057
iteration: 60 loss: 1.3749895012263715 grad: -0.4455878956276825
iteration: 70 loss: 1.1869788107944508 grad: -0.39687296143605244
iteration: 80 loss: 1.0443629793961586 grad: -0.3574706398559615
iteration: 90 loss: 0.9324569595997176 grad: -0.32507585795881416
iteration: 100 loss: 0.842295807127729 grad: -0.2980281687383226
iteration: 110 loss: 0.768096094369329 grad: -0.27513055446792467
iteration: 120 loss: 0.7059592549474639 grad: -0.25550851277471986
iteration: 0 loss: 50.398867565682416 grad: 342.4407962401445
iteration: 10 loss: 6.819270011137963 grad: -0.838067252978405
iteration: 20 loss: 3.7963025729414683 grad: -1.0785817721833313
iteration: 30 loss: 2.633217394357044 grad: -0.8777088211779124
iteration: 40 loss: 2.016643570073194 grad: -0.721345508490008
iteration: 50 loss: 1.6345395726725243 grad: -0.6088460808526223
iteration: 60 loss: 1.3744751831861983 grad: -0.5257915273179585
iteration: 70 loss: 1.186006515834296 grad: -0.4624005708317049
iteration: 80 loss: 1.0431284501265425 grad: -0.4125715650430065
iteration: 90 loss: 0.9310724006497615 grad: -0.3724261802448821
iteration: 100 loss: 0.8408284666024883 grad: -0.3394133796053158
iteration: 110 loss: 0.7665876128666324 grad: -0.3117967160149404
iteration: 120 loss: 0.7044360157343693 grad: -0.2883570992503357
iteration: 0 loss: 50.631939002730675 grad: 263.72419717750205
iteration: 10 loss: 6.879818027575683 grad: 0.13676084930550905
iteration: 20 loss: 3.8318219442429506 grad: -0.6705264659807069
iteration: 30 loss: 2.661379462753261 grad: -0.6379227306309806
iteration: 40 loss: 2.0403750693495897 grad: -0.5568609106921529
iteration: 50 loss: 1.6551276102979335 grad: -0.48583345394399113
iteration: 60 loss: 1.3926822354289305 grad: -0.4285995624144783
iteration: 70 loss: 1.2023388931138057 grad: -0.3826448891650311
iteration: 80 loss: 1.0579435878776167 grad: -0.3452901443693356
iteration: 90 loss: 0.9446331811981591 grad: -0.314462366001515
iteration: 100 loss: 0.8533342667090189 grad: -0.2886458300859434
iteration: 110 loss: 0.778193398059226 grad: -0.26673713491098844
iteration: 120 loss: 0.7152646970100084 grad: -0.24792446377278027
iteration: 130 loss: 0.6617910908423503 grad: -0.23160145135959515
iteration: 0 loss: 50.258245745386105 grad: 306.4502644514377
iteration: 10 loss: 6.878225454773201 grad: -0.8881073330395852
iteration: 20 loss: 3.821441056718248 grad: -1.017757503958678
iteration: 30 loss: 2.649302573662143 grad: -0.8220958851924915
iteration: 40 loss: 2.028678007539094 grad: -0.6751111702015942
iteration: 50 loss: 1.6442715602947415 grad: -0.5702232088622037
iteration: 60 loss: 1.3827080331212445 grad: -0.4929827553117155
iteration: 70 loss: 1.1931761920753738 grad: -0.43406437512673013
iteration: 80 loss: 1.0494993005768265 grad: -0.38774160265875574
iteration: 90 loss: 0.9368177733554561 grad: -0.3503998924102654
iteration: 100 loss: 0.8460687971658196 grad: -0.3196700801103628
iteration: 110 loss: 0.7714103483874003 grad: -0.29394250303130465
iteration: 120 loss: 0.7089068877832522 grad: -0.2720881480170695
iteration: 130 loss: 0.6558101555760086 grad: -0.25329288583062703
iteration: 0 loss: 51.739372232404634 grad: 322.39716610861547
iteration: 10 loss: 6.8026924644559585 grad: -1.7334487524091302
iteration: 20 loss: 3.7907389321893046 grad: -1.3343060769287005
iteration: 30 loss: 2.627958895441969 grad: -1.0087501375928518
iteration: 40 loss: 2.011595073968775 grad: -0.8043269237610065
iteration: 50 loss: 1.6298027914053819 grad: -0.667572783802892
iteration: 60 loss: 1.370074270919345 grad: -0.5702975912061995
iteration: 70 loss: 1.1819268608535574 grad: -0.4977289379370117
iteration: 80 loss: 1.0393421193963066 grad: -0.4415670284534704
iteration: 90 loss: 0.9275489729904869 grad: -0.39683072559806865
iteration: 100 loss: 0.8375391676207652 grad: -0.36036127339496815
iteration: 110 loss: 0.7635066563892988 grad: -0.3300626603467893
iteration: 120 loss: 0.7015408182866684 grad: -0.3044908438199294
iteration: 0 loss: 54.19799892916455 grad: 249.1994604613485
iteration: 10 loss: 6.730957240316581 grad: 0.02044058865667059
iteration: 20 loss: 3.764093365645873 grad: -0.7747103279890384
iteration: 30 loss: 2.61904662336201 grad: -0.7072726591882825
iteration: 40 loss: 2.0098175348845704 grad: -0.6068558752838528
iteration: 50 loss: 1.631252519168811 grad: -0.5242159936411652
iteration: 60 loss: 1.3730866378300561 grad: -0.459413699848776
iteration: 70 loss: 1.1857113128159076 grad: -0.4082017351305498
iteration: 80 loss: 1.0434934530494502 grad: -0.36701347823848973
iteration: 90 loss: 0.9318483553429773 grad: -0.3332829447360476
iteration: 100 loss: 0.841864424580928 grad: -0.3052015217632631
iteration: 110 loss: 0.7677884863267702 grad: -0.28148209548586367
iteration: 120 loss: 0.7057400388848278 grad: -0.2611923634940081
iteration: 0 loss: 52.60687448492697 grad: 280.7573802241503
iteration: 10 loss: 6.787759746392277 grad: -1.1289972341751535
iteration: 20 loss: 3.78638692934328 grad: -1.1898643777309665
iteration: 30 loss: 2.6280673797995076 grad: -0.9449714297810596
iteration: 40 loss: 2.013205146743365 grad: -0.7679653576310526
iteration: 50 loss: 1.6319159851224703 grad: -0.6437246872267561
iteration: 60 loss: 1.3723169941936662 grad: -0.553213992724039
iteration: 70 loss: 1.1841492343948465 grad: -0.48472960097808104
iteration: 80 loss: 1.0414833910653711 grad: -0.43123404581320535
iteration: 90 loss: 0.9295866341533177 grad: -0.3883426883738309
iteration: 100 loss: 0.839467836827184 grad: -0.3532085534298972
iteration: 110 loss: 0.7653286918094967 grad: -0.3239116284761686
iteration: 120 loss: 0.7032619382090078 grad: -0.299113447095041
iteration: 0 loss: 52.8120361125141 grad: 295.45618312940303
iteration: 10 loss: 6.871885919203454 grad: -1.2370281055657368
iteration: 20 loss: 3.826999479464369 grad: -1.1354135477261096
iteration: 30 loss: 2.6523931260691245 grad: -0.882679963899935
iteration: 40 loss: 2.0299404789713495 grad: -0.7126306950267558
iteration: 50 loss: 1.644450974001674 grad: -0.5959216411745731
iteration: 60 loss: 1.382245900143475 grad: -0.511728923366991
iteration: 70 loss: 1.192326985959095 grad: -0.44833966458750635
iteration: 80 loss: 1.0484138981852829 grad: -0.398955868660564
iteration: 90 loss: 0.9355886915185775 grad: -0.35941890880898897
iteration: 100 loss: 0.8447544823665422 grad: -0.32705745997690217
iteration: 110 loss: 0.7700486276384336 grad: -0.30008222501966136
iteration: 120 loss: 0.7075226997789783 grad: -0.27725152179588464
iteration: 130 loss: 0.6544201554225683 grad: -0.2576774288320989
iteration: 0 loss: 49.85377080013735 grad: 282.73261495958525
iteration: 10 loss: 6.726076404825852 grad: -0.3637456401547925
iteration: 20 loss: 3.756701236504465 grad: -0.8866256781167529
iteration: 30 loss: 2.6104167867082735 grad: -0.7710860902731798
iteration: 40 loss: 2.0013372970542234 grad: -0.6524137918078093
iteration: 50 loss: 1.6232881314218834 grad: -0.5600802142573669
iteration: 60 loss: 1.3656999345400271 grad: -0.48916175345309953
iteration: 70 loss: 1.1788736907642074 grad: -0.43368803737380573
iteration: 80 loss: 1.0371522019930703 grad: -0.38933688222410323
iteration: 90 loss: 0.9259482959721275 grad: -0.3531550398094224
iteration: 100 loss: 0.8363547948756191 grad: -0.3231127807354546
iteration: 110 loss: 0.7626246817647122 grad: -0.29778650099847825
iteration: 120 loss: 0.7008835476294735 grad: -0.27615457004071153
iteration: 0 loss: 50.86769597387237 grad: 343.79884842085426
iteration: 10 loss: 6.689409490024693 grad: -0.4722823187517832
iteration: 20 loss: 3.7313618728562274 grad: -0.9453053684665106
iteration: 30 loss: 2.592157751591706 grad: -0.8101414362377192
iteration: 40 loss: 1.987120972343994 grad: -0.679894383555893
iteration: 50 loss: 1.6116551293869799 grad: -0.58043948655253
iteration: 60 loss: 1.355857695918985 grad: -0.5048806287524827
iteration: 70 loss: 1.1703459666220046 grad: -0.44622354366204736
iteration: 80 loss: 1.029630670963136 grad: -0.39959370348710954
iteration: 90 loss: 0.9192218718445392 grad: -0.36172301198734635
iteration: 100 loss: 0.8302725783708577 grad: -0.33039265456238615
iteration: 110 loss: 0.7570750322540583 grad: -0.304060166132207
iteration: 120 loss: 0.6957815327004937 grad: -0.2816262116809703
iteration: 0 loss: 51.76888010470736 grad: 329.4109498707194
iteration: 10 loss: 6.698232186676953 grad: -1.015324160427665
iteration: 20 loss: 3.7457935889909675 grad: -1.1712374480476178
iteration: 30 loss: 2.6011939353161737 grad: -0.9389889745276369
iteration: 40 loss: 1.9928774356719507 grad: -0.766295337793418
iteration: 50 loss: 1.615473525293055 grad: -0.6439328406557234
iteration: 60 loss: 1.3584672539293272 grad: -0.5543530250012454
iteration: 70 loss: 1.172161408326784 grad: -0.4863619842863755
iteration: 80 loss: 1.0309018184559797 grad: -0.433134318318594
iteration: 90 loss: 0.9201070339000084 grad: -0.39038577987878187
iteration: 100 loss: 0.8308764081280473 grad: -0.355321454836399
iteration: 110 loss: 0.7574691876229735 grad: -0.3260502043128663
iteration: 120 loss: 0.6960165427130547 grad: -0.3012504142198794
iteration: 0 loss: 54.57578516437359 grad: 295.8944229341232
iteration: 10 loss: 6.785789147185162 grad: -1.665576293792983
iteration: 20 loss: 3.79695883364035 grad: -1.3094651680551423
iteration: 30 loss: 2.6375919406300454 grad: -0.9906713027548419
iteration: 40 loss: 2.0213028717973263 grad: -0.7907913612771609
iteration: 50 loss: 1.6388775082353029 grad: -0.6570728285402386
iteration: 60 loss: 1.3784029671348683 grad: -0.5618815777457986
iteration: 70 loss: 1.1895501424051376 grad: -0.49079721197068016
iteration: 80 loss: 1.046336573820926 grad: -0.43572968383624433
iteration: 90 loss: 0.9339928645216456 grad: -0.3918248341530829
iteration: 100 loss: 0.8435027674488331 grad: -0.35600338381945523
iteration: 110 loss: 0.7690503908793702 grad: -0.3262208521725228
iteration: 120 loss: 0.7067158789506555 grad: -0.3010677247340958
iteration: 130 loss: 0.6537609193230558 grad: -0.27954116483926694
iteration: 0 loss: 51.31904649449769 grad: 263.7779283428059
iteration: 10 loss: 6.769970464518503 grad: -0.37324565626564754
iteration: 20 loss: 3.7785143119523044 grad: -0.8707664006813576
iteration: 30 loss: 2.624340457211805 grad: -0.7468813640200536
iteration: 40 loss: 2.0114700598571953 grad: -0.6268375311439788
iteration: 50 loss: 1.6312572503825322 grad: -0.5353941492468741
iteration: 60 loss: 1.372285952471595 grad: -0.46602083201985744
iteration: 70 loss: 1.1845032296590199 grad: -0.4121980512880826
iteration: 80 loss: 1.0420814820428281 grad: -0.36941600294801735
iteration: 90 loss: 0.9303424585225664 grad: -0.33466429809792453
iteration: 100 loss: 0.8403262523686201 grad: -0.3059046013500971
iteration: 110 loss: 0.7662533405508095 grad: -0.28172232982163986
iteration: 120 loss: 0.7042282350231207 grad: -0.26111037849799446
iteration: 0 loss: 55.376755843793674 grad: 366.4321674804159
iteration: 10 loss: 6.644516820627442 grad: -1.9229180032762554
iteration: 20 loss: 3.7402366441465347 grad: -1.3389930343745386
iteration: 30 loss: 2.604793983575085 grad: -1.016281563736521
iteration: 40 loss: 1.9989207145889933 grad: -0.811277341726558
iteration: 50 loss: 1.6221238532192397 grad: -0.6737315841594088
iteration: 60 loss: 1.3651096929754865 grad: -0.5757630852804098
iteration: 70 loss: 1.1785745963040675 grad: -0.5026192618904792
iteration: 80 loss: 1.0370109395153535 grad: -0.4459815836817852
iteration: 90 loss: 0.9258963680275553 grad: -0.400848361213103
iteration: 100 loss: 0.8363547000477328 grad: -0.36404409584988046
iteration: 110 loss: 0.7626548872982643 grad: -0.33345993083522996
iteration: 120 loss: 0.7009312388649133 grad: -0.30764206843298175
iteration: 0 loss: 50.12821843007102 grad: 308.7434660310969
iteration: 10 loss: 6.986012130653473 grad: -0.7526347951474078
iteration: 20 loss: 3.873740888214591 grad: -0.9106661701999852
iteration: 30 loss: 2.683962179675456 grad: -0.754400289535418
iteration: 40 loss: 2.0547037288190286 grad: -0.6280835524355036
iteration: 50 loss: 1.665144190125664 grad: -0.5351101867604177
iteration: 60 loss: 1.400143886581108 grad: -0.4653907565391314
iteration: 70 loss: 1.2081515833795244 grad: -0.4115612104412256
iteration: 80 loss: 1.062624211974997 grad: -0.3688685967473531
iteration: 90 loss: 0.9484994657403728 grad: -0.3342251895340933
iteration: 100 loss: 0.8565929943989349 grad: -0.3055676883332298
iteration: 110 loss: 0.7809853678946969 grad: -0.28147444227143836
iteration: 120 loss: 0.7176893571070816 grad: -0.2609375511950437
iteration: 130 loss: 0.6639208438296422 grad: -0.24322400599132551
iteration: 0 loss: 51.871933786759904 grad: 289.4502892138733
iteration: 10 loss: 6.838918210586929 grad: -0.7448301733168436
iteration: 20 loss: 3.8111088923976486 grad: -0.9756539484042441
iteration: 30 loss: 2.645440868698684 grad: -0.8098319105502997
iteration: 40 loss: 2.027059513721559 grad: -0.6732936267366713
iteration: 50 loss: 1.6435998735184347 grad: -0.5726190015021723
iteration: 60 loss: 1.382482657471552 grad: -0.497173878264796
iteration: 70 loss: 1.193174673643088 grad: -0.43899408708555954
iteration: 80 loss: 1.0496130013777722 grad: -0.3929122401314059
iteration: 90 loss: 0.9369900620165688 grad: -0.35556717562803164
iteration: 100 loss: 0.8462687956736193 grad: -0.3247129438508898
iteration: 110 loss: 0.7716207283863448 grad: -0.29880286191656774
iteration: 120 loss: 0.7091177880064689 grad: -0.2767411926794938
iteration: 130 loss: 0.6560159780609948 grad: -0.2577316987888943
iteration: 0 loss: 53.88116633228713 grad: 304.6178214915996
iteration: 10 loss: 6.840998145472192 grad: -0.9468831998686342
iteration: 20 loss: 3.809493570646985 grad: -1.0493954487064887
iteration: 30 loss: 2.642845573332331 grad: -0.8476843466838485
iteration: 40 loss: 2.024268880965792 grad: -0.6954787878546358
iteration: 50 loss: 1.6408724501279721 grad: -0.5867805367603415
iteration: 60 loss: 1.3799008440279399 grad: -0.5067504898873086
iteration: 70 loss: 1.1907581919222552 grad: -0.44573840976704543
iteration: 80 loss: 1.0473585489594783 grad: -0.39780292097245606
iteration: 90 loss: 0.9348862439229296 grad: -0.35918974458089703
iteration: 100 loss: 0.8443020175542518 grad: -0.3274371192379456
iteration: 110 loss: 0.7697775414660348 grad: -0.30087238312819364
iteration: 120 loss: 0.7073857705905869 grad: -0.27832251535688957
iteration: 130 loss: 0.6543839982117045 grad: -0.258941885503856
iteration: 0 loss: 55.993588879305186 grad: 316.7357862361904
iteration: 10 loss: 6.953471591025143 grad: -1.3369598620049472
iteration: 20 loss: 3.866838766492244 grad: -1.2561146227332023
iteration: 30 loss: 2.6801561732621932 grad: -0.9724522359121764
iteration: 40 loss: 2.05156571453356 grad: -0.7817519877046013
iteration: 50 loss: 1.6622563909173054 grad: -0.6514597487321889
iteration: 60 loss: 1.397414933541313 grad: -0.557856169568911
iteration: 70 loss: 1.205556420958973 grad: -0.48762779359014474
iteration: 80 loss: 1.0601522535458487 grad: -0.43307760444600724
iteration: 90 loss: 0.946143052149058 grad: -0.3895145966902987
iteration: 100 loss: 0.8543449097279666 grad: -0.3539355503649435
iteration: 110 loss: 0.7788384857657217 grad: -0.32433481233259376
iteration: 120 loss: 0.7156367355803674 grad: -0.29932420787717706
iteration: 130 loss: 0.6619558592917035 grad: -0.2779133756971598
iteration: 0 loss: 50.008489756020396 grad: 306.347409636863
iteration: 10 loss: 6.803971299692008 grad: -1.2237156199016086
iteration: 20 loss: 3.7927289469952568 grad: -1.2055698603247822
iteration: 30 loss: 2.630786037385984 grad: -0.9485874920564816
iteration: 40 loss: 2.014518492361152 grad: -0.7690877009882213
iteration: 50 loss: 1.6325863538058085 grad: -0.644308338075326
iteration: 60 loss: 1.372656769047353 grad: -0.5537441108422884
iteration: 70 loss: 1.1843047606863282 grad: -0.48532819905625246
iteration: 80 loss: 1.0415302893983691 grad: -0.4319212066010394
iteration: 90 loss: 0.9295669507004359 grad: -0.38910951735654564
iteration: 100 loss: 0.8394062982074089 grad: -0.35403949717569305
iteration: 110 loss: 0.7652404885753558 grad: -0.32479141017612556
iteration: 120 loss: 0.7031567291712897 grad: -0.3000289875132586
iteration: 0 loss: 51.73279107080773 grad: 292.27803760413633
iteration: 10 loss: 6.79064600157273 grad: -0.7979926543353341
iteration: 20 loss: 3.779709298455931 grad: -1.0495074122604295
iteration: 30 loss: 2.62175266324452 grad: -0.8593992140192717
iteration: 40 loss: 2.007941769737127 grad: -0.708711449189748
iteration: 50 loss: 1.6275494513094022 grad: -0.5995487484638948
iteration: 60 loss: 1.3686447941835551 grad: -0.5186164193281446
iteration: 70 loss: 1.181011792331777 grad: -0.45666124037734923
iteration: 80 loss: 1.038763498787582 grad: -0.40785185661190937
iteration: 90 loss: 0.9271985201245446 grad: -0.3684590173838478
iteration: 100 loss: 0.8373479028336575 grad: -0.3360192284866327
iteration: 110 loss: 0.763428952387337 grad: -0.3088501995416661
iteration: 120 loss: 0.7015455409718512 grad: -0.28576783376367754
iteration: 0 loss: 53.488361140109255 grad: 343.95654101350476
iteration: 10 loss: 6.760129878269667 grad: -1.8744796679991969
iteration: 20 loss: 3.78490419724552 grad: -1.3952231454200472
iteration: 30 loss: 2.628387388152822 grad: -1.0466969104003816
iteration: 40 loss: 2.0135769115163797 grad: -0.8313550880597314
iteration: 50 loss: 1.6321505641518674 grad: -0.688300816526199
iteration: 60 loss: 1.3724221155217824 grad: -0.586955399601476
iteration: 70 loss: 1.1841557676838503 grad: -0.511556518001183
iteration: 80 loss: 1.041418309169785 grad: -0.4533222213151418
iteration: 90 loss: 0.9294701058163561 grad: -0.40700901091531005
iteration: 100 loss: 0.839314417185795 grad: -0.36930342286718804
iteration: 110 loss: 0.7651488423598081 grad: -0.33801248599355316
iteration: 120 loss: 0.7030632243256398 grad: -0.3116284105194771
iteration: 0 loss: 52.47973314043946 grad: 272.5854822772842
iteration: 10 loss: 6.759904013245184 grad: -0.6119367979668444
iteration: 20 loss: 3.7714571581429728 grad: -0.9289629808550327
iteration: 30 loss: 2.6185048123174965 grad: -0.7800514499449649
iteration: 40 loss: 2.00653826628534 grad: -0.6518619643965649
iteration: 50 loss: 1.6269963349386671 grad: -0.5562618648379548
iteration: 60 loss: 1.3685350395309632 grad: -0.4841925899137111
iteration: 70 loss: 1.181150851970208 grad: -0.4283925889564191
iteration: 80 loss: 1.0390484383333387 grad: -0.3840611245233273
iteration: 90 loss: 0.9275708533123592 grad: -0.3480466218236377
iteration: 100 loss: 0.8377726256831011 grad: -0.3182309183787021
iteration: 110 loss: 0.7638842903040974 grad: -0.29314922473768623
iteration: 120 loss: 0.7020175548321855 grad: -0.27176044027694457
iteration: 0 loss: 50.0717787685509 grad: 281.8376814385158
iteration: 10 loss: 6.76189644202139 grad: -0.5408312556642072
iteration: 20 loss: 3.7738265718991317 grad: -0.9357665745083684
iteration: 30 loss: 2.6222109497169352 grad: -0.792839004067781
iteration: 40 loss: 2.0105090646709205 grad: -0.6631234375180585
iteration: 50 loss: 1.630866645285964 grad: -0.5653631078706621
iteration: 60 loss: 1.3721936408125492 grad: -0.49147444095406734
iteration: 70 loss: 1.1845744087172947 grad: -0.4342589607958801
iteration: 80 loss: 1.042244498400702 grad: -0.38883821916517186
iteration: 90 loss: 0.9305571035764925 grad: -0.3519794329961461
iteration: 100 loss: 0.8405690227052053 grad: -0.3215012102651119
iteration: 110 loss: 0.766510082206878 grad: -0.2958926417883343
iteration: 120 loss: 0.7044902509762985 grad: -0.2740794134791164
iteration: 0 loss: 53.32232413401766 grad: 298.0108893264285
iteration: 10 loss: 6.63104956186825 grad: -0.30401201127324234
iteration: 20 loss: 3.7121122361957286 grad: -0.9143628866217721
iteration: 30 loss: 2.581789692259008 grad: -0.7911611375876817
iteration: 40 loss: 1.9803227705039965 grad: -0.6656178247331623
iteration: 50 loss: 1.606714520683578 grad: -0.5690791386770924
iteration: 60 loss: 1.3520320815428848 grad: -0.49555259119213424
iteration: 70 loss: 1.1672532052344153 grad: -0.43838660659510764
iteration: 80 loss: 1.027050969595414 grad: -0.3928885539558119
iteration: 90 loss: 0.9170184938684129 grad: -0.3558998296028208
iteration: 100 loss: 0.8283553280993229 grad: -0.3252718020094261
iteration: 110 loss: 0.7553816437444684 grad: -0.2995089116098373
iteration: 120 loss: 0.6942674245262672 grad: -0.27754420586123163
iteration: 0 loss: 52.19817933301938 grad: 248.85568943414862
iteration: 10 loss: 6.814807042657156 grad: -0.15133128702265186
iteration: 20 loss: 3.8180373643123278 grad: -0.8633343774835773
iteration: 30 loss: 2.6570601716330615 grad: -0.7653500051139134
iteration: 40 loss: 2.0387676279193245 grad: -0.648663236960446
iteration: 50 loss: 1.654478816673914 grad: -0.5561758689130107
iteration: 60 loss: 1.3924074589229514 grad: -0.4848921260814138
iteration: 70 loss: 1.2022121886691985 grad: -0.4291487590076796
iteration: 80 loss: 1.0578695977089658 grad: -0.3846450940535876
iteration: 90 loss: 0.9445698312558555 grad: -0.34840144163850834
iteration: 100 loss: 0.8532629777149107 grad: -0.3183603760358189
iteration: 110 loss: 0.7781064887223941 grad: -0.2930771629114681
iteration: 120 loss: 0.7151596802182625 grad: -0.27151532058529965
iteration: 130 loss: 0.6616679588819352 grad: -0.25291514842473295
iteration: 0 loss: 51.32792548222563 grad: 378.9809016283177
iteration: 10 loss: 6.3379836725086784 grad: -2.3971375268222683
iteration: 20 loss: 3.605515976708003 grad: -1.2493020868044973
iteration: 30 loss: 2.527843366128681 grad: -0.9653920592254949
iteration: 40 loss: 1.9481715315688979 grad: -0.7784096781340044
iteration: 50 loss: 1.5855937480752222 grad: -0.6505657386972303
iteration: 60 loss: 1.3372309944484186 grad: -0.5584239878724806
iteration: 70 loss: 1.156392494475726 grad: -0.4890663459812725
iteration: 80 loss: 1.0188030532845005 grad: -0.43503720514100197
iteration: 90 loss: 0.9105861383418827 grad: -0.39178386314071223
iteration: 100 loss: 0.823231918325994 grad: -0.35638342278798135
iteration: 110 loss: 0.7512304423327226 grad: -0.3268782115479337
iteration: 120 loss: 0.6908562643552614 grad: -0.30190956157783044
iteration: 0 loss: 50.71035461237565 grad: 269.744457904352
iteration: 10 loss: 6.621312781608884 grad: 0.14792707962289683
iteration: 20 loss: 3.704837188840305 grad: -0.7431838698915088
iteration: 30 loss: 2.5786537625262183 grad: -0.6931099521894006
iteration: 40 loss: 1.9792602600755345 grad: -0.5993611496231004
iteration: 50 loss: 1.606718223509812 grad: -0.5199926479736718
iteration: 60 loss: 1.3526131933195673 grad: -0.45700958032746253
iteration: 70 loss: 1.168157960418119 grad: -0.4068922743943868
iteration: 80 loss: 1.028139251164248 grad: -0.36639938175590914
iteration: 90 loss: 0.9182090591508231 grad: -0.3331280587578269
iteration: 100 loss: 0.8295992854794101 grad: -0.3053585526161869
iteration: 110 loss: 0.7566487607811971 grad: -0.28185528526394316
iteration: 120 loss: 0.6955386646406871 grad: -0.26171736289065217
iteration: 0 loss: 51.31508166168646 grad: 332.39901104768813
iteration: 10 loss: 6.89204660128404 grad: -1.7910844495889782
iteration: 20 loss: 3.83058999481562 grad: -1.3421436628453478
iteration: 30 loss: 2.653436961430833 grad: -1.0101335866872263
iteration: 40 loss: 2.030252184098117 grad: -0.8044558460099605
iteration: 50 loss: 1.6444765594220703 grad: -0.6674225992043088
iteration: 60 loss: 1.3821394391358643 grad: -0.5701063063843751
iteration: 70 loss: 1.1921537914828066 grad: -0.4975602988299106
iteration: 80 loss: 1.0482055896263773 grad: -0.44143587508391124
iteration: 90 loss: 0.9353620210882913 grad: -0.39673712673649586
iteration: 100 loss: 0.8445188984351262 grad: -0.36030105000927354
iteration: 110 loss: 0.7698096937832485 grad: -0.33003083624562707
iteration: 120 loss: 0.707283785072832 grad: -0.3044827638440435
iteration: 130 loss: 0.6541833303986095 grad: -0.2826307957062325
iteration: 0 loss: 54.68731465818394 grad: 302.6334083896994
iteration: 10 loss: 6.754204853573686 grad: 0.008896625712056107
iteration: 20 loss: 3.7659655321306427 grad: -0.7800700340164325
iteration: 30 loss: 2.617144106075598 grad: -0.7173489372401691
iteration: 40 loss: 2.0070108168281213 grad: -0.6175877986810014
iteration: 50 loss: 1.6282833099048317 grad: -0.5344692241692537
iteration: 60 loss: 1.3701864467102496 grad: -0.4689191241457913
iteration: 70 loss: 1.182954222802602 grad: -0.416945875955652
iteration: 80 loss: 1.0408979154529499 grad: -0.3750570799636207
iteration: 90 loss: 0.9294119723576014 grad: -0.34070303245078054
iteration: 100 loss: 0.8395771623895598 grad: -0.3120725627119099
iteration: 110 loss: 0.7656380131000499 grad: -0.28787045055998095
iteration: 120 loss: 0.7037139261311826 grad: -0.26715538915331866
iteration: 0 loss: 55.42724072229732 grad: 257.44779359299855
iteration: 10 loss: 6.822591724954981 grad: 0.2481695415430148
iteration: 20 loss: 3.807191718553564 grad: -0.7078233300902254
iteration: 30 loss: 2.6457984715738845 grad: -0.6671511161047531
iteration: 40 loss: 2.0288441451297 grad: -0.5778503687146859
iteration: 50 loss: 1.645893705829952 grad: -0.5015568772968612
iteration: 60 loss: 1.384937403525245 grad: -0.44090627885986705
iteration: 70 loss: 1.1956439359472908 grad: -0.3926263690783649
iteration: 80 loss: 1.0520322922587348 grad: -0.35361631953819983
iteration: 90 loss: 0.9393313442268746 grad: -0.3215644326391647
iteration: 100 loss: 0.848521269166006 grad: -0.2948135848300724
iteration: 110 loss: 0.7737820299000966 grad: -0.27217273923870794
iteration: 120 loss: 0.7111897144599507 grad: -0.25277350228042644
iteration: 130 loss: 0.6580023184160382 grad: -0.23597165597095823
iteration: 0 loss: 52.74688088994762 grad: 284.31345859822306
iteration: 10 loss: 6.765268730326335 grad: -0.7651647653067917
iteration: 20 loss: 3.7778352067324374 grad: -1.0346975400484886
iteration: 30 loss: 2.6247889628914645 grad: -0.848971416774036
iteration: 40 loss: 2.0122992337312344 grad: -0.7006174708796802
iteration: 50 loss: 1.6322064095943047 grad: -0.5929077352239025
iteration: 60 loss: 1.3732535777366222 grad: -0.5129738571813527
iteration: 70 loss: 1.1854478306694214 grad: -0.45175078744944663
iteration: 80 loss: 1.0429864830633215 grad: -0.4035029968194539
iteration: 90 loss: 0.9312022805918538 grad: -0.3645554398631766
iteration: 100 loss: 0.8411403183352543 grad: -0.33247776836845566
iteration: 110 loss: 0.7670233325416947 grad: -0.3056091949106503
iteration: 120 loss: 0.7049567877126968 grad: -0.2827802300337111
iteration: 0 loss: 50.9714140105152 grad: 322.3910487923065
iteration: 10 loss: 6.753126173517599 grad: -0.9648682634658166
iteration: 20 loss: 3.758116133689121 grad: -1.116790035126897
iteration: 30 loss: 2.6071474230360847 grad: -0.8999469212455431
iteration: 40 loss: 1.9970187412778118 grad: -0.7371020055278575
iteration: 50 loss: 1.6188683364407965 grad: -0.6210848253136733
iteration: 60 loss: 1.36146179272588 grad: -0.5357999329789824
iteration: 70 loss: 1.1748962809510473 grad: -0.47085938275362665
iteration: 80 loss: 1.0334451143463328 grad: -0.4198876789028738
iteration: 90 loss: 0.922496964107681 grad: -0.3788636100937516
iteration: 100 loss: 0.8331372052620778 grad: -0.34515375283615773
iteration: 110 loss: 0.7596177796298931 grad: -0.3169705813832262
iteration: 120 loss: 0.6980656445407476 grad: -0.293061549965762
iteration: 0 loss: 53.83979364244181 grad: 324.30005432211
iteration: 10 loss: 6.833532894652638 grad: -0.963123587812
iteration: 20 loss: 3.804413227883494 grad: -1.1247448573452776
iteration: 30 loss: 2.6384150878749426 grad: -0.9070919162880281
iteration: 40 loss: 2.0203926607519365 grad: -0.7428422618303052
iteration: 50 loss: 1.6374510313963087 grad: -0.6257912755428907
iteration: 60 loss: 1.3768476276219106 grad: -0.5397700978352986
iteration: 70 loss: 1.1880051125291506 grad: -0.4742901751400412
iteration: 80 loss: 1.0448532711326897 grad: -0.42290986894856675
iteration: 90 loss: 0.9325883739489985 grad: -0.3815667954185825
iteration: 100 loss: 0.842180001766792 grad: -0.347601424438861
iteration: 110 loss: 0.7678063795055178 grad: -0.31920913860356737
iteration: 120 loss: 0.7055453723152244 grad: -0.2951258307202942
iteration: 130 loss: 0.6526580033002408 grad: -0.2744411202509708
iteration: 0 loss: 50.33989068289429 grad: 337.70979350562845
iteration: 10 loss: 6.616813922116804 grad: -1.5136773363107223
iteration: 20 loss: 3.7053224693101003 grad: -1.2747696544631013
iteration: 30 loss: 2.57406782510572 grad: -0.9752435258940246
iteration: 40 loss: 1.9725493094396866 grad: -0.7812438843040054
iteration: 50 loss: 1.5992723687309907 grad: -0.6500874201775073
iteration: 60 loss: 1.3450330260162362 grad: -0.5562957031067725
iteration: 70 loss: 1.1607073254497002 grad: -0.48609656871253
iteration: 80 loss: 1.0209323546243716 grad: -0.4316461550242798
iteration: 90 loss: 0.911290363858701 grad: -0.38820098448815404
iteration: 100 loss: 0.8229797284377376 grad: -0.3527382777679191
iteration: 110 loss: 0.7503230574910628 grad: -0.32324536680108584
iteration: 120 loss: 0.6894939154436873 grad: -0.2983320821724492
iteration: 0 loss: 49.35504141598488 grad: 296.29009217816576
iteration: 10 loss: 6.766835776331515 grad: -0.98472645554652
iteration: 20 loss: 3.7685234127973426 grad: -1.0675518374030792
iteration: 30 loss: 2.61409999446042 grad: -0.8575330846163686
iteration: 40 loss: 2.001976066399989 grad: -0.7024534509221552
iteration: 50 loss: 1.62260273508797 grad: -0.5923183821154138
iteration: 60 loss: 1.3643931345637943 grad: -0.5113984131711711
iteration: 70 loss: 1.1772700569702217 grad: -0.44976413832164774
iteration: 80 loss: 1.0354144319612715 grad: -0.4013611996917743
iteration: 90 loss: 0.924162487786072 grad: -0.36238034006433895
iteration: 100 loss: 0.8345680222907338 grad: -0.3303292596815417
iteration: 110 loss: 0.7608630080920772 grad: -0.3035166365002977
iteration: 120 loss: 0.6991612466775802 grad: -0.28075722526024033
iteration: 0 loss: 51.40083170981943 grad: 344.1146928166981
iteration: 10 loss: 6.723681291256484 grad: -1.7456682671911765
iteration: 20 loss: 3.7578312888961345 grad: -1.327331402843015
iteration: 30 loss: 2.608059583650464 grad: -1.0002047679110895
iteration: 40 loss: 1.9974794204374369 grad: -0.7969767698387948
iteration: 50 loss: 1.618891197842045 grad: -0.6614884224621524
iteration: 60 loss: 1.3611824918587196 grad: -0.5652305937872721
iteration: 70 loss: 1.1744204937823186 grad: -0.49344856124498393
iteration: 80 loss: 1.0328438935547306 grad: -0.4378974562153158
iteration: 90 loss: 0.9218169941535631 grad: -0.39364241701305924
iteration: 100 loss: 0.8324092949747368 grad: -0.35755846293030336
iteration: 110 loss: 0.7588624816011443 grad: -0.32757356372525503
iteration: 120 loss: 0.6972968173773882 grad: -0.3022607573276517
iteration: 0 loss: 50.873866372708335 grad: 302.74118747363883
iteration: 10 loss: 6.790100439409261 grad: -0.7250223444432222
iteration: 20 loss: 3.781330456133095 grad: -1.0196463934541629
iteration: 30 loss: 2.6243214697715302 grad: -0.8386296808981789
iteration: 40 loss: 2.0106907396340663 grad: -0.6929681535321589
iteration: 50 loss: 1.630234038112948 grad: -0.5869974194084588
iteration: 60 loss: 1.3711895428144254 grad: -0.5082475577540639
iteration: 70 loss: 1.1833994603035867 grad: -0.44786582243378414
iteration: 80 loss: 1.040997729109842 grad: -0.4002377705074166
iteration: 90 loss: 0.9292901432924158 grad: -0.36176073346965365
iteration: 100 loss: 0.8393096278586696 grad: -0.3300495020374797
iteration: 110 loss: 0.7652732849115554 grad: -0.30347253510572125
iteration: 120 loss: 0.7032840130251646 grad: -0.28087994240863995
iteration: 0 loss: 54.06890729335365 grad: 268.434759637184
iteration: 10 loss: 6.946648729031081 grad: -0.5326021659407998
iteration: 20 loss: 3.865038748992943 grad: -0.8776209676231561
iteration: 30 loss: 2.6825680420743083 grad: -0.7387590698670454
iteration: 40 loss: 2.0556802678453012 grad: -0.6169005990534817
iteration: 50 loss: 1.667014097028269 grad: -0.5258814472202591
iteration: 60 loss: 1.4023572308055505 grad: -0.4573316290440369
iteration: 70 loss: 1.2104753539252173 grad: -0.40432975386240655
iteration: 80 loss: 1.0649520862150645 grad: -0.36227732555815356
iteration: 90 loss: 0.9507814259007484 grad: -0.32815446924624625
iteration: 100 loss: 0.8588059387398889 grad: -0.2999334186523505
iteration: 110 loss: 0.7831197034137486 grad: -0.27621371312982634
iteration: 120 loss: 0.7197424643799505 grad: -0.2560013206102717
iteration: 130 loss: 0.6658937292567813 grad: -0.23857292576162747
iteration: 0 loss: 52.045288680112144 grad: 294.7539304583435
iteration: 10 loss: 6.910182195412165 grad: -1.0269584344187526
iteration: 20 loss: 3.8320386127755395 grad: -1.0915418884213408
iteration: 30 loss: 2.653430024619824 grad: -0.8723476784097817
iteration: 40 loss: 2.0302334539583584 grad: -0.7125739552790884
iteration: 50 loss: 1.6446100362025968 grad: -0.5998266733972236
iteration: 60 loss: 1.3824091871910325 grad: -0.5172851336807364
iteration: 70 loss: 1.1925230837635106 grad: -0.45456047273709616
iteration: 80 loss: 1.0486432286955856 grad: -0.4053805770234017
iteration: 90 loss: 0.9358449909156082 grad: -0.3658215221759482
iteration: 100 loss: 0.8450308409422757 grad: -0.33332544261446256
iteration: 110 loss: 0.7703390862762635 grad: -0.30616103311075005
iteration: 120 loss: 0.7078225098683685 grad: -0.28311738341764375
iteration: 130 loss: 0.6547256550411888 grad: -0.263323253347038
iteration: 0 loss: 53.4909293272385 grad: 282.6951261441159
iteration: 10 loss: 6.917204785763385 grad: -1.1501690001672231
iteration: 20 loss: 3.8566672206916337 grad: -1.087406934642282
iteration: 30 loss: 2.676287389962164 grad: -0.8573703168186733
iteration: 40 loss: 2.050185905795636 grad: -0.697606070674262
iteration: 50 loss: 1.6620595886949807 grad: -0.586231279035647
iteration: 60 loss: 1.3978381272074165 grad: -0.5051160982732201
iteration: 70 loss: 1.2063225515513212 grad: -0.44364604715477135
iteration: 80 loss: 1.0611125348103665 grad: -0.3955307204651606
iteration: 90 loss: 0.9472125285897577 grad: -0.3568704493872775
iteration: 100 loss: 0.8554728832696128 grad: -0.3251366553859423
iteration: 110 loss: 0.7799937317600697 grad: -0.2986237298069069
iteration: 120 loss: 0.7167996071064987 grad: -0.276141571579555
iteration: 130 loss: 0.6631138423475601 grad: -0.25683539257372706
iteration: 0 loss: 50.69476339554471 grad: 302.7255802253747
iteration: 10 loss: 6.923321273929642 grad: -0.6524500107161284
iteration: 20 loss: 3.8336050754612434 grad: -0.9447001453623587
iteration: 30 loss: 2.6545789896115988 grad: -0.7881197167324068
iteration: 40 loss: 2.031522054139094 grad: -0.655578896641408
iteration: 50 loss: 1.6459989844121414 grad: -0.5574639885521016
iteration: 60 loss: 1.38383706058655 grad: -0.4839027108763095
iteration: 70 loss: 1.193950347624526 grad: -0.4271944359831508
iteration: 80 loss: 1.0500474932013772 grad: -0.3823006681283875
iteration: 90 loss: 0.937214338723317 grad: -0.34593648577423586
iteration: 100 loss: 0.8463594527437305 grad: -0.31590588400708197
iteration: 110 loss: 0.7716246573307383 grad: -0.2906969072522286
iteration: 120 loss: 0.7090647497694921 grad: -0.26923892599476756
iteration: 130 loss: 0.6559254121511086 grad: -0.25075437744821705
iteration: 0 loss: 50.120118078817704 grad: 348.5096156191094
iteration: 10 loss: 6.898597770772002 grad: -0.8658048986824065
iteration: 20 loss: 3.8288368876427823 grad: -1.0330064144241886
iteration: 30 loss: 2.6540687954238407 grad: -0.8472019949625207
iteration: 40 loss: 2.0322118640729885 grad: -0.700224034290741
iteration: 50 loss: 1.6470450081869372 grad: -0.5933141431531175
iteration: 60 loss: 1.3849583818611213 grad: -0.5137999491194778
iteration: 70 loss: 1.1950449934941554 grad: -0.4527904826472403
iteration: 80 loss: 1.051078836887262 grad: -0.40464462623479025
iteration: 90 loss: 0.9381716590008348 grad: -0.36573714819481395
iteration: 100 loss: 0.8472426638909383 grad: -0.3336646746976707
iteration: 110 loss: 0.7724379466140321 grad: -0.3067815099448976
iteration: 120 loss: 0.709813805149803 grad: -0.2839268588382585
iteration: 130 loss: 0.6566161946723035 grad: -0.2642601547712077
iteration: 0 loss: 51.38989405130658 grad: 288.1574355588874
iteration: 10 loss: 6.935350679814871 grad: -0.6529383642926385
iteration: 20 loss: 3.857246006836745 grad: -0.9279722600845403
iteration: 30 loss: 2.675027125043967 grad: -0.769556712916866
iteration: 40 loss: 2.0486611855349524 grad: -0.6386272711731256
iteration: 50 loss: 1.6605731704793554 grad: -0.5424611816169084
iteration: 60 loss: 1.3964568962280943 grad: -0.4706306961452998
iteration: 70 loss: 1.2050546962647621 grad: -0.41537186335020426
iteration: 80 loss: 1.059950256556435 grad: -0.3716804174741437
iteration: 90 loss: 0.9461443215849695 grad: -0.336318194132458
iteration: 100 loss: 0.8544873209287455 grad: -0.30713010846463523
iteration: 110 loss: 0.7790805720027575 grad: -0.28263670398135404
iteration: 120 loss: 0.7159500003645007 grad: -0.26179247728969296
iteration: 130 loss: 0.6623202435288146 grad: -0.24383921659933644
iteration: 0 loss: 52.623426784080905 grad: 324.4104536246007
iteration: 10 loss: 6.7097063482894805 grad: -1.416175958051142
iteration: 20 loss: 3.7436621176452305 grad: -1.2652651855835386
iteration: 30 loss: 2.59761018316873 grad: -0.9803923019519468
iteration: 40 loss: 1.9894500544134892 grad: -0.789368348859782
iteration: 50 loss: 1.6124364564416283 grad: -0.6585831738144469
iteration: 60 loss: 1.3558089811586713 grad: -0.5644492927973104
iteration: 70 loss: 1.1698272246572246 grad: -0.49371965080779967
iteration: 80 loss: 1.0288369819584824 grad: -0.43871729310936486
iteration: 90 loss: 0.9182653835389167 grad: -0.3947532390374467
iteration: 100 loss: 0.8292207380236092 grad: -0.35882013812710944
iteration: 110 loss: 0.7559698170781634 grad: -0.32890659130959027
iteration: 120 loss: 0.6946497421988004 grad: -0.30361872783462285
iteration: 0 loss: 50.319163682687844 grad: 345.4582230459797
iteration: 10 loss: 7.059701258787754 grad: -1.5242567847397832
iteration: 20 loss: 3.908401877800357 grad: -1.2250585486334362
iteration: 30 loss: 2.7036078908581245 grad: -0.9336280508140734
iteration: 40 loss: 2.0672619952544498 grad: -0.747704254966571
iteration: 50 loss: 1.6738152768610812 grad: -0.6224454413440244
iteration: 60 loss: 1.4064565643900757 grad: -0.5329398065433928
iteration: 70 loss: 1.2129271894796974 grad: -0.4659440001749907
iteration: 80 loss: 1.0663432872976377 grad: -0.413959978611115
iteration: 90 loss: 0.9514618816505886 grad: -0.37246434518595173
iteration: 100 loss: 0.8589955492989247 grad: -0.33857740880318876
iteration: 110 loss: 0.78296254669563 grad: -0.31038240386245675
iteration: 120 loss: 0.7193361620596245 grad: -0.2865554063701243
iteration: 130 loss: 0.6653062711040796 grad: -0.2661531233294949
iteration: 0 loss: 51.810294368063815 grad: 241.19133016566076
iteration: 10 loss: 6.982163098988869 grad: 0.09075989503754166
iteration: 20 loss: 3.8700216088072095 grad: -0.5082921212879765
iteration: 30 loss: 2.680981940595137 grad: -0.4903753267631752
iteration: 40 loss: 2.052407993343877 grad: -0.43330909500663994
iteration: 50 loss: 1.663379552263326 grad: -0.38180147853612484
iteration: 60 loss: 1.3987794308748314 grad: -0.3395795155531639
iteration: 70 loss: 1.2070900978286028 grad: -0.3052523687006229
iteration: 80 loss: 1.0617952224368887 grad: -0.27707111484366653
iteration: 90 loss: 0.9478516104599376 grad: -0.25362167390514
iteration: 100 loss: 0.8560883165094476 grad: -0.23384554307779803
iteration: 110 loss: 0.7805953643829443 grad: -0.2169598727480943
iteration: 120 loss: 0.7173922831965353 grad: -0.20238175709823947
iteration: 130 loss: 0.6636998632322336 grad: -0.1896715898865725
iteration: 0 loss: 53.07914153578961 grad: 272.11968590495974
iteration: 10 loss: 6.972736447081885 grad: -0.8700381414442155
iteration: 20 loss: 3.8781065379655293 grad: -0.9811200246545159
iteration: 30 loss: 2.6893224092838284 grad: -0.7992161521409781
iteration: 40 loss: 2.0595213685283356 grad: -0.6599205115064963
iteration: 50 loss: 1.6693221770334192 grad: -0.5593537984611301
iteration: 60 loss: 1.4037764933098267 grad: -0.4847307147993006
iteration: 70 loss: 1.2113423119385516 grad: -0.42750896695544405
iteration: 80 loss: 1.0654579553425636 grad: -0.38234875071698127
iteration: 90 loss: 0.9510420386332871 grad: -0.3458403189186994
iteration: 100 loss: 0.8588950525277969 grad: -0.31573018407076775
iteration: 110 loss: 0.7830861909446454 grad: -0.2904777204746936
iteration: 120 loss: 0.7196197740126163 grad: -0.2689970858500691
iteration: 130 loss: 0.6657053721579277 grad: -0.2505022408134002
iteration: 0 loss: 47.687003952159806 grad: 300.1678392724885
iteration: 10 loss: 6.888826876817637 grad: -0.3365077840617833
iteration: 20 loss: 3.8176851154563254 grad: -0.7322534772712996
iteration: 30 loss: 2.6455780973255556 grad: -0.6508108574410467
iteration: 40 loss: 2.025750776484611 grad: -0.5585695208477908
iteration: 50 loss: 1.641984996884689 grad: -0.4842987491331895
iteration: 60 loss: 1.380883908624748 grad: -0.42610429792510623
iteration: 70 loss: 1.1916866843816107 grad: -0.3799589140837397
iteration: 80 loss: 1.048257283050134 grad: -0.34269042864632093
iteration: 90 loss: 0.9357633295329916 grad: -0.31204483001799926
iteration: 100 loss: 0.8451597993759027 grad: -0.2864352371173605
iteration: 110 loss: 0.7706164497705107 grad: -0.264729740642641
iteration: 120 loss: 0.7082057175830001 grad: -0.24610565023971753
iteration: 130 loss: 0.6551848723264068 grad: -0.2299532726146748
iteration: 0 loss: 50.22168063665378 grad: 291.24801933098274
iteration: 10 loss: 6.919624269723912 grad: -0.4455560881401789
iteration: 20 loss: 3.839833260464564 grad: -0.8995899798321159
iteration: 30 loss: 2.661364150707992 grad: -0.7624452343047655
iteration: 40 loss: 2.0375708380659656 grad: -0.6361824930142708
iteration: 50 loss: 1.6512538727503687 grad: -0.5413318182943747
iteration: 60 loss: 1.38841926080609 grad: -0.46990153740103713
iteration: 70 loss: 1.1979855574626195 grad: -0.41475102152343546
iteration: 80 loss: 1.0536384810606334 grad: -0.37106913808578806
iteration: 90 loss: 0.9404411270715669 grad: -0.33568473990939135
iteration: 100 loss: 0.8492839913187709 grad: -0.3064673336093644
iteration: 110 loss: 0.7742952635265035 grad: -0.28194644025287274
iteration: 120 loss: 0.7115195522903626 grad: -0.2610794773556565
iteration: 130 loss: 0.6581948844810458 grad: -0.24310876892544903
iteration: 0 loss: 54.43950868548477 grad: 253.93873876966188
iteration: 10 loss: 6.913358679257519 grad: 0.19306170033137476
iteration: 20 loss: 3.854473404289365 grad: -0.7352187314476082
iteration: 30 loss: 2.6780469549969648 grad: -0.6846658351606991
iteration: 40 loss: 2.053388768994988 grad: -0.5902045441258504
iteration: 50 loss: 1.6657316377519982 grad: -0.5108062701351361
iteration: 60 loss: 1.401593957282826 grad: -0.4481228134030303
iteration: 70 loss: 1.2100033452304408 grad: -0.3984286372091438
iteration: 80 loss: 1.0646537900298758 grad: -0.358388876718685
iteration: 90 loss: 0.9505914578831338 grad: -0.32556041642194555
iteration: 100 loss: 0.8586857802077826 grad: -0.2982073080426774
iteration: 110 loss: 0.7830456201769597 grad: -0.27508866956556277
iteration: 120 loss: 0.7196992993573407 grad: -0.25530319963405557
iteration: 130 loss: 0.665871519451023 grad: -0.23818416498007755
iteration: 0 loss: 49.62484674226198 grad: 326.12027491003005
iteration: 10 loss: 6.831384463151111 grad: -1.0257573504918978
iteration: 20 loss: 3.79977717387018 grad: -1.0991297790164123
iteration: 30 loss: 2.633885743804471 grad: -0.8827151622848971
iteration: 40 loss: 2.016229307595743 grad: -0.7226445830941166
iteration: 50 loss: 1.633660937429795 grad: -0.6088851407719233
iteration: 60 loss: 1.3733920382090796 grad: -0.5253173900820791
iteration: 70 loss: 1.1848399134554795 grad: -0.4616997225518433
iteration: 80 loss: 1.0419385629079443 grad: -0.4117707963613011
iteration: 90 loss: 0.929890230003242 grad: -0.3715869777520978
iteration: 100 loss: 0.8396701102889726 grad: -0.33856729264662055
iteration: 110 loss: 0.7654611936814448 grad: -0.31096040095366806
iteration: 120 loss: 0.7033452508685927 grad: -0.28753935188017343
iteration: 0 loss: 51.23398537451153 grad: 293.44652476249854
iteration: 10 loss: 6.610090700445653 grad: -0.16618593570815335
iteration: 20 loss: 3.695076184371415 grad: -0.8214184459488016
iteration: 30 loss: 2.5682962722110214 grad: -0.7310315082780423
iteration: 40 loss: 1.9694894310671935 grad: -0.6233688737936758
iteration: 50 loss: 1.5977809456786334 grad: -0.5373972439844271
iteration: 60 loss: 1.3444839342833719 grad: -0.4706525229329147
iteration: 70 loss: 1.160746684130779 grad: -0.4181278511170681
iteration: 80 loss: 1.0213503181296553 grad: -0.3759671576766881
iteration: 90 loss: 0.9119569470210742 grad: -0.3414728388136373
iteration: 100 loss: 0.8238114114881021 grad: -0.3127677043329413
iteration: 110 loss: 0.7512645368898864 grad: -0.28852493022516756
iteration: 120 loss: 0.6905076620104079 grad: -0.26778725746753207
iteration: 0 loss: 52.339783639621785 grad: 316.91676780648095
iteration: 10 loss: 6.788032782613465 grad: -1.5817942345899034
iteration: 20 loss: 3.780501265689881 grad: -1.2893452027602565
iteration: 30 loss: 2.6212615719735535 grad: -0.9819764832522915
iteration: 40 loss: 2.0067494227979212 grad: -0.7854557060866929
iteration: 50 loss: 1.6260309609765913 grad: -0.653117088541129
iteration: 60 loss: 1.3669878837798601 grad: -0.5586393379088981
iteration: 70 loss: 1.1793112449278687 grad: -0.48799030858301007
iteration: 80 loss: 1.0370682869436016 grad: -0.4332217586397218
iteration: 90 loss: 0.9255341242140854 grad: -0.38954002024824697
iteration: 100 loss: 0.8357272757337093 grad: -0.35389496603221426
iteration: 110 loss: 0.7618581604264033 grad: -0.3242576405906478
iteration: 120 loss: 0.7000267899449103 grad: -0.2992275202500077
iteration: 0 loss: 51.29260061864664 grad: 324.86384542229086
iteration: 10 loss: 6.923224294080063 grad: -1.119869524105261
iteration: 20 loss: 3.8376771518760506 grad: -1.1231832115756741
iteration: 30 loss: 2.657644019520252 grad: -0.8958435119080502
iteration: 40 loss: 2.033632981386042 grad: -0.7316378695560649
iteration: 50 loss: 1.6474410593537372 grad: -0.6157066434240459
iteration: 60 loss: 1.3848192652364395 grad: -0.5307941872792679
iteration: 70 loss: 1.1946106990330918 grad: -0.4662589666506384
iteration: 80 loss: 1.0504773306719555 grad: -0.4156629439357734
iteration: 90 loss: 0.9374754704974128 grad: -0.37497204624692404
iteration: 100 loss: 0.8464947767068803 grad: -0.34155400695751104
iteration: 110 loss: 0.771664669963808 grad: -0.3136261213676923
iteration: 120 loss: 0.7090316042707524 grad: -0.2899411282580904
iteration: 130 loss: 0.6558355107023007 grad: -0.2696014972339912
iteration: 0 loss: 52.04596623437246 grad: 366.5779919310175
iteration: 10 loss: 6.525745334448609 grad: -1.6423279037735583
iteration: 20 loss: 3.676741706130873 grad: -1.383152463823405
iteration: 30 loss: 2.5615048102976785 grad: -1.057448598737678
iteration: 40 loss: 1.9660624074035282 grad: -0.8461846067981057
iteration: 50 loss: 1.595621201891101 grad: -0.7033252407149595
iteration: 60 loss: 1.3428854204017129 grad: -0.6011928041090642
iteration: 70 loss: 1.1594282615991356 grad: -0.524790712563133
iteration: 80 loss: 1.0201864265301415 grad: -0.46556660616363144
iteration: 90 loss: 0.9108865750481527 grad: -0.41834487838572093
iteration: 100 loss: 0.8228028934884771 grad: -0.37982627738582325
iteration: 110 loss: 0.7503005108419942 grad: -0.34781386398240877
iteration: 120 loss: 0.6895781298952154 grad: -0.32079031348940545
iteration: 0 loss: 52.84689712275208 grad: 268.74473634481274
iteration: 10 loss: 6.957838383340624 grad: -0.1581860214192276
iteration: 20 loss: 3.8689971364312687 grad: -0.766550020598225
iteration: 30 loss: 2.683315361765587 grad: -0.6836538232864451
iteration: 40 loss: 2.0551609475887744 grad: -0.5843062580972918
iteration: 50 loss: 1.6659435450836677 grad: -0.5047235384026758
iteration: 60 loss: 1.4010380738196049 grad: -0.44278307870663547
iteration: 70 loss: 1.2090497391733068 grad: -0.3939355533773037
iteration: 80 loss: 1.0634915853744464 grad: -0.3546547315105042
iteration: 90 loss: 0.9493236101572868 grad: -0.3224653099074976
iteration: 100 loss: 0.8573708606731171 grad: -0.2956405104221169
iteration: 110 loss: 0.7817179319569566 grad: -0.27295735238569363
iteration: 120 loss: 0.7183792494367315 grad: -0.2535319644635559
iteration: 130 loss: 0.664571267002843 grad: -0.23671232202095396
iteration: 0 loss: 53.13673604512676 grad: 322.46377200740346
iteration: 10 loss: 6.855615727572637 grad: -1.6304200816538217
iteration: 20 loss: 3.8164236394460946 grad: -1.3162450072966279
iteration: 30 loss: 2.6455463461831887 grad: -1.0033944750769899
iteration: 40 loss: 2.025031105897638 grad: -0.8030544789296534
iteration: 50 loss: 1.6406608076098648 grad: -0.6679395733008181
iteration: 60 loss: 1.3791658279538255 grad: -0.5713891335109303
iteration: 70 loss: 1.1897309862336607 grad: -0.4991481130711926
iteration: 80 loss: 1.04616654987314 grad: -0.4431246069986083
iteration: 90 loss: 0.9336034597376138 grad: -0.39843111389574765
iteration: 100 loss: 0.8429731407954932 grad: -0.3619546090446316
iteration: 110 loss: 0.7684303160585014 grad: -0.3316227717254948
iteration: 120 loss: 0.7060377312302741 grad: -0.306004367209197
iteration: 130 loss: 0.6530463241498926 grad: -0.28407986161346943
iteration: 0 loss: 51.495599794275286 grad: 284.65515577008637
iteration: 10 loss: 6.711064730136679 grad: -0.6858290735355126
iteration: 20 loss: 3.752010866052594 grad: -0.9654465738574243
iteration: 30 loss: 2.606675634538698 grad: -0.8037409903666967
iteration: 40 loss: 1.997997665360179 grad: -0.668378405185533
iteration: 50 loss: 1.620263220630725 grad: -0.568390508772809
iteration: 60 loss: 1.362945429909194 grad: -0.49347019272617465
iteration: 70 loss: 1.1763530425559607 grad: -0.4357217301106763
iteration: 80 loss: 1.0348338802168242 grad: -0.3900022492447617
iteration: 90 loss: 0.9238054886638813 grad: -0.35296472409167445
iteration: 100 loss: 0.834364941403237 grad: -0.32237344749950747
iteration: 110 loss: 0.7607688612320502 grad: -0.2966897188768138
iteration: 120 loss: 0.6991458696379595 grad: -0.2748241518068194
iteration: 0 loss: 51.388770144304935 grad: 259.5331911397041
iteration: 10 loss: 6.8414512657414805 grad: -0.02835490094539784
iteration: 20 loss: 3.7998143887115146 grad: -0.6663614954884384
iteration: 30 loss: 2.635713291731463 grad: -0.6169104584406805
iteration: 40 loss: 2.019231470342261 grad: -0.534665731136035
iteration: 50 loss: 1.637230197432845 grad: -0.4652046852778854
iteration: 60 loss: 1.377197539116382 grad: -0.40993564960217804
iteration: 70 loss: 1.188708210423308 grad: -0.3658196793315883
iteration: 80 loss: 1.0457782973053327 grad: -0.3300730598852367
iteration: 90 loss: 0.9336534877390725 grad: -0.3006264814629259
iteration: 100 loss: 0.8433326147036165 grad: -0.275993740557259
iteration: 110 loss: 0.7690118379875706 grad: -0.255103539712504
iteration: 120 loss: 0.7067803770317534 grad: -0.23717245602131984
iteration: 130 loss: 0.653906647293628 grad: -0.22161767691912931
iteration: 0 loss: 54.374533059060056 grad: 264.2883218633597
iteration: 10 loss: 6.8122065206014835 grad: 0.3327433705681425
iteration: 20 loss: 3.787059619879156 grad: -0.6078705886401703
iteration: 30 loss: 2.6290401275005437 grad: -0.6039719186760062
iteration: 40 loss: 2.0152626823397357 grad: -0.5348118535124532
iteration: 50 loss: 1.6346711430907652 grad: -0.4701227339540863
iteration: 60 loss: 1.3754559598866185 grad: -0.41674390613011125
iteration: 70 loss: 1.1874775058677187 grad: -0.37334005761045
iteration: 80 loss: 1.044884945085662 grad: -0.33777508216347674
iteration: 90 loss: 0.9329924820960348 grad: -0.3082586685117479
iteration: 100 loss: 0.8428370198463943 grad: -0.2834355723813097
iteration: 110 loss: 0.7686371767154427 grad: -0.2622997073745831
iteration: 120 loss: 0.7064960765531562 grad: -0.24410139376801837
iteration: 0 loss: 49.94300166005453 grad: 260.8229343297462
iteration: 10 loss: 6.802090162448042 grad: 0.021507268408107138
iteration: 20 loss: 3.7947003854433206 grad: -0.7416447123129097
iteration: 30 loss: 2.636584462760187 grad: -0.6805256569199964
iteration: 40 loss: 2.0216153540628015 grad: -0.5849189191659097
iteration: 50 loss: 1.6399849706455247 grad: -0.5057644724620945
iteration: 60 loss: 1.3799633338053767 grad: -0.44355035224839584
iteration: 70 loss: 1.1913632277465611 grad: -0.39431914247072564
iteration: 80 loss: 1.0482846839635054 grad: -0.3546886472292633
iteration: 90 loss: 0.9360053651105938 grad: -0.3222119933114519
iteration: 100 loss: 0.8455364594390382 grad: -0.2951598123835043
iteration: 110 loss: 0.7710785277481178 grad: -0.2722993599340696
iteration: 120 loss: 0.7087218467647951 grad: -0.2527367120565559
iteration: 130 loss: 0.6557344887797957 grad: -0.23581128600675608
iteration: 0 loss: 49.993820450367714 grad: 312.4259460493324
iteration: 10 loss: 6.828726005140445 grad: -1.2897387603159378
iteration: 20 loss: 3.803114199820894 grad: -1.1452871926512362
iteration: 30 loss: 2.6377610173445563 grad: -0.8929276578592176
iteration: 40 loss: 2.019987153742146 grad: -0.7227382285771211
iteration: 50 loss: 1.6371750659805002 grad: -0.6054347854719505
iteration: 60 loss: 1.3766506647693446 grad: -0.5205463084127903
iteration: 70 loss: 1.1878610115616703 grad: -0.4564874343358429
iteration: 80 loss: 1.0447467229724183 grad: -0.4064972386430618
iteration: 90 loss: 0.9325096324541019 grad: -0.3664225207502516
iteration: 100 loss: 0.8421224841680751 grad: -0.3335869824836474
iteration: 110 loss: 0.7677654403408021 grad: -0.3061935823846654
iteration: 120 loss: 0.7055176207603092 grad: -0.2829928249795193
iteration: 130 loss: 0.6526408971199916 grad: -0.263089783632819
iteration: 0 loss: 55.67290857788558 grad: 316.5021455993436
iteration: 10 loss: 6.831400576950645 grad: -0.8371833292671503
iteration: 20 loss: 3.8149374698238963 grad: -1.0814611410700177
iteration: 30 loss: 2.6499964189320053 grad: -0.8804523916057523
iteration: 40 loss: 2.0312065786267586 grad: -0.7240741722130499
iteration: 50 loss: 1.647255894216042 grad: -0.6115361476288357
iteration: 60 loss: 1.385713081922404 grad: -0.5284044979703741
iteration: 70 loss: 1.1960550706584896 grad: -0.4649151162354359
iteration: 80 loss: 1.0522066407758592 grad: -0.4149795578577174
iteration: 90 loss: 0.9393467250052212 grad: -0.37472703816593345
iteration: 100 loss: 0.8484272819573149 grad: -0.34161043636524746
iteration: 110 loss: 0.7736114688773341 grad: -0.31389516766468945
iteration: 120 loss: 0.7109649213875442 grad: -0.290362883581383
iteration: 130 loss: 0.6577388944325239 grad: -0.27013428206115175
iteration: 0 loss: 53.15328809355298 grad: 348.96968327559955
iteration: 10 loss: 6.899441992818703 grad: -1.7852694125228132
iteration: 20 loss: 3.840310129985276 grad: -1.3366217179755364
iteration: 30 loss: 2.66148761082501 grad: -1.0093030901978715
iteration: 40 loss: 2.036947028746495 grad: -0.8047514931532597
iteration: 50 loss: 1.6501709343757616 grad: -0.6680612012163787
iteration: 60 loss: 1.3870820726072204 grad: -0.570837206220195
iteration: 70 loss: 1.1965152843554279 grad: -0.4982903497413245
iteration: 80 loss: 1.0521058928972542 grad: -0.44212916351766
iteration: 90 loss: 0.9388880422631535 grad: -0.3973809560375411
iteration: 100 loss: 0.8477353846267927 grad: -0.36089268016240345
iteration: 110 loss: 0.7727660779673484 grad: -0.33057183798715556
iteration: 120 loss: 0.7100186172543048 grad: -0.3049764764519322
iteration: 130 loss: 0.6567272309330705 grad: -0.2830811471381398
iteration: 0 loss: 52.59355356654915 grad: 330.58901320814334
iteration: 10 loss: 6.906261189089988 grad: -0.7960790196429333
iteration: 20 loss: 3.833881401768684 grad: -1.0053963917641846
iteration: 30 loss: 2.6573844547358267 grad: -0.8289722439150802
iteration: 40 loss: 2.034684192668175 grad: -0.6865179898146918
iteration: 50 loss: 1.6490272623639777 grad: -0.5824105022372663
iteration: 60 loss: 1.3866198893167414 grad: -0.5048132860747567
iteration: 70 loss: 1.1964793372186995 grad: -0.44519267079679026
iteration: 80 loss: 1.0523432233497838 grad: -0.3980945375377952
iteration: 90 loss: 0.9393037096068507 grad: -0.3600022668748448
iteration: 100 loss: 0.8482685122111516 grad: -0.3285799502799397
iteration: 110 loss: 0.7733765297199088 grad: -0.3022259256522303
iteration: 120 loss: 0.7106793125190879 grad: -0.2798092861259558
iteration: 130 loss: 0.6574195507831643 grad: -0.2605104160700042
iteration: 0 loss: 52.753684884580146 grad: 314.06239684146936
iteration: 10 loss: 6.893463227920248 grad: -1.3361575531236105
iteration: 20 loss: 3.8355466002390544 grad: -1.1216679436821715
iteration: 30 loss: 2.6584938089313255 grad: -0.868078896693891
iteration: 40 loss: 2.0350108747503244 grad: -0.7013384766816604
iteration: 50 loss: 1.648882103910141 grad: -0.5872630930645796
iteration: 60 loss: 1.3862098848993478 grad: -0.5049483684180276
iteration: 70 loss: 1.195923491411122 grad: -0.4429071276792834
iteration: 80 loss: 1.051709789817778 grad: -0.3945148953416952
iteration: 90 loss: 0.9386325748710078 grad: -0.3557262717361228
iteration: 100 loss: 0.8475834095437759 grad: -0.3239429255690016
iteration: 110 loss: 0.7726917397199858 grad: -0.2974234980372801
iteration: 120 loss: 0.7100034373016073 grad: -0.2749586006330079
iteration: 130 loss: 0.6567577039693127 grad: -0.25568263259019247
iteration: 0 loss: 52.46578581545534 grad: 330.13370039405754
iteration: 10 loss: 6.873691340060842 grad: -0.9452276721483286
iteration: 20 loss: 3.822365607166789 grad: -1.0735438571185494
iteration: 30 loss: 2.6498603485309684 grad: -0.8617334906270736
iteration: 40 loss: 2.0288513594838067 grad: -0.7043261857467222
iteration: 50 loss: 1.6442007341121014 grad: -0.5927967258754651
iteration: 60 loss: 1.3824867774236405 grad: -0.5110815395918346
iteration: 70 loss: 1.192861614270772 grad: -0.4489894274943118
iteration: 80 loss: 1.0491265858420527 grad: -0.40032083099488813
iteration: 90 loss: 0.9364091673209756 grad: -0.361187272769817
iteration: 100 loss: 0.8456387217693191 grad: -0.3290519339011074
iteration: 110 loss: 0.7709683521373915 grad: -0.3021975545382735
iteration: 120 loss: 0.7084593964841175 grad: -0.2794232587523649
iteration: 130 loss: 0.6553615433926594 grad: -0.25986528115771135
iteration: 0 loss: 51.11765371242961 grad: 330.6336811822151
iteration: 10 loss: 6.947181664387741 grad: -0.9614488257733844
iteration: 20 loss: 3.8521254457490284 grad: -1.0379634676975038
iteration: 30 loss: 2.667895444707607 grad: -0.8379361951020164
iteration: 40 loss: 2.041673143118136 grad: -0.6885750516170095
iteration: 50 loss: 1.654120441986751 grad: -0.5818687052372479
iteration: 60 loss: 1.3905700677274497 grad: -0.5031785844344071
iteration: 70 loss: 1.19968218402565 grad: -0.4430889920427199
iteration: 80 loss: 1.0550269642972538 grad: -0.395808248762089
iteration: 90 loss: 0.9416096196123362 grad: -0.35767308891557903
iteration: 100 loss: 0.8502890235688276 grad: -0.32627808124521507
iteration: 110 loss: 0.7751748757562134 grad: -0.29998650697107854
iteration: 120 loss: 0.7123003657745606 grad: -0.2776490071014118
iteration: 130 loss: 0.6588961446889864 grad: -0.2584359847397535
iteration: 0 loss: 55.47393447057845 grad: 280.4486340497956
iteration: 10 loss: 6.822189712906989 grad: 0.8304916084853735
iteration: 20 loss: 3.7959145265228718 grad: -0.46765205011955874
iteration: 30 loss: 2.637802107995743 grad: -0.5362520165378883
iteration: 40 loss: 2.023360608537256 grad: -0.4939125689137724
iteration: 50 loss: 1.6420284980950286 grad: -0.4422101589580177
iteration: 60 loss: 1.3821347393682943 grad: -0.39617979897730954
iteration: 70 loss: 1.193565669072541 grad: -0.3573786987809745
iteration: 80 loss: 1.0504656858962667 grad: -0.3249104327855673
iteration: 90 loss: 0.9381372385412099 grad: -0.29759104650651946
iteration: 100 loss: 0.8476053203614604 grad: -0.27439180388149165
iteration: 110 loss: 0.7730782543871183 grad: -0.2544958020431418
iteration: 120 loss: 0.7106507099716427 grad: -0.23726950005294303
iteration: 130 loss: 0.6575932360749654 grad: -0.22222260679376712
iteration: 0 loss: 52.850221215002236 grad: 340.1758946993971
iteration: 10 loss: 6.65271968952803 grad: 0.20261434825704241
iteration: 20 loss: 3.728779383979889 grad: -0.7954062336533956
iteration: 30 loss: 2.598491334717768 grad: -0.7517809073065843
iteration: 40 loss: 1.9958474630114489 grad: -0.6512002511347261
iteration: 50 loss: 1.6208382455383386 grad: -0.5645493573259267
iteration: 60 loss: 1.3648464779441218 grad: -0.4954993582423828
iteration: 70 loss: 1.1789192578273122 grad: -0.44051859435215257
iteration: 80 loss: 1.0377275447289405 grad: -0.39612220640638496
iteration: 90 loss: 0.9268442484147438 grad: -0.35968198936417917
iteration: 100 loss: 0.8374466683186174 grad: -0.3293046492310774
iteration: 110 loss: 0.7638351849487993 grad: -0.30362633511195813
iteration: 120 loss: 0.7021633134522745 grad: -0.28165172053470755
iteration: 0 loss: 54.44133058446889 grad: 298.6658278017745
iteration: 10 loss: 6.70409131844938 grad: -0.3935812531067832
iteration: 20 loss: 3.7467151911295002 grad: -0.9717827958838284
iteration: 30 loss: 2.604082701398759 grad: -0.8245413584824381
iteration: 40 loss: 1.9967230000077671 grad: -0.6879016237998533
iteration: 50 loss: 1.6196801348740502 grad: -0.5852502649476204
iteration: 60 loss: 1.3627530122268003 grad: -0.5079402119083822
iteration: 70 loss: 1.1763935520576028 grad: -0.44824191212492015
iteration: 80 loss: 1.0350185011111122 grad: -0.4009518853293898
iteration: 90 loss: 0.9240813455413363 grad: -0.36264111584873165
iteration: 100 loss: 0.8346991528324824 grad: -0.3310056224988719
iteration: 110 loss: 0.7611402659813806 grad: -0.30445481627541343
iteration: 120 loss: 0.6995404763296061 grad: -0.2818606556912442
iteration: 0 loss: 52.51952131746531 grad: 247.8984194508442
iteration: 10 loss: 6.910469415837352 grad: 0.016535030709528182
iteration: 20 loss: 3.849325707044748 grad: -0.7787344362228585
iteration: 30 loss: 2.672470350765376 grad: -0.7044437797808482
iteration: 40 loss: 2.0480791855054368 grad: -0.601492047593194
iteration: 50 loss: 1.660831247123786 grad: -0.5180746546722583
iteration: 60 loss: 1.3971007273805134 grad: -0.45317359423253245
iteration: 70 loss: 1.2058787987903914 grad: -0.4021304705321088
iteration: 80 loss: 1.0608537444526187 grad: -0.3612122165359204
iteration: 90 loss: 0.9470749217826696 grad: -0.3277818372686744
iteration: 100 loss: 0.8554170912489512 grad: -0.2999997058920278
iteration: 110 loss: 0.7799944398359251 grad: -0.2765654362230744
iteration: 120 loss: 0.7168399847806044 grad: -0.256541628147946
iteration: 130 loss: 0.6631823869963476 grad: -0.2392386447465528
iteration: 0 loss: 51.280671253368794 grad: 231.11679163423548
iteration: 10 loss: 6.846994857753047 grad: 0.43671638075476393
iteration: 20 loss: 3.8136616180554483 grad: -0.5510734195136073
iteration: 30 loss: 2.6485325701151767 grad: -0.5641059692350018
iteration: 40 loss: 2.0304356270694783 grad: -0.5041268243214626
iteration: 50 loss: 1.6470291964258401 grad: -0.4452345782805326
iteration: 60 loss: 1.3858528127435337 grad: -0.39584081907292384
iteration: 70 loss: 1.1964360218296612 grad: -0.3553383769484537
iteration: 80 loss: 1.0527463075934314 grad: -0.3219766637772244
iteration: 90 loss: 0.9399907968143204 grad: -0.29418837287648303
iteration: 100 loss: 0.8491395068129929 grad: -0.2707559559156431
iteration: 110 loss: 0.7743672465758622 grad: -0.2507627876319686
iteration: 120 loss: 0.7117473011915287 grad: -0.23351974903161948
iteration: 130 loss: 0.6585360457092148 grad: -0.21850424737673432
iteration: 0 loss: 52.197028305392266 grad: 259.2648647091966
iteration: 10 loss: 6.898493130344295 grad: 0.31489104289576214
iteration: 20 loss: 3.830766711855135 grad: -0.5467111273961824
iteration: 30 loss: 2.6582186646688486 grad: -0.5558087423953907
iteration: 40 loss: 2.0371876748214905 grad: -0.49712883938224384
iteration: 50 loss: 1.652250890283581 grad: -0.43952327785161405
iteration: 60 loss: 1.390139741933953 grad: -0.39110113821697323
iteration: 70 loss: 1.200091557584611 grad: -0.3513234091273855
iteration: 80 loss: 1.0559450598961617 grad: -0.3185137213541707
iteration: 90 loss: 0.9428424101291223 grad: -0.29115645727111295
iteration: 100 loss: 0.8517174417626269 grad: -0.26806831522447905
iteration: 110 loss: 0.776723235467464 grad: -0.248355593470543
iteration: 120 loss: 0.7139192350116239 grad: -0.23134488640052803
iteration: 130 loss: 0.6605525701278235 grad: -0.21652465376063976
iteration: 0 loss: 49.21405147827779 grad: 306.65024281469175
iteration: 10 loss: 7.000733514571783 grad: -0.4819401205473186
iteration: 20 loss: 3.8806204419200765 grad: -0.8581164669576962
iteration: 30 loss: 2.68926935778307 grad: -0.7335860070668176
iteration: 40 loss: 2.059085322191136 grad: -0.6167467793827248
iteration: 50 loss: 1.6688707299185395 grad: -0.5276886983092491
iteration: 60 loss: 1.403379752685644 grad: -0.4599357196479652
iteration: 70 loss: 1.2110070205484258 grad: -0.40723448606381907
iteration: 80 loss: 1.0651768014768095 grad: -0.365255410411749
iteration: 90 loss: 0.950805712031731 grad: -0.3310979471485128
iteration: 100 loss: 0.858695214515964 grad: -0.30279113522807555
iteration: 110 loss: 0.7829160080486045 grad: -0.27896290806996216
iteration: 120 loss: 0.7194738022827468 grad: -0.25863386310802045
iteration: 130 loss: 0.6655793048724015 grad: -0.24108831088270466
iteration: 0 loss: 52.78347465855716 grad: 285.5330404319999
iteration: 10 loss: 6.84879938319715 grad: -0.6051984487966765
iteration: 20 loss: 3.8046165604864655 grad: -0.9284840471006568
iteration: 30 loss: 2.638684496525829 grad: -0.7811346876021564
iteration: 40 loss: 2.0211574473718925 grad: -0.6526262659487704
iteration: 50 loss: 1.6385252556630638 grad: -0.5565043442719844
iteration: 60 loss: 1.3780878896781028 grad: -0.484015000877857
iteration: 70 loss: 1.189325798274033 grad: -0.42791560851343124
iteration: 80 loss: 1.0462047200487425 grad: -0.38337853999824706
iteration: 90 loss: 0.9339417030705177 grad: -0.34722528090299953
iteration: 100 loss: 0.8435183816317146 grad: -0.31731748698021894
iteration: 110 loss: 0.7691201066062686 grad: -0.29217624836432665
iteration: 120 loss: 0.7068290393331154 grad: -0.270750738768367
iteration: 130 loss: 0.6539088345139962 grad: -0.25227569764181534
iteration: 0 loss: 51.887530726447906 grad: 277.414045816184
iteration: 10 loss: 6.8737855777908825 grad: -0.7296282869396535
iteration: 20 loss: 3.8204072573648773 grad: -0.9555178034130845
iteration: 30 loss: 2.6481635340391305 grad: -0.7874480120122662
iteration: 40 loss: 2.027441830237339 grad: -0.6522488014167345
iteration: 50 loss: 1.6430084165438694 grad: -0.5535489267980862
iteration: 60 loss: 1.3814575325367182 grad: -0.4800024266431577
iteration: 70 loss: 1.1919578228717413 grad: -0.4234927807430182
iteration: 80 loss: 1.0483218301876398 grad: -0.37884529720827265
iteration: 90 loss: 0.9356843900027343 grad: -0.3427272525664492
iteration: 100 loss: 0.8449797938080585 grad: -0.3129261529783321
iteration: 110 loss: 0.7703645370681277 grad: -0.28792540510656317
iteration: 120 loss: 0.7079023580200886 grad: -0.26665431565503317
iteration: 130 loss: 0.6548446864146833 grad: -0.2483369409355013
iteration: 0 loss: 53.89652478015019 grad: 263.76159673304755
iteration: 10 loss: 6.992516563373143 grad: -0.3559543191568397
iteration: 20 loss: 3.8806172862389925 grad: -0.7988243539301314
iteration: 30 loss: 2.6899180303439607 grad: -0.6910051272358546
iteration: 40 loss: 2.0598421349656992 grad: -0.5837160911169722
iteration: 50 loss: 1.6696552578282122 grad: -0.5009838986210406
iteration: 60 loss: 1.4041699712233822 grad: -0.4377097495405853
iteration: 70 loss: 1.2117931484984348 grad: -0.3883249958228531
iteration: 80 loss: 1.0659533299124797 grad: -0.34888662567580203
iteration: 90 loss: 0.9515693103379074 grad: -0.3167288716963714
iteration: 100 loss: 0.8594438851419827 grad: -0.2900310829553634
iteration: 110 loss: 0.7836486264924877 grad: -0.2675215236635772
iteration: 120 loss: 0.7201898285637237 grad: -0.24829010060178378
iteration: 130 loss: 0.6662785920068266 grad: -0.23167040804971523
iteration: 0 loss: 52.49503960028016 grad: 347.8823289980418
iteration: 10 loss: 6.946436488207093 grad: -1.530247988822671
iteration: 20 loss: 3.8549539186119546 grad: -1.2727492974903334
iteration: 30 loss: 2.669434699338787 grad: -0.9764510580765728
iteration: 40 loss: 2.0424053379104574 grad: -0.7838563656054425
iteration: 50 loss: 1.6543892286725745 grad: -0.6532715508868503
iteration: 60 loss: 1.3905634607217507 grad: -0.5596740574196337
iteration: 70 loss: 1.1995057714367994 grad: -0.4894930286095776
iteration: 80 loss: 1.0547429820242293 grad: -0.4349772527033312
iteration: 90 loss: 0.9412564103046696 grad: -0.3914280464602552
iteration: 100 loss: 0.8498910956612401 grad: -0.35584514712723475
iteration: 110 loss: 0.774748369801893 grad: -0.3262274776173979
iteration: 120 loss: 0.7118561685300103 grad: -0.30119086511477955
iteration: 130 loss: 0.6584417385412796 grad: -0.2797480136315697
iteration: 0 loss: 49.42569417281718 grad: 309.35494727652076
iteration: 10 loss: 6.881211243638683 grad: -0.6193144609154859
iteration: 20 loss: 3.81606324365744 grad: -0.9056368809919673
iteration: 30 loss: 2.643135337494269 grad: -0.7586780251356033
iteration: 40 loss: 2.0228973443711102 grad: -0.6334715666267552
iteration: 50 loss: 1.6390294132893772 grad: -0.5403322549070678
iteration: 60 loss: 1.3779676576529132 grad: -0.4702111013387814
iteration: 70 loss: 1.1888698348667608 grad: -0.41596678885383226
iteration: 80 loss: 1.0455624507291594 grad: -0.37289813038845265
iteration: 90 loss: 0.9331957870437112 grad: -0.33792567294538267
iteration: 100 loss: 0.8427168141820403 grad: -0.3089827358638031
iteration: 110 loss: 0.768291748660059 grad: -0.28464167644005633
iteration: 120 loss: 0.7059916685816874 grad: -0.26388866088464186
iteration: 130 loss: 0.6530735582559415 grad: -0.24598551976054633
iteration: 0 loss: 51.48123800806866 grad: 295.8633564103108
iteration: 10 loss: 6.652960150777248 grad: 0.026926913483495823
iteration: 20 loss: 3.732969170149731 grad: -0.7928036551520787
iteration: 30 loss: 2.5997887686373606 grad: -0.7236855592492395
iteration: 40 loss: 1.995747773837138 grad: -0.6200261509693228
iteration: 50 loss: 1.6200998697384585 grad: -0.5348686455386057
iteration: 60 loss: 1.3638165073950652 grad: -0.4682273108714217
iteration: 70 loss: 1.1777639027286981 grad: -0.4156546041657967
iteration: 80 loss: 1.036530129189776 grad: -0.3734342200876103
iteration: 90 loss: 0.9256477711179204 grad: -0.33890109879951447
iteration: 100 loss: 0.8362734630902857 grad: -0.310181479414383
iteration: 110 loss: 0.762696559050503 grad: -0.28594459638313274
iteration: 120 loss: 0.701064518273101 grad: -0.26522817033592705
iteration: 0 loss: 52.049872720653184 grad: 296.0260138036782
iteration: 10 loss: 6.894556769766578 grad: -1.0037406804673397
iteration: 20 loss: 3.834226129406571 grad: -1.0608700360798426
iteration: 30 loss: 2.658711814076841 grad: -0.8477392402930244
iteration: 40 loss: 2.035964115314734 grad: -0.6931992574338981
iteration: 50 loss: 1.6501576308559462 grad: -0.5840569519642236
iteration: 60 loss: 1.3876167810857292 grad: -0.5040805686874716
iteration: 70 loss: 1.197369947206756 grad: -0.44325818034212955
iteration: 80 loss: 1.053150097715595 grad: -0.3955384413347538
iteration: 90 loss: 0.9400437418670332 grad: -0.35713177242686256
iteration: 100 loss: 0.8489541815746303 grad: -0.32556598852719176
iteration: 110 loss: 0.7740171429809379 grad: -0.29916676179883467
iteration: 120 loss: 0.7112819285500998 grad: -0.27676244339288325
iteration: 130 loss: 0.6579896240559719 grad: -0.25750967446910245
iteration: 0 loss: 52.88241172299476 grad: 245.23746281528156
iteration: 10 loss: 6.799654254527261 grad: 0.7358816967869592
iteration: 20 loss: 3.792400591784785 grad: -0.5511641006062016
iteration: 30 loss: 2.6369874854267024 grad: -0.588966700811476
iteration: 40 loss: 2.0231031088944955 grad: -0.5298572517930202
iteration: 50 loss: 1.6418793319754017 grad: -0.46833463943615994
iteration: 60 loss: 1.381981145236213 grad: -0.4160657806896562
iteration: 70 loss: 1.1933806879292206 grad: -0.37304263012150696
iteration: 80 loss: 1.0502468694540683 grad: -0.3375750570433621
iteration: 90 loss: 0.9378889965669259 grad: -0.3080427666464669
iteration: 100 loss: 0.8473334101338217 grad: -0.2831606892438328
iteration: 110 loss: 0.7727880653507203 grad: -0.26195298856290544
iteration: 120 loss: 0.7103468122566234 grad: -0.24368311720719446
iteration: 130 loss: 0.6572793530293976 grad: -0.22779151661107777
iteration: 0 loss: 52.91856412045385 grad: 305.80365177507997
iteration: 10 loss: 6.856548175890442 grad: -1.0708005397678417
iteration: 20 loss: 3.805631777512066 grad: -1.0735628220446247
iteration: 30 loss: 2.636034619248221 grad: -0.8537942945989163
iteration: 40 loss: 2.0172795228678626 grad: -0.6974104367247718
iteration: 50 loss: 1.6342941203250698 grad: -0.5874866766154172
iteration: 60 loss: 1.3738390057712258 grad: -0.5070694007009404
iteration: 70 loss: 1.1851929576637716 grad: -0.44594788928282547
iteration: 80 loss: 1.0422391650977674 grad: -0.3980014403511257
iteration: 90 loss: 0.9301587560517791 grad: -0.359411417478562
iteration: 100 loss: 0.8399172948785153 grad: -0.3276914584566002
iteration: 110 loss: 0.7656930483770504 grad: -0.3011593690022442
iteration: 120 loss: 0.7035653315840797 grad: -0.2786386484433483
iteration: 0 loss: 50.10764237864214 grad: 279.3467548195124
iteration: 10 loss: 6.958981273725096 grad: -0.22848757909440537
iteration: 20 loss: 3.8671529585013475 grad: -0.7578246977369123
iteration: 30 loss: 2.6825234941980356 grad: -0.673937818223788
iteration: 40 loss: 2.055002309087327 grad: -0.5754613914153146
iteration: 50 loss: 1.6661251231886043 grad: -0.4966532129282458
iteration: 60 loss: 1.4014052133821182 grad: -0.4353499154347006
iteration: 70 loss: 1.2095192889031006 grad: -0.3870374480444058
iteration: 80 loss: 1.064016896973912 grad: -0.34821506140362646
iteration: 90 loss: 0.9498774969036442 grad: -0.3164242359553753
iteration: 100 loss: 0.8579370069262289 grad: -0.28994985166481596
iteration: 110 loss: 0.7822863492321879 grad: -0.26757747394008424
iteration: 120 loss: 0.7189437605678161 grad: -0.24842974743391139
iteration: 130 loss: 0.6651280560816055 grad: -0.23185975447614884
iteration: 0 loss: 50.89918508353671 grad: 294.2428268687565
iteration: 10 loss: 6.802996649927784 grad: -1.2957049905471123
iteration: 20 loss: 3.798308466198333 grad: -1.2297368558095094
iteration: 30 loss: 2.6367026318765223 grad: -0.9612555518554682
iteration: 40 loss: 2.019910963132588 grad: -0.7773858862683911
iteration: 50 loss: 1.637398985074097 grad: -0.6504212833973398
iteration: 60 loss: 1.3769620885753036 grad: -0.5585668374543036
iteration: 70 loss: 1.1881850031343013 grad: -0.48930390767548465
iteration: 80 loss: 1.045055828980532 grad: -0.43529923833100864
iteration: 90 loss: 0.9327945050704742 grad: -0.39204343139566483
iteration: 100 loss: 0.8423810583651423 grad: -0.35663041105031057
iteration: 110 loss: 0.7679985967886502 grad: -0.3271094096928946
iteration: 120 loss: 0.7057273507825399 grad: -0.30212464870231115
iteration: 130 loss: 0.652829505309662 grad: -0.2807055734759764
iteration: 0 loss: 49.232505090416865 grad: 377.13368708666275
iteration: 10 loss: 6.5947766038351885 grad: -2.2044459790879247
iteration: 20 loss: 3.727395196497913 grad: -1.47556607568021
iteration: 30 loss: 2.5988848863981646 grad: -1.0974264517068981
iteration: 40 loss: 1.9953140273183139 grad: -0.8682830375522633
iteration: 50 loss: 1.6195428778742924 grad: -0.7172545245391652
iteration: 60 loss: 1.3630792192613048 grad: -0.6107224684224876
iteration: 70 loss: 1.176880360206641 grad: -0.5316858899536573
iteration: 80 loss: 1.0355422819831637 grad: -0.47076255100174946
iteration: 90 loss: 0.9245899935119517 grad: -0.42238275193164776
iteration: 100 loss: 0.8351714018132373 grad: -0.3830406243412793
iteration: 110 loss: 0.7615687843105495 grad: -0.3504224536765101
iteration: 120 loss: 0.6999243418941163 grad: -0.3229408980035583
iteration: 0 loss: 55.45150807837796 grad: 260.2303463176297
iteration: 10 loss: 6.966948126905084 grad: -0.2654346731464472
iteration: 20 loss: 3.875486483464721 grad: -0.7880918966263215
iteration: 30 loss: 2.6902488333722347 grad: -0.6948717231650392
iteration: 40 loss: 2.0618770114823417 grad: -0.5913405313534694
iteration: 50 loss: 1.6722330457125116 grad: -0.5094018700505285
iteration: 60 loss: 1.4068709970649913 grad: -0.4459709857998988
iteration: 70 loss: 1.2144531446157834 grad: -0.3961200725782815
iteration: 80 loss: 1.0685076859068763 grad: -0.3561350724564908
iteration: 90 loss: 0.9539955045808959 grad: -0.3234360253146164
iteration: 100 loss: 0.8617379354005937 grad: -0.29623348866011145
iteration: 110 loss: 0.7858147405201056 grad: -0.2732649759148318
iteration: 120 loss: 0.722235630455378 grad: -0.25362069243232227
iteration: 130 loss: 0.6682128598282279 grad: -0.23663104017642994
iteration: 0 loss: 51.4129862308728 grad: 250.75929134493464
iteration: 10 loss: 6.851019773806869 grad: -0.2003747555071345
iteration: 20 loss: 3.81254894564931 grad: -0.7424422904540275
iteration: 30 loss: 2.6466704896597135 grad: -0.6657688900866906
iteration: 40 loss: 2.0284848237681463 grad: -0.5708047904548881
iteration: 50 loss: 1.6451457115443768 grad: -0.49386975570180014
iteration: 60 loss: 1.3840765191904127 grad: -0.43365453123781955
iteration: 70 loss: 1.1947724475558061 grad: -0.38601701618421613
iteration: 80 loss: 1.051189900457013 grad: -0.34763468578919654
iteration: 90 loss: 0.9385327022702602 grad: -0.31614189258775544
iteration: 100 loss: 0.8477703799702051 grad: -0.2898753534917543
iteration: 110 loss: 0.7730782719072935 grad: -0.2676512343950752
iteration: 120 loss: 0.7105305190432939 grad: -0.24861109690524671
iteration: 130 loss: 0.6573844036184627 grad: -0.23212017589607176
iteration: 0 loss: 49.903001498960045 grad: 266.4594717929595
iteration: 10 loss: 6.955868057172004 grad: 0.14884799564862866
iteration: 20 loss: 3.8549200724638384 grad: -0.593068004448209
iteration: 30 loss: 2.6719323899099603 grad: -0.5753210520848735
iteration: 40 loss: 2.0462163286616817 grad: -0.5070328930246657
iteration: 50 loss: 1.6587343627320892 grad: -0.44506765806318904
iteration: 60 loss: 1.3950695133478968 grad: -0.39435315050971087
iteration: 70 loss: 1.2039950939759942 grad: -0.35324648419930343
iteration: 80 loss: 1.0591310199004262 grad: -0.3196119973929651
iteration: 90 loss: 0.9455044844462877 grad: -0.2917168390446657
iteration: 100 loss: 0.8539838624365902 grad: -0.26826475337283634
iteration: 110 loss: 0.7786825756269206 grad: -0.24829887944858656
iteration: 120 loss: 0.7156348184995626 grad: -0.23110840870972482
iteration: 130 loss: 0.6620709805197293 grad: -0.21615859792586473
iteration: 0 loss: 53.37970797246428 grad: 298.52503881218877
iteration: 10 loss: 6.7725966264294835 grad: -1.5914196774535165
iteration: 20 loss: 3.790612241389557 grad: -1.3459000230976994
iteration: 30 loss: 2.6335978877397626 grad: -1.026437659686117
iteration: 40 loss: 2.0183808318482246 grad: -0.82040075175138
iteration: 50 loss: 1.6365596987052413 grad: -0.6815182471938307
iteration: 60 loss: 1.3764733262096756 grad: -0.5824088260416199
iteration: 70 loss: 1.187892057803517 grad: -0.5083492287129987
iteration: 80 loss: 1.0448799708972893 grad: -0.45098014747085646
iteration: 90 loss: 0.9326922444105218 grad: -0.4052573168288326
iteration: 100 loss: 0.8423268275491864 grad: -0.36797156862560243
iteration: 110 loss: 0.767976612828575 grad: -0.33698898155365603
iteration: 120 loss: 0.7057274819928187 grad: -0.31083729689674733
iteration: 130 loss: 0.6528450369758254 grad: -0.2884687315008312
iteration: 0 loss: 52.55041601210376 grad: 308.235726299981
iteration: 10 loss: 6.857814673672403 grad: -0.6652823088677164
iteration: 20 loss: 3.802046187280873 grad: -0.938882109531911
iteration: 30 loss: 2.6335707829244215 grad: -0.779642616584404
iteration: 40 loss: 2.0157075379680793 grad: -0.6483485498622195
iteration: 50 loss: 1.6332992791446734 grad: -0.55173542002528
iteration: 60 loss: 1.3732206794623958 grad: -0.47940660772576144
iteration: 70 loss: 1.184827792046814 grad: -0.42364846825692126
iteration: 80 loss: 1.042049312129455 grad: -0.37948259091858216
iteration: 90 loss: 0.9300933771630132 grad: -0.34368086176303325
iteration: 100 loss: 0.8399421026847449 grad: -0.31409038878906825
iteration: 110 loss: 0.7657842540940113 grad: -0.2892303603339104
iteration: 120 loss: 0.7037060228334749 grad: -0.2680524891210425
iteration: 0 loss: 52.333133584240095 grad: 335.1524065742588
iteration: 10 loss: 6.853289241670857 grad: -1.6103146226884937
iteration: 20 loss: 3.8164772933844024 grad: -1.3172413236519032
iteration: 30 loss: 2.646571171478172 grad: -1.0063785912249443
iteration: 40 loss: 2.026363529908477 grad: -0.8060139939006704
iteration: 50 loss: 1.6420760793595368 grad: -0.6706036831996239
iteration: 60 loss: 1.3805788737825175 grad: -0.5737542739773218
iteration: 70 loss: 1.1911077878706815 grad: -0.5012551307533114
iteration: 80 loss: 1.047493946440599 grad: -0.44501596011364886
iteration: 90 loss: 0.934877442207587 grad: -0.40014275822831774
iteration: 100 loss: 0.8441938592902095 grad: -0.3635155301851297
iteration: 110 loss: 0.7695997996180598 grad: -0.3330561036089665
iteration: 120 loss: 0.7071587971846302 grad: -0.3073286333999678
iteration: 130 loss: 0.654122042823019 grad: -0.28531001471341577
iteration: 0 loss: 51.74252404963665 grad: 333.26131476421955
iteration: 10 loss: 6.64747979078406 grad: -1.962609874991005
iteration: 20 loss: 3.729687875694708 grad: -1.4102276091804482
iteration: 30 loss: 2.5918833701843225 grad: -1.0664491341529991
iteration: 40 loss: 1.9861877266300987 grad: -0.8498221852331337
iteration: 50 loss: 1.610162875331298 grad: -0.7047442309640545
iteration: 60 loss: 1.354017333164517 grad: -0.6015244260785405
iteration: 70 loss: 1.1683071166214425 grad: -0.5245349740044458
iteration: 80 loss: 1.0274883619742716 grad: -0.4649747049849797
iteration: 90 loss: 0.9170353872256617 grad: -0.4175544245725237
iteration: 100 loss: 0.8280789177447745 grad: -0.3789174549034623
iteration: 110 loss: 0.7548973059782508 grad: -0.34683556365479995
iteration: 120 loss: 0.6936340877327893 grad: -0.3197733675119312
iteration: 0 loss: 51.57966602684916 grad: 314.9165400511851
iteration: 10 loss: 6.717775161152283 grad: -1.5471141048853547
iteration: 20 loss: 3.7427485944619794 grad: -1.2921705503013539
iteration: 30 loss: 2.5950738627248855 grad: -0.997711091368271
iteration: 40 loss: 1.9866521897299043 grad: -0.8025040546143662
iteration: 50 loss: 1.609701299785849 grad: -0.6692813505739623
iteration: 60 loss: 1.353221287667111 grad: -0.5735083923632144
iteration: 70 loss: 1.167402574072102 grad: -0.5015846684341327
iteration: 80 loss: 1.0265690758833443 grad: -0.44566746489659614
iteration: 90 loss: 0.9161413740577534 grad: -0.40097766573213456
iteration: 100 loss: 0.8272266041419905 grad: -0.36445379685202217
iteration: 110 loss: 0.7540922636196477 grad: -0.33404961587954984
iteration: 120 loss: 0.6928767968576143 grad: -0.3083476414655192
iteration: 0 loss: 51.418753122200954 grad: 295.3697910114195
iteration: 10 loss: 6.754804927113417 grad: -0.9010125508657831
iteration: 20 loss: 3.760178636180957 grad: -1.0659009628002571
iteration: 30 loss: 2.608812643426063 grad: -0.8621385263444822
iteration: 40 loss: 1.998393976007099 grad: -0.7076248892519357
iteration: 50 loss: 1.6200388867951419 grad: -0.5971532634841378
iteration: 60 loss: 1.362481429391871 grad: -0.5157744647074434
iteration: 70 loss: 1.1758000395623391 grad: -0.4537137818332713
iteration: 80 loss: 1.0342569772805374 grad: -0.40494313756754796
iteration: 90 loss: 0.9232340712350875 grad: -0.3656503608922109
iteration: 100 loss: 0.8338122471645875 grad: -0.3333344701852661
iteration: 110 loss: 0.7602404279823531 grad: -0.30629554842555495
iteration: 120 loss: 0.698643448137194 grad: -0.2833411334130688
iteration: 0 loss: 58.71722585055622 grad: 342.7576214030392
iteration: 10 loss: 6.673233590625841 grad: -2.0288685619139626
iteration: 20 loss: 3.753613305077744 grad: -1.47978925454819
iteration: 30 loss: 2.612067732794706 grad: -1.1008045057815097
iteration: 40 loss: 2.0034076546154127 grad: -0.8705554941900857
iteration: 50 loss: 1.625124939292205 grad: -0.7188565340117534
iteration: 60 loss: 1.3672306652254684 grad: -0.6119198861406082
iteration: 70 loss: 1.1801340458226728 grad: -0.532625639037593
iteration: 80 loss: 1.0381918702959079 grad: -0.4715288299539562
iteration: 90 loss: 0.9268111125925316 grad: -0.4230264333632067
iteration: 100 loss: 0.8370758698408942 grad: -0.38359403711243223
iteration: 110 loss: 0.7632314029198245 grad: -0.3509070807591555
iteration: 120 loss: 0.7013971969597399 grad: -0.32337157932322924
iteration: 0 loss: 49.306304538129204 grad: 312.0075089822848
iteration: 10 loss: 6.9064279137695515 grad: -0.3669400296974087
iteration: 20 loss: 3.823742440632238 grad: -0.7904341577045957
iteration: 30 loss: 2.647914411182104 grad: -0.6906072449717677
iteration: 40 loss: 2.02662704465547 grad: -0.5871955758077768
iteration: 50 loss: 1.6421950793018865 grad: -0.5061239142706835
iteration: 60 loss: 1.3807602385428999 grad: -0.4434996230682414
iteration: 70 loss: 1.1913866260762338 grad: -0.3942923945386717
iteration: 80 loss: 1.0478620523984896 grad: -0.3548051098302297
iteration: 90 loss: 0.9353175252003066 grad: -0.32249008122314504
iteration: 100 loss: 0.844689027530256 grad: -0.2955858360280581
iteration: 110 loss: 0.7701358730658451 grad: -0.2728511772897511
iteration: 120 loss: 0.7077244871194869 grad: -0.2533919798222536
iteration: 130 loss: 0.6547085516034382 grad: -0.2365501231991708
iteration: 0 loss: 52.66033479381349 grad: 292.04053441155975
iteration: 10 loss: 6.6687956562829065 grad: -1.614006674330685
iteration: 20 loss: 3.737164036231496 grad: -1.317035277280771
iteration: 30 loss: 2.5977436172491832 grad: -1.004296641071559
iteration: 40 loss: 1.9914973257513835 grad: -0.8038626182628014
iteration: 50 loss: 1.6150998944608959 grad: -0.6686842840230449
iteration: 60 loss: 1.3586390867806337 grad: -0.5720831127208494
iteration: 70 loss: 1.1726485921480658 grad: -0.4997971683118261
iteration: 80 loss: 1.0315782074027415 grad: -0.44373285006308866
iteration: 90 loss: 0.9208987524189483 grad: -0.39900219767534173
iteration: 100 loss: 0.8317380503227386 grad: -0.3624919290564803
iteration: 110 loss: 0.7583718230356763 grad: -0.33212945971087876
iteration: 120 loss: 0.6969412372484178 grad: -0.3064832699976112
iteration: 0 loss: 51.55548248761567 grad: 292.3363459035811
iteration: 10 loss: 6.844903665719027 grad: -0.7027264382978122
iteration: 20 loss: 3.8086840158353796 grad: -0.9218051955247184
iteration: 30 loss: 2.6420158518011374 grad: -0.7642516603156789
iteration: 40 loss: 2.0236975534458823 grad: -0.6356566831326907
iteration: 50 loss: 1.6405046989920762 grad: -0.5410388731756619
iteration: 60 loss: 1.3796741643560297 grad: -0.4701620421364525
iteration: 70 loss: 1.190628513202889 grad: -0.4154952047585092
iteration: 80 loss: 1.0472965138582864 grad: -0.3721776832366641
iteration: 90 loss: 0.9348722020445166 grad: -0.33705472430216865
iteration: 100 loss: 0.8443225659965389 grad: -0.30802037806415233
iteration: 110 loss: 0.7698233422223976 grad: -0.28362497257244806
iteration: 120 loss: 0.707450184451938 grad: -0.2628416095704601
iteration: 130 loss: 0.6544622174798694 grad: -0.24492402694027254
iteration: 0 loss: 48.34915668511628 grad: 287.10551987797385
iteration: 10 loss: 6.792987564414819 grad: -0.40679068921915473
iteration: 20 loss: 3.783577518045149 grad: -0.939128951715021
iteration: 30 loss: 2.6255673126763877 grad: -0.8011713760315451
iteration: 40 loss: 2.0112458577589845 grad: -0.6697270097397953
iteration: 50 loss: 1.6303732549432342 grad: -0.5702734023211915
iteration: 60 loss: 1.3710783680012768 grad: -0.49516760156729206
iteration: 70 loss: 1.1831350783083756 grad: -0.4371010416972566
iteration: 80 loss: 1.0406385597188175 grad: -0.3910759659466523
iteration: 90 loss: 0.9288723757887799 grad: -0.3537777855492012
iteration: 100 loss: 0.838856306384266 grad: -0.3229724053975679
iteration: 110 loss: 0.7647994232243449 grad: -0.29711488034738986
iteration: 120 loss: 0.7027995697525594 grad: -0.2751084906605097
iteration: 0 loss: 51.92743416932728 grad: 343.9588381705058
iteration: 10 loss: 6.9459917425696895 grad: -1.9602304063040168
iteration: 20 loss: 3.8611982207198294 grad: -1.4455721250205165
iteration: 30 loss: 2.672259681546083 grad: -1.07588626355661
iteration: 40 loss: 2.043100892654655 grad: -0.8502975034511835
iteration: 50 loss: 1.6538959642340076 grad: -0.7016398157913809
iteration: 60 loss: 1.3893981480835216 grad: -0.5968984322940771
iteration: 70 loss: 1.1979553668175371 grad: -0.519280479899012
iteration: 80 loss: 1.0529730037739518 grad: -0.4595122462529171
iteration: 90 loss: 0.9393658211709159 grad: -0.41209173322690407
iteration: 100 loss: 0.847940793366268 grad: -0.37355890093776245
iteration: 110 loss: 0.7727766230437655 grad: -0.34163250721383964
iteration: 120 loss: 0.7098873420133548 grad: -0.3147489438300764
iteration: 130 loss: 0.656491415832327 grad: -0.2918012332191417
iteration: 0 loss: 50.900435693991874 grad: 291.78475414391835
iteration: 10 loss: 6.808577952709501 grad: -1.2075678910709917
iteration: 20 loss: 3.8066960535254775 grad: -1.1679343468610426
iteration: 30 loss: 2.6433282686848543 grad: -0.9116299473884867
iteration: 40 loss: 2.0251633799093725 grad: -0.7362357094453897
iteration: 50 loss: 1.641685440693064 grad: -0.615475856603741
iteration: 60 loss: 1.3805538004391813 grad: -0.5282942127295607
iteration: 70 loss: 1.1912600517935474 grad: -0.46265040374146527
iteration: 80 loss: 1.047734660604502 grad: -0.41151991542710875
iteration: 90 loss: 0.9351614125705716 grad: -0.3705963365206224
iteration: 100 loss: 0.8444969144347851 grad: -0.3371107324024759
iteration: 110 loss: 0.7699085856367922 grad: -0.3092076342487864
iteration: 120 loss: 0.7074658183650254 grad: -0.28559925015926935
iteration: 130 loss: 0.6544231019299993 grad: -0.26536469172863064
iteration: 0 loss: 54.02357337191788 grad: 278.89732460624236
iteration: 10 loss: 6.940210528549704 grad: -0.5201038831578745
iteration: 20 loss: 3.858857775151178 grad: -0.8919019829471372
iteration: 30 loss: 2.6761753599673948 grad: -0.7537485953498493
iteration: 40 loss: 2.049657648369248 grad: -0.6310805941943507
iteration: 50 loss: 1.661478341464565 grad: -0.5390266078140188
iteration: 60 loss: 1.3972899138619719 grad: -0.46947102513604777
iteration: 70 loss: 1.2058257232469756 grad: -0.41555645214459996
iteration: 80 loss: 1.0606667576204762 grad: -0.37269402111594685
iteration: 90 loss: 0.9468124860547339 grad: -0.33785673136074296
iteration: 100 loss: 0.8551124579045625 grad: -0.3090052959440708
iteration: 110 loss: 0.779667272873832 grad: -0.28472756876050537
iteration: 120 loss: 0.7165022443084375 grad: -0.2640190231621458
iteration: 130 loss: 0.6628414842997782 grad: -0.24614734589940568
iteration: 0 loss: 51.810410206903434 grad: 241.11290419462694
iteration: 10 loss: 6.732939331250319 grad: 0.1330893292680023
iteration: 20 loss: 3.776208836718483 grad: -0.8088378808643258
iteration: 30 loss: 2.628634217457933 grad: -0.7401159853975096
iteration: 40 loss: 2.0171613392228043 grad: -0.6327792685565067
iteration: 50 loss: 1.6370514710578887 grad: -0.5448403886840134
iteration: 60 loss: 1.3778151207872147 grad: -0.4762538519603241
iteration: 70 loss: 1.1896720575925264 grad: -0.4222930802770942
iteration: 80 loss: 1.0468840924107263 grad: -0.3790500466801283
iteration: 90 loss: 0.9348025381218164 grad: -0.34374035181162876
iteration: 100 loss: 0.8444756917355004 grad: -0.3144153065182631
iteration: 110 loss: 0.7701243498583856 grad: -0.28969571881462985
iteration: 120 loss: 0.7078505488365078 grad: -0.2685870045043418
iteration: 130 loss: 0.6549295628429221 grad: -0.2503574895335847
iteration: 0 loss: 53.66068053678232 grad: 323.72102063247524
iteration: 10 loss: 6.790909551080362 grad: -1.1587458936310904
iteration: 20 loss: 3.7797878311559376 grad: -1.1939132900000609
iteration: 30 loss: 2.621611567504839 grad: -0.9424605353270801
iteration: 40 loss: 2.007586607836319 grad: -0.7644257099052687
iteration: 50 loss: 1.6270619593617746 grad: -0.640487621988123
iteration: 60 loss: 1.3680831997603078 grad: -0.5505111467030837
iteration: 70 loss: 1.1804117739635662 grad: -0.48253336822413667
iteration: 80 loss: 1.0381464895763615 grad: -0.4294647814883769
iteration: 90 loss: 0.9265774486452756 grad: -0.3869212872099001
iteration: 100 loss: 0.8367306050492764 grad: -0.3520682866874441
iteration: 110 loss: 0.7628201631435497 grad: -0.3229989141660622
iteration: 120 loss: 0.7009480715743682 grad: -0.2983858944676707
iteration: 0 loss: 49.234812512395756 grad: 313.90785614347317
iteration: 10 loss: 6.841207181081203 grad: -1.024122321873047
iteration: 20 loss: 3.8083519665552887 grad: -1.0931025062105328
iteration: 30 loss: 2.640735750913915 grad: -0.870083462749136
iteration: 40 loss: 2.0218855436045984 grad: -0.7093247004775077
iteration: 50 loss: 1.638469338964321 grad: -0.5963274044833953
iteration: 60 loss: 1.377571173768088 grad: -0.5138077465684522
iteration: 70 loss: 1.1885344606363022 grad: -0.4512113652901341
iteration: 80 loss: 1.0452487776472232 grad: -0.40219805965503597
iteration: 90 loss: 0.9328883084188585 grad: -0.3628143122193389
iteration: 100 loss: 0.8424097172563217 grad: -0.3304892597451212
iteration: 110 loss: 0.7679832842102521 grad: -0.3034861414213716
iteration: 120 loss: 0.7056817800326743 grad: -0.28059212365436964
iteration: 130 loss: 0.6527628454134422 grad: -0.26093570858846404
iteration: 0 loss: 49.48111775961795 grad: 306.3071335442868
iteration: 10 loss: 6.790886469054427 grad: -0.6939949798085543
iteration: 20 loss: 3.7796609729937036 grad: -0.9838466246853483
iteration: 30 loss: 2.622313561789848 grad: -0.818231841202781
iteration: 40 loss: 2.008794392855097 grad: -0.6796412193457366
iteration: 50 loss: 1.6285245122086385 grad: -0.5773825822787856
iteration: 60 loss: 1.3696611127645404 grad: -0.5008441475612375
iteration: 70 loss: 1.1820297524708705 grad: -0.4419063441074388
iteration: 80 loss: 1.0397635186539198 grad: -0.39528588538218296
iteration: 90 loss: 0.9281711037873389 grad: -0.3575476125998792
iteration: 100 loss: 0.8382888218264436 grad: -0.32639895264939767
iteration: 110 loss: 0.7643367963886373 grad: -0.3002633331189667
iteration: 120 loss: 0.7024204205154919 grad: -0.278025375202822
iteration: 0 loss: 56.75643773041084 grad: 322.98743857721735
iteration: 10 loss: 6.581481169325798 grad: -0.38997114953951406
iteration: 20 loss: 3.705073994709396 grad: -0.9448489195385422
iteration: 30 loss: 2.5845164683075215 grad: -0.8078622479465624
iteration: 40 loss: 1.9859807401367933 grad: -0.6777323058797785
iteration: 50 loss: 1.613241593778391 grad: -0.5786437100931858
iteration: 60 loss: 1.3586885121027412 grad: -0.5034121337549504
iteration: 70 loss: 1.173752696887153 grad: -0.44501030031567207
iteration: 80 loss: 1.0332842209403763 grad: -0.3985747427930231
iteration: 90 loss: 0.922950916080605 grad: -0.36085256252987985
iteration: 100 loss: 0.8339850814455861 grad: -0.32963722522514644
iteration: 110 loss: 0.7607211461202248 grad: -0.3033952147297638
iteration: 120 loss: 0.6993348131120662 grad: -0.2810335574285583
iteration: 0 loss: 51.75675424977945 grad: 289.13464912071174
iteration: 10 loss: 6.751596369689403 grad: -0.929781139705758
iteration: 20 loss: 3.7714552499183314 grad: -1.0810207130140266
iteration: 30 loss: 2.6207102605191555 grad: -0.8777648096328365
iteration: 40 loss: 2.0092563272987305 grad: -0.7223654083773398
iteration: 50 loss: 1.6297407058498885 grad: -0.6106355260771024
iteration: 60 loss: 1.371156842716325 grad: -0.5280220372350779
iteration: 70 loss: 1.183609556432409 grad: -0.46485646006670944
iteration: 80 loss: 1.041341097334432 grad: -0.41512470417633707
iteration: 90 loss: 0.9297075353516365 grad: -0.3750017003169725
iteration: 100 loss: 0.8397672446629837 grad: -0.34196770903814866
iteration: 110 loss: 0.7657510877476686 grad: -0.31430477532508483
iteration: 120 loss: 0.7037697963378791 grad: -0.2908048892744098
iteration: 0 loss: 55.616466282220905 grad: 303.98857467642847
iteration: 10 loss: 6.7260737649973885 grad: -1.2140175486491451
iteration: 20 loss: 3.7642571995378686 grad: -1.218029693029989
iteration: 30 loss: 2.6168805399666697 grad: -0.9579028869414323
iteration: 40 loss: 2.0065545407275582 grad: -0.776090363903933
iteration: 50 loss: 1.6275695654214608 grad: -0.6497528890652526
iteration: 60 loss: 1.3692948191077117 grad: -0.5580987760525936
iteration: 70 loss: 1.1819544581183188 grad: -0.4888907273398089
iteration: 80 loss: 1.0398378164203346 grad: -0.4348891003490669
iteration: 90 loss: 0.92832270565135 grad: -0.3916189216466653
iteration: 100 loss: 0.8384788262394316 grad: -0.35618740606031174
iteration: 110 loss: 0.7645435307654225 grad: -0.32664888946777293
iteration: 120 loss: 0.7026315652755412 grad: -0.3016493674253282
iteration: 0 loss: 52.73291600486957 grad: 342.7091856268491
iteration: 10 loss: 6.654202968054009 grad: -1.0730335244138276
iteration: 20 loss: 3.725971207559806 grad: -1.1541230439239365
iteration: 30 loss: 2.589306366208058 grad: -0.9150357506398126
iteration: 40 loss: 1.984822220372404 grad: -0.7440254619194886
iteration: 50 loss: 1.6096149481195425 grad: -0.6243303898116896
iteration: 60 loss: 1.3540019302164514 grad: -0.5371748565581641
iteration: 70 loss: 1.168642005992462 grad: -0.4712059059871073
iteration: 80 loss: 1.028057081955963 grad: -0.41963974650536806
iteration: 90 loss: 0.9177618985421016 grad: -0.3782616371229837
iteration: 100 loss: 0.8289123565031828 grad: -0.34433834709689654
iteration: 110 loss: 0.755802879942373 grad: -0.31602742414663987
iteration: 120 loss: 0.6945875340612392 grad: -0.2920445045826461
iteration: 0 loss: 49.716396810059116 grad: 283.7638549945307
iteration: 10 loss: 6.810445311840133 grad: -0.1667002120757814
iteration: 20 loss: 3.786017547209791 grad: -0.8334547295605598
iteration: 30 loss: 2.626927220965709 grad: -0.7411216296296674
iteration: 40 loss: 2.01264220358681 grad: -0.6309845392950015
iteration: 50 loss: 1.6318602376644444 grad: -0.5431773080422269
iteration: 60 loss: 1.3726087114418988 grad: -0.4751380447917651
iteration: 70 loss: 1.1846685863986914 grad: -0.42168913678405795
iteration: 80 loss: 1.0421500936608523 grad: -0.37885249617029626
iteration: 90 loss: 0.9303477066075009 grad: -0.34385145415914814
iteration: 100 loss: 0.840288056148314 grad: -0.31475778855514425
iteration: 110 loss: 0.7661843456567112 grad: -0.29021111388673315
iteration: 120 loss: 0.7041368672283336 grad: -0.26923155785280833
iteration: 0 loss: 52.27979310037819 grad: 306.8754703747112
iteration: 10 loss: 7.050408296962939 grad: -0.7814201118282839
iteration: 20 loss: 3.9009921390134794 grad: -0.9099704442875729
iteration: 30 loss: 2.6995408857771754 grad: -0.746424373765995
iteration: 40 loss: 2.0650941999918113 grad: -0.6191595897336737
iteration: 50 loss: 1.6727399034333426 grad: -0.52663495479753
iteration: 60 loss: 1.4060411521777496 grad: -0.457654115010176
iteration: 70 loss: 1.2129277583515596 grad: -0.4045656631258371
iteration: 80 loss: 1.0666145796058117 grad: -0.3625422906318082
iteration: 90 loss: 0.9519133473354899 grad: -0.32848386395909146
iteration: 100 loss: 0.8595685678527926 grad: -0.3003329478054205
iteration: 110 loss: 0.7836179796633835 grad: -0.27667826540773965
iteration: 120 loss: 0.7200472128947282 grad: -0.25652237024275903
iteration: 130 loss: 0.6660542140354085 grad: -0.23914147840665007
iteration: 0 loss: 51.48136059067695 grad: 289.32485743866596
iteration: 10 loss: 6.819579285049961 grad: -0.9074534247641347
iteration: 20 loss: 3.8041120724938082 grad: -1.0951842118490072
iteration: 30 loss: 2.6410324618015526 grad: -0.8849742586364149
iteration: 40 loss: 2.023650597268732 grad: -0.7256154959098047
iteration: 50 loss: 1.6407446760651345 grad: -0.6118309888465748
iteration: 60 loss: 1.3799978072126413 grad: -0.5280987949741809
iteration: 70 loss: 1.190963623398354 grad: -0.4642960964055304
iteration: 80 loss: 1.0476163768040683 grad: -0.4141908362862999
iteration: 90 loss: 0.935167589316982 grad: -0.3738464761792325
iteration: 100 loss: 0.8445913546810185 grad: -0.34068271950473755
iteration: 110 loss: 0.7700663223529525 grad: -0.3129469827570094
iteration: 120 loss: 0.7076692686584299 grad: -0.289410595233739
iteration: 130 loss: 0.6546596540890526 grad: -0.26918807424750535
iteration: 0 loss: 51.27379561890966 grad: 344.65691834962405
iteration: 10 loss: 6.816635076581199 grad: -1.2516656506280346
iteration: 20 loss: 3.8018930535706037 grad: -1.203818254784464
iteration: 30 loss: 2.638223655921138 grad: -0.9468439283635323
iteration: 40 loss: 2.0207225771801776 grad: -0.7678464282222375
iteration: 50 loss: 1.6378832637577199 grad: -0.6433924347409797
iteration: 60 loss: 1.3772666143290735 grad: -0.5530356377758946
iteration: 70 loss: 1.1883793996589702 grad: -0.48475929213286945
iteration: 80 loss: 1.045177636222971 grad: -0.43145111735775077
iteration: 90 loss: 0.9328659476546736 grad: -0.3887123316290054
iteration: 100 loss: 0.8424162219881013 grad: -0.3536978792240814
iteration: 110 loss: 0.7680068802362091 grad: -0.32449326975821335
iteration: 120 loss: 0.7057152814828359 grad: -0.2997656010624722
iteration: 130 loss: 0.6528017634141592 grad: -0.27855944376628305
iteration: 0 loss: 52.040280951149185 grad: 322.18183944469587
iteration: 10 loss: 6.853426464437317 grad: -0.8666142508895449
iteration: 20 loss: 3.8203004221457544 grad: -1.0758718001742753
iteration: 30 loss: 2.6519209069576224 grad: -0.8710052770736666
iteration: 40 loss: 2.031904602451422 grad: -0.7141853198208749
iteration: 50 loss: 1.6474071752579882 grad: -0.602058390876793
iteration: 60 loss: 1.385590480680755 grad: -0.519547618687976
iteration: 70 loss: 1.1957862980604081 grad: -0.45669577732711564
iteration: 80 loss: 1.05185772212607 grad: -0.40735489100911804
iteration: 90 loss: 0.9389542130935628 grad: -0.36763895350687104
iteration: 100 loss: 0.848012413746553 grad: -0.3350009189188713
iteration: 110 loss: 0.7731870050876172 grad: -0.30771129633998534
iteration: 120 loss: 0.7105386986764146 grad: -0.2845580481564729
iteration: 130 loss: 0.6573157783373063 grad: -0.26466800316417094
iteration: 0 loss: 52.63576315011855 grad: 256.8896604142718
iteration: 10 loss: 6.917515421226612 grad: 0.4504408182583308
iteration: 20 loss: 3.843414872792602 grad: -0.5310007483658352
iteration: 30 loss: 2.666920052630964 grad: -0.5399028060460286
iteration: 40 loss: 2.0436728264832955 grad: -0.4808533896646804
iteration: 50 loss: 1.657375865124736 grad: -0.4239612773428415
iteration: 60 loss: 1.3943606071044832 grad: -0.37660829507114635
iteration: 70 loss: 1.2036735335652586 grad: -0.3379383822181593
iteration: 80 loss: 1.0590539171378375 grad: -0.30616439317907135
iteration: 90 loss: 0.9455878739828553 grad: -0.2797398219128481
iteration: 100 loss: 0.8541755518241106 grad: -0.2574800443104813
iteration: 110 loss: 0.7789486893647005 grad: -0.23850019173039633
iteration: 120 loss: 0.7159526080909845 grad: -0.22213834385255177
iteration: 130 loss: 0.6624247471500812 grad: -0.2078942612372486
iteration: 0 loss: 51.025413188782274 grad: 246.73980895541808
iteration: 10 loss: 7.0493036293940925 grad: 0.12455153095645544
iteration: 20 loss: 3.8938542837508563 grad: -0.52541432555659
iteration: 30 loss: 2.694417089155567 grad: -0.5143266763993146
iteration: 40 loss: 2.0615290295547557 grad: -0.45723229756727213
iteration: 50 loss: 1.6702025746622398 grad: -0.4041712335671903
iteration: 60 loss: 1.4041926909902196 grad: -0.3601439574024538
iteration: 70 loss: 1.2115568231559914 grad: -0.3241041719369774
iteration: 80 loss: 1.0655854743248379 grad: -0.2943882124294151
iteration: 90 loss: 0.9511357946843915 grad: -0.2695883554884697
iteration: 100 loss: 0.8589804927354717 grad: -0.24862904037368067
iteration: 110 loss: 0.7831754456775261 grad: -0.23070522616594788
iteration: 120 loss: 0.7197183337504033 grad: -0.2152127109339375
iteration: 130 loss: 0.6658153423467493 grad: -0.2016932405672855
iteration: 0 loss: 53.354194170171404 grad: 317.75922562568996
iteration: 10 loss: 6.861571843362113 grad: -1.4718014604515806
iteration: 20 loss: 3.8166867131388824 grad: -1.262098204607505
iteration: 30 loss: 2.646831612383556 grad: -0.9757332071918242
iteration: 40 loss: 2.026948287640359 grad: -0.7862394180079113
iteration: 50 loss: 1.6428617089026871 grad: -0.6566802366029922
iteration: 60 loss: 1.3814683642844598 grad: -0.5633674057493523
iteration: 70 loss: 1.1920431606994581 grad: -0.49318053402079387
iteration: 80 loss: 1.048441915357423 grad: -0.4385418549170283
iteration: 90 loss: 0.9358190411782992 grad: -0.3948255521643146
iteration: 100 loss: 0.8451183968176834 grad: -0.35906359442851077
iteration: 110 loss: 0.7705014291331778 grad: -0.32926940296368806
iteration: 120 loss: 0.7080345499687362 grad: -0.3040651362616733
iteration: 130 loss: 0.6549706769902431 grad: -0.28246593942851533
iteration: 0 loss: 50.36370636707174 grad: 238.3930097448229
iteration: 10 loss: 6.920338402196773 grad: -0.22058512096952185
iteration: 20 loss: 3.847068393228793 grad: -0.6451486275553242
iteration: 30 loss: 2.667929433665408 grad: -0.5749501921889092
iteration: 40 loss: 2.043455505994271 grad: -0.49320786399737243
iteration: 50 loss: 1.6565844318035736 grad: -0.42754244812411757
iteration: 60 loss: 1.3932945313081704 grad: -0.3762242795326045
iteration: 70 loss: 1.2024799192250528 grad: -0.33560229564779354
iteration: 80 loss: 1.0578091949123518 grad: -0.30282954646838317
iteration: 90 loss: 0.9443332821905724 grad: -0.2758969526989855
iteration: 100 loss: 0.8529337651407332 grad: -0.2533967843467722
iteration: 110 loss: 0.777732142353257 grad: -0.23432830804989718
iteration: 120 loss: 0.7147678937813411 grad: -0.217965955475346
iteration: 130 loss: 0.6612750465324925 grad: -0.20377297935892527
iteration: 0 loss: 55.58711826858573 grad: 242.018058798057
iteration: 10 loss: 6.62909532199873 grad: 0.3796965843047197
iteration: 20 loss: 3.7122135040757795 grad: -0.7003225832185824
iteration: 30 loss: 2.5849594798568516 grad: -0.67274953752598
iteration: 40 loss: 1.9845914202916737 grad: -0.5862629738648928
iteration: 50 loss: 1.611296366668615 grad: -0.5103666982920521
iteration: 60 loss: 1.3566131416889469 grad: -0.4494098109835369
iteration: 70 loss: 1.1717054875467428 grad: -0.4006208590048391
iteration: 80 loss: 1.031324927302318 grad: -0.36106676326272213
iteration: 90 loss: 0.9210994126776975 grad: -0.3284944267639538
iteration: 100 loss: 0.832244319053478 grad: -0.30126562873733453
iteration: 110 loss: 0.7590868864807899 grad: -0.2781930320459941
iteration: 120 loss: 0.6977999777558662 grad: -0.25840614431064046
iteration: 0 loss: 52.51201065249928 grad: 246.48027913900418
iteration: 10 loss: 6.826960061918933 grad: 0.02831621831689183
iteration: 20 loss: 3.806639170444586 grad: -0.6901143501994726
iteration: 30 loss: 2.643409249073267 grad: -0.6356338989610959
iteration: 40 loss: 2.0259649037880494 grad: -0.5486256628695244
iteration: 50 loss: 1.6429578758055123 grad: -0.4759825688416516
iteration: 60 loss: 1.3820934203648159 grad: -0.4185487094820405
iteration: 70 loss: 1.1929406489462466 grad: -0.37289693295026355
iteration: 80 loss: 1.049480712834297 grad: -0.33601807633624803
iteration: 90 loss: 0.9369276432880311 grad: -0.3057095075703564
iteration: 100 loss: 0.8462560402894765 grad: -0.2804028416186577
iteration: 110 loss: 0.7716441971114033 grad: -0.25897397490375107
iteration: 120 loss: 0.7091682000684407 grad: -0.2406042685608113
iteration: 130 loss: 0.6560867276754668 grad: -0.22468665197020465
iteration: 0 loss: 53.939448907806124 grad: 331.87531373994057
iteration: 10 loss: 6.698367962771169 grad: -1.4010844381758334
iteration: 20 loss: 3.7504490217883855 grad: -1.2485361952826253
iteration: 30 loss: 2.6058844069619806 grad: -0.9672549926258414
iteration: 40 loss: 1.9972374985727996 grad: -0.7792267738450335
iteration: 50 loss: 1.6194807695779176 grad: -0.6504889558575344
iteration: 60 loss: 1.3621546258036688 grad: -0.5577811257036824
iteration: 70 loss: 1.1755686718457499 grad: -0.48808427035723323
iteration: 80 loss: 1.0340653111212637 grad: -0.43385739051833655
iteration: 90 loss: 0.923057855652711 grad: -0.3904936041413423
iteration: 100 loss: 0.8336406962544888 grad: -0.35503703834532374
iteration: 110 loss: 0.7600688509504917 grad: -0.325509855279376
iteration: 120 loss: 0.6984700069637753 grad: -0.3005408533692341
iteration: 0 loss: 51.84559726108296 grad: 275.9129807102781
iteration: 10 loss: 6.763213249786986 grad: -0.17794411584071418
iteration: 20 loss: 3.776470103168032 grad: -0.8995284223074325
iteration: 30 loss: 2.624779450746287 grad: -0.7914607521737433
iteration: 40 loss: 2.012683615134186 grad: -0.6689673088895959
iteration: 50 loss: 1.6326743758418987 grad: -0.5729035930989903
iteration: 60 loss: 1.3737050725752935 grad: -0.49919577405820587
iteration: 70 loss: 1.1858521282792522 grad: -0.4416896022107461
iteration: 80 loss: 1.0433371781462648 grad: -0.3958382438120412
iteration: 90 loss: 0.9315016041838465 grad: -0.35852593351247064
iteration: 100 loss: 0.8413932994866297 grad: -0.3276137772670937
iteration: 110 loss: 0.7672355569615402 grad: -0.3016050878335502
iteration: 120 loss: 0.7051335541395929 grad: -0.279428549993708
iteration: 0 loss: 55.21940873034778 grad: 329.90261306361333
iteration: 10 loss: 6.731364199016877 grad: -1.9152107933994116
iteration: 20 loss: 3.7735770682464715 grad: -1.4173778801721308
iteration: 30 loss: 2.6223206190317216 grad: -1.0695790926783528
iteration: 40 loss: 2.0097586966487806 grad: -0.8508444277411025
iteration: 50 loss: 1.6295058838635337 grad: -0.7047895036473885
iteration: 60 loss: 1.370470676065121 grad: -0.6011086779952135
iteration: 70 loss: 1.1826498561495513 grad: -0.5239008979687166
iteration: 80 loss: 1.0402167586254853 grad: -0.4642426122087675
iteration: 90 loss: 0.9284863315796098 grad: -0.4167865084141098
iteration: 100 loss: 0.8384922283194101 grad: -0.37814669079062807
iteration: 110 loss: 0.764450103836349 grad: -0.3460795301447148
iteration: 120 loss: 0.7024610969663793 grad: -0.31904122877216057
iteration: 0 loss: 54.792766615142455 grad: 329.7156725275392
iteration: 10 loss: 6.869876443516618 grad: -1.2035384120768404
iteration: 20 loss: 3.8393182430936164 grad: -1.1415242259393814
iteration: 30 loss: 2.66598618557468 grad: -0.8952331476233232
iteration: 40 loss: 2.042683255845821 grad: -0.7259125658398582
iteration: 50 loss: 1.6560329380433607 grad: -0.6085988414892508
iteration: 60 loss: 1.3927328973833772 grad: -0.5235117101629381
iteration: 70 loss: 1.201856827471891 grad: -0.45922401353334913
iteration: 80 loss: 1.0571227684604723 grad: -0.40901666043219376
iteration: 90 loss: 0.9435944821912844 grad: -0.36874671845605245
iteration: 100 loss: 0.8521553815506774 grad: -0.33573867069108176
iteration: 110 loss: 0.7769255939503471 grad: -0.30819344189985054
iteration: 120 loss: 0.7139425365183508 grad: -0.2848588725837578
iteration: 130 loss: 0.6604382731082048 grad: -0.2648374649021763
iteration: 0 loss: 52.17717644844815 grad: 357.7726954390239
iteration: 10 loss: 6.83543744939615 grad: -1.6440020452381523
iteration: 20 loss: 3.831133407914463 grad: -1.3433615966251267
iteration: 30 loss: 2.6625460213721124 grad: -1.0234130687708063
iteration: 40 loss: 2.0406815043463045 grad: -0.818203463462672
iteration: 50 loss: 1.6546043132150523 grad: -0.6798251918144614
iteration: 60 loss: 1.3915781036624304 grad: -0.5810094369397283
iteration: 70 loss: 1.200852424206997 grad: -0.5071329264761966
iteration: 80 loss: 1.0562110789032886 grad: -0.44988736832110715
iteration: 90 loss: 0.942745827437441 grad: -0.40425404761487455
iteration: 100 loss: 0.8513532173920297 grad: -0.3670371358573642
iteration: 110 loss: 0.7761600287425278 grad: -0.3361100455019317
iteration: 120 loss: 0.7132072680255988 grad: -0.3100047624825972
iteration: 130 loss: 0.6597290439011555 grad: -0.28767608316049315
iteration: 0 loss: 52.307013001738845 grad: 272.94933047501854
iteration: 10 loss: 6.957125643400683 grad: -0.4063048565880768
iteration: 20 loss: 3.8645716052622614 grad: -0.8344898880600227
iteration: 30 loss: 2.6793594899061812 grad: -0.7171002320699631
iteration: 40 loss: 2.0518096235842593 grad: -0.604086101702058
iteration: 50 loss: 1.663086314990654 grad: -0.5175519778405769
iteration: 60 loss: 1.3985672317715976 grad: -0.4515795523032689
iteration: 70 loss: 1.206882826391517 grad: -0.4001887995208969
iteration: 80 loss: 1.0615675454477738 grad: -0.35920634180418776
iteration: 90 loss: 0.9475969761319902 grad: -0.3258272735981127
iteration: 100 loss: 0.8558072452661608 grad: -0.29814203191566624
iteration: 110 loss: 0.7802908691544623 grad: -0.2748194982436658
iteration: 120 loss: 0.717068021325965 grad: -0.25490847329921784
iteration: 130 loss: 0.6633593990096746 grad: -0.2377132384841374
iteration: 0 loss: 51.209595461947565 grad: 276.638475020642
iteration: 10 loss: 6.955729039006118 grad: -0.3493517835259248
iteration: 20 loss: 3.852881212788485 grad: -0.7877423159261945
iteration: 30 loss: 2.6688939189639314 grad: -0.6871279916075321
iteration: 40 loss: 2.0430464511195665 grad: -0.5833467133720824
iteration: 50 loss: 1.655690641453475 grad: -0.5022655532735086
iteration: 60 loss: 1.3922194801459888 grad: -0.4397779989367506
iteration: 70 loss: 1.2013456173249737 grad: -0.3907575969445626
iteration: 80 loss: 1.0566695406746476 grad: -0.3514670756029275
iteration: 90 loss: 0.9432131716520499 grad: -0.31934232308162125
iteration: 100 loss: 0.8518446349084198 grad: -0.2926156337863206
iteration: 110 loss: 0.7766788094954378 grad: -0.27004403145194533
iteration: 120 loss: 0.7137518021823284 grad: -0.2507335468722374
iteration: 130 loss: 0.6602958974486186 grad: -0.2340270121762044
iteration: 0 loss: 48.37455205190891 grad: 315.6099165100177
iteration: 10 loss: 6.661960432599823 grad: -1.2154401087919813
iteration: 20 loss: 3.722681482814261 grad: -1.1827317709781804
iteration: 30 loss: 2.5862958325739203 grad: -0.9327470422037254
iteration: 40 loss: 1.982481809860019 grad: -0.7574436317798945
iteration: 50 loss: 1.6077646030134516 grad: -0.635215168477246
iteration: 60 loss: 1.3524899685384382 grad: -0.5463192953949423
iteration: 70 loss: 1.1673688244959934 grad: -0.47906548983485375
iteration: 80 loss: 1.0269580825751374 grad: -0.42650908130639253
iteration: 90 loss: 0.9167941184624727 grad: -0.38434464443076405
iteration: 100 loss: 0.8280463026857929 grad: -0.34978263978247237
iteration: 110 loss: 0.7550176735296439 grad: -0.3209434118022096
iteration: 120 loss: 0.6938679747618566 grad: -0.296516911567447
iteration: 0 loss: 50.19840556227653 grad: 304.3778320114339
iteration: 10 loss: 6.83922548712425 grad: -1.1449557150312666
iteration: 20 loss: 3.8055204556188653 grad: -1.0884986636687315
iteration: 30 loss: 2.637447853243678 grad: -0.8600454600830616
iteration: 40 loss: 2.018698432282318 grad: -0.7003419345962462
iteration: 50 loss: 1.6355174907992642 grad: -0.588753482206944
iteration: 60 loss: 1.3748680296678024 grad: -0.5073898598424362
iteration: 70 loss: 1.1860604623491946 grad: -0.44569080681810486
iteration: 80 loss: 1.0429773927748185 grad: -0.3973762185587316
iteration: 90 loss: 0.9307938880839918 grad: -0.3585452540645174
iteration: 100 loss: 0.8404696323971786 grad: -0.3266654557289445
iteration: 110 loss: 0.7661781998078202 grad: -0.30002715321617723
iteration: 120 loss: 0.7039953507832334 grad: -0.27743668386315523
iteration: 0 loss: 50.16118844731287 grad: 363.269252681442
iteration: 10 loss: 6.490380923210399 grad: -1.6326721686088135
iteration: 20 loss: 3.6603058403036566 grad: -1.2446419457124183
iteration: 30 loss: 2.55381170373936 grad: -0.9686235698682741
iteration: 40 loss: 1.9624484640931272 grad: -0.7821527357859317
iteration: 50 loss: 1.5941286686066136 grad: -0.6538159561393817
iteration: 60 loss: 1.3425865768545835 grad: -0.5611076509555551
iteration: 70 loss: 1.1598382026553404 grad: -0.49126593965944176
iteration: 80 loss: 1.0210322514821284 grad: -0.436847279367625
iteration: 90 loss: 0.9120058179078941 grad: -0.3932839158956703
iteration: 100 loss: 0.8240945501035888 grad: -0.3576357719889067
iteration: 110 loss: 0.7516996351913473 grad: -0.32793090707848505
iteration: 120 loss: 0.6910418528135861 grad: -0.30279969142751517
iteration: 0 loss: 52.0550978262064 grad: 295.6885482670282
iteration: 10 loss: 6.925215455912828 grad: -1.1360053495406146
iteration: 20 loss: 3.849428372544439 grad: -1.1429182747477715
iteration: 30 loss: 2.6689458153914116 grad: -0.9046479597345791
iteration: 40 loss: 2.0436847469407606 grad: -0.7360247252916591
iteration: 50 loss: 1.6563445960151428 grad: -0.6181232134895576
iteration: 60 loss: 1.3927681157563463 grad: -0.532230018287053
iteration: 70 loss: 1.2017752412396228 grad: -0.4671644283618919
iteration: 80 loss: 1.0569929563036815 grad: -0.41626345950232274
iteration: 90 loss: 0.9434479137094886 grad: -0.37538902791078854
iteration: 100 loss: 0.8520069905199225 grad: -0.3418566811290438
iteration: 110 loss: 0.7767824644507031 grad: -0.31385577252831187
iteration: 120 loss: 0.7138078612815661 grad: -0.2901232747502094
iteration: 130 loss: 0.6603132653340666 grad: -0.2697523718149877
iteration: 0 loss: 49.01177916016522 grad: 344.37765632189837
iteration: 10 loss: 6.648529754124121 grad: -1.756936058774849
iteration: 20 loss: 3.7124720716814905 grad: -1.3638243971968542
iteration: 30 loss: 2.5756137036235653 grad: -1.0334762543454121
iteration: 40 loss: 1.9722215789622417 grad: -0.8252177925338405
iteration: 50 loss: 1.5981989523348519 grad: -0.6855277864383943
iteration: 60 loss: 1.3436455616957077 grad: -0.5859753706325419
iteration: 70 loss: 1.1591951279097388 grad: -0.5116063338453934
iteration: 80 loss: 1.0193855537309817 grad: -0.45399358638266735
iteration: 90 loss: 0.9097538373694491 grad: -0.40806749088944777
iteration: 100 loss: 0.821476082547592 grad: -0.37060704218715884
iteration: 110 loss: 0.7488633370362963 grad: -0.33947166181786487
iteration: 120 loss: 0.6880829441111431 grad: -0.31318478490055024
iteration: 0 loss: 51.34219203688858 grad: 291.4243628938226
iteration: 10 loss: 7.069306596789292 grad: -0.3415388945875022
iteration: 20 loss: 3.913783878705999 grad: -0.7963025170448574
iteration: 30 loss: 2.7110929375566726 grad: -0.6966496662629975
iteration: 40 loss: 2.075380596175116 grad: -0.5919540036317197
iteration: 50 loss: 1.681891166145926 grad: -0.5098103926855015
iteration: 60 loss: 1.414230678517511 grad: -0.44639183898247103
iteration: 70 loss: 1.2203128627444046 grad: -0.3965981139446244
iteration: 80 loss: 1.073325186805877 grad: -0.35667029622045987
iteration: 90 loss: 0.9580540362689631 grad: -0.3240175860538856
iteration: 100 loss: 0.8652232555180656 grad: -0.296849446842721
iteration: 110 loss: 0.7888543669653452 grad: -0.2739049483983326
iteration: 120 loss: 0.7249203888174468 grad: -0.25427632156531366
iteration: 130 loss: 0.6706094363805112 grad: -0.2372958151301825
iteration: 0 loss: 53.49738510732382 grad: 274.50172859945815
iteration: 10 loss: 6.99535430972781 grad: -0.8627952185903147
iteration: 20 loss: 3.8854588428131054 grad: -0.964126033574006
iteration: 30 loss: 2.694199406690545 grad: -0.7846934302071881
iteration: 40 loss: 2.0634061393340164 grad: -0.647858809419653
iteration: 50 loss: 1.6726195342933103 grad: -0.5491577291640553
iteration: 60 loss: 1.4066628129783731 grad: -0.4759381759927241
iteration: 70 loss: 1.213916795241444 grad: -0.41979535722135386
iteration: 80 loss: 1.0677847575607715 grad: -0.37548487346587234
iteration: 90 loss: 0.9531661574670497 grad: -0.339660582354747
iteration: 100 loss: 0.8608497172740499 grad: -0.31011192068056237
iteration: 110 loss: 0.7848968301170978 grad: -0.28532791531830265
iteration: 120 loss: 0.7213063673911247 grad: -0.2642437217119419
iteration: 130 loss: 0.667283945851688 grad: -0.2460885212869364
iteration: 0 loss: 50.542726066170026 grad: 337.2196298791879
iteration: 10 loss: 6.976750115090495 grad: -0.7769192134892107
iteration: 20 loss: 3.86920808769198 grad: -0.9827144351485047
iteration: 30 loss: 2.6797877903837946 grad: -0.8101054231958885
iteration: 40 loss: 2.0507143986028495 grad: -0.6708104285956135
iteration: 50 loss: 1.6613628963136762 grad: -0.5690937568055106
iteration: 60 loss: 1.3965808253177452 grad: -0.49331568736539666
iteration: 70 loss: 1.204800856413013 grad: -0.4351094698274314
iteration: 80 loss: 1.0594721723122016 grad: -0.38913601035229994
iteration: 90 loss: 0.9455298679386446 grad: -0.35195624400227277
iteration: 100 loss: 0.8537895005797096 grad: -0.32128734125532354
iteration: 110 loss: 0.7783326366423519 grad: -0.29556481674575086
iteration: 120 loss: 0.7151734396079106 grad: -0.27368442653720526
iteration: 130 loss: 0.6615292558330608 grad: -0.2548461052707214
iteration: 0 loss: 50.32179166737251 grad: 273.13138449810845
iteration: 10 loss: 6.805323422975065 grad: -0.5406455488690942
iteration: 20 loss: 3.7785398046999035 grad: -0.9297313472271678
iteration: 30 loss: 2.6192356959132996 grad: -0.7838078986999366
iteration: 40 loss: 2.005527481093519 grad: -0.6543445566109525
iteration: 50 loss: 1.6254201332594675 grad: -0.5574595072964119
iteration: 60 loss: 1.3667861808633626 grad: -0.4844701645258974
iteration: 70 loss: 1.1793803493596897 grad: -0.4280477451737037
iteration: 80 loss: 1.0373180921704839 grad: -0.38329938565383437
iteration: 90 loss: 0.925905591940715 grad: -0.34700598728227733
iteration: 100 loss: 0.836181023488693 grad: -0.3170043801221595
iteration: 110 loss: 0.7623673995768431 grad: -0.29180020036952486
iteration: 120 loss: 0.7005729676467044 grad: -0.2703327594350423
iteration: 0 loss: 49.457218374921574 grad: 344.24087292946547
iteration: 10 loss: 6.673398173887852 grad: -1.3710426411310999
iteration: 20 loss: 3.725526740578475 grad: -1.1896675583985594
iteration: 30 loss: 2.5872796231403963 grad: -0.9390109257016823
iteration: 40 loss: 1.9828322158734244 grad: -0.7620731971318109
iteration: 50 loss: 1.60783715890317 grad: -0.6387836177173994
iteration: 60 loss: 1.3524244835221824 grad: -0.5492107375860487
iteration: 70 loss: 1.1672303098189745 grad: -0.4815032274943122
iteration: 80 loss: 1.0267800368827589 grad: -0.428626706664804
iteration: 90 loss: 0.9165950186634704 grad: -0.38622607337828185
iteration: 100 loss: 0.8278368266513264 grad: -0.3514831311666302
iteration: 110 loss: 0.7548041887966974 grad: -0.3225009057785563
iteration: 120 loss: 0.6936543510222773 grad: -0.2979584171249905
iteration: 0 loss: 51.16371257275194 grad: 255.4193188519404
iteration: 10 loss: 6.82608255041867 grad: -0.08698830626385029
iteration: 20 loss: 3.8063820051708137 grad: -0.8257415923603766
iteration: 30 loss: 2.6435140872728713 grad: -0.7367582669783135
iteration: 40 loss: 2.0260983954481575 grad: -0.626435047060591
iteration: 50 loss: 1.643048732326158 grad: -0.5384073402315731
iteration: 60 loss: 1.3821336796628119 grad: -0.47031981691197045
iteration: 70 loss: 1.1929367694301205 grad: -0.416937892293157
iteration: 80 loss: 1.0494412912956828 grad: -0.3742299380040282
iteration: 90 loss: 0.9368603776738795 grad: -0.33938665383956035
iteration: 100 loss: 0.8461671806480808 grad: -0.3104614860730796
iteration: 110 loss: 0.7715386564693124 grad: -0.28608404886673217
iteration: 120 loss: 0.7090498029696061 grad: -0.26526916757407437
iteration: 130 loss: 0.6559584496444586 grad: -0.2472934823759499
iteration: 0 loss: 50.76073760347458 grad: 325.1382019772372
iteration: 10 loss: 6.90983354002955 grad: -0.8814898235824952
iteration: 20 loss: 3.8322430324303904 grad: -1.0151648903511576
iteration: 30 loss: 2.65386023961169 grad: -0.8210723766177926
iteration: 40 loss: 2.030728186569046 grad: -0.6741051474194518
iteration: 50 loss: 1.6451128051454693 grad: -0.569064723123355
iteration: 60 loss: 1.3828991964796016 grad: -0.49169008892534394
iteration: 70 loss: 1.1929929820767442 grad: -0.43267751140464467
iteration: 80 loss: 1.0490910372291182 grad: -0.3862948440274323
iteration: 90 loss: 0.9362709389784372 grad: -0.34891884402042284
iteration: 100 loss: 0.8454360443384261 grad: -0.3181730906873218
iteration: 110 loss: 0.7707249707651003 grad: -0.2924425333135662
iteration: 120 loss: 0.7081905549019377 grad: -0.2705942910807269
iteration: 130 loss: 0.6550772817004558 grad: -0.25181148144863863
iteration: 0 loss: 51.93682834727799 grad: 253.70168507422693
iteration: 10 loss: 6.957253812270417 grad: 0.30844326796033616
iteration: 20 loss: 3.864969495898673 grad: -0.6205213732753726
iteration: 30 loss: 2.68231238609732 grad: -0.6129075182391416
iteration: 40 loss: 2.055651193510626 grad: -0.5411254178474378
iteration: 50 loss: 1.6671386855774388 grad: -0.47464946649206025
iteration: 60 loss: 1.4025668021527053 grad: -0.4200410915966016
iteration: 70 loss: 1.2107285152204978 grad: -0.37576903243479226
iteration: 80 loss: 1.065225242767941 grad: -0.3395744803766663
iteration: 90 loss: 0.9510611833619887 grad: -0.309590568536816
iteration: 100 loss: 0.859084686635759 grad: -0.28441317439106
iteration: 110 loss: 0.7833931553550428 grad: -0.2630041048478917
iteration: 120 loss: 0.7200082772959501 grad: -0.24459199600406784
iteration: 130 loss: 0.6661507125392994 grad: -0.22859681967392761
iteration: 0 loss: 51.510109820262024 grad: 321.3903884764278
iteration: 10 loss: 6.9298774123710905 grad: -1.4736297906335913
iteration: 20 loss: 3.853454413024255 grad: -1.2105359491119139
iteration: 30 loss: 2.6699228799040857 grad: -0.9318657278028959
iteration: 40 loss: 2.0432439447832538 grad: -0.7504739237429802
iteration: 50 loss: 1.65523954329468 grad: -0.6269561895206819
iteration: 60 loss: 1.391346685675134 grad: -0.5381062644788007
iteration: 70 loss: 1.2002089242225586 grad: -0.47129807143157
iteration: 80 loss: 1.0553710130275662 grad: -0.41928725436752023
iteration: 90 loss: 0.9418185171601346 grad: -0.3776648153232002
iteration: 100 loss: 0.8503965891147179 grad: -0.3436061224767049
iteration: 110 loss: 0.7752055037961493 grad: -0.3152220675405183
iteration: 120 loss: 0.7122719334056042 grad: -0.2912029321147426
iteration: 130 loss: 0.6588219578256113 grad: -0.27061271512957524
iteration: 0 loss: 54.914540280719045 grad: 277.9812984598435
iteration: 10 loss: 6.8821786947868615 grad: -1.152735759712837
iteration: 20 loss: 3.835789658023135 grad: -1.1774668682135871
iteration: 30 loss: 2.6612686745341474 grad: -0.9279201020716112
iteration: 40 loss: 2.0382916053245026 grad: -0.7530489820572677
iteration: 50 loss: 1.6521327339077372 grad: -0.631418275809606
iteration: 60 loss: 1.3892793374442673 grad: -0.5430853828443185
iteration: 70 loss: 1.19877788017493 grad: -0.47631060955198
iteration: 80 loss: 1.054353683990154 grad: -0.42415002718587047
iteration: 90 loss: 0.9410827071980582 grad: -0.3823109027898966
iteration: 100 loss: 0.8498592603893733 grad: -0.34801721014250203
iteration: 110 loss: 0.7748121284023661 grad: -0.31940082642221224
iteration: 120 loss: 0.7119853913070322 grad: -0.29516091020829327
iteration: 130 loss: 0.65861621517607 grad: -0.27436478635616257
iteration: 0 loss: 54.71114967200399 grad: 298.5114796381953
iteration: 10 loss: 6.850223966467018 grad: -1.539538235663083
iteration: 20 loss: 3.814356170558891 grad: -1.2662891660044973
iteration: 30 loss: 2.6447064028329805 grad: -0.967877844558346
iteration: 40 loss: 2.0248234746270763 grad: -0.7758730214467568
iteration: 50 loss: 1.6408022531959432 grad: -0.6461844286778041
iteration: 60 loss: 1.3795097309500968 grad: -0.5533966199507032
iteration: 70 loss: 1.1901959348671716 grad: -0.48389142842569455
iteration: 80 loss: 1.0467047242790282 grad: -0.4299332548053701
iteration: 90 loss: 0.9341855121737436 grad: -0.3868467687538237
iteration: 100 loss: 0.8435804412326682 grad: -0.3516519305758864
iteration: 110 loss: 0.7690507316907645 grad: -0.32236344164823927
iteration: 120 loss: 0.7066632121114625 grad: -0.29760912910205206
iteration: 130 loss: 0.6536714631232082 grad: -0.2764108123367036
iteration: 0 loss: 51.25281803417556 grad: 293.9799153667324
iteration: 10 loss: 6.707343133680627 grad: -0.1675277561915002
iteration: 20 loss: 3.7404505429028423 grad: -0.8576552256275
iteration: 30 loss: 2.5998621345018944 grad: -0.7670025630850944
iteration: 40 loss: 1.9940519435166275 grad: -0.6540972458954366
iteration: 50 loss: 1.6179704633057719 grad: -0.5633166795083641
iteration: 60 loss: 1.3616499504088841 grad: -0.49273894365480164
iteration: 70 loss: 1.1756875351537597 grad: -0.4372097558533352
iteration: 80 loss: 1.0345814195724499 grad: -0.39267236545225753
iteration: 90 loss: 0.923831728019319 grad: -0.3562693318380516
iteration: 100 loss: 0.8345832892129114 grad: -0.32600706218289577
iteration: 110 loss: 0.761121628212591 grad: -0.30047519072341045
iteration: 120 loss: 0.6995935523278934 grad: -0.2786560845240523
iteration: 0 loss: 51.57163409027844 grad: 243.7208034590511
iteration: 10 loss: 6.917554275908515 grad: 0.10775074600497292
iteration: 20 loss: 3.8365083396230477 grad: -0.5877064445961312
iteration: 30 loss: 2.66047272486193 grad: -0.5643516150353288
iteration: 40 loss: 2.038141200225461 grad: -0.49613934553947725
iteration: 50 loss: 1.652611495598814 grad: -0.43514350799756774
iteration: 60 loss: 1.3901966920414204 grad: -0.38545173495454604
iteration: 70 loss: 1.1999817244801223 grad: -0.34525482546579883
iteration: 80 loss: 1.055739906910648 grad: -0.3123977409092467
iteration: 90 loss: 0.9425819403514155 grad: -0.28516183868104317
iteration: 100 loss: 0.851425211935996 grad: -0.26227047811105086
iteration: 110 loss: 0.7764136431380642 grad: -0.2427846989840186
iteration: 120 loss: 0.7136013187910066 grad: -0.22600847984839517
iteration: 130 loss: 0.6602321078568242 grad: -0.21141891071039337
iteration: 0 loss: 53.69610921406177 grad: 305.57867030280306
iteration: 10 loss: 6.863493141906061 grad: -0.6137638187263993
iteration: 20 loss: 3.8195540444411646 grad: -0.9639257318043939
iteration: 30 loss: 2.649904769809754 grad: -0.8045701047917113
iteration: 40 loss: 2.029832797966471 grad: -0.6687966759115633
iteration: 50 loss: 1.6454969306358345 grad: -0.5683933998673086
iteration: 60 loss: 1.3838685793595522 grad: -0.4932042801269442
iteration: 70 loss: 1.1942371750294776 grad: -0.435292729186081
iteration: 80 loss: 1.0504581820409014 grad: -0.3894763567554301
iteration: 90 loss: 0.9376822648368116 grad: -0.352382785216938
iteration: 100 loss: 0.8468492363492921 grad: -0.3217609221259937
iteration: 110 loss: 0.7721169944899718 grad: -0.296062700315715
iteration: 120 loss: 0.7095490248346489 grad: -0.2741929301430243
iteration: 130 loss: 0.6563958807915614 grad: -0.25535679520496335
iteration: 0 loss: 49.396204448874535 grad: 311.6975763893138
iteration: 10 loss: 6.711775636727907 grad: -1.2363537702790168
iteration: 20 loss: 3.7427266859724777 grad: -1.1936594628483266
iteration: 30 loss: 2.5970060018030794 grad: -0.9337280510518435
iteration: 40 loss: 1.9892009671845947 grad: -0.7554743483621695
iteration: 50 loss: 1.6124106413309909 grad: -0.6323617528074112
iteration: 60 loss: 1.3559174726805479 grad: -0.5432735704802243
iteration: 70 loss: 1.1700167752086683 grad: -0.47607830327616585
iteration: 80 loss: 1.0290756098046254 grad: -0.4236709169201227
iteration: 90 loss: 0.9185334202533423 grad: -0.38168273318164936
iteration: 100 loss: 0.8295057853857046 grad: -0.34729829623543496
iteration: 110 loss: 0.7562638936860409 grad: -0.3186274917337466
iteration: 120 loss: 0.6949476268276323 grad: -0.2943565187872345
iteration: 0 loss: 56.7468220916818 grad: 406.8504233632455
iteration: 10 loss: 3.8670385993102836 grad: -0.7742946623967366
iteration: 20 loss: 2.209256432839621 grad: -0.8740174905309946
iteration: 30 loss: 1.5558497951249344 grad: -0.7085520462441828
iteration: 40 loss: 1.202956946283352 grad: -0.5837151461976351
iteration: 50 loss: 0.9814168691880861 grad: -0.4940386685947765
iteration: 60 loss: 0.8292105660375042 grad: -0.4276648075045412
iteration: 70 loss: 0.7181165304758989 grad: -0.37684827186092923
iteration: 80 loss: 0.633422135511947 grad: -0.3367872034369341
iteration: 90 loss: 0.566696282945238 grad: -0.30442831563937617
iteration: 100 loss: 0.5127571877842869 grad: -0.2777588566458743
iteration: 0 loss: 45.1655148004563 grad: 375.5671640845411
iteration: 10 loss: 4.2922434703728785 grad: -0.45871080316971974
iteration: 20 loss: 2.3939123491835828 grad: -0.6700538224287739
iteration: 30 loss: 1.667551723380918 grad: -0.5634067676838019
iteration: 40 loss: 1.281468535548811 grad: -0.4722133454932171
iteration: 50 loss: 1.0414214441458916 grad: -0.4040492451934016
iteration: 60 loss: 0.8775589829285143 grad: -0.35252266401031446
iteration: 70 loss: 0.7585053773067226 grad: -0.31252831873346154
iteration: 80 loss: 0.6680543274214457 grad: -0.2806835922766868
iteration: 90 loss: 0.5969829086796132 grad: -0.2547622999239799
iteration: 100 loss: 0.5396530849644808 grad: -0.23326477749615132
iteration: 0 loss: 44.88082219997488 grad: 381.3758628266014
iteration: 10 loss: 4.2721735136361865 grad: -0.9126993763410263
iteration: 20 loss: 2.390062962984509 grad: -0.8337178871019983
iteration: 30 loss: 1.6662711573859186 grad: -0.6570941603350917
iteration: 40 loss: 1.2809476076489856 grad: -0.5359872991446837
iteration: 50 loss: 1.0411932615031054 grad: -0.45161635645511544
iteration: 60 loss: 0.8774612688130219 grad: -0.39006563647139236
iteration: 70 loss: 0.7584710206608013 grad: -0.34332022802435247
iteration: 80 loss: 0.6680522379429434 grad: -0.3066501459509162
iteration: 90 loss: 0.596997511690626 grad: -0.27712497812122816
iteration: 100 loss: 0.5396761391684777 grad: -0.2528433275601984
iteration: 0 loss: 54.669425960620835 grad: 423.11447138012863
iteration: 10 loss: 3.957406937875889 grad: -0.5998387071779252
iteration: 20 loss: 2.2542582844391537 grad: -0.6984837099206609
iteration: 30 loss: 1.5878772414666855 grad: -0.578328882456002
iteration: 40 loss: 1.228587153435621 grad: -0.4831894177086405
iteration: 50 loss: 1.0030564482610977 grad: -0.4131571093218932
iteration: 60 loss: 0.8480496546490444 grad: -0.3604673475145725
iteration: 70 loss: 0.7348517840266802 grad: -0.3196347603077792
iteration: 80 loss: 0.6485055499631265 grad: -0.2871364815673538
iteration: 90 loss: 0.5804417537941927 grad: -0.26068204008177454
iteration: 100 loss: 0.5253934324834766 grad: -0.23873705914340618
iteration: 0 loss: 48.685986632500295 grad: 362.9114180607344
iteration: 10 loss: 4.226048675863736 grad: -0.5107295862531372
iteration: 20 loss: 2.367267624831584 grad: -0.691387656284979
iteration: 30 loss: 1.6525206839181588 grad: -0.5795483024035378
iteration: 40 loss: 1.2715740178704933 grad: -0.4854651304543648
iteration: 50 loss: 1.0343001228220188 grad: -0.41531446194889526
iteration: 60 loss: 0.8721263559445188 grad: -0.3623181003501454
iteration: 70 loss: 0.7541877043971578 grad: -0.3211887888803687
iteration: 80 loss: 0.664516942852143 grad: -0.28844054528475216
iteration: 90 loss: 0.5940161440564886 grad: -0.26178288388181437
iteration: 100 loss: 0.5371182071657213 grad: -0.23967376415084274
iteration: 0 loss: 48.37900635451351 grad: 315.88427155254305
iteration: 10 loss: 4.106279780726874 grad: 0.02523955393161325
iteration: 20 loss: 2.3117073727591806 grad: -0.5412249988543449
iteration: 30 loss: 1.6182871950932993 grad: -0.5070804781014586
iteration: 40 loss: 1.2473368777036327 grad: -0.4420520313974303
iteration: 50 loss: 1.0157128330502845 grad: -0.38604276964001805
iteration: 60 loss: 0.8571226311401419 grad: -0.3410529955715017
iteration: 70 loss: 0.7416408460650762 grad: -0.30492778374267576
iteration: 80 loss: 0.6537512262545367 grad: -0.2755331224614951
iteration: 90 loss: 0.5845969671259312 grad: -0.25124267052295046
iteration: 100 loss: 0.528750696317812 grad: -0.23087290750231626
iteration: 0 loss: 45.20440167898914 grad: 371.8140729688017
iteration: 10 loss: 4.247760791131648 grad: -0.0065689667379627715
iteration: 20 loss: 2.3701808840210106 grad: -0.4993931718323673
iteration: 30 loss: 1.6528971319416996 grad: -0.4744483197042143
iteration: 40 loss: 1.2714241766272856 grad: -0.41737299995565136
iteration: 50 loss: 1.0340470813204197 grad: -0.366756035268171
iteration: 60 loss: 0.8718789801844816 grad: -0.32548057837875477
iteration: 70 loss: 0.7539737229938676 grad: -0.2920114504374316
iteration: 80 loss: 0.664340318118155 grad: -0.2645847639563562
iteration: 90 loss: 0.5938737981835981 grad: -0.24179746789258733
iteration: 100 loss: 0.5370053662443091 grad: -0.22260557783463628
iteration: 0 loss: 43.42181624366545 grad: 298.83189724051755
iteration: 10 loss: 4.361470641403974 grad: 0.5116125127530968
iteration: 20 loss: 2.4229756994528873 grad: -0.3093715149299344
iteration: 30 loss: 1.6872913694394562 grad: -0.3648327856837194
iteration: 40 loss: 1.2970271588466693 grad: -0.34281568751899383
iteration: 50 loss: 1.0544948134252907 grad: -0.311408916791338
iteration: 60 loss: 0.888931316048787 grad: -0.28208055997967374
iteration: 70 loss: 0.7686163420772903 grad: -0.2566727044881351
iteration: 80 loss: 0.6771814671064518 grad: -0.23500575987488737
iteration: 90 loss: 0.6053156520717742 grad: -0.2165125452980157
iteration: 100 loss: 0.5473279583788595 grad: -0.20062909091071057
iteration: 0 loss: 60.09326627660444 grad: 396.54522828759707
iteration: 10 loss: 3.7725457414194765 grad: -0.45248836601780074
iteration: 20 loss: 2.1774142801254626 grad: -0.6470457491904056
iteration: 30 loss: 1.5461845210931477 grad: -0.551701271913194
iteration: 40 loss: 1.2027284701394323 grad: -0.467205316361225
iteration: 50 loss: 0.985673005520973 grad: -0.4027344184093261
iteration: 60 loss: 0.8357183594828463 grad: -0.3533102513744355
iteration: 70 loss: 0.7257638298388801 grad: -0.31454917269393035
iteration: 80 loss: 0.6416159475091429 grad: -0.2834393459682567
iteration: 90 loss: 0.5751054399255818 grad: -0.2579547783474179
iteration: 100 loss: 0.5211912676940383 grad: -0.23670939300298677
iteration: 0 loss: 44.77345464743652 grad: 363.8778723031064
iteration: 10 loss: 4.256960261645583 grad: -0.09868317431189608
iteration: 20 loss: 2.3809682943551707 grad: -0.5862872079539334
iteration: 30 loss: 1.661639468519008 grad: -0.5348963482676385
iteration: 40 loss: 1.2784547589464472 grad: -0.46158804260894304
iteration: 50 loss: 1.0398259961736007 grad: -0.40074341999049146
iteration: 60 loss: 0.8767378270159415 grad: -0.3526251186606416
iteration: 70 loss: 0.7581392439762401 grad: -0.31433497942830546
iteration: 80 loss: 0.6679696621509638 grad: -0.28336785849788027
iteration: 90 loss: 0.5970787167937239 grad: -0.2578924479418059
iteration: 100 loss: 0.5398674568929313 grad: -0.2366034473599164
iteration: 0 loss: 44.49309921085935 grad: 328.1190727220345
iteration: 10 loss: 4.2204654380670945 grad: -0.3228541460728185
iteration: 20 loss: 2.3633430694049737 grad: -0.6568628645014404
iteration: 30 loss: 1.6494235767341623 grad: -0.5700064645459129
iteration: 40 loss: 1.2689827577489103 grad: -0.4833011091178031
iteration: 50 loss: 1.0320590453657958 grad: -0.4158962567258589
iteration: 60 loss: 0.8701455344028711 grad: -0.3640275719584214
iteration: 70 loss: 0.7524094157211452 grad: -0.3233633786747273
iteration: 80 loss: 0.6629014815250891 grad: -0.2907815914525421
iteration: 90 loss: 0.5925348267723662 grad: -0.2641480829390742
iteration: 100 loss: 0.535749538708966 grad: -0.24199386370206083
iteration: 0 loss: 45.44785268666963 grad: 396.42559280442447
iteration: 10 loss: 4.182288578539308 grad: -1.1879246150131366
iteration: 20 loss: 2.353554881247584 grad: -0.9974387012491615
iteration: 30 loss: 1.6440590114290734 grad: -0.7654232384075972
iteration: 40 loss: 1.2649914852533006 grad: -0.6152476625983465
iteration: 50 loss: 1.0287017127621683 grad: -0.5134247652416601
iteration: 60 loss: 0.8671662843828265 grad: -0.44037557715614806
iteration: 70 loss: 0.7496946930650665 grad: -0.38554180317686004
iteration: 80 loss: 0.6603904711232644 grad: -0.3429016215205578
iteration: 90 loss: 0.590190015772557 grad: -0.3088049297599858
iteration: 100 loss: 0.5335452803274993 grad: -0.2809199513135149
iteration: 0 loss: 47.4280189527616 grad: 326.94306086618775
iteration: 10 loss: 4.2468820082208785 grad: 0.7114141147764614
iteration: 20 loss: 2.364845783433421 grad: -0.20806659231910102
iteration: 30 loss: 1.6490022448823434 grad: -0.2991962340126161
iteration: 40 loss: 1.2686449538789268 grad: -0.2945905406417333
iteration: 50 loss: 1.032006151478568 grad: -0.27339998036859015
iteration: 60 loss: 0.8703364488065662 grad: -0.2507702621407163
iteration: 70 loss: 0.752780110836449 grad: -0.23008783645580153
iteration: 80 loss: 0.6633994224827223 grad: -0.21193063748141105
iteration: 90 loss: 0.5931213338306985 grad: -0.1961457565305078
iteration: 100 loss: 0.5363968709645881 grad: -0.18241465920272826
iteration: 0 loss: 45.737622718165085 grad: 369.1146746770249
iteration: 10 loss: 4.213738797968455 grad: -0.8339102614607308
iteration: 20 loss: 2.360597956489569 grad: -0.8198894490528791
iteration: 30 loss: 1.6464863647093724 grad: -0.654572967966955
iteration: 40 loss: 1.266003297007294 grad: -0.5365044280982096
iteration: 50 loss: 1.029163456755131 grad: -0.45310334961697646
iteration: 60 loss: 0.867383276104771 grad: -0.39185339418246806
iteration: 70 loss: 0.7497935757094804 grad: -0.3451602235177662
iteration: 80 loss: 0.6604297465515546 grad: -0.30844432817409717
iteration: 90 loss: 0.5901988028837298 grad: -0.2788354503271427
iteration: 100 loss: 0.5335388473615074 grad: -0.25445792803803097
iteration: 0 loss: 49.188035099318 grad: 379.5678435119704
iteration: 10 loss: 4.190138421483922 grad: -0.21840818385211408
iteration: 20 loss: 2.35817611711218 grad: -0.5511509649528432
iteration: 30 loss: 1.6499878760905218 grad: -0.4931015032849079
iteration: 40 loss: 1.271377359182266 grad: -0.4245913849346495
iteration: 50 loss: 1.0350927745929435 grad: -0.3690081634802602
iteration: 60 loss: 0.8733720532798709 grad: -0.3252909791237434
iteration: 70 loss: 0.7556423427274074 grad: -0.2905339144258854
iteration: 80 loss: 0.666059463076346 grad: -0.2624040539332791
iteration: 90 loss: 0.5955831511069349 grad: -0.23923198241864557
iteration: 100 loss: 0.5386754845465641 grad: -0.21983775051076654
iteration: 0 loss: 46.97796463356458 grad: 434.50071948980616
iteration: 10 loss: 3.841096791986026 grad: -1.1255962740028589
iteration: 20 loss: 2.205255492382879 grad: -0.9830136272595438
iteration: 30 loss: 1.5586381468921735 grad: -0.7598035753562284
iteration: 40 loss: 1.208258855759678 grad: -0.6135350928635892
iteration: 50 loss: 0.9876632869062715 grad: -0.5135743983627005
iteration: 60 loss: 0.8357425434474366 grad: -0.4414660040396591
iteration: 70 loss: 0.7246364026189606 grad: -0.3871197926264369
iteration: 80 loss: 0.6397919180600927 grad: -0.34472868272167206
iteration: 90 loss: 0.5728538606505357 grad: -0.3107492874623717
iteration: 100 loss: 0.5186782273690782 grad: -0.28290632656019354
iteration: 0 loss: 48.93920135189962 grad: 367.98904607591686
iteration: 10 loss: 4.203167692002642 grad: -0.5969821652511116
iteration: 20 loss: 2.353507373134438 grad: -0.7123719228358494
iteration: 30 loss: 1.6423082704679308 grad: -0.589060058701362
iteration: 40 loss: 1.2633687652487837 grad: -0.4911551593920057
iteration: 50 loss: 1.0274114911769836 grad: -0.4192589194423526
iteration: 60 loss: 0.8661755677350232 grad: -0.3653049386634468
iteration: 70 loss: 0.7489418842400979 grad: -0.32358418013375345
iteration: 80 loss: 0.6598217775331663 grad: -0.2904393077880826
iteration: 90 loss: 0.5897637359809612 grad: -0.2634991774353534
iteration: 100 loss: 0.5332299772141017 grad: -0.2411794948554672
iteration: 0 loss: 43.04353834015526 grad: 334.9691070955636
iteration: 10 loss: 4.222928186559784 grad: 0.17563743468016874
iteration: 20 loss: 2.357751925557874 grad: -0.47894817445348625
iteration: 30 loss: 1.644230156401756 grad: -0.4684600485159359
iteration: 40 loss: 1.264589748705062 grad: -0.41416683832229867
iteration: 50 loss: 1.0283368464822409 grad: -0.36429973194187615
iteration: 60 loss: 0.8669468927176406 grad: -0.3232875975286038
iteration: 70 loss: 0.7496200217888145 grad: -0.28994591970479966
iteration: 80 loss: 0.6604369723932823 grad: -0.2626060116983153
iteration: 90 loss: 0.5903327888138599 grad: -0.2398929403157945
iteration: 100 loss: 0.5337630866327924 grad: -0.22077101847015446
iteration: 0 loss: 57.7383604104404 grad: 436.11052450043854
iteration: 10 loss: 3.8507644391918556 grad: -0.3777440229279102
iteration: 20 loss: 2.205995836434836 grad: -0.6640917768695419
iteration: 30 loss: 1.5601622439350484 grad: -0.5735312174075593
iteration: 40 loss: 1.2104408825816377 grad: -0.4866103748339339
iteration: 50 loss: 0.9901755225758858 grad: -0.4193757511821391
iteration: 60 loss: 0.8383902460241601 grad: -0.3676333833215269
iteration: 70 loss: 0.7273139296840285 grad: -0.32701330338211787
iteration: 80 loss: 0.6424426938032908 grad: -0.294413460239887
iteration: 90 loss: 0.575448019149917 grad: -0.2677218978703982
iteration: 100 loss: 0.5212006848583273 grad: -0.24548581522589483
iteration: 0 loss: 44.34789799707449 grad: 341.6801332584042
iteration: 10 loss: 4.269951876575181 grad: 0.20974779413480538
iteration: 20 loss: 2.385238685170903 grad: -0.4399119673187345
iteration: 30 loss: 1.6639331250886988 grad: -0.4421085023600401
iteration: 40 loss: 1.280041051877106 grad: -0.39493803024980906
iteration: 50 loss: 1.041079082827144 grad: -0.34934240073623046
iteration: 60 loss: 0.8778020413473396 grad: -0.3111174539233438
iteration: 70 loss: 0.7590814142438996 grad: -0.2797177615662908
iteration: 80 loss: 0.6688251894085873 grad: -0.25380008236932744
iteration: 90 loss: 0.5978684128207615 grad: -0.23216952341618674
iteration: 100 loss: 0.540604610529375 grad: -0.21389705470504267
iteration: 0 loss: 44.442724987599625 grad: 392.8004058201886
iteration: 10 loss: 4.202560870848753 grad: -0.6290925741882779
iteration: 20 loss: 2.355306497474058 grad: -0.7209469629940584
iteration: 30 loss: 1.6442148402616938 grad: -0.5909398360654496
iteration: 40 loss: 1.2651921163886237 grad: -0.4906565317139425
iteration: 50 loss: 1.0291219715316153 grad: -0.4178405173487011
iteration: 60 loss: 0.8677745091862644 grad: -0.36353630908312146
iteration: 70 loss: 0.7504381881931708 grad: -0.321709646145766
iteration: 80 loss: 0.6612257082810257 grad: -0.28856925228677166
iteration: 90 loss: 0.5910850533998862 grad: -0.26168395731303895
iteration: 100 loss: 0.5344774148750069 grad: -0.23944094811783395
iteration: 0 loss: 45.72887176240552 grad: 411.8738135675607
iteration: 10 loss: 4.092809507820055 grad: -1.2504698941391652
iteration: 20 loss: 2.3081921645717887 grad: -0.9859598234388439
iteration: 30 loss: 1.6156693994204365 grad: -0.7518543030291056
iteration: 40 loss: 1.2450735194106115 grad: -0.603531327859041
iteration: 50 loss: 1.0136976419610784 grad: -0.5035721799088735
iteration: 60 loss: 0.8553047805392618 grad: -0.4320166261015066
iteration: 70 loss: 0.7399854303024437 grad: -0.37834652755217113
iteration: 80 loss: 0.6522318363793768 grad: -0.3366196838889258
iteration: 90 loss: 0.5831929863831895 grad: -0.3032510523367817
iteration: 100 loss: 0.5274457510126509 grad: -0.27595614468888907
iteration: 0 loss: 54.70318048741981 grad: 322.69193215183
iteration: 10 loss: 4.073429093372769 grad: 0.15439612942308872
iteration: 20 loss: 2.3044197840626195 grad: -0.4875900160602621
iteration: 30 loss: 1.6169766517337765 grad: -0.47475937648522515
iteration: 40 loss: 1.2480511527839389 grad: -0.41951233670619426
iteration: 50 loss: 1.0172294624186096 grad: -0.3690024282061364
iteration: 60 loss: 0.8589692654102857 grad: -0.3274858782145972
iteration: 70 loss: 0.7436097046431975 grad: -0.2937295858281082
iteration: 80 loss: 0.6557436226888224 grad: -0.26604190075541745
iteration: 90 loss: 0.5865641884840691 grad: -0.24303329594485373
iteration: 100 loss: 0.5306686598310493 grad: -0.22365761589399008
iteration: 0 loss: 41.55173401686796 grad: 361.52647539517
iteration: 10 loss: 4.2420392102386275 grad: -0.924541903336382
iteration: 20 loss: 2.366763781293602 grad: -0.9092725138861
iteration: 30 loss: 1.6481781670115445 grad: -0.7191803639221541
iteration: 40 loss: 1.266164064297341 grad: -0.5855471872548568
iteration: 50 loss: 1.0286796083371634 grad: -0.49209159135721203
iteration: 60 loss: 0.866601875339091 grad: -0.4239453446195257
iteration: 70 loss: 0.748872247347456 grad: -0.37227733780457445
iteration: 80 loss: 0.6594470191893875 grad: -0.3318264329016173
iteration: 90 loss: 0.589196168052736 grad: -0.299322822352127
iteration: 100 loss: 0.532538992867227 grad: -0.27264341617802745
iteration: 0 loss: 47.847846541299674 grad: 379.37135030966533
iteration: 10 loss: 4.224749641294528 grad: -0.8648702369045633
iteration: 20 loss: 2.3655178167286977 grad: -0.8206611210973274
iteration: 30 loss: 1.6491740163759128 grad: -0.647153393446793
iteration: 40 loss: 1.2676975431569106 grad: -0.5274453415274832
iteration: 50 loss: 1.0303396561673914 grad: -0.44408524795484194
iteration: 60 loss: 0.8682572991136691 grad: -0.38333523921412893
iteration: 70 loss: 0.7504764064198587 grad: -0.33724225601381075
iteration: 80 loss: 0.6609839428309153 grad: -0.3011128834299527
iteration: 90 loss: 0.5906621981171983 grad: -0.2720421039489658
iteration: 100 loss: 0.5339356282258284 grad: -0.24814699811760027
iteration: 0 loss: 49.13019725835693 grad: 363.95441186784285
iteration: 10 loss: 4.119681818688108 grad: -0.12391446768468557
iteration: 20 loss: 2.3113373708335363 grad: -0.5715528381279988
iteration: 30 loss: 1.6164002412377323 grad: -0.5232561434892615
iteration: 40 loss: 1.245340328558293 grad: -0.45438225572086
iteration: 50 loss: 1.013857912316656 grad: -0.3966036971619031
iteration: 60 loss: 0.8554450093062603 grad: -0.35049053635443683
iteration: 70 loss: 0.7401281810453342 grad: -0.3135240879289257
iteration: 80 loss: 0.6523819314909154 grad: -0.2834477009737535
iteration: 90 loss: 0.5833500867976743 grad: -0.2585824188779564
iteration: 100 loss: 0.5276081837345455 grad: -0.23771703690235013
iteration: 0 loss: 61.33204213957199 grad: 438.5270684782266
iteration: 10 loss: 3.761439670517203 grad: 0.012073514957364986
iteration: 20 loss: 2.15536004747446 grad: -0.5323086372880999
iteration: 30 loss: 1.527247978776276 grad: -0.5043775826520795
iteration: 40 loss: 1.1867502432607828 grad: -0.44324007263669996
iteration: 50 loss: 0.9719584776532263 grad: -0.3892748057284799
iteration: 60 loss: 0.8237344632244376 grad: -0.34532812375156113
iteration: 70 loss: 0.7151328449352728 grad: -0.30971024631249383
iteration: 80 loss: 0.6320675576481666 grad: -0.28052919566761886
iteration: 90 loss: 0.5664414436049893 grad: -0.2562877130667288
iteration: 0 loss: 57.88585982016539 grad: 420.43798778762346
iteration: 10 loss: 3.8035475861173094 grad: -0.44281652611707717
iteration: 20 loss: 2.169739021327752 grad: -0.7381813015700256
iteration: 30 loss: 1.5335311631325899 grad: -0.6240879340152704
iteration: 40 loss: 1.1897746936285165 grad: -0.5237578606445832
iteration: 50 loss: 0.9734224583837865 grad: -0.4484334676377806
iteration: 60 loss: 0.8243648165585827 grad: -0.3913771972182832
iteration: 70 loss: 0.7152842697891256 grad: -0.3470285189096146
iteration: 80 loss: 0.6319299612039024 grad: -0.31167903450538037
iteration: 90 loss: 0.5661237064926106 grad: -0.2828808979675636
iteration: 0 loss: 44.520602148093964 grad: 378.1934899574422
iteration: 10 loss: 4.22128235510817 grad: -1.246560689484419
iteration: 20 loss: 2.368914494287434 grad: -0.9803960684325896
iteration: 30 loss: 1.6524760311871516 grad: -0.7468957601829873
iteration: 40 loss: 1.2704824095755447 grad: -0.5993916148882084
iteration: 50 loss: 1.0326720519559454 grad: -0.5000647873965107
iteration: 60 loss: 0.870234876572712 grad: -0.4289869352210062
iteration: 70 loss: 0.7521779579614386 grad: -0.37568573616959394
iteration: 80 loss: 0.6624680980816762 grad: -0.33425100502792027
iteration: 90 loss: 0.5919722894400778 grad: -0.30111888667861475
iteration: 100 loss: 0.535104057798559 grad: -0.2740191286600149
iteration: 0 loss: 46.95218941069982 grad: 341.40006929666356
iteration: 10 loss: 4.1759039758582155 grad: -0.21010249595923017
iteration: 20 loss: 2.340563189827125 grad: -0.5972089116958397
iteration: 30 loss: 1.6343215207651733 grad: -0.5268689176051338
iteration: 40 loss: 1.2577501616507851 grad: -0.44948446084406046
iteration: 50 loss: 1.02314669943218 grad: -0.38818558596673464
iteration: 60 loss: 0.8627749172132595 grad: -0.34063326156591045
iteration: 70 loss: 0.7461353293899099 grad: -0.3031771880974675
iteration: 80 loss: 0.6574460251908518 grad: -0.2730688300974061
iteration: 90 loss: 0.5877131510694033 grad: -0.24839706568191539
iteration: 100 loss: 0.5314326082814825 grad: -0.22783438968131803
iteration: 0 loss: 66.98120429048491 grad: 465.5707741240989
iteration: 10 loss: 3.5486561251933297 grad: -0.884759549547482
iteration: 20 loss: 2.0702395116928445 grad: -0.8367109666281952
iteration: 30 loss: 1.4811539540728518 grad: -0.6658564400391696
iteration: 40 loss: 1.15797041708137 grad: -0.5465992036526952
iteration: 50 loss: 0.9524328292828944 grad: -0.4628103829396901
iteration: 60 loss: 0.8097424574313912 grad: -0.4012684873919592
iteration: 70 loss: 0.7047102519434868 grad: -0.3542622072228623
iteration: 80 loss: 0.6240774010431612 grad: -0.3172088324746919
iteration: 90 loss: 0.5601796452228882 grad: -0.28725216344831084
iteration: 0 loss: 43.22897828720093 grad: 397.377289722789
iteration: 10 loss: 4.339142001468397 grad: -0.5462576121936853
iteration: 20 loss: 2.41450437081837 grad: -0.6632264920343334
iteration: 30 loss: 1.680043108274731 grad: -0.5536627708872818
iteration: 40 loss: 1.2903303901778764 grad: -0.4640507358606851
iteration: 50 loss: 1.0482747532477417 grad: -0.3974748499755637
iteration: 60 loss: 0.8831490374870389 grad: -0.3471807843265274
iteration: 70 loss: 0.763230695249025 grad: -0.3081164219134477
iteration: 80 loss: 0.6721512856564996 grad: -0.27698003049940456
iteration: 90 loss: 0.6006026814260291 grad: -0.2516076758977396
iteration: 100 loss: 0.5428980296567054 grad: -0.23054331794912603
iteration: 0 loss: 46.58508382358979 grad: 371.91489987819284
iteration: 10 loss: 4.216876937600236 grad: -0.5310775967320351
iteration: 20 loss: 2.3621832030964716 grad: -0.7010310451317638
iteration: 30 loss: 1.6487094568819884 grad: -0.5883397619284665
iteration: 40 loss: 1.2684535169267044 grad: -0.4934444688366231
iteration: 50 loss: 1.0316304530992553 grad: -0.42248764697111174
iteration: 60 loss: 0.8697799511397865 grad: -0.36876771232054756
iteration: 70 loss: 0.7520869291784038 grad: -0.3270128300296452
iteration: 80 loss: 0.662610375631547 grad: -0.29372934093470393
iteration: 90 loss: 0.5922676702803074 grad: -0.2666135278510198
iteration: 100 loss: 0.5355013421171448 grad: -0.24411037169489236
iteration: 0 loss: 52.612600931895734 grad: 390.07010370712544
iteration: 10 loss: 3.983832842130472 grad: -0.4700696885770182
iteration: 20 loss: 2.2654478409721874 grad: -0.6817776182005321
iteration: 30 loss: 1.5947683880998822 grad: -0.5744381714295077
iteration: 40 loss: 1.2335770324036937 grad: -0.48251339983188785
iteration: 50 loss: 1.006985423908356 grad: -0.41354035924103755
iteration: 60 loss: 0.8513007294740088 grad: -0.3612391138965639
iteration: 70 loss: 0.7376305517827859 grad: -0.32054416189123386
iteration: 80 loss: 0.6509352261928143 grad: -0.2880791712843879
iteration: 90 loss: 0.5826021984176308 grad: -0.2616123003722748
iteration: 100 loss: 0.5273394975502015 grad: -0.23963488327584992
iteration: 0 loss: 55.578839208916676 grad: 405.22699410150375
iteration: 10 loss: 4.07077103902603 grad: -0.8865383960031263
iteration: 20 loss: 2.3149915484417845 grad: -0.888259609475089
iteration: 30 loss: 1.6277061084404687 grad: -0.7031142977723215
iteration: 40 loss: 1.2576724159030164 grad: -0.573316347787465
iteration: 50 loss: 1.0257307097212602 grad: -0.48250973243193496
iteration: 60 loss: 0.866513669442859 grad: -0.4162155492573698
iteration: 70 loss: 0.7503610418108441 grad: -0.36588438476933327
iteration: 80 loss: 0.6618378106526589 grad: -0.32642959839869934
iteration: 90 loss: 0.5921093922404246 grad: -0.29468929221553114
iteration: 100 loss: 0.5357504648384499 grad: -0.2686089644015942
iteration: 0 loss: 47.393100545553644 grad: 391.96047456875726
iteration: 10 loss: 4.164048927805073 grad: -0.8068859283966112
iteration: 20 loss: 2.334873815826582 grad: -0.8620531307155326
iteration: 30 loss: 1.6305111345478012 grad: -0.6909869028979704
iteration: 40 loss: 1.2547984191719892 grad: -0.5666400556414086
iteration: 50 loss: 1.0206852478788113 grad: -0.4785234659986819
iteration: 60 loss: 0.8606346910582116 grad: -0.41375104076937963
iteration: 70 loss: 0.7442253063671326 grad: -0.3643584126212511
iteration: 80 loss: 0.65571135691293 grad: -0.3255178477389393
iteration: 90 loss: 0.5861179477098465 grad: -0.29419717428940073
iteration: 100 loss: 0.5299518866058861 grad: -0.2684129251085385
iteration: 0 loss: 50.80245776844851 grad: 374.79507973804
iteration: 10 loss: 4.131449100283135 grad: -0.4935234805399812
iteration: 20 loss: 2.3218202239979537 grad: -0.729707492039464
iteration: 30 loss: 1.6231644774255238 grad: -0.6146709936548012
iteration: 40 loss: 1.249955975034151 grad: -0.5152864478235786
iteration: 50 loss: 1.0171925364966228 grad: -0.44075238425744034
iteration: 60 loss: 0.8579640881823913 grad: -0.38432462141942964
iteration: 70 loss: 0.7420979210828591 grad: -0.3404954209575104
iteration: 80 loss: 0.6539644645326503 grad: -0.3055886142228036
iteration: 90 loss: 0.584649535626175 grad: -0.2771752993542299
iteration: 100 loss: 0.5286944079109823 grad: -0.2536152495575715
iteration: 0 loss: 48.3698554246895 grad: 438.1859854239368
iteration: 10 loss: 3.852616878273073 grad: -1.1933674621687977
iteration: 20 loss: 2.2110356004134455 grad: -0.9899088204148979
iteration: 30 loss: 1.5628846275158552 grad: -0.7564724427416176
iteration: 40 loss: 1.2118605145603232 grad: -0.6085099686245423
iteration: 50 loss: 0.9908797194387013 grad: -0.5084952834975636
iteration: 60 loss: 0.8386801060104186 grad: -0.436708509079528
iteration: 70 loss: 0.7273517659869785 grad: -0.38275058330582706
iteration: 80 loss: 0.6423213335459547 grad: -0.34073004154919007
iteration: 90 loss: 0.5752233474173739 grad: -0.3070822674855367
iteration: 100 loss: 0.5209078038531681 grad: -0.2795300190145129
iteration: 0 loss: 50.382191783951924 grad: 351.85600449087485
iteration: 10 loss: 4.108485547000575 grad: -0.2944357802346628
iteration: 20 loss: 2.3118727740705833 grad: -0.6177850447486084
iteration: 30 loss: 1.6175960428663942 grad: -0.5411916213094173
iteration: 40 loss: 1.2464297941392033 grad: -0.46198455781868875
iteration: 50 loss: 1.0147884616087801 grad: -0.39958472456130123
iteration: 60 loss: 0.8562431652167519 grad: -0.3511519596514095
iteration: 70 loss: 0.7408233456924542 grad: -0.31293695405644634
iteration: 80 loss: 0.6529969816065204 grad: -0.28216125493590966
iteration: 90 loss: 0.5839018392561311 grad: -0.25689851284050147
iteration: 100 loss: 0.5281089826591845 grad: -0.23581018271552895
iteration: 0 loss: 43.83052810347123 grad: 361.9573927356154
iteration: 10 loss: 4.199925861553269 grad: -0.39469581942929155
iteration: 20 loss: 2.3499753184263352 grad: -0.6736080271641618
iteration: 30 loss: 1.6396328652505294 grad: -0.5764781030454829
iteration: 40 loss: 1.2612777219575781 grad: -0.4862317629696005
iteration: 50 loss: 1.0257140074967621 grad: -0.41727930267934454
iteration: 60 loss: 0.8647556826449545 grad: -0.36464065760107606
iteration: 70 loss: 0.7477265085407949 grad: -0.32356152375702507
iteration: 80 loss: 0.6587625352363333 grad: -0.2907445603925272
iteration: 90 loss: 0.5888271819713654 grad: -0.263973829180408
iteration: 100 loss: 0.5323921268046351 grad: -0.24173872676170255
iteration: 0 loss: 61.02675611229775 grad: 381.57686896673266
iteration: 10 loss: 3.839008497140061 grad: 0.04416218697327186
iteration: 20 loss: 2.200736629705551 grad: -0.5441213033399426
iteration: 30 loss: 1.554775632633912 grad: -0.511839691084891
iteration: 40 loss: 1.204975423102207 grad: -0.44736603552760645
iteration: 50 loss: 0.9848278307887003 grad: -0.3914610976115544
iteration: 60 loss: 0.833258044361752 grad: -0.3463779986464086
iteration: 70 loss: 0.7224326487284998 grad: -0.3100724011872701
iteration: 80 loss: 0.6378170195603988 grad: -0.2804626975684414
iteration: 90 loss: 0.5710685667828247 grad: -0.25594814935880394
iteration: 100 loss: 0.5170522233593147 grad: -0.23535780230945252
iteration: 0 loss: 42.83994290527101 grad: 322.67611567484096
iteration: 10 loss: 4.269778129254022 grad: -0.13732438681007503
iteration: 20 loss: 2.3907820906904984 grad: -0.615045009509593
iteration: 30 loss: 1.6683513838596893 grad: -0.551345254439247
iteration: 40 loss: 1.2833257858577864 grad: -0.4719806539554449
iteration: 50 loss: 1.0435497799527516 grad: -0.4078300872343369
iteration: 60 loss: 0.8796988760961374 grad: -0.35772839556610786
iteration: 70 loss: 0.7605653196025163 grad: -0.3181600946919083
iteration: 80 loss: 0.6700042284019632 grad: -0.2863231411453847
iteration: 90 loss: 0.5988168686934511 grad: -0.26023005522296455
iteration: 100 loss: 0.5413749198212578 grad: -0.2384872737795118
iteration: 0 loss: 58.89602887860269 grad: 480.4341650022949
iteration: 10 loss: 3.4059684925150164 grad: -0.2698009907925848
iteration: 20 loss: 1.982745048154522 grad: -0.7801791348579719
iteration: 30 loss: 1.4224468240748716 grad: -0.6222131220025158
iteration: 40 loss: 1.1151785941666608 grad: -0.5137826793754657
iteration: 50 loss: 0.9194771735609389 grad: -0.43722179763177826
iteration: 60 loss: 0.7833583575815365 grad: -0.38064322318625315
iteration: 70 loss: 0.6829705530258252 grad: -0.3371914784512089
iteration: 80 loss: 0.6057620985444704 grad: -0.3027797337687586
iteration: 90 loss: 0.5444740341961899 grad: -0.27484739912860434
iteration: 0 loss: 53.00139101201933 grad: 347.224939460163
iteration: 10 loss: 4.0188113575117805 grad: 0.30188001611054316
iteration: 20 loss: 2.2678557800601413 grad: -0.43191100615404293
iteration: 30 loss: 1.5907510367014766 grad: -0.44411378024125475
iteration: 40 loss: 1.227849846867643 grad: -0.39986114701622494
iteration: 50 loss: 1.0008925265856718 grad: -0.3552691032771549
iteration: 60 loss: 0.8452988581574242 grad: -0.3173388883487456
iteration: 70 loss: 0.7318810803886994 grad: -0.28593359075578834
iteration: 80 loss: 0.6454878422210558 grad: -0.25987619935998724
iteration: 90 loss: 0.5774615949587976 grad: -0.2380470385723224
iteration: 100 loss: 0.5224921589999046 grad: -0.21955337665493357
iteration: 0 loss: 42.05951148755664 grad: 424.2922688496386
iteration: 10 loss: 4.24379077132591 grad: -1.3533557533205092
iteration: 20 loss: 2.3737514560481774 grad: -1.0060242110345032
iteration: 30 loss: 1.6539665419070595 grad: -0.7601788844743081
iteration: 40 loss: 1.2709082534659253 grad: -0.6082045791713905
iteration: 50 loss: 1.0326689769815196 grad: -0.506621865784972
iteration: 60 loss: 0.8700363958564594 grad: -0.43419564459266735
iteration: 70 loss: 0.7518851957748169 grad: -0.3800016520570717
iteration: 80 loss: 0.6621297891088861 grad: -0.3379337849165659
iteration: 90 loss: 0.5916136696330547 grad: -0.3043300092530858
iteration: 100 loss: 0.5347388304021892 grad: -0.2768656264695881
iteration: 0 loss: 63.823035299800246 grad: 388.7382025883692
iteration: 10 loss: 3.890288290300842 grad: 0.30141887762225805
iteration: 20 loss: 2.2266433670668615 grad: -0.4301279410208584
iteration: 30 loss: 1.5730900476406864 grad: -0.4460359127184793
iteration: 40 loss: 1.2194100772560226 grad: -0.4031151851631892
iteration: 50 loss: 0.9968292639320101 grad: -0.35892227102739027
iteration: 60 loss: 0.8435653186539143 grad: -0.321028066863529
iteration: 70 loss: 0.7314827666839185 grad: -0.28951385866429413
iteration: 80 loss: 0.6458929689735503 grad: -0.26329230013749716
iteration: 90 loss: 0.578365380620281 grad: -0.24128259314256573
iteration: 100 loss: 0.5237105924082822 grad: -0.2226092581775677
iteration: 0 loss: 61.35957486311097 grad: 333.1704458208643
iteration: 10 loss: 4.047791942211998 grad: 0.37754801527596454
iteration: 20 loss: 2.2963029221406774 grad: -0.4104017045950646
iteration: 30 loss: 1.6143481255296799 grad: -0.4264049737231792
iteration: 40 loss: 1.2476058495777496 grad: -0.38423846841700016
iteration: 50 loss: 1.0177885199981243 grad: -0.34137321623322026
iteration: 60 loss: 0.8600267695550511 grad: -0.3048921207658901
iteration: 70 loss: 0.7449212588617229 grad: -0.2747003308144059
iteration: 80 loss: 0.6571813906563296 grad: -0.24966327924398882
iteration: 90 loss: 0.5880575742741546 grad: -0.22869853700861462
iteration: 100 loss: 0.532177260693805 grad: -0.21094363030616328
iteration: 0 loss: 46.311694805774756 grad: 366.16542503409187
iteration: 10 loss: 4.185547728439112 grad: -0.5129279852486335
iteration: 20 loss: 2.347361948431874 grad: -0.7284562182081744
iteration: 30 loss: 1.6391834558975957 grad: -0.6098669412539639
iteration: 40 loss: 1.2614797041990806 grad: -0.5096301424756211
iteration: 50 loss: 1.0261494361821777 grad: -0.43506650175513983
iteration: 60 loss: 0.8652759330184794 grad: -0.37888504748297147
iteration: 70 loss: 0.7482709475545396 grad: -0.33538561583333465
iteration: 80 loss: 0.6593045569758776 grad: -0.3008194094732028
iteration: 90 loss: 0.5893548617120608 grad: -0.27273005940570433
iteration: 100 loss: 0.532900351431222 grad: -0.24946800699671368
iteration: 0 loss: 51.04167751586037 grad: 412.1645960668102
iteration: 10 loss: 3.945430760437706 grad: -0.6214345128905003
iteration: 20 loss: 2.243407527088576 grad: -0.7677286402926098
iteration: 30 loss: 1.578748191297605 grad: -0.6337977579681475
iteration: 40 loss: 1.2208352588736902 grad: -0.5274499377315833
iteration: 50 loss: 0.9963560090057565 grad: -0.4495157621980238
iteration: 60 loss: 0.8421615911429399 grad: -0.39113110358051456
iteration: 70 loss: 0.7296049406626149 grad: -0.34604732513373815
iteration: 80 loss: 0.6437756853041389 grad: -0.3102730114896435
iteration: 90 loss: 0.5761366803423119 grad: -0.2812256709999042
iteration: 100 loss: 0.5214432020528131 grad: -0.25718239053831954
iteration: 0 loss: 56.36351459475306 grad: 414.1729851560669
iteration: 10 loss: 3.887685495699233 grad: -0.5074479951205269
iteration: 20 loss: 2.224593189089567 grad: -0.7361883231666831
iteration: 30 loss: 1.5716554367585642 grad: -0.6195373408806844
iteration: 40 loss: 1.2184331230695202 grad: -0.5196252603257178
iteration: 50 loss: 0.9961512736783995 grad: -0.44475657690378934
iteration: 60 loss: 0.8430826372622587 grad: -0.3880591008610642
iteration: 70 loss: 0.7311308147452358 grad: -0.3439961008797223
iteration: 80 loss: 0.6456309621084156 grad: -0.3088817363989963
iteration: 90 loss: 0.5781668687146024 grad: -0.2802823780026777
iteration: 100 loss: 0.5235579354313648 grad: -0.25655468099237266
iteration: 0 loss: 53.16342867066956 grad: 429.73940776711834
iteration: 10 loss: 3.690823014008484 grad: -0.8109351558336256
iteration: 20 loss: 2.1262787667677583 grad: -0.8433501921438675
iteration: 30 loss: 1.5084022883717643 grad: -0.6692263824650653
iteration: 40 loss: 1.172645209014759 grad: -0.5480502974998505
iteration: 50 loss: 0.9606422957887678 grad: -0.4630823288992766
iteration: 60 loss: 0.8142695807774462 grad: -0.4008091506214268
iteration: 70 loss: 0.7069907278467905 grad: -0.3533434147851958
iteration: 80 loss: 0.6249192035017007 grad: -0.31599906020710294
iteration: 90 loss: 0.5600676230608521 grad: -0.28585854172639796
iteration: 0 loss: 44.725827951233605 grad: 380.2156802227186
iteration: 10 loss: 4.1500472034732745 grad: -0.6610995569504683
iteration: 20 loss: 2.325985521533684 grad: -0.7656594010055083
iteration: 30 loss: 1.6238293584663304 grad: -0.6242214991002916
iteration: 40 loss: 1.2494067449007078 grad: -0.5163381185220615
iteration: 50 loss: 1.016161743280918 grad: -0.43846331472741473
iteration: 60 loss: 0.8567390298587104 grad: -0.3806156522767056
iteration: 70 loss: 0.7408053832065723 grad: -0.33619218064767836
iteration: 80 loss: 0.6526643650640008 grad: -0.3010782783591571
iteration: 90 loss: 0.5833710669978907 grad: -0.27264882632589305
iteration: 100 loss: 0.5274516479942328 grad: -0.24916866452401198
iteration: 0 loss: 47.621863618483985 grad: 438.56083407208246
iteration: 10 loss: 3.929975590042638 grad: -1.1644605062901119
iteration: 20 loss: 2.2437368051404603 grad: -0.9356010059655574
iteration: 30 loss: 1.5798070746067507 grad: -0.7198503806606998
iteration: 40 loss: 1.2215711034687047 grad: -0.5812373841063594
iteration: 50 loss: 0.9967613224929334 grad: -0.4869715906114881
iteration: 60 loss: 0.8423241612304082 grad: -0.4190596207518018
iteration: 70 loss: 0.7295987320267159 grad: -0.36787854282838084
iteration: 80 loss: 0.6436530835046803 grad: -0.3279378879296354
iteration: 90 loss: 0.5759336633941943 grad: -0.2959010275565688
iteration: 100 loss: 0.5211845276264946 grad: -0.2696297668721184
iteration: 0 loss: 51.15716665395689 grad: 388.6138363667177
iteration: 10 loss: 4.111475004356552 grad: -0.36623515847807986
iteration: 20 loss: 2.315308362324851 grad: -0.6763745929397333
iteration: 30 loss: 1.6207994226075575 grad: -0.5802171611908592
iteration: 40 loss: 1.2492110961467338 grad: -0.4898581588359906
iteration: 50 loss: 1.0171874311180213 grad: -0.4206875146903938
iteration: 60 loss: 0.8583289114693468 grad: -0.3678262821494561
iteration: 70 loss: 0.742656591714463 grad: -0.32653905745444955
iteration: 80 loss: 0.6546255939047114 grad: -0.29353252148315373
iteration: 90 loss: 0.5853627196285164 grad: -0.26659066316728214
iteration: 100 loss: 0.5294306386286652 grad: -0.24420148543620473
iteration: 0 loss: 42.25251784766281 grad: 346.33808235172876
iteration: 10 loss: 4.348390952242673 grad: -0.4133080899139532
iteration: 20 loss: 2.4242220026306893 grad: -0.6333556571213986
iteration: 30 loss: 1.6877739936355118 grad: -0.5369985234942345
iteration: 40 loss: 1.2965758549707145 grad: -0.45181437265972235
iteration: 50 loss: 1.053471348313327 grad: -0.3875045761750251
iteration: 60 loss: 0.8875831660957374 grad: -0.33864259934958396
iteration: 70 loss: 0.7670907316413501 grad: -0.30059351016821
iteration: 80 loss: 0.6755654601464481 grad: -0.27022799782155504
iteration: 90 loss: 0.6036614781866608 grad: -0.24546759634511775
iteration: 100 loss: 0.5456673819789185 grad: -0.2249043155054979
iteration: 0 loss: 46.27395657940119 grad: 377.6159203166611
iteration: 10 loss: 4.247347558208276 grad: -0.6986486282665896
iteration: 20 loss: 2.368559429290971 grad: -0.7801427219969532
iteration: 30 loss: 1.6500606557441726 grad: -0.6318743079631461
iteration: 40 loss: 1.2681707483316544 grad: -0.5214524899447631
iteration: 50 loss: 1.030716464104733 grad: -0.4422923295412934
iteration: 60 loss: 0.8686107429208195 grad: -0.3836684710166988
iteration: 70 loss: 0.7508238900073924 grad: -0.33872480344138156
iteration: 80 loss: 0.6613284680020528 grad: -0.30323798807876934
iteration: 90 loss: 0.5910030848148463 grad: -0.2745283265351885
iteration: 100 loss: 0.534271530127295 grad: -0.25083013353668016
iteration: 0 loss: 39.59516832461273 grad: 363.42695869982845
iteration: 10 loss: 4.354708526112683 grad: -0.9043937938515082
iteration: 20 loss: 2.424856120711452 grad: -0.8159378936288608
iteration: 30 loss: 1.6865286731836022 grad: -0.6434010004840647
iteration: 40 loss: 1.29473195315553 grad: -0.5251193984301793
iteration: 50 loss: 1.0514529308786884 grad: -0.4426564396950343
iteration: 60 loss: 0.8855501301718918 grad: -0.3824601530685904
iteration: 70 loss: 0.7651072278781612 grad: -0.3367207529014946
iteration: 80 loss: 0.6736567059959022 grad: -0.3008254869896217
iteration: 90 loss: 0.6018356648960022 grad: -0.2719145798913055
iteration: 100 loss: 0.5439249773549322 grad: -0.24813141824272708
iteration: 0 loss: 46.5276192852657 grad: 388.52818553835357
iteration: 10 loss: 4.2373333388047945 grad: -0.42604969690663713
iteration: 20 loss: 2.366402774078159 grad: -0.6617815144432114
iteration: 30 loss: 1.6500194145136944 grad: -0.5624948429282397
iteration: 40 loss: 1.268861286038745 grad: -0.47360397827763256
iteration: 50 loss: 1.0316852001030632 grad: -0.4062303073580547
iteration: 60 loss: 0.8696803535007844 grad: -0.3549519412063637
iteration: 70 loss: 0.7519173783323574 grad: -0.314989150367574
iteration: 80 loss: 0.6624104898953026 grad: -0.283085296693521
iteration: 90 loss: 0.5920572511264172 grad: -0.2570678070447021
iteration: 100 loss: 0.5352906356191108 grad: -0.2354612470005718
iteration: 0 loss: 53.958596623295186 grad: 445.22120339277245
iteration: 10 loss: 3.9056241394256093 grad: -0.47826114761521527
iteration: 20 loss: 2.233939516643328 grad: -0.6782477367427
iteration: 30 loss: 1.5784320860387362 grad: -0.5768587630205931
iteration: 40 loss: 1.2238824484611954 grad: -0.48752112433394607
iteration: 50 loss: 1.000744720821372 grad: -0.4195241985110588
iteration: 60 loss: 0.84706372381565 grad: -0.3674886484849199
iteration: 70 loss: 0.7346472973148676 grad: -0.3267412952324237
iteration: 80 loss: 0.6487811880852363 grad: -0.2940822605333126
iteration: 90 loss: 0.5810203022950357 grad: -0.26736261011410273
iteration: 100 loss: 0.5261657299251138 grad: -0.2451138447013508
iteration: 0 loss: 42.93105668459213 grad: 370.6175695974776
iteration: 10 loss: 4.329534060284129 grad: -0.4370926422355743
iteration: 20 loss: 2.410411847138965 grad: -0.6505269374258132
iteration: 30 loss: 1.677298986069011 grad: -0.549072800860599
iteration: 40 loss: 1.288197255766053 grad: -0.4610144361507176
iteration: 50 loss: 1.0465076098043524 grad: -0.39491550706788364
iteration: 60 loss: 0.8816328030319792 grad: -0.3448437063351445
iteration: 70 loss: 0.7618999116629394 grad: -0.3059249049008931
iteration: 80 loss: 0.6709642478977003 grad: -0.274904898712508
iteration: 90 loss: 0.5995308062441093 grad: -0.24963435259779493
iteration: 100 loss: 0.5419207106170606 grad: -0.22866228828260066
iteration: 0 loss: 51.05607185234174 grad: 414.66074046108656
iteration: 10 loss: 3.8718271097401074 grad: -0.9203239569193841
iteration: 20 loss: 2.21647739030397 grad: -0.8872824023268955
iteration: 30 loss: 1.563323174183717 grad: -0.7026478070363418
iteration: 40 loss: 1.2102216118047513 grad: -0.5739875227556374
iteration: 50 loss: 0.9883075346561285 grad: -0.48387493481518024
iteration: 60 loss: 0.8356870424430863 grad: -0.4179623799697923
iteration: 70 loss: 0.7241884622583463 grad: -0.3678296777596094
iteration: 80 loss: 0.6391173632827386 grad: -0.3284674273706137
iteration: 90 loss: 0.5720476916599595 grad: -0.29675777128073805
iteration: 100 loss: 0.5177973207155446 grad: -0.27067160414727576
iteration: 0 loss: 41.20388997402795 grad: 440.16582189558704
iteration: 10 loss: 4.227366101331179 grad: -1.1566350812014445
iteration: 20 loss: 2.37830069103645 grad: -0.899421825136398
iteration: 30 loss: 1.6625311146329047 grad: -0.6894276313614498
iteration: 40 loss: 1.2800923669647504 grad: -0.5558514151133289
iteration: 50 loss: 1.0415822360771265 grad: -0.465339487291524
iteration: 60 loss: 0.8784377028950572 grad: -0.4002554257594749
iteration: 70 loss: 0.7597336311645946 grad: -0.3512634795809813
iteration: 80 loss: 0.6694500050874178 grad: -0.3130621710118207
iteration: 90 loss: 0.5984501574780552 grad: -0.28243855605358964
iteration: 100 loss: 0.5411393853885536 grad: -0.25733743593673364
iteration: 0 loss: 43.884131915845664 grad: 312.96721758369847
iteration: 10 loss: 4.342443214257855 grad: 0.20854001186588117
iteration: 20 loss: 2.411365793057195 grad: -0.29992004631291985
iteration: 30 loss: 1.6770185669103574 grad: -0.3174040470457943
iteration: 40 loss: 1.2878760322396374 grad: -0.29079065979188967
iteration: 50 loss: 1.0463186048361257 grad: -0.26163214322463213
iteration: 60 loss: 0.8815798804399823 grad: -0.23600353971202975
iteration: 70 loss: 0.7619582154063044 grad: -0.21435635064196817
iteration: 80 loss: 0.6711083252742934 grad: -0.19613692071079503
iteration: 90 loss: 0.5997398480824152 grad: -0.1807032132076072
iteration: 100 loss: 0.5421786724887583 grad: -0.16750839713634644
iteration: 0 loss: 41.48253621313438 grad: 350.12098134732923
iteration: 10 loss: 4.386818697216437 grad: -0.704849437191155
iteration: 20 loss: 2.436876118663101 grad: -0.7421810406492175
iteration: 30 loss: 1.6936524048404118 grad: -0.6013337099108553
iteration: 40 loss: 1.299762396250034 grad: -0.4968043676699661
iteration: 50 loss: 1.055340370666851 grad: -0.4218110849568977
iteration: 60 loss: 0.8887212897223187 grad: -0.36622431828551927
iteration: 70 loss: 0.7677882092154339 grad: -0.32357551217092795
iteration: 80 loss: 0.6759811839583563 grad: -0.28987704661371844
iteration: 90 loss: 0.6038890759207127 grad: -0.26259686607657196
iteration: 100 loss: 0.5457652006781576 grad: -0.24006549397716015
iteration: 0 loss: 41.37205736490953 grad: 385.37356221840724
iteration: 10 loss: 4.280833092265311 grad: -0.08504713525637932
iteration: 20 loss: 2.374943170148485 grad: -0.4695525182355385
iteration: 30 loss: 1.6522723627996874 grad: -0.441858055344699
iteration: 40 loss: 1.269307788485879 grad: -0.3891562455237188
iteration: 50 loss: 1.0314961436302341 grad: -0.34277988370242785
iteration: 60 loss: 0.869251114654286 grad: -0.304922113281226
iteration: 70 loss: 0.7514025627266495 grad: -0.27414730226803546
iteration: 80 loss: 0.6618758534135519 grad: -0.24886184220677227
iteration: 90 loss: 0.5915317953686926 grad: -0.22780155941820535
iteration: 100 loss: 0.5347869410818682 grad: -0.2100243512208106
iteration: 0 loss: 44.64584644597248 grad: 374.32415046641574
iteration: 10 loss: 4.2585200473663045 grad: -0.31414623595687297
iteration: 20 loss: 2.3801123285037318 grad: -0.6276522305271095
iteration: 30 loss: 1.6594174148023162 grad: -0.5417855510938828
iteration: 40 loss: 1.2758348788069354 grad: -0.4580105589733552
iteration: 50 loss: 1.0371572613149427 grad: -0.3934282056557908
iteration: 60 loss: 0.8741470489938944 grad: -0.3439705894429315
iteration: 70 loss: 0.7556704162644068 grad: -0.3053183261536373
iteration: 80 loss: 0.6656339219185529 grad: -0.2744162347739344
iteration: 90 loss: 0.5948738103637962 grad: -0.24919584048464363
iteration: 100 loss: 0.5377858497632505 grad: -0.2282419848754669
iteration: 0 loss: 50.24079284109528 grad: 328.89191229472726
iteration: 10 loss: 4.266052312746772 grad: 0.16154186496703904
iteration: 20 loss: 2.39297877478194 grad: -0.49598399565165263
iteration: 30 loss: 1.6715954692010035 grad: -0.47752959642806025
iteration: 40 loss: 1.286698997242514 grad: -0.419405195152293
iteration: 50 loss: 1.0468124653279203 grad: -0.3675460126006321
iteration: 60 loss: 0.8827861804380022 grad: -0.32538692361527577
iteration: 70 loss: 0.7634671490853758 grad: -0.29132965283995305
iteration: 80 loss: 0.672728846149353 grad: -0.26351521392978966
iteration: 90 loss: 0.6013782567037279 grad: -0.24047244272295865
iteration: 100 loss: 0.5437880843399564 grad: -0.22111300001226392
iteration: 0 loss: 46.67942896126701 grad: 417.2536300098633
iteration: 10 loss: 4.064249999405134 grad: -0.689565516870734
iteration: 20 loss: 2.2957276406114446 grad: -0.7851888940532112
iteration: 30 loss: 1.6092218070763342 grad: -0.6403403320845522
iteration: 40 loss: 1.2412733407885836 grad: -0.5296869791410728
iteration: 50 loss: 1.011270023732405 grad: -0.4497235132209049
iteration: 60 loss: 0.8536722592668246 grad: -0.3902961366149329
iteration: 70 loss: 0.7388506472315054 grad: -0.34465371820078483
iteration: 80 loss: 0.651426823248626 grad: -0.30857814890077406
iteration: 90 loss: 0.5826160552822909 grad: -0.2793742068013661
iteration: 100 loss: 0.5270319481033684 grad: -0.25525881794759914
iteration: 0 loss: 60.04990158356784 grad: 376.7034541467402
iteration: 10 loss: 3.885636448883154 grad: 0.1890432292856471
iteration: 20 loss: 2.2091988460810255 grad: -0.4530521523317605
iteration: 30 loss: 1.5556110063974466 grad: -0.45142571435861173
iteration: 40 loss: 1.203611288963657 grad: -0.4038352196338938
iteration: 50 loss: 0.9827520372125285 grad: -0.3581756028331223
iteration: 60 loss: 0.8309801269639469 grad: -0.3198245960486268
iteration: 70 loss: 0.7201485005148243 grad: -0.2882162570572101
iteration: 80 loss: 0.635604989246448 grad: -0.26203789761445373
iteration: 90 loss: 0.5689583858676005 grad: -0.24012157829681163
iteration: 100 loss: 0.5150523124722729 grad: -0.2215559957438738
iteration: 0 loss: 48.02043116821524 grad: 405.8417829636219
iteration: 10 loss: 4.129200872054753 grad: -1.1156395913711123
iteration: 20 loss: 2.3249525028102407 grad: -0.9358981157692452
iteration: 30 loss: 1.6251727059846581 grad: -0.7239928839042868
iteration: 40 loss: 1.2510793490650114 grad: -0.5849827595973176
iteration: 50 loss: 1.0177583475388845 grad: -0.4899067347849091
iteration: 60 loss: 0.8581779670817841 grad: -0.4212885898620078
iteration: 70 loss: 0.7420841488338241 grad: -0.3695553468142958
iteration: 80 loss: 0.6537999845554623 grad: -0.32919162539688474
iteration: 90 loss: 0.5843835453106294 grad: -0.2968299267856829
iteration: 100 loss: 0.5283593295454755 grad: -0.2703070636155447
iteration: 0 loss: 43.28230778608202 grad: 413.997591576292
iteration: 10 loss: 4.144524838200348 grad: -0.7615343564454569
iteration: 20 loss: 2.3317071321782596 grad: -0.7989387090237824
iteration: 30 loss: 1.631590888778481 grad: -0.6467059559697175
iteration: 40 loss: 1.2572566289242622 grad: -0.5338121316817104
iteration: 50 loss: 1.0236031477303102 grad: -0.4528401105107988
iteration: 60 loss: 0.8636636522837762 grad: -0.3928401670455045
iteration: 70 loss: 0.7472211223599019 grad: -0.34682243738304896
iteration: 80 loss: 0.6586132102347619 grad: -0.31047721278707885
iteration: 90 loss: 0.5889019701465424 grad: -0.28106742585707745
iteration: 100 loss: 0.5326114005162129 grad: -0.25678822488789865
iteration: 0 loss: 60.50576466216626 grad: 466.19279800907395
iteration: 10 loss: 3.5322431262933276 grad: -0.5869266609672101
iteration: 20 loss: 2.0553298805593427 grad: -0.8351616535661396
iteration: 30 loss: 1.4685267560960462 grad: -0.6854186670504869
iteration: 40 loss: 1.1470210592101053 grad: -0.5689481726275721
iteration: 50 loss: 0.9427542668895512 grad: -0.4841731629749084
iteration: 60 loss: 0.8010628141246756 grad: -0.42085556632746834
iteration: 70 loss: 0.6968383323307783 grad: -0.3720435191810314
iteration: 80 loss: 0.61687296808817 grad: -0.3333520628752329
iteration: 90 loss: 0.5535364809494984 grad: -0.3019605777952379
iteration: 0 loss: 49.40059550199017 grad: 347.2273310437707
iteration: 10 loss: 4.276667379785274 grad: -0.0289864427157687
iteration: 20 loss: 2.392254445134313 grad: -0.5080134110310175
iteration: 30 loss: 1.6691241548467752 grad: -0.4740041943379829
iteration: 40 loss: 1.2840322236963237 grad: -0.4140824755250452
iteration: 50 loss: 1.0442800352837531 grad: -0.36259387423064776
iteration: 60 loss: 0.8804548678516023 grad: -0.32114875671249116
iteration: 70 loss: 0.7613357292849204 grad: -0.2877771604950461
iteration: 80 loss: 0.6707784758135528 grad: -0.2605484432713525
iteration: 90 loss: 0.5995871938879728 grad: -0.2379906212758719
iteration: 100 loss: 0.542136072085751 grad: -0.2190301826541109
iteration: 0 loss: 49.91775358739044 grad: 412.03099524218464
iteration: 10 loss: 4.037171052539303 grad: -1.0979952694059005
iteration: 20 loss: 2.2951508801742917 grad: -0.9423214132285558
iteration: 30 loss: 1.6120507942277982 grad: -0.7311291156588148
iteration: 40 loss: 1.2445823321207337 grad: -0.5917622656007324
iteration: 50 loss: 1.0144566198190328 grad: -0.49616247097935173
iteration: 60 loss: 0.8566053249252367 grad: -0.42703094018892734
iteration: 70 loss: 0.7415196387861546 grad: -0.37483535655182565
iteration: 80 loss: 0.6538535697723918 grad: -0.33406528906534716
iteration: 90 loss: 0.584829092248459 grad: -0.3013484084599264
iteration: 100 loss: 0.5290587131825801 grad: -0.27451459207869394
iteration: 0 loss: 48.81967721550568 grad: 366.0075605903285
iteration: 10 loss: 4.089090638017462 grad: -0.3211717016053836
iteration: 20 loss: 2.303109429627098 grad: -0.632140126311183
iteration: 30 loss: 1.6117711691490513 grad: -0.5511573205588667
iteration: 40 loss: 1.2419764152568447 grad: -0.4690900343007155
iteration: 50 loss: 1.0111425269559815 grad: -0.40486376073240515
iteration: 60 loss: 0.8531370916221849 grad: -0.3552193028156076
iteration: 70 loss: 0.7381076197194819 grad: -0.31616575573008626
iteration: 80 loss: 0.6505786794835082 grad: -0.2847876879743417
iteration: 90 loss: 0.5817187831847729 grad: -0.2590783366465926
iteration: 100 loss: 0.5261172674312217 grad: -0.23764993922749278
iteration: 0 loss: 46.39209618109523 grad: 335.8608549188714
iteration: 10 loss: 4.203162541577347 grad: 0.08376034715854447
iteration: 20 loss: 2.3514910015449213 grad: -0.4333086268426737
iteration: 30 loss: 1.641111487807145 grad: -0.421782948941986
iteration: 40 loss: 1.2627695285149125 grad: -0.37414553443786724
iteration: 50 loss: 1.027185703055445 grad: -0.3302928540882281
iteration: 60 loss: 0.8661851475398262 grad: -0.29404107245993816
iteration: 70 loss: 0.7491034807698352 grad: -0.26442913178327127
iteration: 80 loss: 0.6600836519975349 grad: -0.2400482269742516
iteration: 90 loss: 0.590092688516713 grad: -0.21972253054388757
iteration: 100 loss: 0.5336040498272765 grad: -0.20255892999162656
iteration: 0 loss: 55.43622909968615 grad: 340.9681042610991
iteration: 10 loss: 4.013446965985829 grad: 0.47107848946282993
iteration: 20 loss: 2.274505158435659 grad: -0.31566557611602597
iteration: 30 loss: 1.5991337628760116 grad: -0.36531345741323
iteration: 40 loss: 1.236174528936069 grad: -0.34177287013756463
iteration: 50 loss: 1.008747967063879 grad: -0.30988527138591826
iteration: 60 loss: 0.8526105050360787 grad: -0.28041900106725426
iteration: 70 loss: 0.7386699075161066 grad: -0.25500480687855254
iteration: 80 loss: 0.6518008333668232 grad: -0.23338524615966602
iteration: 90 loss: 0.583349489622213 grad: -0.21496076088747548
iteration: 100 loss: 0.5280022716264781 grad: -0.19915290759329132
iteration: 0 loss: 40.74852832150482 grad: 337.01115568762503
iteration: 10 loss: 4.257504556183738 grad: 0.03843052039612502
iteration: 20 loss: 2.3725365810690744 grad: -0.5104311865822572
iteration: 30 loss: 1.653025394872698 grad: -0.47856225849487743
iteration: 40 loss: 1.2707209146050114 grad: -0.41720625340398965
iteration: 50 loss: 1.0330063489491708 grad: -0.3644193114575665
iteration: 60 loss: 0.8707057593240445 grad: -0.3220483733129676
iteration: 70 loss: 0.7527615271307116 grad: -0.2880363898218253
iteration: 80 loss: 0.6631340962849208 grad: -0.26036278216024433
iteration: 90 loss: 0.5926954797578219 grad: -0.2374925301515849
iteration: 100 loss: 0.5358653111098151 grad: -0.2183104958509336
iteration: 0 loss: 42.445958760424155 grad: 399.6789975182454
iteration: 10 loss: 4.246285007256533 grad: -0.9227488714567196
iteration: 20 loss: 2.372145554527337 grad: -0.836963224616671
iteration: 30 loss: 1.6522489509252822 grad: -0.6591250899861049
iteration: 40 loss: 1.269407506341767 grad: -0.5372955093491112
iteration: 50 loss: 1.031389666725731 grad: -0.4525143996578408
iteration: 60 loss: 0.8689422470314722 grad: -0.3907224661607878
iteration: 70 loss: 0.7509406967542005 grad: -0.3438281733649373
iteration: 80 loss: 0.6613062168902285 grad: -0.3070618841923268
iteration: 90 loss: 0.5908886995910673 grad: -0.2774719557377893
iteration: 100 loss: 0.534095177939972 grad: -0.25314510603858764
iteration: 0 loss: 62.81995649717749 grad: 405.110363148625
iteration: 10 loss: 3.9208256650605438 grad: -0.4068866097499123
iteration: 20 loss: 2.249256442160872 grad: -0.6818470886062701
iteration: 30 loss: 1.5896262726544343 grad: -0.584756338130709
iteration: 40 loss: 1.2323250288043461 grad: -0.4949710370524587
iteration: 50 loss: 1.0073881609043218 grad: -0.42606861947713803
iteration: 60 loss: 0.852477758894524 grad: -0.37322376513752203
iteration: 70 loss: 0.739182724433373 grad: -0.33181797002499547
iteration: 80 loss: 0.6526642448549699 grad: -0.29862871092235116
iteration: 90 loss: 0.584403386304323 grad: -0.2714780503719526
iteration: 100 loss: 0.5291554433107938 grad: -0.24887391327598016
iteration: 0 loss: 50.94498434033585 grad: 444.08857781588745
iteration: 10 loss: 3.959213449994381 grad: -1.0608116930868787
iteration: 20 loss: 2.2576175834466 grad: -0.9309585706758328
iteration: 30 loss: 1.5910804825324787 grad: -0.7187153060507685
iteration: 40 loss: 1.2315798594152263 grad: -0.5812345514705529
iteration: 50 loss: 1.0058444040083818 grad: -0.4874281809420391
iteration: 60 loss: 0.8506468447357789 grad: -0.4197150033217726
iteration: 70 loss: 0.7372747552662573 grad: -0.3686159520147848
iteration: 80 loss: 0.6507710288836445 grad: -0.32870062650932963
iteration: 90 loss: 0.5825654782008148 grad: -0.29666037950050955
iteration: 100 loss: 0.5273896798627736 grad: -0.2703710415504146
iteration: 0 loss: 55.55626042838956 grad: 421.7878471185637
iteration: 10 loss: 3.912720745368667 grad: -0.32249664371011016
iteration: 20 loss: 2.2314871892771904 grad: -0.6394032153563933
iteration: 30 loss: 1.575623744072676 grad: -0.5511943755254362
iteration: 40 loss: 1.2215136918663865 grad: -0.46811484705147655
iteration: 50 loss: 0.9988300290230488 grad: -0.40400687833058296
iteration: 60 loss: 0.8455188610067704 grad: -0.35465273635679734
iteration: 70 loss: 0.7333911229555848 grad: -0.31586447490836306
iteration: 80 loss: 0.6477497538386078 grad: -0.28469436827494543
iteration: 90 loss: 0.5801653242734904 grad: -0.2591403607241064
iteration: 100 loss: 0.5254509276527725 grad: -0.23782583669106866
iteration: 0 loss: 48.948003587032375 grad: 401.99184149644196
iteration: 10 loss: 4.146892288175133 grad: -0.894802155452795
iteration: 20 loss: 2.337218584750396 grad: -0.7895951121227749
iteration: 30 loss: 1.6355373300371783 grad: -0.6239244649148559
iteration: 40 loss: 1.2601764602871464 grad: -0.5105359715771838
iteration: 50 loss: 1.025881255528599 grad: -0.4312729355493129
iteration: 60 loss: 0.8655181996656954 grad: -0.37325485408277714
iteration: 70 loss: 0.7487803666709955 grad: -0.3290674040148925
iteration: 80 loss: 0.6599569336503919 grad: -0.2943219862640839
iteration: 90 loss: 0.5900824307235862 grad: -0.2662908691468352
iteration: 100 loss: 0.5336643646909248 grad: -0.24319892425904546
iteration: 0 loss: 58.912815362154475 grad: 421.0502544564863
iteration: 10 loss: 3.913147356730819 grad: -0.47897847706709706
iteration: 20 loss: 2.237941583162286 grad: -0.6872666831809333
iteration: 30 loss: 1.5804518282154525 grad: -0.5794494366449507
iteration: 40 loss: 1.2249598386062892 grad: -0.48666795806888796
iteration: 50 loss: 1.0013287099770671 grad: -0.4170091288707525
iteration: 60 loss: 0.8473683038262358 grad: -0.3641815240096766
iteration: 70 loss: 0.7347843736806681 grad: -0.32307807502207087
iteration: 80 loss: 0.6488136656239476 grad: -0.2902900500138538
iteration: 90 loss: 0.5809855780615346 grad: -0.26356293915965423
iteration: 100 loss: 0.5260870359325054 grad: -0.24137233281053483
iteration: 0 loss: 50.54337292535338 grad: 422.38517762521485
iteration: 10 loss: 4.022985910177877 grad: -0.49407820701061494
iteration: 20 loss: 2.282314027417628 grad: -0.6735897373621208
iteration: 30 loss: 1.6053514003060214 grad: -0.5668107895160983
iteration: 40 loss: 1.2413527530767086 grad: -0.4770151879316428
iteration: 50 loss: 1.013185471074435 grad: -0.40963285450178744
iteration: 60 loss: 0.8564894119652474 grad: -0.35842867321302757
iteration: 70 loss: 0.7421114491491029 grad: -0.3184970972403278
iteration: 80 loss: 0.654890464060207 grad: -0.28657581575077273
iteration: 90 loss: 0.5861499241925064 grad: -0.26050561633106684
iteration: 100 loss: 0.5305609618353628 grad: -0.23882394191225312
iteration: 0 loss: 64.59603614110341 grad: 361.35682513669894
iteration: 10 loss: 3.9604590283312806 grad: 0.8846640924553146
iteration: 20 loss: 2.25957840627205 grad: -0.1720439061575991
iteration: 30 loss: 1.594013538105049 grad: -0.2905106254288068
iteration: 40 loss: 1.234696727752552 grad: -0.29471614886939584
iteration: 50 loss: 1.0088871993765347 grad: -0.27695506792563784
iteration: 60 loss: 0.8535399843131845 grad: -0.25575187463704685
iteration: 70 loss: 0.7400034908953951 grad: -0.23563703341596975
iteration: 80 loss: 0.653341218468623 grad: -0.21764620478950866
iteration: 90 loss: 0.5849894320440399 grad: -0.20183125901373225
iteration: 100 loss: 0.5296809722163247 grad: -0.18797270882644623
iteration: 0 loss: 72.61417831452553 grad: 433.74385086052473
iteration: 10 loss: 3.58843502407436 grad: 0.4455528794264545
iteration: 20 loss: 2.080636852393623 grad: -0.35693607622858914
iteration: 30 loss: 1.4866305505153667 grad: -0.4100955246078637
iteration: 40 loss: 1.161786255718694 grad: -0.38377587787756795
iteration: 50 loss: 0.9554410426875702 grad: -0.34811003190393147
iteration: 60 loss: 0.8122676087910308 grad: -0.31504043609614785
iteration: 70 loss: 0.7069084060191347 grad: -0.2864468574720066
iteration: 80 loss: 0.6260361445599135 grad: -0.26208118671544417
iteration: 90 loss: 0.5619536214360944 grad: -0.2412930471304956
iteration: 0 loss: 64.55659156392336 grad: 383.26914221766725
iteration: 10 loss: 3.8818005555326223 grad: 0.010560753108994111
iteration: 20 loss: 2.220940263963092 grad: -0.5723782625934956
iteration: 30 loss: 1.5681199509525123 grad: -0.5324948329387571
iteration: 40 loss: 1.2149750523758485 grad: -0.4629437987215299
iteration: 50 loss: 0.9928309059936592 grad: -0.40364627434371586
iteration: 60 loss: 0.8399297911824678 grad: -0.35620398189935343
iteration: 70 loss: 0.7281521739116876 grad: -0.318191758790475
iteration: 80 loss: 0.6428214794589787 grad: -0.28730516166282705
iteration: 90 loss: 0.5755163677901526 grad: -0.26180802196544206
iteration: 100 loss: 0.5210544665273615 grad: -0.24044361069278453
iteration: 0 loss: 44.047442203552464 grad: 321.3900578198028
iteration: 10 loss: 4.307742507726206 grad: 0.011405428297992563
iteration: 20 loss: 2.402279501164561 grad: -0.5440284850891417
iteration: 30 loss: 1.6738864320795157 grad: -0.5014696351840869
iteration: 40 loss: 1.2866632868656944 grad: -0.4338259248614479
iteration: 50 loss: 1.04585068886945 grad: -0.3771849956462576
iteration: 60 loss: 0.8814303459090215 grad: -0.3322772435580463
iteration: 70 loss: 0.7619493770981453 grad: -0.296495184134112
iteration: 80 loss: 0.6711593285192314 grad: -0.26752883655950443
iteration: 90 loss: 0.599811861730607 grad: -0.24367979287067176
iteration: 100 loss: 0.5422525714935463 grad: -0.2237348550522609
iteration: 0 loss: 44.93915507834727 grad: 301.53256543285437
iteration: 10 loss: 4.246736729485705 grad: 0.4129508583692697
iteration: 20 loss: 2.370926797705951 grad: -0.34157745145912843
iteration: 30 loss: 1.6536867438541198 grad: -0.3766452160898137
iteration: 40 loss: 1.2721115573510196 grad: -0.34662424044036916
iteration: 50 loss: 1.0346359007799864 grad: -0.3114839788507441
iteration: 60 loss: 0.8723884464617103 grad: -0.28024074309662905
iteration: 70 loss: 0.7544207819323261 grad: -0.25380054359949333
iteration: 80 loss: 0.6647379192438827 grad: -0.23156860251091466
iteration: 90 loss: 0.5942316051086478 grad: -0.21277345325248986
iteration: 100 loss: 0.5373306296348594 grad: -0.19674267366083786
iteration: 0 loss: 49.395671903435186 grad: 335.78330321517274
iteration: 10 loss: 4.2473669063575805 grad: 0.38272614915676473
iteration: 20 loss: 2.3717116758283407 grad: -0.319359662047046
iteration: 30 loss: 1.654649097658421 grad: -0.35914257319521314
iteration: 40 loss: 1.2731516299935388 grad: -0.3332340478636934
iteration: 50 loss: 1.035691828632604 grad: -0.30087205311943205
iteration: 60 loss: 0.8734295036120702 grad: -0.27154888549128986
iteration: 70 loss: 0.7554325940070405 grad: -0.2464921259644974
iteration: 80 loss: 0.6657143624977586 grad: -0.22529526201069278
iteration: 90 loss: 0.5951707413563451 grad: -0.20729924676375533
iteration: 100 loss: 0.5382326484158594 grad: -0.19190161107996023
iteration: 0 loss: 45.01738977768737 grad: 394.21134673598783
iteration: 10 loss: 4.290803561904753 grad: -0.3686727445541933
iteration: 20 loss: 2.399125357228265 grad: -0.6178807125161926
iteration: 30 loss: 1.6734726385028476 grad: -0.5322005087968517
iteration: 40 loss: 1.2871200506082128 grad: -0.45099657866115817
iteration: 50 loss: 1.046627282583689 grad: -0.3883620496928911
iteration: 60 loss: 0.8823223902044116 grad: -0.3402511306203445
iteration: 70 loss: 0.7628710841652471 grad: -0.3025378991664456
iteration: 80 loss: 0.6720723633039928 grad: -0.2723067459255007
iteration: 90 loss: 0.6006988923873823 grad: -0.2475780194248235
iteration: 100 loss: 0.5431062020813276 grad: -0.22699281063952947
iteration: 0 loss: 48.41236994128853 grad: 366.82709937672314
iteration: 10 loss: 4.1145108427195245 grad: -0.31760836959810873
iteration: 20 loss: 2.3174479949552733 grad: -0.6181384243425747
iteration: 30 loss: 1.6231108914728958 grad: -0.5411492138672629
iteration: 40 loss: 1.2515774647648952 grad: -0.4619442978859012
iteration: 50 loss: 1.0195114210352834 grad: -0.39948481888847576
iteration: 60 loss: 0.8605658463996985 grad: -0.35097254838210423
iteration: 70 loss: 0.74479014396411 grad: -0.3126829932988352
iteration: 80 loss: 0.6566529435987666 grad: -0.2818451008206919
iteration: 90 loss: 0.5872872008807132 grad: -0.25653308668887564
iteration: 100 loss: 0.5312581873956581 grad: -0.23540681078148057
iteration: 0 loss: 44.90569965449561 grad: 354.9021186990292
iteration: 10 loss: 4.2646919707906985 grad: -0.5188282694205993
iteration: 20 loss: 2.3755211886295085 grad: -0.6860178420530665
iteration: 30 loss: 1.6539551976457005 grad: -0.5716591420690231
iteration: 40 loss: 1.2707444984664356 grad: -0.47752092297391435
iteration: 50 loss: 1.0325876072114293 grad: -0.40785849488862513
iteration: 60 loss: 0.8700572728737991 grad: -0.35544418655394
iteration: 70 loss: 0.7519909825456869 grad: -0.3148711749750752
iteration: 80 loss: 0.6623000930617722 grad: -0.28262352251127315
iteration: 90 loss: 0.5918315607187569 grad: -0.25640773834230085
iteration: 100 loss: 0.534991354599945 grad: -0.23468682380297967
iteration: 0 loss: 45.29347543200356 grad: 341.2652350354975
iteration: 10 loss: 4.358916440606456 grad: -0.26183739472716416
iteration: 20 loss: 2.4235680054333155 grad: -0.5597796393670424
iteration: 30 loss: 1.6860988297468629 grad: -0.49091478592158644
iteration: 40 loss: 1.2949631687070409 grad: -0.41931703192265535
iteration: 50 loss: 1.0520694423006385 grad: -0.36292695342577386
iteration: 60 loss: 0.8863858855545252 grad: -0.3191742875999232
iteration: 70 loss: 0.7660663392954864 grad: -0.28465637857566806
iteration: 80 loss: 0.6746826598894752 grad: -0.2568561920228069
iteration: 90 loss: 0.6028942628631326 grad: -0.23403133121020586
iteration: 100 loss: 0.5449950691935638 grad: -0.2149724765602648
iteration: 0 loss: 51.65878976374663 grad: 443.6153381750511
iteration: 10 loss: 3.996869208787716 grad: -1.0716692821447382
iteration: 20 loss: 2.27766777659198 grad: -0.9001387390643782
iteration: 30 loss: 1.6040837064917717 grad: -0.7009523920567766
iteration: 40 loss: 1.2409445452434407 grad: -0.5693057238262023
iteration: 50 loss: 1.0130358905478467 grad: -0.4786943406133916
iteration: 60 loss: 0.8564156893555512 grad: -0.4129560809489209
iteration: 70 loss: 0.7420511519701243 grad: -0.36317992223698603
iteration: 80 loss: 0.6548218074268665 grad: -0.3242029672195291
iteration: 90 loss: 0.5860661780263233 grad: -0.2928575154839585
iteration: 100 loss: 0.5304611931329749 grad: -0.2671000620640872
iteration: 0 loss: 50.01947649958311 grad: 396.2602962212958
iteration: 10 loss: 4.109680216968172 grad: -0.3617020163384055
iteration: 20 loss: 2.313724262979659 grad: -0.6113496322144248
iteration: 30 loss: 1.6197806679080549 grad: -0.5261815918152248
iteration: 40 loss: 1.24866682635841 grad: -0.4465922716631779
iteration: 50 loss: 1.016964669397649 grad: -0.3853016348743026
iteration: 60 loss: 0.8583178372249759 grad: -0.33818035580004574
iteration: 70 loss: 0.742785581967713 grad: -0.3011847903356804
iteration: 80 loss: 0.6548481612688771 grad: -0.2714782725445927
iteration: 90 loss: 0.5856482143921808 grad: -0.24713822545789507
iteration: 100 loss: 0.5297584507173848 grad: -0.22684489854784384
iteration: 0 loss: 59.58045951380712 grad: 380.5070124296228
iteration: 10 loss: 3.940216341061593 grad: 0.2490064164185728
iteration: 20 loss: 2.249318668842079 grad: -0.4613984188028586
iteration: 30 loss: 1.5853423710622834 grad: -0.4639999550474827
iteration: 40 loss: 1.2268288532139773 grad: -0.414095470324657
iteration: 50 loss: 1.0016500136475357 grad: -0.3659883358524493
iteration: 60 loss: 0.8468443388361596 grad: -0.32573046846510767
iteration: 70 loss: 0.733780800626846 grad: -0.2927010671015179
iteration: 80 loss: 0.6475338420646453 grad: -0.2654622049932779
iteration: 90 loss: 0.5795483445430033 grad: -0.24274437056671433
iteration: 100 loss: 0.5245644574399028 grad: -0.22356386186028016
iteration: 0 loss: 47.08588500625326 grad: 379.45440513702573
iteration: 10 loss: 4.240280682932404 grad: -0.678746744667474
iteration: 20 loss: 2.374037395516635 grad: -0.7540552880658771
iteration: 30 loss: 1.6563123920584324 grad: -0.6145835438412424
iteration: 40 loss: 1.2739531486918994 grad: -0.5086968133584308
iteration: 50 loss: 1.0359034371236207 grad: -0.4322356138379069
iteration: 60 loss: 0.8732590959399534 grad: -0.3753964494891484
iteration: 70 loss: 0.755014303865747 grad: -0.3317189852630396
iteration: 80 loss: 0.6651338053494246 grad: -0.2971762474245665
iteration: 90 loss: 0.5944835090969552 grad: -0.26919689378554634
iteration: 100 loss: 0.5374756319014605 grad: -0.24607992530127848
iteration: 0 loss: 53.74465068822769 grad: 318.0502932920767
iteration: 10 loss: 4.136558604387904 grad: 0.6721959960185757
iteration: 20 loss: 2.332087575729268 grad: -0.3164927229020381
iteration: 30 loss: 1.634177114905304 grad: -0.382449976742429
iteration: 40 loss: 1.2604054058377352 grad: -0.35861950800294484
iteration: 50 loss: 1.0268251863891154 grad: -0.3243136728540495
iteration: 60 loss: 0.8667953642508853 grad: -0.29248587278297733
iteration: 70 loss: 0.7502084469037569 grad: -0.26510360414491196
iteration: 80 loss: 0.6614430664817519 grad: -0.2418980885478841
iteration: 90 loss: 0.5915773293218081 grad: -0.22219933918971485
iteration: 100 loss: 0.5351413549235324 grad: -0.20536083818787998
iteration: 0 loss: 52.49475926501323 grad: 391.72192311319463
iteration: 10 loss: 4.047647338699914 grad: -0.6837249543023591
iteration: 20 loss: 2.2945543193172027 grad: -0.7366955470513159
iteration: 30 loss: 1.6100513966013172 grad: -0.6018352539851457
iteration: 40 loss: 1.2424554580486735 grad: -0.4996498535921271
iteration: 50 loss: 1.012458438984843 grad: -0.4256217112019457
iteration: 60 loss: 0.8547830480253072 grad: -0.3704065534333103
iteration: 70 loss: 0.7398681268639187 grad: -0.32785321581786114
iteration: 80 loss: 0.6523548708255476 grad: -0.2941162748781905
iteration: 90 loss: 0.5834637462300055 grad: -0.26673225122604755
iteration: 100 loss: 0.5278089892419137 grad: -0.24406650683931624
iteration: 0 loss: 42.383938440747755 grad: 359.1032802405303
iteration: 10 loss: 4.338808662786257 grad: -0.11131914935335366
iteration: 20 loss: 2.412535412380211 grad: -0.5163723060466936
iteration: 30 loss: 1.6793178893540657 grad: -0.47254469219474216
iteration: 40 loss: 1.2902762638756362 grad: -0.41002786110457845
iteration: 50 loss: 1.0485658439833143 grad: -0.3577486139483177
iteration: 60 loss: 0.8836217859410763 grad: -0.31612184106810337
iteration: 70 loss: 0.7637994957696553 grad: -0.28280461233806026
iteration: 80 loss: 0.6727690795844203 grad: -0.25572578467624674
iteration: 90 loss: 0.6012425619994495 grad: -0.23335414568739898
iteration: 100 loss: 0.5435441806240636 grad: -0.21458953399221117
iteration: 0 loss: 41.158225611235615 grad: 377.9213186267124
iteration: 10 loss: 4.242615290549069 grad: -1.0001441237850086
iteration: 20 loss: 2.368889335904731 grad: -0.9139149644089927
iteration: 30 loss: 1.650458387299254 grad: -0.7183756845620733
iteration: 40 loss: 1.268367258348421 grad: -0.5846368287200243
iteration: 50 loss: 1.0307509882577919 grad: -0.4916627373144265
iteration: 60 loss: 0.8685345379758886 grad: -0.4239595859871721
iteration: 70 loss: 0.7506739639918372 grad: -0.3726239899806435
iteration: 80 loss: 0.6611296119784295 grad: -0.3324103301231436
iteration: 90 loss: 0.5907718289040531 grad: -0.30007328850413917
iteration: 100 loss: 0.53401902661087 grad: -0.27350976723875636
iteration: 0 loss: 45.64020556163438 grad: 478.32658460374785
iteration: 10 loss: 3.605810130437517 grad: -1.161822716047923
iteration: 20 loss: 2.1054913752478073 grad: -0.9735734341065956
iteration: 30 loss: 1.5031277698317767 grad: -0.7479453586363114
iteration: 40 loss: 1.1729005506510723 grad: -0.6045891795147476
iteration: 50 loss: 0.963235772632288 grad: -0.5072083121680951
iteration: 60 loss: 0.8179226094935075 grad: -0.43698015548008146
iteration: 70 loss: 0.7111183345052929 grad: -0.3839741676218513
iteration: 80 loss: 0.6292305879258658 grad: -0.3425470192208331
iteration: 90 loss: 0.5644107174736994 grad: -0.3092718963566793
iteration: 0 loss: 48.28867170790667 grad: 336.5880913929477
iteration: 10 loss: 4.280755165551827 grad: -0.16318545832329995
iteration: 20 loss: 2.4004255312915617 grad: -0.5446434797650878
iteration: 30 loss: 1.6768298891359914 grad: -0.49240057217003463
iteration: 40 loss: 1.2908192333656272 grad: -0.425225247310008
iteration: 50 loss: 1.0502370893128832 grad: -0.3699723385177827
iteration: 60 loss: 0.885726805502517 grad: -0.32628898236361836
iteration: 70 loss: 0.7660485059873694 grad: -0.2914708880793194
iteration: 80 loss: 0.6750316996533663 grad: -0.26325275220700606
iteration: 90 loss: 0.6034583212666885 grad: -0.23998956441284675
iteration: 100 loss: 0.5456855945974646 grad: -0.2205101251227679
iteration: 0 loss: 42.47178477043615 grad: 323.77858542242575
iteration: 10 loss: 4.27253960391123 grad: -0.09873454466929862
iteration: 20 loss: 2.3789582634246718 grad: -0.5031705850633437
iteration: 30 loss: 1.6570608172384957 grad: -0.463630931015533
iteration: 40 loss: 1.2736912140354766 grad: -0.403789580475453
iteration: 50 loss: 1.0353715967116015 grad: -0.35317056315267525
iteration: 60 loss: 0.8726776782718016 grad: -0.3126301387463093
iteration: 70 loss: 0.7544554794309822 grad: -0.28006024740465685
iteration: 80 loss: 0.6646202751820222 grad: -0.2535173111081702
iteration: 90 loss: 0.5940200013393554 grad: -0.23154315532944997
iteration: 100 loss: 0.5370602198234503 grad: -0.21308165846705424
iteration: 0 loss: 41.31315685891497 grad: 343.3873080704725
iteration: 10 loss: 4.335065005226583 grad: 0.17193924603901117
iteration: 20 loss: 2.4046231212526603 grad: -0.38406232709301497
iteration: 30 loss: 1.672410112949406 grad: -0.39027235531623594
iteration: 40 loss: 1.2844677503228261 grad: -0.35155434088957815
iteration: 50 loss: 1.0436264456136748 grad: -0.31291695262557473
iteration: 60 loss: 0.8793518984020093 grad: -0.28004100163553003
iteration: 70 loss: 0.7600520804016924 grad: -0.2527731421201707
iteration: 80 loss: 0.6694373911812529 grad: -0.2301049832052592
iteration: 90 loss: 0.5982480282872199 grad: -0.21107995687423148
iteration: 100 loss: 0.540827803945955 grad: -0.19493438213387734
iteration: 0 loss: 44.26918628506027 grad: 384.48178003074935
iteration: 10 loss: 4.236520795062006 grad: -1.2408882662465517
iteration: 20 loss: 2.372266284151341 grad: -1.0049819067109431
iteration: 30 loss: 1.6535571692745015 grad: -0.7682383495134397
iteration: 40 loss: 1.2708032960757587 grad: -0.6168906944675723
iteration: 50 loss: 1.0326702666862233 grad: -0.5146213800874712
iteration: 60 loss: 0.8700786974914908 grad: -0.44134738851661925
iteration: 70 loss: 0.7519435557174305 grad: -0.38637664607644595
iteration: 80 loss: 0.6621937255689855 grad: -0.34364164424814075
iteration: 90 loss: 0.5916785395293118 grad: -0.3094735628652849
iteration: 100 loss: 0.5348025455693708 grad: -0.2815317834871926
iteration: 0 loss: 53.013629906753025 grad: 395.62053726519787
iteration: 10 loss: 4.032993606107663 grad: -0.41922925451146625
iteration: 20 loss: 2.2833482526282918 grad: -0.6355766629261411
iteration: 30 loss: 1.6028398362781529 grad: -0.5430720460172261
iteration: 40 loss: 1.2375184987968169 grad: -0.45986373718350537
iteration: 50 loss: 1.0088907373816172 grad: -0.39626828398248537
iteration: 60 loss: 0.85209755513721 grad: -0.34751957725058424
iteration: 70 loss: 0.7377833209031699 grad: -0.3093072155243963
iteration: 80 loss: 0.6506972887149541 grad: -0.2786554751661851
iteration: 90 loss: 0.5821207648510112 grad: -0.25356013423408574
iteration: 100 loss: 0.5267043005981329 grad: -0.2326497888298911
iteration: 0 loss: 45.79622429427732 grad: 427.65276203571767
iteration: 10 loss: 4.06009257424516 grad: -1.1157384044485952
iteration: 20 loss: 2.3040953274177642 grad: -0.9422724800997779
iteration: 30 loss: 1.6175646975196865 grad: -0.732115751465079
iteration: 40 loss: 1.2485653066954165 grad: -0.5931617330980631
iteration: 50 loss: 1.0175751765853998 grad: -0.497664083531738
iteration: 60 loss: 0.8591696719331594 grad: -0.4285163256512637
iteration: 70 loss: 0.7436989507098339 grad: -0.37626086189517627
iteration: 80 loss: 0.6557500601556967 grad: -0.3354169352626758
iteration: 90 loss: 0.5865091406650539 grad: -0.30262438053229135
iteration: 100 loss: 0.5305678144155866 grad: -0.2757180322699509
iteration: 0 loss: 47.53978660690845 grad: 423.488752399719
iteration: 10 loss: 3.852754531085132 grad: -1.1838736197821396
iteration: 20 loss: 2.1988309095664094 grad: -0.9904569230294876
iteration: 30 loss: 1.55021342353786 grad: -0.7631864025461197
iteration: 40 loss: 1.2002553178891007 grad: -0.61616342892389
iteration: 50 loss: 0.9804642535997449 grad: -0.5160037959466944
iteration: 60 loss: 0.8293250428285485 grad: -0.44380464709984824
iteration: 70 loss: 0.7188983316364514 grad: -0.38938478931424914
iteration: 80 loss: 0.6346284163052835 grad: -0.34691893588980804
iteration: 90 loss: 0.568174399261972 grad: -0.312861976895933
iteration: 100 loss: 0.5144083434682795 grad: -0.28494026830243757
iteration: 0 loss: 45.87223856227849 grad: 402.40631761643414
iteration: 10 loss: 4.047710872548785 grad: -1.1374677148447976
iteration: 20 loss: 2.280217898377197 grad: -0.9497672429657482
iteration: 30 loss: 1.5957027804540138 grad: -0.7387060701642874
iteration: 40 loss: 1.2295619899120374 grad: -0.5986728003950528
iteration: 50 loss: 1.0010077314709633 grad: -0.5023826678501749
iteration: 60 loss: 0.8445604517055059 grad: -0.43264100471163053
iteration: 70 loss: 0.7306638155395045 grad: -0.3799239411620237
iteration: 80 loss: 0.6439961206461621 grad: -0.338710402566457
iteration: 90 loss: 0.5758135300522388 grad: -0.3056149046289072
iteration: 100 loss: 0.5207589733722258 grad: -0.27845559050210633
iteration: 0 loss: 46.99243949405495 grad: 379.9496957010932
iteration: 10 loss: 4.150947823264025 grad: -0.6154866589771888
iteration: 20 loss: 2.3269340658678797 grad: -0.7567684482108714
iteration: 30 loss: 1.6244083356666785 grad: -0.6228979002501563
iteration: 40 loss: 1.2498242327767664 grad: -0.5170882536616214
iteration: 50 loss: 1.0164953172711235 grad: -0.4398900600171578
iteration: 60 loss: 0.8570222386661457 grad: -0.3822606540178768
iteration: 70 loss: 0.7410549702662634 grad: -0.3378821422575897
iteration: 80 loss: 0.652889680760092 grad: -0.302742897637196
iteration: 90 loss: 0.5835778166951555 grad: -0.2742594758938389
iteration: 100 loss: 0.5276435682342047 grad: -0.2507148618487704
iteration: 0 loss: 63.1675656521172 grad: 436.1172492726465
iteration: 10 loss: 3.591500969409191 grad: -1.1176035003786307
iteration: 20 loss: 2.0998770419042887 grad: -0.955277640955253
iteration: 30 loss: 1.5008187826305486 grad: -0.7410796219950022
iteration: 40 loss: 1.171912773102163 grad: -0.6011987866536429
iteration: 50 loss: 0.9628603409725077 grad: -0.5052917081717684
iteration: 60 loss: 0.8178604525182064 grad: -0.43580711839871344
iteration: 70 loss: 0.711226953448948 grad: -0.3832189486388301
iteration: 80 loss: 0.6294358639183883 grad: -0.3420445912867025
iteration: 90 loss: 0.5646715194953603 grad: -0.3089309259270733
iteration: 0 loss: 50.11450576042766 grad: 399.85549384276015
iteration: 10 loss: 4.114579327188122 grad: -0.09416687430493118
iteration: 20 loss: 2.3147751566234547 grad: -0.49958232098570865
iteration: 30 loss: 1.6209154304230073 grad: -0.46346751722654295
iteration: 40 loss: 1.2499033256125254 grad: -0.4052800456698754
iteration: 50 loss: 1.0182221173281918 grad: -0.3554491983196202
iteration: 60 loss: 0.8595523883725285 grad: -0.31528241778002425
iteration: 70 loss: 0.7439776090757071 grad: -0.28287609160869503
iteration: 80 loss: 0.6559902456335542 grad: -0.25638506270744077
iteration: 90 loss: 0.5867388444915543 grad: -0.23440164418810883
iteration: 100 loss: 0.5307989087720092 grad: -0.21589700501600978
iteration: 0 loss: 41.19321891533547 grad: 374.80588859150095
iteration: 10 loss: 4.1905875587816475 grad: -1.2415037826485436
iteration: 20 loss: 2.3459797176171624 grad: -0.9949724341222052
iteration: 30 loss: 1.6348933765633442 grad: -0.7607824796941158
iteration: 40 loss: 1.2562956165098844 grad: -0.6112088334557587
iteration: 50 loss: 1.0207945347796836 grad: -0.5100824225511918
iteration: 60 loss: 0.8600233108230565 grad: -0.43758532400022354
iteration: 70 loss: 0.743223507303438 grad: -0.38317178851424466
iteration: 80 loss: 0.6544955738528518 grad: -0.340854228569766
iteration: 90 loss: 0.5847878324706554 grad: -0.3070098797671092
iteration: 100 loss: 0.528566028265893 grad: -0.27932619479121734
iteration: 0 loss: 48.69622569601742 grad: 375.825485295891
iteration: 10 loss: 4.213133062916892 grad: -0.43695250150803033
iteration: 20 loss: 2.3584912771029023 grad: -0.6346212775923011
iteration: 30 loss: 1.6454182091497387 grad: -0.5395743506019264
iteration: 40 loss: 1.2655878669126486 grad: -0.4554345820121294
iteration: 50 loss: 1.0291263957866994 grad: -0.3915459275091301
iteration: 60 loss: 0.8675707329270236 grad: -0.34278335055836906
iteration: 70 loss: 0.750117778769684 grad: -0.3046815158737776
iteration: 80 loss: 0.6608385353728078 grad: -0.2741937245502059
iteration: 90 loss: 0.5906599625526393 grad: -0.24928187834660223
iteration: 100 loss: 0.5340318315702461 grad: -0.22855804839686217
iteration: 0 loss: 44.3504311909151 grad: 369.1100954118837
iteration: 10 loss: 4.1937597516259135 grad: -0.2611270456158429
iteration: 20 loss: 2.3455903686250497 grad: -0.6585172367815748
iteration: 30 loss: 1.6360893786329804 grad: -0.57253243307257
iteration: 40 loss: 1.2582299779566133 grad: -0.48465449850711056
iteration: 50 loss: 1.0230189391061437 grad: -0.4164565010251706
iteration: 60 loss: 0.862332383358824 grad: -0.36412856585692677
iteration: 70 loss: 0.7455216367261115 grad: -0.3232030611636974
iteration: 80 loss: 0.6567380169460855 grad: -0.29047320779312813
iteration: 90 loss: 0.5869545203111722 grad: -0.2637572970485189
iteration: 100 loss: 0.5306492932722077 grad: -0.24155954839128166
iteration: 0 loss: 47.0814355999546 grad: 437.50756722012045
iteration: 10 loss: 4.0611779370554935 grad: -1.3350693832697793
iteration: 20 loss: 2.3049013817067547 grad: -1.0449490545924043
iteration: 30 loss: 1.6182476775499957 grad: -0.7893348362244318
iteration: 40 loss: 1.249181275566157 grad: -0.6305026003330889
iteration: 50 loss: 1.018132851562727 grad: -0.5243726945122831
iteration: 60 loss: 0.8596737670967823 grad: -0.4487877546170224
iteration: 70 loss: 0.7441546222139963 grad: -0.3922971839921499
iteration: 80 loss: 0.6561626490914189 grad: -0.34849612429853366
iteration: 90 loss: 0.5868837168917377 grad: -0.31354446614131715
iteration: 100 loss: 0.5309089532094565 grad: -0.28500593452226636
iteration: 0 loss: 44.333651343366746 grad: 376.1830973306998
iteration: 10 loss: 4.240036990474602 grad: -0.8767695667049626
iteration: 20 loss: 2.371987639329001 grad: -0.8507736529183995
iteration: 30 loss: 1.6532181681905471 grad: -0.671661355928676
iteration: 40 loss: 1.2706030724876571 grad: -0.5473647425200794
iteration: 50 loss: 1.0325838650686074 grad: -0.4606615269041246
iteration: 60 loss: 0.8700711628593208 grad: -0.39744104731227264
iteration: 70 loss: 0.7519893485265873 grad: -0.34947112058207136
iteration: 80 loss: 0.6622756161242581 grad: -0.31187714191092736
iteration: 90 loss: 0.5917849667812747 grad: -0.2816365613909144
iteration: 100 loss: 0.5349256576622462 grad: -0.25678833384169747
iteration: 0 loss: 53.23012674410457 grad: 358.2666945284591
iteration: 10 loss: 4.1714420670194325 grad: -0.23870153685696838
iteration: 20 loss: 2.352704087326608 grad: -0.5754386252098506
iteration: 30 loss: 1.6475013555102371 grad: -0.5106068696733
iteration: 40 loss: 1.2700342697175158 grad: -0.43847402004969926
iteration: 50 loss: 1.0343023661661475 grad: -0.38064130336684665
iteration: 60 loss: 0.8728859530873375 grad: -0.33536176759020164
iteration: 70 loss: 0.7553383847457966 grad: -0.29944041541844557
iteration: 80 loss: 0.6658710203105329 grad: -0.2704017119354254
iteration: 90 loss: 0.5954711128008718 grad: -0.24649671167578502
iteration: 100 loss: 0.5386155894045508 grad: -0.22649695188953684
iteration: 0 loss: 44.323306904941234 grad: 313.9658717332649
iteration: 10 loss: 4.198216114935638 grad: 0.08800901061208219
iteration: 20 loss: 2.3558987027615994 grad: -0.5720526281544581
iteration: 30 loss: 1.6460139757969872 grad: -0.5331429289486465
iteration: 40 loss: 1.2670676458630459 grad: -0.46163135557559054
iteration: 50 loss: 1.0308341524711813 grad: -0.4010218536117905
iteration: 60 loss: 0.869288871388893 grad: -0.35285488023498707
iteration: 70 loss: 0.7517700321643794 grad: -0.3144774921223649
iteration: 80 loss: 0.6624002174198309 grad: -0.2834350023693535
iteration: 90 loss: 0.592126549261097 grad: -0.2579032841771734
iteration: 100 loss: 0.5354067916234933 grad: -0.23657457799622506
iteration: 0 loss: 56.521353932650406 grad: 414.53876339973334
iteration: 10 loss: 3.9908784213987682 grad: -0.6430866653105975
iteration: 20 loss: 2.269161286856822 grad: -0.7984508863239212
iteration: 30 loss: 1.5955664342885607 grad: -0.6552670753016978
iteration: 40 loss: 1.232842135580003 grad: -0.5435024285407023
iteration: 50 loss: 1.0054634833854008 grad: -0.4621386470821916
iteration: 60 loss: 0.8493740538797696 grad: -0.40142314445474575
iteration: 70 loss: 0.7355024402746825 grad: -0.35466923440967046
iteration: 80 loss: 0.6487181582951962 grad: -0.3176487107985677
iteration: 90 loss: 0.5803602878373071 grad: -0.2876412682864651
iteration: 100 loss: 0.5251099239454892 grad: -0.2628389541841626
iteration: 0 loss: 46.808677727430926 grad: 401.56265195865217
iteration: 10 loss: 4.185204185481904 grad: -0.7205983583212869
iteration: 20 loss: 2.3471851170521734 grad: -0.7707490818247764
iteration: 30 loss: 1.6386020730134823 grad: -0.6238041530797556
iteration: 40 loss: 1.2607227024216792 grad: -0.5146992345539317
iteration: 50 loss: 1.0253297307178235 grad: -0.4365306192270345
iteration: 60 loss: 0.8644445552151658 grad: -0.3786630138668502
iteration: 70 loss: 0.7474513625144771 grad: -0.3343109420518477
iteration: 80 loss: 0.6585074910172725 grad: -0.2992977990340578
iteration: 90 loss: 0.5885848699124174 grad: -0.27097502922071853
iteration: 100 loss: 0.532158896142226 grad: -0.24759825251125794
iteration: 0 loss: 46.516537088595584 grad: 392.9636944280235
iteration: 10 loss: 4.133122634672128 grad: -0.4314057146820187
iteration: 20 loss: 2.3211363378089698 grad: -0.6842079034251481
iteration: 30 loss: 1.6229569708051796 grad: -0.5834003988881923
iteration: 40 loss: 1.2501026936394986 grad: -0.4918048339091485
iteration: 50 loss: 1.0175412608205079 grad: -0.42209236532862054
iteration: 60 loss: 0.8584245360004222 grad: -0.3689262272762663
iteration: 70 loss: 0.7426186899791157 grad: -0.3274420889218571
iteration: 80 loss: 0.6545157452483594 grad: -0.2942973211028182
iteration: 90 loss: 0.5852136278165496 grad: -0.267252928268701
iteration: 100 loss: 0.5292606095498823 grad: -0.24478466405792637
iteration: 0 loss: 72.71526668458151 grad: 412.18204970815486
iteration: 10 loss: 3.634670224520257 grad: -0.0051660819381800505
iteration: 20 loss: 2.1085118949717154 grad: -0.5060400120050567
iteration: 30 loss: 1.5034232826649878 grad: -0.4819595478894287
iteration: 40 loss: 1.1728638102581912 grad: -0.42588403928459595
iteration: 50 loss: 0.9632476228473272 grad: -0.37572631867558226
iteration: 60 loss: 0.8180354847643023 grad: -0.33452477793680746
iteration: 70 loss: 0.7113215235916538 grad: -0.3009110288121737
iteration: 80 loss: 0.6295038550698254 grad: -0.27322591848516026
iteration: 90 loss: 0.564735820725797 grad: -0.2501264783402648
iteration: 0 loss: 43.87220101520393 grad: 370.1913898121313
iteration: 10 loss: 4.196417546278016 grad: -0.6690087181315971
iteration: 20 loss: 2.3489428688705876 grad: -0.7839630183789522
iteration: 30 loss: 1.638938108685786 grad: -0.6442167668538591
iteration: 40 loss: 1.2607088113329816 grad: -0.5348290994019627
iteration: 50 loss: 1.0252147599308046 grad: -0.4550529653502768
iteration: 60 loss: 0.8643035992959912 grad: -0.39546808889685525
iteration: 70 loss: 0.7473100896563665 grad: -0.3495591373360186
iteration: 80 loss: 0.6583748637679657 grad: -0.313192804444493
iteration: 90 loss: 0.588463606130476 grad: -0.2837057352109683
iteration: 100 loss: 0.5320492683785315 grad: -0.2593263371995879
iteration: 0 loss: 54.506141320400936 grad: 388.5997971968564
iteration: 10 loss: 4.018579265322842 grad: -0.7780602805885946
iteration: 20 loss: 2.2849969277134816 grad: -0.8387402301639779
iteration: 30 loss: 1.605900605754538 grad: -0.6799186241197483
iteration: 40 loss: 1.2403802508373785 grad: -0.5607951310138333
iteration: 50 loss: 1.0113466038359216 grad: -0.47514529615897405
iteration: 60 loss: 0.8541745555495172 grad: -0.41166269392995847
iteration: 70 loss: 0.7395442840973953 grad: -0.3629988617395515
iteration: 80 loss: 0.6522011060295964 grad: -0.32459485707430613
iteration: 90 loss: 0.5834155999535721 grad: -0.2935476226235168
iteration: 100 loss: 0.5278281764632827 grad: -0.2679406651959627
iteration: 0 loss: 66.09658675865386 grad: 437.3984822529315
iteration: 10 loss: 3.6635048064488482 grad: -0.36747999607497894
iteration: 20 loss: 2.1263178486215937 grad: -0.6752026227473154
iteration: 30 loss: 1.5132058954023622 grad: -0.5809539036630358
iteration: 40 loss: 1.1782709744647533 grad: -0.4919145088260817
iteration: 50 loss: 0.9661483379438157 grad: -0.42339650351769065
iteration: 60 loss: 0.819418181230958 grad: -0.37082271367235353
iteration: 70 loss: 0.7117435135167149 grad: -0.3296317173225264
iteration: 80 loss: 0.6292976781875371 grad: -0.29662063292451535
iteration: 90 loss: 0.5641093009556822 grad: -0.26962110729248534
iteration: 0 loss: 48.22854153130336 grad: 365.0978128347316
iteration: 10 loss: 4.178105763519935 grad: -0.005563878210026255
iteration: 20 loss: 2.335648004510941 grad: -0.5437955496935812
iteration: 30 loss: 1.6307997582387295 grad: -0.5062214766256863
iteration: 40 loss: 1.2552840212206733 grad: -0.44091697923710776
iteration: 50 loss: 1.0213432309662742 grad: -0.3851599106307962
iteration: 60 loss: 0.8613987664912615 grad: -0.3404749660318237
iteration: 70 loss: 0.7450473447269442 grad: -0.3046127032911169
iteration: 80 loss: 0.6565605778183371 grad: -0.27542760359134705
iteration: 90 loss: 0.5869751786633721 grad: -0.2513001899814934
iteration: 100 loss: 0.5308052677258026 grad: -0.2310564376989726
iteration: 0 loss: 51.38329029692704 grad: 393.06056899724126
iteration: 10 loss: 4.117012140238702 grad: -0.42766215857823825
iteration: 20 loss: 2.3294575949836474 grad: -0.5884001424647239
iteration: 30 loss: 1.635359815148604 grad: -0.5033839377585235
iteration: 40 loss: 1.2628848555824665 grad: -0.4275185662699035
iteration: 50 loss: 1.0297740435715406 grad: -0.3693730977690438
iteration: 60 loss: 0.8698837473278384 grad: -0.32465828837444227
iteration: 70 loss: 0.7532918930043541 grad: -0.289507508112743
iteration: 80 loss: 0.6644560555858086 grad: -0.2612419658483261
iteration: 90 loss: 0.5944909122103623 grad: -0.23805090541546206
iteration: 100 loss: 0.537944453568302 grad: -0.21869134039672977
iteration: 0 loss: 47.226940910908915 grad: 373.2834968564349
iteration: 10 loss: 4.209651717630137 grad: -0.6148192658029575
iteration: 20 loss: 2.358742666765052 grad: -0.7792475905202507
iteration: 30 loss: 1.6467531660683066 grad: -0.6426719244501761
iteration: 40 loss: 1.267099537067307 grad: -0.5335183410183613
iteration: 50 loss: 1.0305770568554016 grad: -0.4536860734105055
iteration: 60 loss: 0.8689014496606599 grad: -0.3940501025016567
iteration: 70 loss: 0.7513222350955236 grad: -0.34812325740721406
iteration: 80 loss: 0.661926100790879 grad: -0.311764722751342
iteration: 90 loss: 0.5916438173877313 grad: -0.28230170427538853
iteration: 100 loss: 0.5349250295781034 grad: -0.2579557681295065
iteration: 0 loss: 54.25397447311928 grad: 438.6997107023823
iteration: 10 loss: 3.9422512936226584 grad: -0.7280944940538717
iteration: 20 loss: 2.252021885132308 grad: -0.7971016713591637
iteration: 30 loss: 1.587687336224793 grad: -0.6507944270746835
iteration: 40 loss: 1.2288943706260975 grad: -0.5394044323292885
iteration: 50 loss: 1.0034875823093485 grad: -0.45869642979713066
iteration: 60 loss: 0.848492247465236 grad: -0.39855480359416895
iteration: 70 loss: 0.7352694065301958 grad: -0.3522611438342119
iteration: 80 loss: 0.6488875391702013 grad: -0.3156053111753794
iteration: 90 loss: 0.5807868454104953 grad: -0.28588870264664334
iteration: 100 loss: 0.5257037904917855 grad: -0.26132100546269255
iteration: 0 loss: 54.85401416839491 grad: 413.0368360627917
iteration: 10 loss: 4.042909361335784 grad: -0.5086258253846616
iteration: 20 loss: 2.3006302308515596 grad: -0.7140448228530907
iteration: 30 loss: 1.6176425430121113 grad: -0.599832342050853
iteration: 40 loss: 1.2498698381055378 grad: -0.5026175012969035
iteration: 50 loss: 1.0193489876375927 grad: -0.4299406890031641
iteration: 60 loss: 0.8611136183261888 grad: -0.37498124025342
iteration: 70 loss: 0.7456814678920306 grad: -0.33231058891203935
iteration: 80 loss: 0.6577099809414491 grad: -0.2983300443817828
iteration: 90 loss: 0.5884177509131157 grad: -0.27066917712047434
iteration: 100 loss: 0.5324123287449435 grad: -0.24772989262538797
iteration: 0 loss: 51.967085741990125 grad: 333.80589004800885
iteration: 10 loss: 4.2353361028845855 grad: 0.40249590308483696
iteration: 20 loss: 2.3717938007870627 grad: -0.3153314371992709
iteration: 30 loss: 1.6564959846029772 grad: -0.34958344337485814
iteration: 40 loss: 1.2752791936981422 grad: -0.32258401554629285
iteration: 50 loss: 1.0377693347420724 grad: -0.29048693450556057
iteration: 60 loss: 0.8753759077564858 grad: -0.2617980210193716
iteration: 70 loss: 0.7572352318978285 grad: -0.23743918468794226
iteration: 80 loss: 0.667380875576865 grad: -0.2169070111915546
iteration: 90 loss: 0.5967142174863098 grad: -0.199514746233886
iteration: 100 loss: 0.5396666964292804 grad: -0.18465622705322576
iteration: 0 loss: 44.68460221420093 grad: 320.57489693564037
iteration: 10 loss: 4.38068721630506 grad: 0.21835953291313176
iteration: 20 loss: 2.42295980529033 grad: -0.31127057735947483
iteration: 30 loss: 1.683057480634571 grad: -0.33277371765396657
iteration: 40 loss: 1.2918287583399268 grad: -0.3063562527456073
iteration: 50 loss: 1.0492344547786738 grad: -0.2763978520318486
iteration: 60 loss: 0.8838883021907019 grad: -0.2497461677429299
iteration: 70 loss: 0.7638706618760228 grad: -0.22708643843558335
iteration: 80 loss: 0.6727428047531082 grad: -0.20793478714430186
iteration: 90 loss: 0.6011684378937306 grad: -0.19166434395807
iteration: 100 loss: 0.5434484833528007 grad: -0.17772483950525136
iteration: 0 loss: 43.99256692948316 grad: 407.8161353501466
iteration: 10 loss: 4.248775470272118 grad: -1.1235978543189016
iteration: 20 loss: 2.374788089390222 grad: -0.9449247079208623
iteration: 30 loss: 1.6552032406204737 grad: -0.7345795320662318
iteration: 40 loss: 1.2722886889187892 grad: -0.5954018637050514
iteration: 50 loss: 1.0340827411956028 grad: -0.49967283679876345
iteration: 60 loss: 0.8714275008316938 grad: -0.4303169132233754
iteration: 70 loss: 0.7532288258174215 grad: -0.37788239626871045
iteration: 80 loss: 0.6634163728716967 grad: -0.3368862849413028
iteration: 90 loss: 0.5928409565414189 grad: -0.3039643248669678
iteration: 100 loss: 0.5359080434506971 grad: -0.27694735862714726
iteration: 0 loss: 39.26165124387355 grad: 308.6658836288833
iteration: 10 loss: 4.348096343903473 grad: -0.07355770159444602
iteration: 20 loss: 2.4146039703092272 grad: -0.42212063712501924
iteration: 30 loss: 1.6785193061247232 grad: -0.39290310114895705
iteration: 40 loss: 1.2884806748446946 grad: -0.3442793355349588
iteration: 50 loss: 1.0464329798530179 grad: -0.302465784922052
iteration: 60 loss: 0.8814098905797271 grad: -0.26869438413670343
iteration: 70 loss: 0.7616163825585069 grad: -0.24140858701396403
iteration: 80 loss: 0.6706600681460176 grad: -0.21907616337839078
iteration: 90 loss: 0.5992250986586938 grad: -0.2005233011074577
iteration: 100 loss: 0.5416227496005542 grad: -0.1848903947826378
iteration: 0 loss: 59.39940514873309 grad: 312.65702860067285
iteration: 10 loss: 3.8957139002473014 grad: 0.45573972995079154
iteration: 20 loss: 2.228514428878663 grad: -0.39502492808152473
iteration: 30 loss: 1.5725467694244137 grad: -0.4248192945316511
iteration: 40 loss: 1.2178424498799194 grad: -0.38707465173120914
iteration: 50 loss: 0.9948384571828022 grad: -0.3457962483996101
iteration: 60 loss: 0.8414192517648602 grad: -0.3098534009655502
iteration: 70 loss: 0.7293072846609903 grad: -0.27976487023870356
iteration: 80 loss: 0.6437489147821741 grad: -0.2546404433148789
iteration: 90 loss: 0.5762819708989727 grad: -0.2335050235894548
iteration: 100 loss: 0.5217009415024553 grad: -0.21554638725465314
iteration: 0 loss: 48.62153357389347 grad: 320.6341960563059
iteration: 10 loss: 4.182345649166067 grad: 0.09371536948751764
iteration: 20 loss: 2.3477247774377576 grad: -0.4518162556477514
iteration: 30 loss: 1.6408681210861935 grad: -0.43558087308458027
iteration: 40 loss: 1.2635221141717508 grad: -0.38435003798160156
iteration: 50 loss: 1.028240933170764 grad: -0.3381434825283374
iteration: 60 loss: 0.8673096715004415 grad: -0.30029879801003495
iteration: 70 loss: 0.750210640189173 grad: -0.2695602148273461
iteration: 80 loss: 0.6611409658601339 grad: -0.24435077236224212
iteration: 90 loss: 0.5910895541087484 grad: -0.22339570013728072
iteration: 100 loss: 0.5345391062077066 grad: -0.20574104106622326
iteration: 0 loss: 56.72422428463867 grad: 423.02003342981953
iteration: 10 loss: 3.8026407052022724 grad: -0.8406269527870575
iteration: 20 loss: 2.1889055850257053 grad: -0.825092622949753
iteration: 30 loss: 1.5495593773355474 grad: -0.6621267530486135
iteration: 40 loss: 1.2025800314352388 grad: -0.5454583827410354
iteration: 50 loss: 0.9838483016272387 grad: -0.46248604028462204
iteration: 60 loss: 0.8330563273511044 grad: -0.4011773139489412
iteration: 70 loss: 0.7226826915051044 grad: -0.35420276206329465
iteration: 80 loss: 0.6383383513097711 grad: -0.3171128469522139
iteration: 90 loss: 0.5717553508101528 grad: -0.28710061221220545
iteration: 100 loss: 0.5178396483121436 grad: -0.26232113540105495
iteration: 0 loss: 51.80668318037351 grad: 354.48366378047444
iteration: 10 loss: 4.144062663404012 grad: -0.02319280993997619
iteration: 20 loss: 2.3283048458355635 grad: -0.5975314181346258
iteration: 30 loss: 1.6285128021972097 grad: -0.5482589842757273
iteration: 40 loss: 1.2545857850261448 grad: -0.47363094987629184
iteration: 50 loss: 1.0212723215453563 grad: -0.41139937066443216
iteration: 60 loss: 0.8616058760120805 grad: -0.36211732520648676
iteration: 70 loss: 0.7453836049335223 grad: -0.32287439799844614
iteration: 80 loss: 0.6569557250370714 grad: -0.2911221620532291
iteration: 90 loss: 0.5873937674365483 grad: -0.2649915973837138
iteration: 100 loss: 0.5312286737640864 grad: -0.24314868742532197
iteration: 0 loss: 49.10834301106979 grad: 420.41969647848754
iteration: 10 loss: 3.8182750986256906 grad: -0.9949791143718557
iteration: 20 loss: 2.1984122290269954 grad: -0.9732472996227985
iteration: 30 loss: 1.5568187524733152 grad: -0.7575204206371011
iteration: 40 loss: 1.2084633394343158 grad: -0.6139524331522384
iteration: 50 loss: 0.9887891420536025 grad: -0.5151298266976478
iteration: 60 loss: 0.8373098552278448 grad: -0.4435214878596475
iteration: 70 loss: 0.7264126935397557 grad: -0.3893827414258072
iteration: 80 loss: 0.6416564338227214 grad: -0.3470551826120072
iteration: 90 loss: 0.5747410362445904 grad: -0.31306576175032363
iteration: 100 loss: 0.5205516095873629 grad: -0.28517478422356424
iteration: 0 loss: 59.455434066307696 grad: 420.7983361294809
iteration: 10 loss: 3.895429723031033 grad: -0.6399343808732185
iteration: 20 loss: 2.2350286558612837 grad: -0.7323169766455648
iteration: 30 loss: 1.5811711168592988 grad: -0.6014293138937614
iteration: 40 loss: 1.2268644122124732 grad: -0.5002920189857198
iteration: 50 loss: 1.0036464896266724 grad: -0.42658643032325905
iteration: 60 loss: 0.8498043738141363 grad: -0.3714564303564215
iteration: 70 loss: 0.7372142003572332 grad: -0.32890013315303257
iteration: 80 loss: 0.6511826707527771 grad: -0.2951263140028243
iteration: 90 loss: 0.5832706130711565 grad: -0.26769329835831535
iteration: 100 loss: 0.5282798917050141 grad: -0.2449757370991077
iteration: 0 loss: 46.031988861072456 grad: 455.17045742503177
iteration: 10 loss: 3.853053183300827 grad: -1.060003000797265
iteration: 20 loss: 2.2176432298935898 grad: -0.9164271519029981
iteration: 30 loss: 1.5710570991896846 grad: -0.7151294814520239
iteration: 40 loss: 1.2200867667739161 grad: -0.5814778918555754
iteration: 50 loss: 0.9987187034649154 grad: -0.4892098416685916
iteration: 60 loss: 0.8460204122692713 grad: -0.4221388018073786
iteration: 70 loss: 0.7341911744460131 grad: -0.37128718451678455
iteration: 80 loss: 0.6486935819766999 grad: -0.33143268235745155
iteration: 90 loss: 0.5811718344845179 grad: -0.29936189021689535
iteration: 100 loss: 0.5264758808507395 grad: -0.272997323502465
iteration: 0 loss: 43.80662815841949 grad: 351.42950473601115
iteration: 10 loss: 4.334779269189684 grad: -0.26953387102092397
iteration: 20 loss: 2.4114489336405067 grad: -0.5837587725371256
iteration: 30 loss: 1.6782211250496486 grad: -0.5121880372423466
iteration: 40 loss: 1.2891153904611201 grad: -0.43705748759995894
iteration: 50 loss: 1.0473933437498835 grad: -0.37780755190727766
iteration: 60 loss: 0.8824704311299766 grad: -0.33184818135691035
iteration: 70 loss: 0.7626852192249565 grad: -0.29561696365245105
iteration: 80 loss: 0.6716984708406347 grad: -0.26646387789472015
iteration: 90 loss: 0.6002173686954186 grad: -0.24255121138150637
iteration: 100 loss: 0.5425637087011058 grad: -0.2226028491048374
iteration: 0 loss: 45.04049540376662 grad: 356.33688144398945
iteration: 10 loss: 4.3039919114676195 grad: -0.2264987505329364
iteration: 20 loss: 2.393166481131547 grad: -0.548889790700491
iteration: 30 loss: 1.6657030699251332 grad: -0.4885142418306433
iteration: 40 loss: 1.2797572705512308 grad: -0.42001779578257625
iteration: 50 loss: 1.0399942197237082 grad: -0.36487752771736437
iteration: 60 loss: 0.8763909192731593 grad: -0.3216355407693555
iteration: 70 loss: 0.7575486695668587 grad: -0.28730107750900546
iteration: 80 loss: 0.6672660122208899 grad: -0.25953004173642114
iteration: 90 loss: 0.5963287880478095 grad: -0.2366595197593594
iteration: 100 loss: 0.5391067336246455 grad: -0.21751915503557334
iteration: 0 loss: 40.9489485823023 grad: 404.6698833176306
iteration: 10 loss: 4.1221745453434036 grad: -0.9384932354454736
iteration: 20 loss: 2.3140720880253203 grad: -0.871500527083395
iteration: 30 loss: 1.6159042317634968 grad: -0.6899548468716156
iteration: 40 loss: 1.2433558155816256 grad: -0.5637145649679473
iteration: 50 loss: 1.0112197861593069 grad: -0.4752499609029499
iteration: 60 loss: 0.8525390000712089 grad: -0.41051896127986065
iteration: 70 loss: 0.7371409050017725 grad: -0.3612764637800895
iteration: 80 loss: 0.6494068867016888 grad: -0.3226108418760837
iteration: 90 loss: 0.580434591202406 grad: -0.2914623039665991
iteration: 100 loss: 0.5247755464542107 grad: -0.2658384038529997
iteration: 0 loss: 44.473410490468495 grad: 390.0262227098917
iteration: 10 loss: 4.191104231065972 grad: -0.818543409897438
iteration: 20 loss: 2.3475298396766777 grad: -0.7908985433335429
iteration: 30 loss: 1.6376406784745747 grad: -0.6306308970855825
iteration: 40 loss: 1.2594272795712724 grad: -0.5174016708909834
iteration: 50 loss: 1.0239769427188217 grad: -0.4375353594012154
iteration: 60 loss: 0.8631250456867086 grad: -0.3788492893196099
iteration: 70 loss: 0.7461950251887505 grad: -0.3340633821103003
iteration: 80 loss: 0.6573219535517141 grad: -0.2988068094803865
iteration: 90 loss: 0.5874691768238914 grad: -0.27034336160215294
iteration: 100 loss: 0.5311088591110589 grad: -0.2468849330813111
iteration: 0 loss: 53.58464088714973 grad: 460.8799403408193
iteration: 10 loss: 3.5717546143130843 grad: -0.7289187370562986
iteration: 20 loss: 2.069529726241109 grad: -0.7961624515326872
iteration: 30 loss: 1.4755094419159087 grad: -0.6442345563052547
iteration: 40 loss: 1.151318990255897 grad: -0.5331814138437939
iteration: 50 loss: 0.9458270972731125 grad: -0.4535475413559006
iteration: 60 loss: 0.8034893428021364 grad: -0.39439201393364615
iteration: 70 loss: 0.6988838985719419 grad: -0.3488888867702208
iteration: 80 loss: 0.6186730307956829 grad: -0.31284952476218575
iteration: 90 loss: 0.5551662858656115 grad: -0.2836135774686053
iteration: 0 loss: 42.09315072767847 grad: 380.245851390034
iteration: 10 loss: 4.341506521056297 grad: -0.8948983736268377
iteration: 20 loss: 2.414597130123308 grad: -0.8587420147762141
iteration: 30 loss: 1.6791410215027596 grad: -0.6800984301906623
iteration: 40 loss: 1.288993524614935 grad: -0.5553608044641622
iteration: 50 loss: 1.0467582088886873 grad: -0.46804784288676876
iteration: 60 loss: 0.8815724479021892 grad: -0.4042387892181755
iteration: 70 loss: 0.7616524633733041 grad: -0.3557426599262825
iteration: 80 loss: 0.6706004681964259 grad: -0.31768771936086826
iteration: 90 loss: 0.599093506365865 grad: -0.28704459402233096
iteration: 100 loss: 0.5414369277819291 grad: -0.2618437812343447
iteration: 0 loss: 50.405041323099255 grad: 439.1164944262195
iteration: 10 loss: 3.881230862789628 grad: -1.116238465266508
iteration: 20 loss: 2.2059510242910703 grad: -0.9458616831661212
iteration: 30 loss: 1.551984523558572 grad: -0.7365777477295703
iteration: 40 loss: 1.20001675999728 grad: -0.5979139542882189
iteration: 50 loss: 0.9793320446230444 grad: -0.5023984661210568
iteration: 60 loss: 0.827768133314211 grad: -0.4330953007884278
iteration: 70 loss: 0.7171407811103812 grad: -0.38063026619550955
iteration: 80 loss: 0.6327860901904139 grad: -0.33956176750099903
iteration: 90 loss: 0.5663102237723169 grad: -0.3065477173165799
iteration: 100 loss: 0.5125574118112788 grad: -0.27943093650529427
iteration: 0 loss: 46.26680587116277 grad: 375.9497595588815
iteration: 10 loss: 4.330283727482476 grad: -0.21239262957869884
iteration: 20 loss: 2.4151624879260782 grad: -0.5455611826692891
iteration: 30 loss: 1.68380497766384 grad: -0.48948574018143537
iteration: 40 loss: 1.2949433745787824 grad: -0.4224063092709154
iteration: 50 loss: 1.053013310779699 grad: -0.36770521155787733
iteration: 60 loss: 0.8877595946804088 grad: -0.32453807905776605
iteration: 70 loss: 0.7676261612359667 grad: -0.29013491868180347
iteration: 80 loss: 0.6763088678396592 grad: -0.262239632276344
iteration: 90 loss: 0.6045254381302088 grad: -0.23922686272016966
iteration: 100 loss: 0.5465991706089219 grad: -0.21994283009830315
iteration: 0 loss: 40.24718892169162 grad: 354.2524105434304
iteration: 10 loss: 4.400793895184382 grad: -0.707125728749666
iteration: 20 loss: 2.442568882337913 grad: -0.7260883403722738
iteration: 30 loss: 1.6974574166628869 grad: -0.5884251637488754
iteration: 40 loss: 1.3027176329064787 grad: -0.48669032075832297
iteration: 50 loss: 1.0577899827983195 grad: -0.41364731285856665
iteration: 60 loss: 0.8908260748504621 grad: -0.3594346361996771
iteration: 70 loss: 0.7696389348488992 grad: -0.31778699282408956
iteration: 80 loss: 0.6776352885602716 grad: -0.28484271128404176
iteration: 90 loss: 0.6053857265595649 grad: -0.25814748830310663
iteration: 100 loss: 0.5471325370903369 grad: -0.23608122466669465
iteration: 0 loss: 52.99578257204671 grad: 429.9388913626759
iteration: 10 loss: 3.978632635779377 grad: -0.3328552885201812
iteration: 20 loss: 2.26807632359305 grad: -0.6049920314091162
iteration: 30 loss: 1.599224067752361 grad: -0.527172535765773
iteration: 40 loss: 1.238335366078073 grad: -0.44993207250318
iteration: 50 loss: 1.0116219727024558 grad: -0.38941426985354377
iteration: 60 loss: 0.8556942638187635 grad: -0.34248251574059
iteration: 70 loss: 0.7417567476325282 grad: -0.30544079703353866
iteration: 80 loss: 0.6548027808005993 grad: -0.2755910279804705
iteration: 90 loss: 0.5862305306898385 grad: -0.2510707359397666
iteration: 100 loss: 0.5307504302025502 grad: -0.23058755544345436
iteration: 0 loss: 46.936731789893905 grad: 352.07684878201513
iteration: 10 loss: 4.171978463821564 grad: -0.30641473253683704
iteration: 20 loss: 2.3313565939494185 grad: -0.6352145699079788
iteration: 30 loss: 1.6263636021355772 grad: -0.5520086798680144
iteration: 40 loss: 1.2510621627421146 grad: -0.46892032475207446
iteration: 50 loss: 1.0174404811979334 grad: -0.40422562969362286
iteration: 60 loss: 0.8578174133517265 grad: -0.35435365709737743
iteration: 70 loss: 0.7417593646384624 grad: -0.3151880738074294
iteration: 80 loss: 0.6535319477199912 grad: -0.28375793033247465
iteration: 90 loss: 0.5841736326306504 grad: -0.25802946306503965
iteration: 100 loss: 0.5282024207247311 grad: -0.23660075212865905
iteration: 0 loss: 51.49946079069596 grad: 438.1475755555821
iteration: 10 loss: 3.6902262006519777 grad: -0.783890460260062
iteration: 20 loss: 2.118939903390087 grad: -0.7411498449209115
iteration: 30 loss: 1.5037392240658598 grad: -0.6103824116907659
iteration: 40 loss: 1.169881464511532 grad: -0.5094254501931841
iteration: 50 loss: 0.9590652419052194 grad: -0.43557657978811387
iteration: 60 loss: 0.8134476482248995 grad: -0.3801232144161932
iteration: 70 loss: 0.7066643138647454 grad: -0.33716968718593004
iteration: 80 loss: 0.6249269272913641 grad: -0.3029804009039263
iteration: 90 loss: 0.5603055211843391 grad: -0.27514058283734655
iteration: 0 loss: 46.46054123078704 grad: 330.79359741319
iteration: 10 loss: 4.206645636239128 grad: -0.017088839268348987
iteration: 20 loss: 2.3561195156517813 grad: -0.5568473764041253
iteration: 30 loss: 1.645277745015433 grad: -0.512683828246915
iteration: 40 loss: 1.2662463099807775 grad: -0.4438101004405035
iteration: 50 loss: 1.0300787413358525 grad: -0.38609958718246695
iteration: 60 loss: 0.8686198180919085 grad: -0.34028689403223056
iteration: 70 loss: 0.7511801491435274 grad: -0.3037456253563148
iteration: 80 loss: 0.6618775675139493 grad: -0.2741403979702668
iteration: 90 loss: 0.5916599218832992 grad: -0.24974966473369556
iteration: 100 loss: 0.5349868078786709 grad: -0.22934132184765593
iteration: 0 loss: 53.14273582479024 grad: 415.0530071215622
iteration: 10 loss: 3.9571213615288965 grad: -0.5383728206371671
iteration: 20 loss: 2.2560298468719835 grad: -0.6684734247105785
iteration: 30 loss: 1.5899059599596512 grad: -0.5579925057022318
iteration: 40 loss: 1.2306136861688257 grad: -0.46772791949864806
iteration: 50 loss: 1.0050020466562108 grad: -0.40069806224366544
iteration: 60 loss: 0.8498909964512843 grad: -0.3500531978191467
iteration: 70 loss: 0.7365859121210551 grad: -0.3107027657908306
iteration: 80 loss: 0.6501372411851001 grad: -0.27932663817697073
iteration: 90 loss: 0.5819784861335144 grad: -0.25375010250758456
iteration: 100 loss: 0.5268432880138789 grad: -0.23250986600293871
iteration: 0 loss: 44.46559491814816 grad: 328.48727962964756
iteration: 10 loss: 4.312632712781248 grad: 0.26333070710596435
iteration: 20 loss: 2.4050525181731413 grad: -0.41908471627419414
iteration: 30 loss: 1.6765766065785266 grad: -0.42860614358415633
iteration: 40 loss: 1.2892457259232204 grad: -0.3849762327988004
iteration: 50 loss: 1.0482855296461986 grad: -0.34151616627353887
iteration: 60 loss: 0.8837096401208658 grad: -0.3047217832617048
iteration: 70 loss: 0.7640800468697804 grad: -0.2743442160824148
iteration: 80 loss: 0.6731534874512694 grad: -0.2491904772810471
iteration: 90 loss: 0.6016826483451868 grad: -0.22815049413689267
iteration: 100 loss: 0.5440125031908875 grad: -0.21034681741035977
iteration: 0 loss: 47.21009437384677 grad: 411.0559656315439
iteration: 10 loss: 4.223316199069028 grad: -1.0207663674866194
iteration: 20 loss: 2.371059614885367 grad: -0.8672629198510237
iteration: 30 loss: 1.6553108219167867 grad: -0.6791692588780616
iteration: 40 loss: 1.2734506768933485 grad: -0.5530938564961281
iteration: 50 loss: 1.035563538036513 grad: -0.46573961007449965
iteration: 60 loss: 0.8729806574333452 grad: -0.40213716288513096
iteration: 70 loss: 0.7547618962567785 grad: -0.35387485705796656
iteration: 80 loss: 0.6648938731483957 grad: -0.31603084938208326
iteration: 90 loss: 0.5942506435655808 grad: -0.28556750695359223
iteration: 100 loss: 0.5372476766472325 grad: -0.26051772334389867
iteration: 0 loss: 48.0848756171032 grad: 358.77118126014636
iteration: 10 loss: 4.241955205870784 grad: -0.898779743520344
iteration: 20 loss: 2.3760668362761366 grad: -0.8671032091385653
iteration: 30 loss: 1.6575833710820171 grad: -0.6869640971001516
iteration: 40 loss: 1.2747647272022236 grad: -0.5613415928089833
iteration: 50 loss: 1.0364417827524697 grad: -0.4733730072305139
iteration: 60 loss: 0.8736265189079198 grad: -0.40903800414902597
iteration: 70 loss: 0.7552694027657377 grad: -0.3601066990304548
iteration: 80 loss: 0.6653119917824287 grad: -0.32168540111912924
iteration: 90 loss: 0.5946072397867449 grad: -0.29072991329211384
iteration: 100 loss: 0.5375597426635234 grad: -0.26525994693995014
iteration: 0 loss: 44.31901937188749 grad: 381.87648785699105
iteration: 10 loss: 4.210919509174098 grad: -1.1626759149965216
iteration: 20 loss: 2.362522059199033 grad: -0.941891459013592
iteration: 30 loss: 1.6484380092885906 grad: -0.7240324784877674
iteration: 40 loss: 1.267682905914667 grad: -0.583855357700315
iteration: 50 loss: 1.0305947762704735 grad: -0.4885991207846226
iteration: 60 loss: 0.8686165458317109 grad: -0.4200510098588682
iteration: 70 loss: 0.750870861920248 grad: -0.368448366208137
iteration: 80 loss: 0.661382843662564 grad: -0.32822005352657124
iteration: 90 loss: 0.5910514565226467 grad: -0.29598210469680236
iteration: 100 loss: 0.5343089674647259 grad: -0.26956756567837714
iteration: 0 loss: 53.91744459191816 grad: 377.3973351124947
iteration: 10 loss: 3.9623201362219747 grad: 0.1232313796788143
iteration: 20 loss: 2.2528334493890725 grad: -0.519066164961631
iteration: 30 loss: 1.5858112433365932 grad: -0.504851637945589
iteration: 40 loss: 1.2264817078156822 grad: -0.4461797813610009
iteration: 50 loss: 1.0010447467425778 grad: -0.3924420793975343
iteration: 60 loss: 0.8461627934349384 grad: -0.3482237734952707
iteration: 70 loss: 0.7330909053263316 grad: -0.3122514640050091
iteration: 80 loss: 0.6468623845218066 grad: -0.2827396980351838
iteration: 90 loss: 0.5789055332517653 grad: -0.258214272542362
iteration: 100 loss: 0.5239533689498951 grad: -0.23756253153099668
iteration: 0 loss: 42.39176277092758 grad: 316.11558435594134
iteration: 10 loss: 4.306400511536636 grad: 0.14851937687100358
iteration: 20 loss: 2.394342255220801 grad: -0.3847483929821527
iteration: 30 loss: 1.6668644041110383 grad: -0.3870078364751043
iteration: 40 loss: 1.2808800563529985 grad: -0.34756456628266436
iteration: 50 loss: 1.0410558333390492 grad: -0.3089626204019875
iteration: 60 loss: 0.877385838042342 grad: -0.27631556228940307
iteration: 70 loss: 0.7584789717207472 grad: -0.24931367112224934
iteration: 80 loss: 0.6681365168625152 grad: -0.22690176329718853
iteration: 90 loss: 0.5971449950819657 grad: -0.20811008254982605
iteration: 100 loss: 0.5398739958781004 grad: -0.19217286752955753
iteration: 0 loss: 56.52835015194219 grad: 391.7417952287413
iteration: 10 loss: 4.012770070620098 grad: -0.25253800176635355
iteration: 20 loss: 2.2821075300793052 grad: -0.6117556173975234
iteration: 30 loss: 1.6058384103733012 grad: -0.5393863411539233
iteration: 40 loss: 1.2416207562280925 grad: -0.4611498039678952
iteration: 50 loss: 1.0131939477942895 grad: -0.3990335671224815
iteration: 60 loss: 0.856301848478655 grad: -0.3506941350940737
iteration: 70 loss: 0.7417879523416311 grad: -0.3125122751849172
iteration: 80 loss: 0.6544752987414623 grad: -0.2817498291887931
iteration: 90 loss: 0.585673987901497 grad: -0.2564941247316938
iteration: 100 loss: 0.5300456132294626 grad: -0.23541140620839765
iteration: 0 loss: 48.95479379572221 grad: 398.6840342155616
iteration: 10 loss: 3.9625588435281895 grad: -0.790360462647757
iteration: 20 loss: 2.251641188678894 grad: -0.8164871383810268
iteration: 30 loss: 1.5825925855376475 grad: -0.6566218034909023
iteration: 40 loss: 1.222577217054591 grad: -0.5403685503888384
iteration: 50 loss: 0.9969863727881768 grad: -0.45762723562444196
iteration: 60 loss: 0.8421608174784193 grad: -0.3965660798575978
iteration: 70 loss: 0.7292284192182954 grad: -0.34985191315730446
iteration: 80 loss: 0.6431688797114558 grad: -0.3130191617798474
iteration: 90 loss: 0.5753868942919408 grad: -0.2832512540768166
iteration: 100 loss: 0.5206049780784021 grad: -0.2586990352091557
iteration: 0 loss: inf grad: 4064.670163408508
iteration: 0 loss: inf grad: 3814.7452293395445
iteration: 0 loss: inf grad: 3860.237498849524
iteration: 0 loss: inf grad: 4231.744843343592
iteration: 0 loss: inf grad: 3680.682718511753
iteration: 0 loss: inf grad: 3285.9709200159928
iteration: 0 loss: inf grad: 3797.9571023334975
iteration: 0 loss: inf grad: 3110.3605878393464
iteration: 0 loss: inf grad: 3981.4901243522427
iteration: 0 loss: inf grad: 3701.289098758971
iteration: 0 loss: inf grad: 3388.7103246011443
iteration: 0 loss: inf grad: 3981.324321606203
iteration: 0 loss: inf grad: 3370.6597842516226
iteration: 0 loss: inf grad: 3764.1206065752885
iteration: 0 loss: inf grad: 3821.2133728344197
iteration: 0 loss: inf grad: 4325.471352863087
iteration: 0 loss: inf grad: 3712.5777729258916
iteration: 0 loss: inf grad: 3465.9978752877605
iteration: 0 loss: inf grad: 4337.450124880823
iteration: 0 loss: inf grad: 3511.2573333351156
iteration: 0 loss: inf grad: 3976.6635828936733
iteration: 0 loss: inf grad: 4133.37083042874
iteration: 0 loss: inf grad: 3337.998598609458
iteration: 0 loss: inf grad: 3672.579059460361
iteration: 0 loss: inf grad: 3854.679242368269
iteration: 0 loss: inf grad: 3715.819280398574
iteration: 0 loss: inf grad: 4356.720036673285
iteration: 0 loss: inf grad: 4218.615092146338
iteration: 0 loss: inf grad: 3799.1727123048963
iteration: 0 loss: inf grad: 3499.6680079031257
iteration: 0 loss: inf grad: 4607.832498244394
iteration: 0 loss: inf grad: 4010.1044022208935
iteration: 0 loss: inf grad: 3786.9137474270256
iteration: 0 loss: inf grad: 3931.1196283068452
iteration: 0 loss: inf grad: 4053.4642540489226
iteration: 0 loss: inf grad: 3941.732046133114
iteration: 0 loss: inf grad: 3779.8246908892243
iteration: 0 loss: inf grad: 4366.073534364143
iteration: 0 loss: inf grad: 3603.183129301701
iteration: 0 loss: inf grad: 3668.3547320463213
iteration: 0 loss: inf grad: 3846.896363747811
iteration: 0 loss: inf grad: 3337.5403401399235
iteration: 0 loss: inf grad: 4747.550448264843
iteration: 0 loss: inf grad: 3539.6525296053824
iteration: 0 loss: inf grad: 4244.151077189932
iteration: 0 loss: inf grad: 3929.8556785881683
iteration: 0 loss: inf grad: 3458.7559103408007
iteration: 0 loss: inf grad: 3733.0581511889523
iteration: 0 loss: inf grad: 4119.574682841421
iteration: 0 loss: inf grad: 4138.748184832517
iteration: 0 loss: inf grad: 4275.105282076312
iteration: 0 loss: inf grad: 3843.9056990699587
iteration: 0 loss: inf grad: 4381.432656612644
iteration: 0 loss: inf grad: 3919.6662162447997
iteration: 0 loss: inf grad: 3545.672579190086
iteration: 0 loss: inf grad: 3820.114746340228
iteration: 0 loss: inf grad: 3704.672684266724
iteration: 0 loss: inf grad: 3920.3559217366137
iteration: 0 loss: inf grad: 4447.590291299297
iteration: 0 loss: inf grad: 3770.0580013439003
iteration: 0 loss: inf grad: 4158.888058242703
iteration: 0 loss: inf grad: 4388.408822212043
iteration: 0 loss: inf grad: 3261.5334349007553
iteration: 0 loss: inf grad: 3583.2150175352826
iteration: 0 loss: inf grad: 3878.24271657198
iteration: 0 loss: inf grad: 3792.3966201912144
iteration: 0 loss: inf grad: 3424.0653003146917
iteration: 0 loss: inf grad: 4178.896364083343
iteration: 0 loss: inf grad: 3825.5969004549056
iteration: 0 loss: inf grad: 4070.0364604022334
iteration: 0 loss: inf grad: 4151.033493520541
iteration: 0 loss: inf grad: 4617.118559304716
iteration: 0 loss: inf grad: 3568.960187637069
iteration: 0 loss: inf grad: 4121.056289774878
iteration: 0 loss: inf grad: 3712.796865839917
iteration: 0 loss: inf grad: 3447.47314860712
iteration: 0 loss: inf grad: 3477.6683021696854
iteration: 0 loss: inf grad: 3453.3304849999286
iteration: 0 loss: inf grad: 4011.7445455140423
iteration: 0 loss: inf grad: 4067.423069132956
iteration: 0 loss: inf grad: 4410.515097802854
iteration: 0 loss: inf grad: 4211.166248695905
iteration: 0 loss: inf grad: 4048.1472217430032
iteration: 0 loss: inf grad: 4216.089898268134
iteration: 0 loss: inf grad: 4218.613685050683
iteration: 0 loss: inf grad: 3671.3825985393532
iteration: 0 loss: inf grad: 4334.258210372542
iteration: 0 loss: inf grad: 3883.5740025498103
iteration: 0 loss: inf grad: 3326.669118489581
iteration: 0 loss: inf grad: 3161.454608952603
iteration: 0 loss: inf grad: 3465.3934481314623
iteration: 0 loss: inf grad: 3979.2386827674036
iteration: 0 loss: inf grad: 3736.2519336694977
iteration: 0 loss: inf grad: 3619.854033183873
iteration: 0 loss: inf grad: 3506.212538254747
iteration: 0 loss: inf grad: 4393.3366387467395
iteration: 0 loss: inf grad: 3986.252323852054
iteration: 0 loss: inf grad: 3850.9211936684305
iteration: 0 loss: inf grad: 3816.7988297825573
iteration: 0 loss: inf grad: 3290.750155424482
iteration: 0 loss: inf grad: 3941.8879471065507
iteration: 0 loss: inf grad: 3644.0287371603963
iteration: 0 loss: inf grad: 3838.276194356772
iteration: 0 loss: inf grad: 4723.550052960734
iteration: 0 loss: inf grad: 3462.323095689959
iteration: 0 loss: inf grad: 3331.589763432547
iteration: 0 loss: inf grad: 3507.8517981157306
iteration: 0 loss: inf grad: 3906.5517402319906
iteration: 0 loss: inf grad: 4008.139399716126
iteration: 0 loss: inf grad: 4278.065490497849
iteration: 0 loss: inf grad: 4202.3411019944815
iteration: 0 loss: inf grad: 4043.087718535862
iteration: 0 loss: inf grad: 3835.6460577573484
iteration: 0 loss: inf grad: 4329.426938420353
iteration: 0 loss: inf grad: 4042.3356336441725
iteration: 0 loss: inf grad: 3795.6093887462093
iteration: 0 loss: inf grad: 3808.3530087321433
iteration: 0 loss: inf grad: 3737.7245964516287
iteration: 0 loss: inf grad: 4336.619196763668
iteration: 0 loss: inf grad: 3827.999169837817
iteration: 0 loss: inf grad: 3643.879564834535
iteration: 0 loss: inf grad: 3256.769226352161
iteration: 0 loss: inf grad: 4156.065237382243
iteration: 0 loss: inf grad: 4045.106552323178
iteration: 0 loss: inf grad: 3985.6798888293965
iteration: 0 loss: inf grad: 4141.648674457321
iteration: 0 loss: inf grad: 3751.020437721586
iteration: 0 loss: inf grad: 3913.2184402362323
iteration: 0 loss: inf grad: 4347.820152956717
iteration: 0 loss: inf grad: 3719.032034342631
iteration: 0 loss: inf grad: 3978.7610409390986
iteration: 0 loss: inf grad: 3793.9910375615855
iteration: 0 loss: inf grad: 4373.644554916059
iteration: 0 loss: inf grad: 4144.372968974958
iteration: 0 loss: inf grad: 3444.0389187864666
iteration: 0 loss: inf grad: 3336.563768070032
iteration: 0 loss: inf grad: 4109.74489289219
iteration: 0 loss: inf grad: 3201.149064145356
iteration: 0 loss: inf grad: 3236.9058836343356
iteration: 0 loss: inf grad: 3314.8870740689463
iteration: 0 loss: inf grad: 4218.786230922249
iteration: 0 loss: inf grad: 3615.94585123898
iteration: 0 loss: inf grad: 4206.068300144385
iteration: 0 loss: inf grad: 4213.392754620294
iteration: 0 loss: inf grad: 4510.019168346746
iteration: 0 loss: inf grad: 3577.834702564788
iteration: 0 loss: inf grad: 3620.099175668749
iteration: 0 loss: inf grad: 4070.5480916238894
iteration: 0 loss: inf grad: 3938.1413032150276
iteration: 0 loss: inf grad: 4558.745299514412
iteration: 0 loss: inf grad: 3861.249275810147
iteration: 0 loss: inf grad: 4365.589093341874
iteration: 0 loss: inf grad: 3831.1178439967994
iteration: 0 loss: inf grad: 3614.467037866265
iteration: 0 loss: inf grad: 4281.815800719451
iteration: 0 loss: inf grad: 3593.1447136156867
iteration: 0 loss: inf grad: 4355.4013893091305
iteration: 0 loss: inf grad: 3414.9339367074235
iteration: 0 loss: inf grad: 4150.010005964375
iteration: 0 loss: inf grad: 3389.3535268355267
iteration: 0 loss: inf grad: 4126.994875908673
iteration: 0 loss: inf grad: 3679.7835032928315
iteration: 0 loss: inf grad: 3830.1281128822325
iteration: 0 loss: inf grad: 3830.249195854828
iteration: 0 loss: inf grad: 3263.530831885629
iteration: 0 loss: inf grad: 3959.036368976206
iteration: 0 loss: inf grad: 4005.062490112029
gradient_descent.py:22: RuntimeWarning: overflow encountered in exp
  term2 -= np.sum(np.log(np.exp(C) + np.exp(-C)))
gradient_descent.py:57: RuntimeWarning: invalid value encountered in double_scalars
  diff = np.absolute(f_history[-1]-f_history[-2])
iteration: 0 loss: 62.09826814225366 grad: 228.6069898774509
iteration: 10 loss: 11.269069139340196 grad: -1.6834315996906932
iteration: 20 loss: 6.510238586198077 grad: -1.6846888859928129
iteration: 30 loss: 4.560190365096002 grad: -1.3443893458615306
iteration: 40 loss: 3.5038913037172033 grad: -1.0956295112936556
iteration: 50 loss: 2.8430635906804373 grad: -0.9192896130910334
iteration: 60 loss: 2.3912015439148657 grad: -0.7901015300025215
iteration: 70 loss: 2.062951007186918 grad: -0.6920487099224655
iteration: 80 loss: 1.8137980641298037 grad: -0.6153279875664145
iteration: 90 loss: 1.6182794184383882 grad: -0.5537638091662517
iteration: 100 loss: 1.460787060825954 grad: -0.5033173180302357
iteration: 110 loss: 1.331225853925141 grad: -0.4612513718971496
iteration: 120 loss: 1.2227793958372373 grad: -0.4256514244995982
iteration: 130 loss: 1.1306800937126023 grad: -0.3951408475010913
iteration: 140 loss: 1.0514938510441838 grad: -0.36870572891171227
iteration: 150 loss: 0.9826846810060075 grad: -0.345583469892414
iteration: 160 loss: 0.9223393387929222 grad: -0.32518982848539607
iteration: 170 loss: 0.8689873548603579 grad: -0.30706986335772707
iteration: 0 loss: 64.15545991067837 grad: 205.31639922022828
iteration: 10 loss: 11.86080119214514 grad: -0.5020881196175582
iteration: 20 loss: 6.803119973628864 grad: -1.1963499471533683
iteration: 30 loss: 4.757734368892918 grad: -1.0504056725424686
iteration: 40 loss: 3.6541626512817773 grad: -0.8884773758150954
iteration: 50 loss: 2.964826911625633 grad: -0.760719218995547
iteration: 60 loss: 2.4937837245384302 grad: -0.6623858374267864
iteration: 70 loss: 2.1516934694294916 grad: -0.5855756982512167
iteration: 80 loss: 1.8920584801037628 grad: -0.5243209262353107
iteration: 90 loss: 1.6883124352746728 grad: -0.47448947181878104
iteration: 100 loss: 1.524183061663073 grad: -0.433229045373698
iteration: 110 loss: 1.389150319778227 grad: -0.39853757240581594
iteration: 120 loss: 1.2761128717920243 grad: -0.36897951052727324
iteration: 130 loss: 1.1801046463844862 grad: -0.34350333998280813
iteration: 140 loss: 1.0975489005600552 grad: -0.3213233221006264
iteration: 150 loss: 1.0258044065299972 grad: -0.30184155817863184
iteration: 160 loss: 0.962878526784979 grad: -0.2845955644385653
iteration: 170 loss: 0.9072397392212472 grad: -0.2692222842639532
iteration: 0 loss: 67.07608753608652 grad: 210.49250159521605
iteration: 10 loss: 11.829012283867355 grad: -1.1783466882233564
iteration: 20 loss: 6.784631518744928 grad: -1.4613198726023064
iteration: 30 loss: 4.745365390055006 grad: -1.1896378812706785
iteration: 40 loss: 3.6453270453085596 grad: -0.9774298707202025
iteration: 50 loss: 2.958176698620801 grad: -0.8244113403825921
iteration: 60 loss: 2.488570841592512 grad: -0.711336564385068
iteration: 70 loss: 2.147475842162542 grad: -0.6250139486946548
iteration: 80 loss: 1.888559596535825 grad: -0.5571715817345716
iteration: 90 loss: 1.68535075922831 grad: -0.5025354746881435
iteration: 100 loss: 1.521634458887175 grad: -0.45762997278811807
iteration: 110 loss: 1.3869269483166136 grad: -0.4200864829891703
iteration: 120 loss: 1.2741507395885079 grad: -0.38824095578765155
iteration: 130 loss: 1.1783559638214351 grad: -0.36089251066346073
iteration: 140 loss: 1.0959772055635684 grad: -0.3371539114225356
iteration: 150 loss: 1.0243813565882027 grad: -0.3163560100744922
iteration: 160 loss: 0.9615817281701311 grad: -0.29798491174519465
iteration: 170 loss: 0.9060512414668913 grad: -0.28163957754388846
iteration: 0 loss: 62.28748701085415 grad: 236.80096733076672
iteration: 10 loss: 11.817378638745573 grad: -1.1817572375396495
iteration: 20 loss: 6.74693215979253 grad: -1.3853152261950785
iteration: 30 loss: 4.70793750636734 grad: -1.139749633292933
iteration: 40 loss: 3.6117510008680256 grad: -0.9430962301428631
iteration: 50 loss: 2.9284513647127244 grad: -0.7990752124476482
iteration: 60 loss: 2.462142667994654 grad: -0.6916234510749544
iteration: 70 loss: 2.123788394316472 grad: -0.6090672180372324
iteration: 80 loss: 1.8671488098272924 grad: -0.5438895517207489
iteration: 90 loss: 1.665845829343983 grad: -0.49122133163179416
iteration: 100 loss: 1.503741061918266 grad: -0.44781948391109494
iteration: 110 loss: 1.3704102355165808 grad: -0.4114569968263001
iteration: 120 loss: 1.2588215526676654 grad: -0.38056027094975287
iteration: 130 loss: 1.1640603249772374 grad: -0.35398859345984507
iteration: 140 loss: 1.0825884398399135 grad: -0.3308961413849341
iteration: 150 loss: 1.0117941913119715 grad: -0.310643113048219
iteration: 160 loss: 0.9497078460672128 grad: -0.29273693662303873
iteration: 170 loss: 0.8948158989630677 grad: -0.2767924074337985
iteration: 0 loss: 64.64610871143007 grad: 198.38200508865546
iteration: 10 loss: 11.768156137550415 grad: -0.7076312163246866
iteration: 20 loss: 6.747421214216419 grad: -1.309979698904568
iteration: 30 loss: 4.7203506513888405 grad: -1.105289514504619
iteration: 40 loss: 3.6270806825060307 grad: -0.921320938916198
iteration: 50 loss: 2.944098426537907 grad: -0.7834336329032721
iteration: 60 loss: 2.4772663216469315 grad: -0.6796057963442518
iteration: 70 loss: 2.138126176320549 grad: -0.5994351869388288
iteration: 80 loss: 1.880649102055031 grad: -0.535937574030386
iteration: 90 loss: 1.6785366058082383 grad: -0.4845090112429346
iteration: 100 loss: 1.515678695153011 grad: -0.442054082587892
iteration: 110 loss: 1.3816585374400925 grad: -0.40643468200871036
iteration: 120 loss: 1.2694431330920783 grad: -0.37613391949306374
iteration: 130 loss: 1.1741132251199815 grad: -0.3500489031510384
iteration: 140 loss: 1.0921250827542612 grad: -0.3273599496174112
iteration: 150 loss: 1.0208613457784437 grad: -0.3074458615067953
iteration: 160 loss: 0.9583470113056219 grad: -0.28982763911577514
iteration: 170 loss: 0.9030638369767158 grad: -0.2741301901921848
iteration: 0 loss: 62.00663285031007 grad: 168.00065335597435
iteration: 10 loss: 11.491386410287056 grad: 0.2491440058949846
iteration: 20 loss: 6.637120488672488 grad: -1.0666559993244216
iteration: 30 loss: 4.6554159098624375 grad: -1.006028142238108
iteration: 40 loss: 3.5814810666783243 grad: -0.8707909685657961
iteration: 50 loss: 2.908894394314261 grad: -0.7541823619501038
iteration: 60 loss: 2.4484919791765276 grad: -0.6612219331101448
iteration: 70 loss: 2.1137129391774825 grad: -0.5872159892745321
iteration: 80 loss: 1.8593908919493431 grad: -0.5274875437290899
iteration: 90 loss: 1.6596709982324105 grad: -0.4784963834699342
iteration: 100 loss: 1.498693389627409 grad: -0.4376875743999251
iteration: 110 loss: 1.366192567573514 grad: -0.40321892888602495
iteration: 120 loss: 1.2552323604588558 grad: -0.3737454648607089
iteration: 130 loss: 1.160958332700995 grad: -0.34826908630917536
iteration: 140 loss: 1.079871797089006 grad: -0.32603659864202206
iteration: 150 loss: 1.009387685646123 grad: -0.30647043434820254
iteration: 160 loss: 0.9475547665932192 grad: -0.2891210597377425
iteration: 170 loss: 0.8928726972761235 grad: -0.2736337840822668
iteration: 0 loss: 64.94281173512287 grad: 201.92565340057126
iteration: 10 loss: 11.85077632224387 grad: 0.11969139723903116
iteration: 20 loss: 6.7584232307334045 grad: -0.9515585812192633
iteration: 30 loss: 4.7231189248270145 grad: -0.9199453615080051
iteration: 40 loss: 3.6281690375358266 grad: -0.8072453717633101
iteration: 50 loss: 2.94475456539261 grad: -0.7052589171865713
iteration: 60 loss: 2.47780529041806 grad: -0.6221216004711563
iteration: 70 loss: 2.1386377840617983 grad: -0.5550304975919584
iteration: 80 loss: 1.8811585296648032 grad: -0.5003717817502533
iteration: 90 loss: 1.6790489375107427 grad: -0.45522416079194244
iteration: 100 loss: 1.5161928432183809 grad: -0.41741046554324407
iteration: 110 loss: 1.3821718715791411 grad: -0.38532954313140255
iteration: 120 loss: 1.2699530167981885 grad: -0.357796288810669
iteration: 130 loss: 1.1746174693232867 grad: -0.33392225271831
iteration: 140 loss: 1.0926220181245712 grad: -0.31303162884743113
iteration: 150 loss: 1.0213497692956977 grad: -0.2946028453213873
iteration: 160 loss: 0.9588261013568213 grad: -0.2782277717293062
iteration: 170 loss: 0.9035330704683392 grad: -0.2635829182568715
iteration: 0 loss: 66.06118361305226 grad: 158.0415743742746
iteration: 10 loss: 11.993818723813236 grad: 1.1782977960163517
iteration: 20 loss: 6.853399918752282 grad: -0.6382162436907279
iteration: 30 loss: 4.793973631934932 grad: -0.7465390125585507
iteration: 40 loss: 3.685065648007615 grad: -0.6918346540123081
iteration: 50 loss: 2.992464185896758 grad: -0.6210701350884005
iteration: 60 loss: 2.5189610324220912 grad: -0.5570987671144468
iteration: 70 loss: 2.174861065182375 grad: -0.5027816373286934
iteration: 80 loss: 1.9135254241381185 grad: -0.4571418705988141
iteration: 90 loss: 1.7083131639039328 grad: -0.4186422979531116
iteration: 100 loss: 1.5429042691840882 grad: -0.3858963199665643
iteration: 110 loss: 1.406744658060151 grad: -0.3577840100618061
iteration: 120 loss: 1.292707253191731 grad: -0.33342823936801574
iteration: 130 loss: 1.1958058640679652 grad: -0.3121458358120995
iteration: 140 loss: 1.1124476901822247 grad: -0.29340251622474434
iteration: 150 loss: 1.0399784982440574 grad: -0.27677710034606595
iteration: 160 loss: 0.9763949960787283 grad: -0.2619344343885669
iteration: 170 loss: 0.9201568490997157 grad: -0.24860521304467803
iteration: 0 loss: 65.62836287669424 grad: 222.30712804495383
iteration: 10 loss: 11.770804270897502 grad: -0.9173678498325502
iteration: 20 loss: 6.741711034030769 grad: -1.3658627173146196
iteration: 30 loss: 4.71356225491474 grad: -1.142874070177584
iteration: 40 loss: 3.6202678424522428 grad: -0.9502536761676007
iteration: 50 loss: 2.9375433622405 grad: -0.8067733011771973
iteration: 60 loss: 2.4710502050371277 grad: -0.6989968719365924
iteration: 70 loss: 2.1322602649514177 grad: -0.615906472280304
iteration: 80 loss: 1.875118507494583 grad: -0.5501768315501363
iteration: 90 loss: 1.6733172955246531 grad: -0.49699629966019
iteration: 100 loss: 1.5107445181697172 grad: -0.45313586835923403
iteration: 110 loss: 1.37698414217452 grad: -0.4163680047565379
iteration: 120 loss: 1.2650050991272688 grad: -0.3851139834728824
iteration: 130 loss: 1.1698903996460681 grad: -0.35822696512825314
iteration: 140 loss: 1.0880985657087299 grad: -0.33485528168514045
iteration: 150 loss: 1.017014328681977 grad: -0.3143539689942761
iteration: 160 loss: 0.9546645655476079 grad: -0.29622604040009537
iteration: 170 loss: 0.8995326957485965 grad: -0.280082567439209
iteration: 0 loss: 64.93639972422669 grad: 197.87550746923745
iteration: 10 loss: 11.861248678762278 grad: 0.14563018163230546
iteration: 20 loss: 6.777427004583057 grad: -1.020300215067929
iteration: 30 loss: 4.739409481209846 grad: -0.9842177196788374
iteration: 40 loss: 3.641713175144353 grad: -0.8597819637787696
iteration: 50 loss: 2.956152884461558 grad: -0.7479529644499707
iteration: 60 loss: 2.4875634617679183 grad: -0.6573355238645404
iteration: 70 loss: 2.147126222409249 grad: -0.5845856597362382
iteration: 80 loss: 1.8886449557170304 grad: -0.5255810260856506
iteration: 90 loss: 1.6857293744484112 grad: -0.47703209492471954
iteration: 100 loss: 1.5222135959264629 grad: -0.43650687249759884
iteration: 110 loss: 1.3876443104939178 grad: -0.40222769216285276
iteration: 120 loss: 1.2749635031925457 grad: -0.3728853955035166
iteration: 130 loss: 1.1792341084517994 grad: -0.34750273810446275
iteration: 140 loss: 1.0968993377759118 grad: -0.3253391574248903
iteration: 150 loss: 1.0253320231108773 grad: -0.30582500507895916
iteration: 160 loss: 0.962549644466187 grad: -0.28851586119826467
iteration: 170 loss: 0.9070280977736563 grad: -0.2730604239199048
iteration: 0 loss: 66.26596970250338 grad: 177.22714132252423
iteration: 10 loss: 11.6472191374764 grad: -0.2633350831967888
iteration: 20 loss: 6.691654856274454 grad: -1.2065857266526412
iteration: 30 loss: 4.687213739888483 grad: -1.0715892862546148
iteration: 40 loss: 3.6042691158438007 grad: -0.9090885153586745
iteration: 50 loss: 2.92695208963716 grad: -0.7794576170682492
iteration: 60 loss: 2.4636259410697843 grad: -0.6792633217397456
iteration: 70 loss: 2.126843122217658 grad: -0.6008188423608609
iteration: 80 loss: 1.8710489917926596 grad: -0.5381663631834975
iteration: 90 loss: 1.6701933333936474 grad: -0.48714289743689604
iteration: 100 loss: 1.508307384124526 grad: -0.44486116423501604
iteration: 110 loss: 1.3750600363871606 grad: -0.4092884972818151
iteration: 120 loss: 1.2634731036607512 grad: -0.37896437382775744
iteration: 130 loss: 1.168663900295831 grad: -0.3528172921477663
iteration: 140 loss: 1.087113975334143 grad: -0.33004558749444063
iteration: 150 loss: 1.0162239732506515 grad: -0.310038624911419
iteration: 160 loss: 0.9540320461159023 grad: -0.29232367037107404
iteration: 170 loss: 0.8990297775231656 grad: -0.2765293551614285
iteration: 0 loss: 65.624171507209 grad: 221.500149028817
iteration: 10 loss: 11.543103040125226 grad: -1.7071466709644527
iteration: 20 loss: 6.671891596563 grad: -1.7045788735570147
iteration: 30 loss: 4.678572250845884 grad: -1.3502919052794418
iteration: 40 loss: 3.597859845207371 grad: -1.095979620869647
iteration: 50 loss: 2.921119057027185 grad: -0.9172827560155234
iteration: 60 loss: 2.458010199843639 grad: -0.7870421196202066
iteration: 70 loss: 2.121372686036011 grad: -0.6885304915182573
iteration: 80 loss: 1.865718350605262 grad: -0.6116421691689784
iteration: 90 loss: 1.6650092200082904 grad: -0.550059611281928
iteration: 100 loss: 1.50327549107723 grad: -0.4996724772111509
iteration: 110 loss: 1.3701825688286506 grad: -0.45770579222403074
iteration: 120 loss: 1.2587491146675183 grad: -0.42222428508737986
iteration: 130 loss: 1.1640901585987278 grad: -0.3918397306365557
iteration: 140 loss: 1.0826857637124372 grad: -0.36553167582404883
iteration: 150 loss: 1.0119356963468817 grad: -0.3425338617485533
iteration: 160 loss: 0.9498776595710297 grad: -0.32226006246346517
iteration: 170 loss: 0.8950030786701807 grad: -0.304254340833459
iteration: 0 loss: 62.96946456252836 grad: 175.59316624012635
iteration: 10 loss: 11.886505950656453 grad: 1.2817758402079962
iteration: 20 loss: 6.762530825213037 grad: -0.5142953676847386
iteration: 30 loss: 4.7226922902682835 grad: -0.6670705779693069
iteration: 40 loss: 3.627023386666927 grad: -0.6337995873233503
iteration: 50 loss: 2.9436673167966463 grad: -0.5751186291071434
iteration: 60 loss: 2.476935392518705 grad: -0.5189152680972247
iteration: 70 loss: 2.137993215793358 grad: -0.4700486594939586
iteration: 80 loss: 1.880710770627502 grad: -0.428467314074155
iteration: 90 loss: 1.6787643029663493 grad: -0.39311945722265973
iteration: 100 loss: 1.5160409901549097 grad: -0.36289842445956944
iteration: 110 loss: 1.3821275312941246 grad: -0.3368578676163608
iteration: 120 loss: 1.269995727094159 grad: -0.31423451531457247
iteration: 130 loss: 1.1747308105913445 grad: -0.29442327621516573
iteration: 140 loss: 1.0927928269055138 grad: -0.27694536735164904
iteration: 150 loss: 1.0215674623874624 grad: -0.2614202360403304
iteration: 160 loss: 0.9590821308321954 grad: -0.24754320778392752
iteration: 170 loss: 0.9038204954120126 grad: -0.2350683232031608
iteration: 0 loss: 65.96914194488146 grad: 202.72478619463257
iteration: 10 loss: 11.582599514412571 grad: -1.2884487715181165
iteration: 20 loss: 6.667550101159577 grad: -1.4483609681777008
iteration: 30 loss: 4.670440703035018 grad: -1.178148559608193
iteration: 40 loss: 3.5907578124293567 grad: -0.9701161040273345
iteration: 50 loss: 2.9154539718744505 grad: -0.8197258461802797
iteration: 60 loss: 2.4535509843636563 grad: -0.7082351054186435
iteration: 70 loss: 2.1178466950124277 grad: -0.6228994219791946
iteration: 80 loss: 1.8629055493762197 grad: -0.5556971014747927
iteration: 90 loss: 1.6627447131456228 grad: -0.50149128246258
iteration: 100 loss: 1.501437376067894 grad: -0.4568839945549802
iteration: 110 loss: 1.3686802905881037 grad: -0.4195524966886998
iteration: 120 loss: 1.2575146547438552 grad: -0.38786088097229215
iteration: 130 loss: 1.1630717594384392 grad: -0.3606261536072123
iteration: 140 loss: 1.0818435209017396 grad: -0.3369728023323465
iteration: 150 loss: 1.0112384547317725 grad: -0.31623957365911526
iteration: 160 loss: 0.9493008071864427 grad: -0.2979180159208096
iteration: 170 loss: 0.894526960010601 grad: -0.2816109223760561
iteration: 0 loss: 65.23802974388778 grad: 210.3775050383523
iteration: 10 loss: 11.985639373999406 grad: -0.27302480690598946
iteration: 20 loss: 6.828896452660708 grad: -1.038511725139999
iteration: 30 loss: 4.76763338309386 grad: -0.9493993930605421
iteration: 40 loss: 3.65985451469855 grad: -0.8185522870124852
iteration: 50 loss: 2.9690231400223968 grad: -0.7090520228403409
iteration: 60 loss: 2.4973188259381542 grad: -0.622297664980223
iteration: 70 loss: 2.1548757965483674 grad: -0.5533077028189364
iteration: 80 loss: 1.8950185667168542 grad: -0.49760433454957387
iteration: 90 loss: 1.6911122577831024 grad: -0.4518731460717192
iteration: 100 loss: 1.5268553874297637 grad: -0.413739698360457
iteration: 110 loss: 1.3917144146177056 grad: -0.3814963853215387
iteration: 120 loss: 1.2785812941672041 grad: -0.3538973708552114
iteration: 130 loss: 1.1824863842800204 grad: -0.3300180198063466
iteration: 140 loss: 1.0998509025076104 grad: -0.30916021342694777
iteration: 150 loss: 1.0280323671933151 grad: -0.2907882424691437
iteration: 160 loss: 0.965037309877723 grad: -0.27448482640960026
iteration: 170 loss: 0.909333616609274 grad: -0.2599204565539694
iteration: 0 loss: 66.21994381445501 grad: 245.926234151315
iteration: 10 loss: 11.611732219524708 grad: -2.1060184214199436
iteration: 20 loss: 6.6752153010147595 grad: -1.7954814575482847
iteration: 30 loss: 4.66748880237678 grad: -1.3952970341136544
iteration: 40 loss: 3.5832414177332548 grad: -1.125178219169202
iteration: 50 loss: 2.905988926043516 grad: -0.9387691530349852
iteration: 60 loss: 2.443331003002682 grad: -0.8039885594335682
iteration: 70 loss: 2.107446786663538 grad: -0.7024849451885935
iteration: 80 loss: 1.8526114280497579 grad: -0.6234740994033652
iteration: 90 loss: 1.652698444046688 grad: -0.5603067617220896
iteration: 100 loss: 1.4917064130336257 grad: -0.5086912549252705
iteration: 110 loss: 1.3592921383747125 grad: -0.4657449342576824
iteration: 120 loss: 1.2484750918218774 grad: -0.42946443393146605
iteration: 130 loss: 1.1543747683219656 grad: -0.39841628002388674
iteration: 140 loss: 1.0734768467954878 grad: -0.3715487362145173
iteration: 150 loss: 1.0031867742397316 grad: -0.3480732034422227
iteration: 160 loss: 0.9415475512740737 grad: -0.32738706638759163
iteration: 170 loss: 0.8870553611322148 grad: -0.30902208743546866
iteration: 0 loss: 63.69738598883107 grad: 204.07229827556367
iteration: 10 loss: 11.688434799086798 grad: -1.0308934697543384
iteration: 20 loss: 6.713330968870775 grad: -1.3534467289615084
iteration: 30 loss: 4.696506457584912 grad: -1.1248252726046593
iteration: 40 loss: 3.6079228163214547 grad: -0.9347395155814707
iteration: 50 loss: 2.927821695906294 grad: -0.7939558527044015
iteration: 60 loss: 2.4630182602468267 grad: -0.6883208681671711
iteration: 70 loss: 2.1254135292089233 grad: -0.6068764866642142
iteration: 80 loss: 1.8691511652461321 grad: -0.5424223866332711
iteration: 90 loss: 1.668029153527393 grad: -0.4902468884609471
iteration: 100 loss: 1.5059970696704774 grad: -0.4471923442032023
iteration: 110 loss: 1.372677455155347 grad: -0.4110813229924646
iteration: 120 loss: 1.261064653201381 grad: -0.3803705772225351
iteration: 130 loss: 1.1662591180859387 grad: -0.3539387931597632
iteration: 140 loss: 1.0847317532268541 grad: -0.3309529953910663
iteration: 150 loss: 1.013876325945396 grad: -0.3107821483137495
iteration: 160 loss: 0.9517264644089003 grad: -0.292939814802331
iteration: 170 loss: 0.8967707348416628 grad: -0.27704517797910555
iteration: 0 loss: 63.11767441852996 grad: 179.14227922104186
iteration: 10 loss: 11.622008881550384 grad: 0.590116200748863
iteration: 20 loss: 6.671474083580216 grad: -0.9083988348382448
iteration: 30 loss: 4.672947085994523 grad: -0.9158039380034058
iteration: 40 loss: 3.5934888386273087 grad: -0.8088735517025062
iteration: 50 loss: 2.9183632918054006 grad: -0.7073544385537428
iteration: 60 loss: 2.456520163262336 grad: -0.6237206354285285
iteration: 70 loss: 2.1208002894245923 grad: -0.5560433485675474
iteration: 80 loss: 1.8658022059548371 grad: -0.5008885706912081
iteration: 90 loss: 1.6655634199727585 grad: -0.4553550217821079
iteration: 100 loss: 1.5041686572522353 grad: -0.41724971188312643
iteration: 110 loss: 1.371321138869751 grad: -0.3849509920565791
iteration: 120 loss: 1.2600656586465364 grad: -0.3572560441490886
iteration: 130 loss: 1.1655354639050612 grad: -0.33326259879697334
iteration: 140 loss: 1.0842234822919388 grad: -0.3122844350537958
iteration: 150 loss: 1.0135386921756435 grad: -0.29379221504966613
iteration: 160 loss: 0.9515254839278978 grad: -0.2773720208356
iteration: 170 loss: 0.8966802002100849 grad: -0.26269603224659444
iteration: 0 loss: 61.75115584158968 grad: 245.57539515500542
iteration: 10 loss: 11.74715471441916 grad: -0.8689339868217733
iteration: 20 loss: 6.733055206590259 grad: -1.39017556963257
iteration: 30 loss: 4.7043570111113695 grad: -1.1767525461896002
iteration: 40 loss: 3.6108594456627956 grad: -0.9813071867711844
iteration: 50 loss: 2.9284049982515667 grad: -0.8336019757498911
iteration: 60 loss: 2.462371116472241 grad: -0.7221065065678025
iteration: 70 loss: 2.1240898298399107 grad: -0.6360027821321312
iteration: 80 loss: 1.8674474231121083 grad: -0.5678586458528327
iteration: 90 loss: 1.6661137059955267 grad: -0.5127285465754159
iteration: 100 loss: 1.5039696559162457 grad: -0.4672737025749747
iteration: 110 loss: 1.3705990029090407 grad: -0.4291840721238841
iteration: 120 loss: 1.258973148717194 grad: -0.3968200763932044
iteration: 130 loss: 1.1641785054453893 grad: -0.3689898758441082
iteration: 140 loss: 1.0826771384902052 grad: -0.3448081775031945
iteration: 150 loss: 1.0118571282639757 grad: -0.32360456152858713
iteration: 160 loss: 0.9497483804789226 grad: -0.3048625055789367
iteration: 170 loss: 0.8948369940166727 grad: -0.2881778881217734
iteration: 0 loss: 63.99541559962691 grad: 184.26942689300495
iteration: 10 loss: 11.785406403375502 grad: 0.47823574244269756
iteration: 20 loss: 6.759648827197617 grad: -0.8662074906904198
iteration: 30 loss: 4.733536546521422 grad: -0.8722599335786708
iteration: 40 loss: 3.639831620155498 grad: -0.7731362755890441
iteration: 50 loss: 2.955979969447154 grad: -0.6783786741066824
iteration: 60 loss: 2.4882232305395435 grad: -0.5998300794701333
iteration: 70 loss: 2.148220859511267 grad: -0.5359447405644002
iteration: 80 loss: 1.8899730769424588 grad: -0.4836651804875496
iteration: 90 loss: 1.6871812045637853 grad: -0.4403589772352089
iteration: 100 loss: 1.523726122332598 grad: -0.4040155300313354
iteration: 110 loss: 1.3891800501903824 grad: -0.3731371173752439
iteration: 120 loss: 1.2764996463717415 grad: -0.3466065173239232
iteration: 130 loss: 1.1807566231120126 grad: -0.32358179753215677
iteration: 140 loss: 1.0983996099854183 grad: -0.3034201555675155
iteration: 150 loss: 1.0268048654553157 grad: -0.2856241155359008
iteration: 160 loss: 0.9639920765604405 grad: -0.26980356813755235
iteration: 170 loss: 0.9084385791313087 grad: -0.2556487810467398
iteration: 0 loss: 65.50001741164127 grad: 217.40144668705
iteration: 10 loss: 11.744047302753353 grad: -0.9138369684358303
iteration: 20 loss: 6.722713682341981 grad: -1.325532645755877
iteration: 30 loss: 4.701496742123006 grad: -1.1150761580963475
iteration: 40 loss: 3.6120929113890683 grad: -0.9287278314259653
iteration: 50 loss: 2.9317082443072153 grad: -0.7890978799337847
iteration: 60 loss: 2.466722781946332 grad: -0.6840186652874023
iteration: 70 loss: 2.128958406630235 grad: -0.6029527174135705
iteration: 80 loss: 1.8725443858073627 grad: -0.5388055660616813
iteration: 90 loss: 1.6712772201195465 grad: -0.4868963843156736
iteration: 100 loss: 1.509107167775834 grad: -0.4440783936018551
iteration: 110 loss: 1.3756573819178728 grad: -0.4081792590479745
iteration: 120 loss: 1.2639224152149753 grad: -0.37765899901155486
iteration: 130 loss: 1.169002601145125 grad: -0.35139886936645837
iteration: 140 loss: 1.0873685133511488 grad: -0.32856809795690867
iteration: 150 loss: 1.016413476506841 grad: -0.3085375857136142
iteration: 160 loss: 0.9541706275058046 grad: -0.29082260243003183
iteration: 170 loss: 0.8991280323477274 grad: -0.2750438311610356
iteration: 0 loss: 66.58343320584017 grad: 230.38276402728673
iteration: 10 loss: 11.542371147025877 grad: -2.0953944239097364
iteration: 20 loss: 6.6549588639661375 grad: -1.7258952014929179
iteration: 30 loss: 4.660357813742642 grad: -1.3371508140960566
iteration: 40 loss: 3.581179166259682 grad: -1.0786767505889816
iteration: 50 loss: 2.906284009020675 grad: -0.9007066210194137
iteration: 60 loss: 2.444837674752927 grad: -0.7720513422623251
iteration: 70 loss: 2.109608873330713 grad: -0.6751284289084839
iteration: 80 loss: 1.855133226395124 grad: -0.5996454293282987
iteration: 90 loss: 1.655412575139797 grad: -0.5392648456934066
iteration: 100 loss: 1.4945138882463984 grad: -0.48989828742043773
iteration: 110 loss: 1.3621326322871192 grad: -0.44880002917102624
iteration: 120 loss: 1.2513111528531804 grad: -0.4140617572423943
iteration: 130 loss: 1.1571829670781633 grad: -0.3843177862243077
iteration: 140 loss: 1.0762425929589303 grad: -0.35856588411608187
iteration: 150 loss: 1.0059011713666968 grad: -0.33605436511152853
iteration: 160 loss: 0.9442054309945227 grad: -0.3162086495937391
iteration: 170 loss: 0.8896540238570609 grad: -0.29858214345891054
iteration: 0 loss: 62.516112049921006 grad: 173.12739350302405
iteration: 10 loss: 11.616260171531373 grad: 0.47500940507038086
iteration: 20 loss: 6.669315640897366 grad: -1.0122796763321185
iteration: 30 loss: 4.671988577470694 grad: -0.9739652159227046
iteration: 40 loss: 3.593086145887314 grad: -0.8466933612636208
iteration: 50 loss: 2.918232107105847 grad: -0.7345836710714972
iteration: 60 loss: 2.456524913934655 grad: -0.644661845761485
iteration: 70 loss: 2.12087259353418 grad: -0.5728862207648392
iteration: 80 loss: 1.8659060436093982 grad: -0.5148749020110852
iteration: 90 loss: 1.6656792515706746 grad: -0.4672487340754764
iteration: 100 loss: 1.5042857303248474 grad: -0.42755095737944293
iteration: 110 loss: 1.3714335595426002 grad: -0.39400360186562866
iteration: 120 loss: 1.2601702939484147 grad: -0.365305794761721
iteration: 130 loss: 1.1656307793277847 grad: -0.3404908569744073
iteration: 140 loss: 1.084308877735318 grad: -0.3188286160036736
iteration: 150 loss: 1.0136141132413947 grad: -0.29975877048859045
iteration: 160 loss: 0.9515911900711713 grad: -0.2828449906244758
iteration: 170 loss: 0.8967366246067534 grad: -0.2677428531356981
iteration: 0 loss: 67.14832119048954 grad: 197.21002278019603
iteration: 10 loss: 11.557452991959797 grad: -1.20777687916232
iteration: 20 loss: 6.654357132436664 grad: -1.50827173073541
iteration: 30 loss: 4.661345406167129 grad: -1.2425892741438451
iteration: 40 loss: 3.5832131735923296 grad: -1.0257873347147588
iteration: 50 loss: 2.9087553417162284 grad: -0.8666281995882199
iteration: 60 loss: 2.4474421246017384 grad: -0.7480279061922198
iteration: 70 loss: 2.112203982683985 grad: -0.6571099635478033
iteration: 80 loss: 1.8576538783318854 grad: -0.5855058068627926
iteration: 90 loss: 1.657830931935347 grad: -0.5277817202501172
iteration: 100 loss: 1.4968204134422856 grad: -0.48031901448180764
iteration: 110 loss: 1.3643268327794182 grad: -0.4406355947718792
iteration: 120 loss: 1.2533969234759752 grad: -0.40698037770560747
iteration: 130 loss: 1.1591661945527365 grad: -0.3780861818721787
iteration: 140 loss: 1.0781299072286856 grad: -0.3530150555384566
iteration: 150 loss: 1.007699296389009 grad: -0.3310587260783034
iteration: 160 loss: 0.9459208555160579 grad: -0.31167280764378663
iteration: 170 loss: 0.8912928453429975 grad: -0.2944322459398274
iteration: 0 loss: 65.25161941056899 grad: 209.03904716256875
iteration: 10 loss: 11.728505649693012 grad: -1.3275079808797825
iteration: 20 loss: 6.743340653753545 grad: -1.4770185026151692
iteration: 30 loss: 4.715180300346467 grad: -1.1868166868390722
iteration: 40 loss: 3.620136214086952 grad: -0.9702985360734847
iteration: 50 loss: 2.9362425377219306 grad: -0.8161189236964932
iteration: 60 loss: 2.4690723087472346 grad: -0.7028705165515181
iteration: 70 loss: 2.1299084321919755 grad: -0.6167366283536739
iteration: 80 loss: 1.872572466213073 grad: -0.5492190783233253
iteration: 90 loss: 1.6706840868949835 grad: -0.4949513099539806
iteration: 100 loss: 1.5080885501270545 grad: -0.4504174929384186
iteration: 110 loss: 1.3743443882485735 grad: -0.4132314905211182
iteration: 120 loss: 1.2624050433978482 grad: -0.38172209461534046
iteration: 130 loss: 1.1673439133438395 grad: -0.354686188388314
iteration: 140 loss: 1.0856134622957307 grad: -0.33123664351109083
iteration: 150 loss: 1.014594562644933 grad: -0.3107055000241926
iteration: 160 loss: 0.9523116193559376 grad: -0.2925804975718207
iteration: 170 loss: 0.8972464696003044 grad: -0.27646235885408754
iteration: 0 loss: 59.200067611728514 grad: 197.94713269786368
iteration: 10 loss: 11.484424002567515 grad: -0.1497086353740676
iteration: 20 loss: 6.629675685337836 grad: -1.1603515155232604
iteration: 30 loss: 4.648180973011594 grad: -1.0487952382769303
iteration: 40 loss: 3.574876414517348 grad: -0.897338411065904
iteration: 50 loss: 2.9029393839122433 grad: -0.7736092168301555
iteration: 60 loss: 2.4431151392142385 grad: -0.6767865186077862
iteration: 70 loss: 2.108832031584494 grad: -0.6003616504952962
iteration: 80 loss: 1.8549323845833319 grad: -0.5389608299956226
iteration: 90 loss: 1.6555733262346786 grad: -0.4887308649357712
iteration: 100 loss: 1.4949059631604045 grad: -0.44695818690013467
iteration: 110 loss: 1.3626739087429252 grad: -0.4117121771925387
iteration: 120 loss: 1.251948316763405 grad: -0.381594657898574
iteration: 130 loss: 1.1578805890898034 grad: -0.3555734998372405
iteration: 140 loss: 1.0769766942610823 grad: -0.33287261250652195
iteration: 150 loss: 1.0066552967339455 grad: -0.3128983750601658
iteration: 160 loss: 0.9449681780766199 grad: -0.29518960595653554
iteration: 170 loss: 0.8904174473784885 grad: -0.2793829179460685
iteration: 0 loss: 60.3991458244158 grad: 246.41845159521205
iteration: 10 loss: 11.624095093080912 grad: -0.47695827160542537
iteration: 20 loss: 6.653635504076234 grad: -1.2276381188970278
iteration: 30 loss: 4.649669613099335 grad: -1.0949546742371008
iteration: 40 loss: 3.569686106116602 grad: -0.932277135227035
iteration: 50 loss: 2.8954987328658963 grad: -0.8008032977426355
iteration: 60 loss: 2.434993186432497 grad: -0.6984732557998716
iteration: 70 loss: 2.100655406515092 grad: -0.6180412180885535
iteration: 80 loss: 1.8469635469007037 grad: -0.5536530843925986
iteration: 90 loss: 1.6479191720899176 grad: -0.5011436589412475
iteration: 100 loss: 1.487602720282997 grad: -0.45759448647197604
iteration: 110 loss: 1.3557247384364515 grad: -0.420937720029576
iteration: 120 loss: 1.2453409953016654 grad: -0.38968117408573527
iteration: 130 loss: 1.1515961637664573 grad: -0.3627268565156119
iteration: 140 loss: 1.0709938173679967 grad: -0.3392515424971052
iteration: 150 loss: 1.0009524016588165 grad: -0.3186272128946832
iteration: 160 loss: 0.9395245452884741 grad: -0.3003671298032002
iteration: 170 loss: 0.8852136885452637 grad: -0.284088607680146
iteration: 0 loss: 61.02130623236847 grad: 235.84004958071938
iteration: 10 loss: 11.613361506314835 grad: -1.1638788792432202
iteration: 20 loss: 6.680243060205736 grad: -1.4829456018590244
iteration: 30 loss: 4.669297936267743 grad: -1.238392479325446
iteration: 40 loss: 3.582991237402439 grad: -1.0293917054870754
iteration: 50 loss: 2.904636364652809 grad: -0.873079467311201
iteration: 60 loss: 2.441404620999823 grad: -0.755418761889151
iteration: 70 loss: 2.1052306592211028 grad: -0.664667719308402
iteration: 80 loss: 1.850261508120928 grad: -0.5929101806700914
iteration: 90 loss: 1.6503024834636646 grad: -0.5349038336922725
iteration: 100 loss: 1.4893146265696322 grad: -0.4871146965333632
iteration: 110 loss: 1.3569333797313035 grad: -0.44709905562057817
iteration: 120 loss: 1.2461657056167248 grad: -0.4131230700250718
iteration: 130 loss: 1.1521236064820435 grad: -0.38392680063954066
iteration: 140 loss: 1.0712882007650562 grad: -0.3585747116454545
iteration: 150 loss: 1.0010621349766469 grad: -0.3363586187044593
iteration: 160 loss: 0.9394866892516389 grad: -0.3167331381849588
iteration: 170 loss: 0.8850570009972629 grad: -0.29927175928361394
iteration: 0 loss: 68.21429097982104 grad: 210.60299576660168
iteration: 10 loss: 11.551766483144114 grad: -1.8548196636556535
iteration: 20 loss: 6.666699179725666 grad: -1.7110733273027667
iteration: 30 loss: 4.673880274966822 grad: -1.3303000421803566
iteration: 40 loss: 3.594705514255483 grad: -1.0727911926701874
iteration: 50 loss: 2.91916491172647 grad: -0.8955803188174492
iteration: 60 loss: 2.456893679287904 grad: -0.7676369098670359
iteration: 70 loss: 2.1208340366658622 grad: -0.6713269015501908
iteration: 80 loss: 1.8655820322192085 grad: -0.5963513718787976
iteration: 90 loss: 1.6651566198914802 grad: -0.5363862129016662
iteration: 100 loss: 1.5036252829586374 grad: -0.487360063887267
iteration: 110 loss: 1.3706779821900867 grad: -0.4465425029226302
iteration: 120 loss: 1.2593498148235498 grad: -0.41203764073377686
iteration: 130 loss: 1.1647670218135318 grad: -0.38248951714619195
iteration: 140 loss: 1.083417458292512 grad: -0.3569034240412053
iteration: 150 loss: 1.0127064026954196 grad: -0.3345334924765771
iteration: 160 loss: 0.9506755208202032 grad: -0.31480966045619463
iteration: 170 loss: 0.8958191287520094 grad: -0.2972888672295313
iteration: 0 loss: 62.986889832083506 grad: 183.36074449319608
iteration: 10 loss: 11.579851842532952 grad: -0.07652787981338054
iteration: 20 loss: 6.656766664763051 grad: -1.1223190915614008
iteration: 30 loss: 4.662481858331913 grad: -1.0158570223925378
iteration: 40 loss: 3.584771417002354 grad: -0.8665181407654466
iteration: 50 loss: 2.9107685285780462 grad: -0.7448677954154937
iteration: 60 loss: 2.4497658677064464 grad: -0.6501364413282555
iteration: 70 loss: 2.114714153571392 grad: -0.5756924836017987
iteration: 80 loss: 1.860263958005114 grad: -0.516102499442328
iteration: 90 loss: 1.6604835889011156 grad: -0.4675000111354215
iteration: 100 loss: 1.4994782362415953 grad: -0.42717993787889713
iteration: 110 loss: 1.366965597108488 grad: -0.39322837698988855
iteration: 120 loss: 1.2560011057163882 grad: -0.36426575458114274
iteration: 130 loss: 1.1617260414388346 grad: -0.3392777275963331
iteration: 140 loss: 1.0806395198529903 grad: -0.3175041730095942
iteration: 150 loss: 1.0101553666350727 grad: -0.29836538468393126
iteration: 160 loss: 0.9483218235792182 grad: -0.28141213359102035
iteration: 170 loss: 0.8936383317134393 grad: -0.2662912337031502
iteration: 0 loss: 62.84073018593131 grad: 264.7830757592979
iteration: 10 loss: 11.772602216355153 grad: -1.9854088120432474
iteration: 20 loss: 6.75686368961504 grad: -1.743139776292327
iteration: 30 loss: 4.71867593255638 grad: -1.3655176741258899
iteration: 40 loss: 3.619533144444507 grad: -1.1037526860957738
iteration: 50 loss: 2.933747282688976 grad: -0.9216345099256441
iteration: 60 loss: 2.4656546587570616 grad: -0.7895773443627081
iteration: 70 loss: 2.126042870066819 grad: -0.6900172003131471
iteration: 80 loss: 1.8685082471693435 grad: -0.6124877518639038
iteration: 90 loss: 1.6665583761638862 grad: -0.550496231408673
iteration: 100 loss: 1.503978899811252 grad: -0.49983988272369917
iteration: 110 loss: 1.370295028400423 grad: -0.45769183459966845
iteration: 120 loss: 1.2584408684753148 grad: -0.4220861659420375
iteration: 130 loss: 1.1634782857918253 grad: -0.3916157819310334
iteration: 140 loss: 1.081852736335391 grad: -0.365248201174035
iteration: 150 loss: 1.010940788996281 grad: -0.34220926815992664
iteration: 160 loss: 0.9487642029555652 grad: -0.32190744068148114
iteration: 170 loss: 0.8938032056635166 grad: -0.30388312416323704
iteration: 0 loss: 65.86272013385029 grad: 217.24055487591266
iteration: 10 loss: 11.963527394735923 grad: -0.9374252845188896
iteration: 20 loss: 6.831392373019423 grad: -1.2343324569931722
iteration: 30 loss: 4.7715385890182915 grad: -1.0442008896335357
iteration: 40 loss: 3.663468552810049 grad: -0.8757482262923147
iteration: 50 loss: 2.9722116759994606 grad: -0.7480993842251304
iteration: 60 loss: 2.500133656869986 grad: -0.6511365834575162
iteration: 70 loss: 2.157382938912592 grad: -0.575787950544712
iteration: 80 loss: 1.8972734355251801 grad: -0.5158247550655144
iteration: 90 loss: 1.6931583182688221 grad: -0.467080000010189
iteration: 100 loss: 1.5287264625578494 grad: -0.42672271053371935
iteration: 110 loss: 1.3934370393265205 grad: -0.3927822537338673
iteration: 120 loss: 1.2801765505821756 grad: -0.3638521766805519
iteration: 130 loss: 1.1839712480762252 grad: -0.3389050056581867
iteration: 140 loss: 1.1012392195390353 grad: -0.317174136208615
iteration: 150 loss: 1.0293355655470473 grad: -0.29807671284443343
iteration: 160 loss: 0.9662649247961368 grad: -0.2811620661774542
iteration: 170 loss: 0.9104936825329705 grad: -0.2660764553966406
iteration: 0 loss: 64.22881919434914 grad: 204.32269541431836
iteration: 10 loss: 11.697078596169975 grad: -0.7851764407638122
iteration: 20 loss: 6.725376965615107 grad: -1.2719028105762762
iteration: 30 loss: 4.708300662440996 grad: -1.090537994896132
iteration: 40 loss: 3.6185612328599333 grad: -0.9176629146741736
iteration: 50 loss: 2.937269769326535 grad: -0.7846274305212555
iteration: 60 loss: 2.471424444889718 grad: -0.6829906465195943
iteration: 70 loss: 2.1329423601656674 grad: -0.6038040923245119
iteration: 80 loss: 1.8759457502584487 grad: -0.5407070916035412
iteration: 90 loss: 1.6742065272932947 grad: -0.4893843537593002
iteration: 100 loss: 1.5116514443930513 grad: -0.44688297411293365
iteration: 110 loss: 1.377884677993734 grad: -0.41113880635433836
iteration: 120 loss: 1.265886166935533 grad: -0.38067477690898044
iteration: 130 loss: 1.1707451230919022 grad: -0.35440995772108413
iteration: 140 loss: 1.0889236439661132 grad: -0.3315369248866518
iteration: 150 loss: 1.0178085495111355 grad: -0.31144130245801255
iteration: 160 loss: 0.9554279390225185 grad: -0.2936477945990815
iteration: 170 loss: 0.9002659384256979 grad: -0.2777831978424176
iteration: 0 loss: 65.53885125678222 grad: 217.27775391622643
iteration: 10 loss: 11.813034764570126 grad: -0.9629638133574361
iteration: 20 loss: 6.753475590345095 grad: -1.3562507043661298
iteration: 30 loss: 4.717493500199907 grad: -1.1389722336563242
iteration: 40 loss: 3.621242160470708 grad: -0.9482164808733737
iteration: 50 loss: 2.9372137212499596 grad: -0.8052284263255962
iteration: 60 loss: 2.470097631773156 grad: -0.6975776524464394
iteration: 70 loss: 2.131001373018948 grad: -0.6145216462236573
iteration: 80 loss: 1.8737130243486966 grad: -0.5488096417752553
iteration: 90 loss: 1.6718504214817254 grad: -0.4956489656453097
iteration: 100 loss: 1.509263362288948 grad: -0.45181395357857446
iteration: 110 loss: 1.3755152331607199 grad: -0.41507626772587136
iteration: 120 loss: 1.2635633655845182 grad: -0.38385559613021136
iteration: 130 loss: 1.1684840206259486 grad: -0.3570036639726714
iteration: 140 loss: 1.086731702710784 grad: -0.33366771108562704
iteration: 150 loss: 1.0156887175714309 grad: -0.3132020031741646
iteration: 160 loss: 0.9533804699654783 grad: -0.29510903619571915
iteration: 170 loss: 0.8982894907144741 grad: -0.2789995494172648
iteration: 0 loss: 66.77774226532847 grad: 225.32796072043354
iteration: 10 loss: 11.9421478368374 grad: -1.2812774739165467
iteration: 20 loss: 6.835073069628792 grad: -1.5929779863284013
iteration: 30 loss: 4.774283752081043 grad: -1.2871806966200916
iteration: 40 loss: 3.6641851655788495 grad: -1.050901265882862
iteration: 50 loss: 2.9715244218195536 grad: -0.8819292397324136
iteration: 60 loss: 2.498574154144907 grad: -0.757938608733815
iteration: 70 loss: 2.1552954019908115 grad: -0.6638271833589517
iteration: 80 loss: 1.8948727036685424 grad: -0.5902136816079627
iteration: 90 loss: 1.6905791372934635 grad: -0.5311635833986754
iteration: 100 loss: 1.5260544708586499 grad: -0.4827917412677986
iteration: 110 loss: 1.3907273496146113 grad: -0.44246534486758793
iteration: 120 loss: 1.2774649315060742 grad: -0.4083436089157372
iteration: 130 loss: 1.1812809719392383 grad: -0.3791036352096768
iteration: 140 loss: 1.0985853451781458 grad: -0.353771590278025
iteration: 150 loss: 1.0267276736861028 grad: -0.3316153674901329
iteration: 160 loss: 0.9637088993591036 grad: -0.31207432069646107
iteration: 170 loss: 0.9079928911595826 grad: -0.29471204086653646
iteration: 0 loss: 62.09699169781129 grad: 218.49155922609938
iteration: 10 loss: 11.586700161877305 grad: -1.3324460470634758
iteration: 20 loss: 6.683039871418259 grad: -1.560203537605106
iteration: 30 loss: 4.679270612511242 grad: -1.2659596006576137
iteration: 40 loss: 3.5950625555475115 grad: -1.0390613020960577
iteration: 50 loss: 2.917098975958528 grad: -0.8756319677466889
iteration: 60 loss: 2.4536170414391396 grad: -0.7548916844807247
iteration: 70 loss: 2.1169522447207783 grad: -0.6627479806580179
iteration: 80 loss: 1.8614158411313506 grad: -0.5903649675678391
iteration: 90 loss: 1.6608832441680037 grad: -0.5321042257742157
iteration: 100 loss: 1.4993452717953915 grad: -0.48424751924486353
iteration: 110 loss: 1.366449011986912 grad: -0.4442600797882936
iteration: 120 loss: 1.255204712875225 grad: -0.41036094231313425
iteration: 130 loss: 1.1607238175295358 grad: -0.38126494980663456
iteration: 140 loss: 1.0794851980239484 grad: -0.35602278087742334
iteration: 150 loss: 1.008888616750648 grad: -0.3339186523242976
iteration: 160 loss: 0.9469723420676762 grad: -0.31440302522978336
iteration: 170 loss: 0.8922286129365024 grad: -0.29704719007698804
iteration: 0 loss: 61.83816843136856 grad: 207.4304067068363
iteration: 10 loss: 11.574439561978373 grad: -0.7038998920439754
iteration: 20 loss: 6.6616634332075675 grad: -1.3580827846015
iteration: 30 loss: 4.6641079394934915 grad: -1.159499228437015
iteration: 40 loss: 3.5842992322118747 grad: -0.9706105106939398
iteration: 50 loss: 2.9091373408134378 grad: -0.8264917053173537
iteration: 60 loss: 2.4474944692966676 grad: -0.7171453239332772
iteration: 70 loss: 2.1120905930664033 grad: -0.6324075704482186
iteration: 80 loss: 1.857453851631073 grad: -0.5651727235941215
iteration: 90 loss: 1.6575852916617722 grad: -0.5106714328395909
iteration: 100 loss: 1.4965516011985445 grad: -0.46566554006844896
iteration: 110 loss: 1.3640476061491014 grad: -0.42790521715551455
iteration: 120 loss: 1.2531147238575036 grad: -0.3957883946234758
iteration: 130 loss: 1.1588854215907636 grad: -0.3681474283451561
iteration: 140 loss: 1.077853162053998 grad: -0.3441131150934581
iteration: 150 loss: 1.0074280891015983 grad: -0.32302602425547483
iteration: 160 loss: 0.9456560215382772 grad: -0.304377318067565
iteration: 170 loss: 0.8910347974092061 grad: -0.2877683621996813
iteration: 0 loss: 67.56733930427276 grad: 247.2859537442046
iteration: 10 loss: 11.713305645149644 grad: -2.2216088764222164
iteration: 20 loss: 6.728799679820853 grad: -1.795192325016846
iteration: 30 loss: 4.702992400755895 grad: -1.3843020515460907
iteration: 40 loss: 3.6096535189011445 grad: -1.1132868248586316
iteration: 50 loss: 2.9270248592738426 grad: -0.9275148683465119
iteration: 60 loss: 2.460828201169699 grad: -0.7936521867637953
iteration: 70 loss: 2.122437762404223 grad: -0.693053438505761
iteration: 80 loss: 1.8657324445397203 grad: -0.614859229139748
iteration: 90 loss: 1.664369267358862 grad: -0.5524084446243279
iteration: 100 loss: 1.5022187352473007 grad: -0.501416666883286
iteration: 110 loss: 1.3688570494100119 grad: -0.45901332425630437
iteration: 120 loss: 1.2572503966012762 grad: -0.423207187150608
iteration: 130 loss: 1.1624816113832244 grad: -0.3925755309976289
iteration: 140 loss: 1.0810103053535882 grad: -0.3660757004507136
iteration: 150 loss: 1.0102228714008148 grad: -0.3429266240190879
iteration: 160 loss: 0.9481480517661939 grad: -0.322531886176165
iteration: 170 loss: 0.8932711426061608 grad: -0.30442835378907657
iteration: 0 loss: 62.9361377900342 grad: 191.47840415437722
iteration: 10 loss: 11.568556257903062 grad: -0.7167451573211476
iteration: 20 loss: 6.660996780752942 grad: -1.278654848048915
iteration: 30 loss: 4.6644660174781825 grad: -1.092950358772376
iteration: 40 loss: 3.5850603647832857 grad: -0.9175877388977038
iteration: 50 loss: 2.91006662636379 grad: -0.7836440682107284
iteration: 60 loss: 2.4484900288593137 grad: -0.6817512143790017
iteration: 70 loss: 2.1131035852264093 grad: -0.6025651557024508
iteration: 80 loss: 1.8584595580851286 grad: -0.5395655207700759
iteration: 90 loss: 1.6585709761614205 grad: -0.48837115804096753
iteration: 100 loss: 1.4975108449361456 grad: -0.4460017052481409
iteration: 110 loss: 1.3649774575011924 grad: -0.41038190789633544
iteration: 120 loss: 1.2540141811977348 grad: -0.380030739708473
iteration: 130 loss: 1.1597545904722788 grad: -0.35386647099376417
iteration: 140 loss: 1.0786927716688461 grad: -0.3310822240185546
iteration: 150 loss: 1.0082392082592213 grad: -0.3110646888553719
iteration: 160 loss: 0.9464398892233806 grad: -0.2933397750423416
iteration: 170 loss: 0.8917927209414632 grad: -0.27753545377155003
iteration: 0 loss: 63.78807971843013 grad: 198.9775996533919
iteration: 10 loss: 11.557862213985532 grad: -0.40225477511236984
iteration: 20 loss: 6.645021278642823 grad: -1.2029166441977799
iteration: 30 loss: 4.65715683300947 grad: -1.0688652552187872
iteration: 40 loss: 3.58245920074573 grad: -0.9085749536100657
iteration: 50 loss: 2.9099291847249087 grad: -0.7800437366095208
iteration: 60 loss: 2.4496873120907283 grad: -0.6803172781192052
iteration: 70 loss: 2.1150432711921314 grad: -0.6020376225539468
iteration: 80 loss: 1.8608144153706483 grad: -0.5394087456031055
iteration: 90 loss: 1.661152136607515 grad: -0.48834474541095546
iteration: 100 loss: 1.5002055399184593 grad: -0.44599540020995004
iteration: 110 loss: 1.3677166880948293 grad: -0.4103460241028758
iteration: 120 loss: 1.256755075800687 grad: -0.37994476950252853
iteration: 130 loss: 1.1624703554900242 grad: -0.35372417446050053
iteration: 140 loss: 1.081366753930855 grad: -0.33088427418117716
iteration: 150 loss: 1.0108612758317577 grad: -0.31081495168338824
iteration: 160 loss: 0.949004166140102 grad: -0.29304340486542746
iteration: 170 loss: 0.8942961370994866 grad: -0.27719792158855416
iteration: 0 loss: 59.735339668483675 grad: 211.14443833760737
iteration: 10 loss: 11.449195441158611 grad: -0.12475008213769756
iteration: 20 loss: 6.586619996540363 grad: -1.1983946475815104
iteration: 30 loss: 4.613002868206587 grad: -1.0837057930029725
iteration: 40 loss: 3.5459702453082502 grad: -0.9237042541275736
iteration: 50 loss: 2.878608812903828 grad: -0.7933665219069306
iteration: 60 loss: 2.422187681440846 grad: -0.6919109210743601
iteration: 70 loss: 2.0905106569672913 grad: -0.6122258832039231
iteration: 80 loss: 1.8386612300485219 grad: -0.5484766728723606
iteration: 90 loss: 1.6409527455306157 grad: -0.4965107775715083
iteration: 100 loss: 1.481640390489371 grad: -0.45342356072495565
iteration: 110 loss: 1.3505392517304244 grad: -0.4171605057922533
iteration: 120 loss: 1.2407709660506456 grad: -0.38624113881345
iteration: 130 loss: 1.1475235018861398 grad: -0.35957727533101463
iteration: 140 loss: 1.0673299108740055 grad: -0.3363537189384796
iteration: 150 loss: 0.9976293152373149 grad: -0.31594896081721496
iteration: 160 loss: 0.9364892213554247 grad: -0.29788153072854123
iteration: 170 loss: 0.8824240138820972 grad: -0.28177301291072593
iteration: 0 loss: 65.57826587772952 grad: 172.73221282773042
iteration: 10 loss: 11.691116783174865 grad: 0.32879628033909836
iteration: 20 loss: 6.734564252660014 grad: -1.0920077653230487
iteration: 30 loss: 4.723678605410566 grad: -1.0282524428450333
iteration: 40 loss: 3.634870943287958 grad: -0.886419226667915
iteration: 50 loss: 2.952976108202289 grad: -0.7651223019230741
iteration: 60 loss: 2.4861210404054543 grad: -0.6690061974266877
iteration: 70 loss: 2.1465794243143717 grad: -0.5928395849171859
iteration: 80 loss: 1.888587215807175 grad: -0.5315910668062421
iteration: 90 loss: 1.6859474780084467 grad: -0.48150197833055014
iteration: 100 loss: 1.5225890424158024 grad: -0.4398814350021625
iteration: 110 loss: 1.388108325609003 grad: -0.4048004458431514
iteration: 120 loss: 1.2754748771386735 grad: -0.37485702296235535
iteration: 130 loss: 1.179767587199959 grad: -0.3490146122605905
iteration: 140 loss: 1.0974392305791876 grad: -0.3264934273125317
iteration: 150 loss: 1.0258685340745615 grad: -0.3066971013700651
iteration: 160 loss: 0.9630766935411388 grad: -0.2891625437204224
iteration: 170 loss: 0.9075419883780672 grad: -0.2735251113585325
iteration: 0 loss: 64.54333862953031 grad: 274.86334313213047
iteration: 10 loss: 11.64577472311953 grad: -2.458798657644639
iteration: 20 loss: 6.684399249601525 grad: -1.7166481674921479
iteration: 30 loss: 4.669197009774879 grad: -1.3378709152905328
iteration: 40 loss: 3.5826920429073486 grad: -1.082451162419058
iteration: 50 loss: 2.9047236285230777 grad: -0.9052896185340442
iteration: 60 loss: 2.441871832608864 grad: -0.7767048536047139
iteration: 70 loss: 2.1059856971456528 grad: -0.6795997885697285
iteration: 80 loss: 1.851219773755681 grad: -0.6038547361561275
iteration: 90 loss: 1.6513998611136147 grad: -0.5431979443130699
iteration: 100 loss: 1.4905048739167548 grad: -0.49356633917776527
iteration: 110 loss: 1.3581835578640329 grad: -0.4522231867529817
iteration: 120 loss: 1.2474523580783832 grad: -0.4172623484712914
iteration: 130 loss: 1.153429970756158 grad: -0.38731748668716803
iteration: 140 loss: 1.0726022519057468 grad: -0.3613846295593731
iteration: 150 loss: 1.0023752252564422 grad: -0.338710034830932
iteration: 160 loss: 0.9407925994851625 grad: -0.3187170866631934
iteration: 170 loss: 0.8863512737728921 grad: -0.3009573124647039
iteration: 0 loss: 59.62706842545265 grad: 189.41545475706351
iteration: 10 loss: 11.36836911060273 grad: 0.6852676635271844
iteration: 20 loss: 6.549603301964163 grad: -0.970300545040053
iteration: 30 loss: 4.595105513806382 grad: -0.9643432013881134
iteration: 40 loss: 3.536716953255026 grad: -0.8457814995154709
iteration: 50 loss: 2.8737706709119433 grad: -0.7366810594746597
iteration: 60 loss: 2.4198178803560704 grad: -0.6479072086027395
iteration: 70 loss: 2.0896124198789865 grad: -0.5765525086245706
iteration: 80 loss: 1.8386814210486013 grad: -0.5186467615972478
iteration: 90 loss: 1.641564903340788 grad: -0.47098180896502506
iteration: 100 loss: 1.482642347945247 grad: -0.4311786705986361
iteration: 110 loss: 1.3518009504962472 grad: -0.3974969804193168
iteration: 120 loss: 1.2422061423070028 grad: -0.3686547447223377
iteration: 130 loss: 1.149073513938633 grad: -0.34369491466095836
iteration: 140 loss: 1.0689540972677853 grad: -0.32189218104506356
iteration: 150 loss: 0.9992990341656465 grad: -0.30268857518687353
iteration: 160 loss: 0.938184049907787 grad: -0.28564873312872674
iteration: 170 loss: 0.8841292588697549 grad: -0.2704284564464047
iteration: 0 loss: 67.03125816690127 grad: 237.94566988097884
iteration: 10 loss: 11.786548713765383 grad: -2.216677116227351
iteration: 20 loss: 6.755749462933388 grad: -1.7541343371858362
iteration: 30 loss: 4.719570590949181 grad: -1.3503954876052433
iteration: 40 loss: 3.6221547435311137 grad: -1.0868432109069057
iteration: 50 loss: 2.937293082258128 grad: -0.9066230980756071
iteration: 60 loss: 2.4696415924964144 grad: -0.7767480527328328
iteration: 70 loss: 2.13020627326585 grad: -0.6790609016111604
iteration: 80 loss: 1.8727038129958269 grad: -0.6030489685612863
iteration: 90 loss: 1.6707072555288753 grad: -0.5422764713394814
iteration: 100 loss: 1.508038529866884 grad: -0.49260533906740656
iteration: 110 loss: 1.374243416554319 grad: -0.45126209262008316
iteration: 120 loss: 1.2622678927553517 grad: -0.4163216283564132
iteration: 130 loss: 1.1671807439236297 grad: -0.386407440000072
iteration: 140 loss: 1.085431458363101 grad: -0.3605099606271369
iteration: 150 loss: 1.014398919299269 grad: -0.3378723265632644
iteration: 160 loss: 0.9521061625961341 grad: -0.31791618048909653
iteration: 170 loss: 0.8970340596773151 grad: -0.30019209632160004
iteration: 0 loss: 61.257553817638204 grad: 214.27501904417116
iteration: 10 loss: 11.654534194649592 grad: 0.2128831516439662
iteration: 20 loss: 6.684161153572817 grad: -1.029255574213318
iteration: 30 loss: 4.6783507436966 grad: -0.9881111404207878
iteration: 40 loss: 3.5957520755578116 grad: -0.8616606400620284
iteration: 50 loss: 2.9190453812261694 grad: -0.7492079318444035
iteration: 60 loss: 2.456325551519264 grad: -0.658414829477388
iteration: 70 loss: 2.1200895996880305 grad: -0.5856320744258294
iteration: 80 loss: 1.864776639735117 grad: -0.5266370464636481
iteration: 90 loss: 1.6643423910605137 grad: -0.4781063361383472
iteration: 100 loss: 1.502826366112293 grad: -0.4375968000670293
iteration: 110 loss: 1.3699053201179263 grad: -0.4033277071434915
iteration: 120 loss: 1.258607840735625 grad: -0.3739897025095811
iteration: 130 loss: 1.1640568849156807 grad: -0.3486062793323247
iteration: 140 loss: 1.082738649638238 grad: -0.3264378225959206
iteration: 150 loss: 1.0120576018749405 grad: -0.30691558096299837
iteration: 160 loss: 0.9500550461399061 grad: -0.28959590137094227
iteration: 170 loss: 0.8952251785407167 grad: -0.27412810799185305
iteration: 0 loss: 61.2305267801079 grad: 179.00281688869455
iteration: 10 loss: 11.745210615734074 grad: 0.812208507371601
iteration: 20 loss: 6.750220387523396 grad: -0.9154214744812534
iteration: 30 loss: 4.727531126585911 grad: -0.924597846317843
iteration: 40 loss: 3.63436919894523 grad: -0.8138292199254804
iteration: 50 loss: 2.9506777369108685 grad: -0.7098888308811603
iteration: 60 loss: 2.4830539070935216 grad: -0.6248738594176445
iteration: 70 loss: 2.1432036827999665 grad: -0.5563954792545487
iteration: 80 loss: 1.8851236823473465 grad: -0.5007623578642616
iteration: 90 loss: 1.6825062464250053 grad: -0.4549362562990226
iteration: 100 loss: 1.5192253982640151 grad: -0.4166494060123972
iteration: 110 loss: 1.3848490753746112 grad: -0.3842377319199506
iteration: 120 loss: 1.2723315375453075 grad: -0.35647341437962543
iteration: 130 loss: 1.1767433181308578 grad: -0.3324389378775155
iteration: 140 loss: 1.0945326173401058 grad: -0.3114385017467333
iteration: 150 loss: 1.0230757090528393 grad: -0.29293659124730265
iteration: 160 loss: 0.9603925494823836 grad: -0.27651521646538496
iteration: 170 loss: 0.9049608795617132 grad: -0.2618438217837029
iteration: 0 loss: 64.90076373726562 grad: 199.25357421601691
iteration: 10 loss: 11.583862745243984 grad: -0.5479707268565512
iteration: 20 loss: 6.665595999478393 grad: -1.3086484140008388
iteration: 30 loss: 4.6686929794339616 grad: -1.1273808595424348
iteration: 40 loss: 3.5893169489928285 grad: -0.94520485731155
iteration: 50 loss: 2.914296571002772 grad: -0.8053434852702714
iteration: 60 loss: 2.452631161954659 grad: -0.6990852057294329
iteration: 70 loss: 2.1171198819774872 grad: -0.6167065138542543
iteration: 80 loss: 1.8623343945496758 grad: -0.5513296942838865
iteration: 90 loss: 1.6622993668949924 grad: -0.4983251560753231
iteration: 100 loss: 1.5010943187922665 grad: -0.4545471948263352
iteration: 110 loss: 1.3684210048915537 grad: -0.4178098797902527
iteration: 120 loss: 1.2573244894302578 grad: -0.386556687092606
iteration: 130 loss: 1.162939036812118 grad: -0.3596532437622534
iteration: 140 loss: 1.0817588564615852 grad: -0.3362551602868931
iteration: 150 loss: 1.0111942436167902 grad: -0.3157218491283762
iteration: 160 loss: 0.9492908364673812 grad: -0.29755901183184885
iteration: 170 loss: 0.8945461146084365 grad: -0.281379393714523
iteration: 0 loss: 62.37609317344095 grad: 229.42482738757494
iteration: 10 loss: 11.572853077412798 grad: -0.9974123403185368
iteration: 20 loss: 6.636202529145934 grad: -1.4326130628277083
iteration: 30 loss: 4.641149926268967 grad: -1.2013523199052438
iteration: 40 loss: 3.5650441010308316 grad: -0.9982047922995622
iteration: 50 loss: 2.8928927248997836 grad: -0.8464915009204255
iteration: 60 loss: 2.4335716981353968 grad: -0.7325572133104851
iteration: 70 loss: 2.0999699558016633 grad: -0.6448003521995158
iteration: 80 loss: 1.846756728794224 grad: -0.5754530174117256
iteration: 90 loss: 1.6480338206192342 grad: -0.519403828377591
iteration: 100 loss: 1.4879382614631966 grad: -0.473222261673546
iteration: 110 loss: 1.3562144004730394 grad: -0.4345429550476495
iteration: 120 loss: 1.2459391218587619 grad: -0.4016907262448358
iteration: 130 loss: 1.1522707800123415 grad: -0.3734496487065166
iteration: 140 loss: 1.071722086674113 grad: -0.34891748373272335
iteration: 150 loss: 1.00171774931972 grad: -0.32741151011415504
iteration: 160 loss: 0.9403147822205359 grad: -0.30840604320945253
iteration: 170 loss: 0.8860197553407158 grad: -0.29148998910221513
iteration: 0 loss: 64.27876572040704 grad: 231.20703882589964
iteration: 10 loss: 11.763274443872454 grad: -1.033396901069211
iteration: 20 loss: 6.745440387669987 grad: -1.4406382303305483
iteration: 30 loss: 4.713269718021324 grad: -1.2076799489717536
iteration: 40 loss: 3.617651664182011 grad: -1.0046135125884803
iteration: 50 loss: 2.9338367484326473 grad: -0.8525406298635887
iteration: 60 loss: 2.466874530323118 grad: -0.738109095372969
iteration: 70 loss: 2.1279254289050353 grad: -0.6498629004003906
iteration: 80 loss: 1.8707819050466903 grad: -0.5800755978654383
iteration: 90 loss: 1.669059322961361 grad: -0.5236419082767397
iteration: 100 loss: 1.5066052612180763 grad: -0.47712678266510306
iteration: 110 loss: 1.3729819565876988 grad: -0.4381578659036548
iteration: 120 loss: 1.2611464063119953 grad: -0.4050531254573705
iteration: 130 loss: 1.1661750310377101 grad: -0.37659066339216496
iteration: 140 loss: 1.084522759576554 grad: -0.35186325256756695
iteration: 150 loss: 1.0135724449011254 grad: -0.3301840705991668
iteration: 160 loss: 0.9513500792555863 grad: -0.3110240761052517
iteration: 170 loss: 0.8963387754082429 grad: -0.2939694247939387
iteration: 0 loss: 62.76860074961692 grad: 243.17170295515288
iteration: 10 loss: 11.445272834173403 grad: -1.741687001716537
iteration: 20 loss: 6.591356609094419 grad: -1.640919919694131
iteration: 30 loss: 4.610482139918201 grad: -1.2987694418308064
iteration: 40 loss: 3.5399193034619945 grad: -1.0556799227598068
iteration: 50 loss: 2.871067770361854 grad: -0.88473826445416
iteration: 60 loss: 2.414110425036721 grad: -0.7599576368754692
iteration: 70 loss: 2.0823491881663525 grad: -0.6654436921773879
iteration: 80 loss: 1.8306327458509175 grad: -0.5915873906331669
iteration: 90 loss: 1.6331598399936162 grad: -0.5323728505851902
iteration: 100 loss: 1.4741276128116128 grad: -0.48388047794433475
iteration: 110 loss: 1.3433210336849946 grad: -0.4434607705594197
iteration: 120 loss: 1.2338459976688847 grad: -0.40926393315900583
iteration: 130 loss: 1.1408823780360846 grad: -0.3799617654824324
iteration: 140 loss: 1.0609592951616584 grad: -0.3545770906920382
iteration: 150 loss: 0.9915142381400983 grad: -0.33237557365548503
iteration: 160 loss: 0.9306143505798652 grad: -0.3127950148530139
iteration: 170 loss: 0.8767743463444359 grad: -0.29539788236238357
iteration: 0 loss: 61.93082441911003 grad: 209.79246946032163
iteration: 10 loss: 11.560952867834617 grad: -1.2367263791173353
iteration: 20 loss: 6.650885936875304 grad: -1.4203857027686784
iteration: 30 loss: 4.653725089484231 grad: -1.1676544586241766
iteration: 40 loss: 3.5747156869123997 grad: -0.9654683999485362
iteration: 50 loss: 2.9003994259358645 grad: -0.8173896649930812
iteration: 60 loss: 2.4395335584102793 grad: -0.7069651079179478
iteration: 70 loss: 2.1048125578995496 grad: -0.6221815736775507
iteration: 80 loss: 1.8507679749334949 grad: -0.5552909120009509
iteration: 90 loss: 1.6514121423305037 grad: -0.5012727770280889
iteration: 100 loss: 1.490823913995841 grad: -0.4567842157088582
iteration: 110 loss: 1.3587090266826387 grad: -0.4195306957305807
iteration: 120 loss: 1.2481180913890222 grad: -0.38789177170020367
iteration: 130 loss: 1.154191140246646 grad: -0.36069340463174493
iteration: 140 loss: 1.0734278120356644 grad: -0.33706551163405174
iteration: 150 loss: 1.0032432907806983 grad: -0.3163502608888803
iteration: 160 loss: 0.9416875218348237 grad: -0.2980414292960806
iteration: 170 loss: 0.8872617389138213 grad: -0.28174330319504803
iteration: 0 loss: 65.34924063539715 grad: 247.68977021322613
iteration: 10 loss: 11.536582215421545 grad: -2.0498886200170228
iteration: 20 loss: 6.634013091275805 grad: -1.7308228870946847
iteration: 30 loss: 4.641725918480336 grad: -1.3426784117661295
iteration: 40 loss: 3.565500566923509 grad: -1.0823179486515062
iteration: 50 loss: 2.8929345713260606 grad: -0.9031802308024223
iteration: 60 loss: 2.433258916816406 grad: -0.7738658993956512
iteration: 70 loss: 2.099396957802831 grad: -0.676551751731907
iteration: 80 loss: 1.8460011075880258 grad: -0.6008197098430914
iteration: 90 loss: 1.647152632087041 grad: -0.5402698123375529
iteration: 100 loss: 1.4869722937336602 grad: -0.49078143968546795
iteration: 110 loss: 1.3551926846003384 grad: -0.4495913256020519
iteration: 120 loss: 1.244882391097483 grad: -0.41478108091128735
iteration: 130 loss: 1.1511939229303607 grad: -0.3849789355063038
iteration: 140 loss: 1.0706358501112268 grad: -0.3591788124371584
iteration: 150 loss: 1.0006299188449619 grad: -0.33662649575905146
iteration: 160 loss: 0.9392310056550328 grad: -0.3167456797453413
iteration: 170 loss: 0.8849441227951668 grad: -0.29908855435528725
iteration: 0 loss: 62.63303951882571 grad: 213.21655812582847
iteration: 10 loss: 11.581366037150639 grad: -0.5766620092538632
iteration: 20 loss: 6.655721240260928 grad: -1.3336535907084426
iteration: 30 loss: 4.660329380968051 grad: -1.1434192538754904
iteration: 40 loss: 3.5824944865731925 grad: -0.9567370337877497
iteration: 50 loss: 2.908579760188414 grad: -0.8143510113453009
iteration: 60 loss: 2.447704045776804 grad: -0.7064727824127199
iteration: 70 loss: 2.1127764889705016 grad: -0.6229569424917625
iteration: 80 loss: 1.8584383735598584 grad: -0.5567335995881084
iteration: 90 loss: 1.6587571986643028 grad: -0.503072829311247
iteration: 100 loss: 1.4978394890620372 grad: -0.45877086932943256
iteration: 110 loss: 1.3654046298206595 grad: -0.4216055453912651
iteration: 120 loss: 1.2545096029329712 grad: -0.3899963844833553
iteration: 130 loss: 1.1602969901346114 grad: -0.3627924810318047
iteration: 140 loss: 1.0792669728844388 grad: -0.33913763124114943
iteration: 150 loss: 1.008834243425223 grad: -0.31838256500521606
iteration: 160 loss: 0.9470477496354927 grad: -0.3000264528588059
iteration: 170 loss: 0.8924075113756192 grad: -0.283677030152569
iteration: 0 loss: 68.83555888255376 grad: 187.75343365473339
iteration: 10 loss: 11.959121882160616 grad: -0.2874497190453956
iteration: 20 loss: 6.8266437779852 grad: -1.1112106255770264
iteration: 30 loss: 4.771869429571532 grad: -0.9887712620305698
iteration: 40 loss: 3.6660305221864955 grad: -0.8401543471736369
iteration: 50 loss: 2.9757184298800894 grad: -0.7214415768139294
iteration: 60 loss: 2.5040161036326936 grad: -0.6295825270796233
iteration: 70 loss: 2.1613747267213266 grad: -0.5575806988201154
iteration: 80 loss: 1.901244757396026 grad: -0.5000083354332844
iteration: 90 loss: 1.6970451835627913 grad: -0.45307090560921687
iteration: 100 loss: 1.5324982931515891 grad: -0.4141351052958899
iteration: 110 loss: 1.3970808474995549 grad: -0.3813458183056726
iteration: 120 loss: 1.2836888051381885 grad: -0.3533691429353749
iteration: 130 loss: 1.1873535204387984 grad: -0.32922575972349455
iteration: 140 loss: 1.1044957808981053 grad: -0.30818244147415463
iteration: 150 loss: 1.0324720321136738 grad: -0.2896802861308463
iteration: 160 loss: 0.9692874847328312 grad: -0.2732863107594651
iteration: 170 loss: 0.9134086541994706 grad: -0.2586601467957874
iteration: 0 loss: 64.87422096230209 grad: 208.91400674323805
iteration: 10 loss: 11.772692249808994 grad: -1.118301309911329
iteration: 20 loss: 6.747222666322242 grad: -1.3984881183325861
iteration: 30 loss: 4.71447006803886 grad: -1.161540284216653
iteration: 40 loss: 3.619191586489168 grad: -0.9645028994341591
iteration: 50 loss: 2.935672112325557 grad: -0.8178664074429388
iteration: 60 loss: 2.4688735970696807 grad: -0.7078975572255304
iteration: 70 loss: 2.1299937384395973 grad: -0.6232694864966521
iteration: 80 loss: 1.8728614880313794 grad: -0.5564232166032267
iteration: 90 loss: 1.671116208958913 grad: -0.5024021190922836
iteration: 100 loss: 1.5086201532893606 grad: -0.45788883250196927
iteration: 110 loss: 1.3749444072224668 grad: -0.42060051034530954
iteration: 120 loss: 1.263051308737059 grad: -0.38892251755230556
iteration: 130 loss: 1.1680205070532876 grad: -0.36168385489909927
iteration: 140 loss: 1.086308875914421 grad: -0.3380160657528337
iteration: 150 loss: 1.01530042402944 grad: -0.3172621826268465
iteration: 160 loss: 0.9530218091869177 grad: -0.298916420625144
iteration: 170 loss: 0.8979565030354024 grad: -0.2825832606290913
iteration: 0 loss: 69.3248271540446 grad: 198.48605649234165
iteration: 10 loss: 11.766103397421231 grad: -1.1928568005639026
iteration: 20 loss: 6.765487413604825 grad: -1.4151242706171876
iteration: 30 loss: 4.738926578685798 grad: -1.1537160321401134
iteration: 40 loss: 3.643771820129876 grad: -0.9501346438043925
iteration: 50 loss: 2.9588328222702414 grad: -0.8028303372988532
iteration: 60 loss: 2.490324920729232 grad: -0.6936475223097887
iteration: 70 loss: 2.149799381864589 grad: -0.6101012275128326
iteration: 80 loss: 1.8911784402301401 grad: -0.5443220889320836
iteration: 90 loss: 1.6881136350643828 grad: -0.49127172383664275
iteration: 100 loss: 1.5244542788046518 grad: -0.447618909515629
iteration: 110 loss: 1.3897521900327197 grad: -0.4110876361480672
iteration: 120 loss: 1.276950507799649 grad: -0.3800755800340525
iteration: 130 loss: 1.1811117562082556 grad: -0.35342443094427883
iteration: 140 loss: 1.0986781702830408 grad: -0.33027717966587805
iteration: 150 loss: 1.0270214606851533 grad: -0.3099866877428441
iteration: 160 loss: 0.9641580161127504 grad: -0.29205544093612124
iteration: 170 loss: 0.908562737677626 grad: -0.2760948275888156
iteration: 0 loss: 64.29177683310557 grad: 214.11777695800873
iteration: 10 loss: 11.84385405008869 grad: -0.6771896929209874
iteration: 20 loss: 6.765785768070352 grad: -1.2255710040001395
iteration: 30 loss: 4.72499341650287 grad: -1.0605754668678493
iteration: 40 loss: 3.6269463130546353 grad: -0.8945137369846388
iteration: 50 loss: 2.941998195860296 grad: -0.7654785558251598
iteration: 60 loss: 2.4743014228828626 grad: -0.6665809797602712
iteration: 70 loss: 2.134788493719415 grad: -0.5894266853410994
iteration: 80 loss: 1.8771772730027474 grad: -0.5279120266326188
iteration: 90 loss: 1.675052297310272 grad: -0.47786198084112885
iteration: 100 loss: 1.512245316776402 grad: -0.4364087611652029
iteration: 110 loss: 1.3783088559170706 grad: -0.4015437556237924
iteration: 120 loss: 1.2661931606912318 grad: -0.3718280541145313
iteration: 130 loss: 1.1709695783227454 grad: -0.34620799555124
iteration: 140 loss: 1.0890890434280995 grad: -0.32389623846795756
iteration: 150 loss: 1.017931174452852 grad: -0.304293564782552
iteration: 160 loss: 0.9555192964774257 grad: -0.28693635425670766
iteration: 170 loss: 0.900334295184414 grad: -0.27146055271516084
iteration: 0 loss: 63.37233360625169 grad: 248.53153215371202
iteration: 10 loss: 11.915779370327982 grad: -1.0155755334142702
iteration: 20 loss: 6.793559875976371 grad: -1.3339963467330163
iteration: 30 loss: 4.740084346919668 grad: -1.1294205857715105
iteration: 40 loss: 3.6366392565254997 grad: -0.9460598603797372
iteration: 50 loss: 2.948839886373993 grad: -0.8068116301772699
iteration: 60 loss: 2.479430259826007 grad: -0.7010638390098312
iteration: 70 loss: 2.1387972042366115 grad: -0.6189727018783981
iteration: 80 loss: 1.8804079775880547 grad: -0.5537268579482488
iteration: 90 loss: 1.6777177220107495 grad: -0.5007578379594578
iteration: 100 loss: 1.514485401279506 grad: -0.45695975252953247
iteration: 110 loss: 1.3802198219553412 grad: -0.4201708765849521
iteration: 120 loss: 1.267843572890177 grad: -0.38884918432322424
iteration: 130 loss: 1.1724097514393592 grad: -0.3618689758318683
iteration: 140 loss: 1.090356821521668 grad: -0.3383910480523527
iteration: 150 loss: 1.0190556251404401 grad: -0.3177778969333262
iteration: 160 loss: 0.9565231532671781 grad: -0.29953702660042336
iteration: 170 loss: 0.901235619467405 grad: -0.2832821980940687
iteration: 0 loss: 65.70639970598239 grad: 202.3928562416773
iteration: 10 loss: 11.872306931253123 grad: -0.47397998721255197
iteration: 20 loss: 6.801594548265215 grad: -1.1882636846075632
iteration: 30 loss: 4.756833128981436 grad: -1.0393207934510802
iteration: 40 loss: 3.6542888118888186 grad: -0.877616676696403
iteration: 50 loss: 2.9656424911698362 grad: -0.750886048986766
iteration: 60 loss: 2.495021380125391 grad: -0.6536291039560596
iteration: 70 loss: 2.1531832822180066 grad: -0.5777701575687478
iteration: 80 loss: 1.8936950809844264 grad: -0.5173205166734064
iteration: 90 loss: 1.6900298632838051 grad: -0.46816473708876727
iteration: 100 loss: 1.5259393940883372 grad: -0.4274731162722867
iteration: 110 loss: 1.3909185598355511 grad: -0.3932639852258163
iteration: 120 loss: 1.2778754911447654 grad: -0.36411846634620904
iteration: 130 loss: 1.181850247288532 grad: -0.3389982156981295
iteration: 140 loss: 1.0992701267668137 grad: -0.3171278249463988
iteration: 150 loss: 1.027496606438283 grad: -0.2979175077677952
iteration: 160 loss: 0.9645388804563647 grad: -0.2809111895375785
iteration: 170 loss: 0.9088666779075951 grad: -0.26575091759647823
iteration: 0 loss: 64.0992411470009 grad: 232.2618649706283
iteration: 10 loss: 11.60971877441621 grad: -1.6559692584687904
iteration: 20 loss: 6.6548944858748404 grad: -1.6163241325130553
iteration: 30 loss: 4.647936850295124 grad: -1.2965169027289871
iteration: 40 loss: 3.566279140125144 grad: -1.0598773516310178
iteration: 50 loss: 2.891399742854074 grad: -0.8909465527722997
iteration: 60 loss: 2.4306725384035004 grad: -0.7667056924781612
iteration: 70 loss: 2.0963325738156198 grad: -0.6721837361541698
iteration: 80 loss: 1.8427404460767733 grad: -0.5981065909295639
iteration: 90 loss: 1.6438411506046164 grad: -0.5385933372518876
iteration: 100 loss: 1.4836871027561178 grad: -0.4897822880436118
iteration: 110 loss: 1.3519747403621212 grad: -0.44904935393399587
iteration: 120 loss: 1.2417527626226645 grad: -0.41455556723759646
iteration: 130 loss: 1.1481624393605587 grad: -0.38497675036590917
iteration: 140 loss: 1.0677058867785159 grad: -0.35933652628952306
iteration: 150 loss: 0.9978011321328598 grad: -0.3368998563772949
iteration: 160 loss: 0.936500934794863 grad: -0.31710318241834246
iteration: 170 loss: 0.8823091451203336 grad: -0.2995073818443479
iteration: 0 loss: 67.6901763160672 grad: 247.7631099563526
iteration: 10 loss: 12.038463423651097 grad: -1.8013582015435454
iteration: 20 loss: 6.875681658650716 grad: -1.5866506892915562
iteration: 30 loss: 4.79909813573427 grad: -1.2491954772419371
iteration: 40 loss: 3.682090820603786 grad: -1.0136517550746476
iteration: 50 loss: 2.9855726450103 grad: -0.8490369338788071
iteration: 60 loss: 2.510147487510453 grad: -0.7292216795283604
iteration: 70 loss: 2.165138354874547 grad: -0.63859855705254
iteration: 80 loss: 1.9034342413290852 grad: -0.567832473172462
iteration: 90 loss: 1.6981520477930148 grad: -0.5111134474884562
iteration: 100 loss: 1.5328408648589091 grad: -0.4646693779058232
iteration: 110 loss: 1.3968728800323746 grad: -0.42595609019200187
iteration: 120 loss: 1.2830781630308261 grad: -0.3931998499912087
iteration: 130 loss: 1.1864450509729068 grad: -0.36512825273861954
iteration: 140 loss: 1.1033653899742355 grad: -0.3408057397661429
iteration: 150 loss: 1.031175648268717 grad: -0.31952952719729666
iteration: 160 loss: 0.9678669382424645 grad: -0.3007617025378573
iteration: 170 loss: 0.9118956328072934 grad: -0.2840836945849237
iteration: 0 loss: 65.2410090947378 grad: 167.5656619698099
iteration: 10 loss: 11.96398510136644 grad: 0.2903957913001974
iteration: 20 loss: 6.829033153960774 grad: -0.7408803677515965
iteration: 30 loss: 4.7702638947812375 grad: -0.7306157140866341
iteration: 40 loss: 3.662957313698743 grad: -0.6465493528656
iteration: 50 loss: 2.97219100975388 grad: -0.5683948455852725
iteration: 60 loss: 2.5004339900948582 grad: -0.5039719486547022
iteration: 70 loss: 2.1578980629499878 grad: -0.45160060398608004
iteration: 80 loss: 1.8979346504913173 grad: -0.4086893653992836
iteration: 90 loss: 1.6939197875681202 grad: -0.37307500477920386
iteration: 100 loss: 1.5295567828209393 grad: -0.34312181177049894
iteration: 110 loss: 1.3943142160348792 grad: -0.31761599303256327
iteration: 120 loss: 1.2810848970633826 grad: -0.2956532595187608
iteration: 130 loss: 1.1848994101395958 grad: -0.2765521427066032
iteration: 140 loss: 1.1021788815410631 grad: -0.2597920023919035
iteration: 150 loss: 1.0302805817696439 grad: -0.2449694854351882
iteration: 160 loss: 0.9672107236092828 grad: -0.23176789503732564
iteration: 170 loss: 0.9114368498042247 grad: -0.21993544179698196
iteration: 0 loss: 68.11794547983779 grad: 191.55537346177346
iteration: 10 loss: 11.944955813819433 grad: -0.9679227524683587
iteration: 20 loss: 6.841940687723102 grad: -1.2945598942075622
iteration: 30 loss: 4.783421160058176 grad: -1.0878461610970023
iteration: 40 loss: 3.6740268805479817 grad: -0.9085467685774405
iteration: 50 loss: 2.981334430207632 grad: -0.7738647012142559
iteration: 60 loss: 2.508047205672979 grad: -0.6721100677962233
iteration: 70 loss: 2.1643193277617767 grad: -0.5933353016705865
iteration: 80 loss: 1.903420830602458 grad: -0.5308220168690287
iteration: 90 loss: 1.6986622787974903 grad: -0.48011638241549603
iteration: 100 loss: 1.5336990348840283 grad: -0.4382107858524661
iteration: 110 loss: 1.3979651671470492 grad: -0.4030208065398798
iteration: 120 loss: 1.2843285218133464 grad: -0.37306378656572675
iteration: 130 loss: 1.1878014024834804 grad: -0.3472594925757432
iteration: 140 loss: 1.104791352690096 grad: -0.3248037373294953
iteration: 150 loss: 1.0326454005968033 grad: -0.30508622787002304
iteration: 160 loss: 0.9693619251084287 grad: -0.28763583180986185
iteration: 170 loss: 0.913402396878899 grad: -0.2720832590276563
iteration: 0 loss: 62.6775302282123 grad: 211.42378550836617
iteration: 10 loss: 11.783542373014143 grad: -0.4268255814694273
iteration: 20 loss: 6.731758738675705 grad: -0.9909524370591646
iteration: 30 loss: 4.703761000681962 grad: -0.9010103501867254
iteration: 40 loss: 3.6125854441189733 grad: -0.779386847539957
iteration: 50 loss: 2.931660261624364 grad: -0.6777443830936836
iteration: 60 loss: 2.4665042391272682 grad: -0.5968917725733983
iteration: 70 loss: 2.1286952119235965 grad: -0.532301619283541
iteration: 80 loss: 1.8722818676123594 grad: -0.47992902551502703
iteration: 90 loss: 1.6710312856730112 grad: -0.43676968462840493
iteration: 100 loss: 1.5088823401922788 grad: -0.40066103011015475
iteration: 110 loss: 1.3754537151867638 grad: -0.37004038909903164
iteration: 120 loss: 1.2637382954816614 grad: -0.3437625792398616
iteration: 130 loss: 1.1688359134368098 grad: -0.320974208128322
iteration: 140 loss: 1.0872171247166411 grad: -0.30102856956739665
iteration: 150 loss: 1.016275423900879 grad: -0.2834277489448723
iteration: 160 loss: 0.9540441766733404 grad: -0.2677827307884968
iteration: 170 loss: 0.8990116812931526 grad: -0.25378548045957244
iteration: 0 loss: 63.90562116629977 grad: 204.17720733245028
iteration: 10 loss: 11.857135150959039 grad: -0.16428022728844496
iteration: 20 loss: 6.774672734187871 grad: -1.120884740257034
iteration: 30 loss: 4.734430316053352 grad: -1.0141812691273724
iteration: 40 loss: 3.6357734524417538 grad: -0.8643356427101341
iteration: 50 loss: 2.949967160809344 grad: -0.7421742313096289
iteration: 60 loss: 2.4814504639844843 grad: -0.6470866462530234
iteration: 70 loss: 2.1412205780101514 grad: -0.5724280891136535
iteration: 80 loss: 1.8829975765238487 grad: -0.5127255132095278
iteration: 90 loss: 1.6803525832378496 grad: -0.464079085891122
iteration: 100 loss: 1.517101940548731 grad: -0.4237601299415738
iteration: 110 loss: 1.3827844784845267 grad: -0.389838836815526
iteration: 120 loss: 1.270339154837116 grad: -0.36092504321170976
iteration: 130 loss: 1.174828249941997 grad: -0.3359973572238453
iteration: 140 loss: 1.0926954669229085 grad: -0.31429094685233483
iteration: 150 loss: 1.0213146497437926 grad: -0.2952229487908501
iteration: 160 loss: 0.9587044893291519 grad: -0.2783420139738507
iteration: 170 loss: 0.903342127168385 grad: -0.26329353568523306
iteration: 0 loss: 65.59175351979657 grad: 175.01018249800921
iteration: 10 loss: 11.915065712498642 grad: 0.9995871855705665
iteration: 20 loss: 6.816456691143602 grad: -0.8768993965560908
iteration: 30 loss: 4.769069522409402 grad: -0.9107918002559081
iteration: 40 loss: 3.665427458613935 grad: -0.8067191090302319
iteration: 50 loss: 2.9758848052787967 grad: -0.7052864105856324
iteration: 60 loss: 2.5044630509204664 grad: -0.621434997471526
iteration: 70 loss: 2.161912229110261 grad: -0.5535866298771093
iteration: 80 loss: 1.9017937580932662 grad: -0.49833671998339285
iteration: 90 loss: 1.697572266530642 grad: -0.45276565980179584
iteration: 100 loss: 1.5329901265064025 grad: -0.4146609651235281
iteration: 110 loss: 1.3975332898284747 grad: -0.3823868517464178
iteration: 120 loss: 1.284101963505195 grad: -0.3547311132922666
iteration: 130 loss: 1.1877294084408112 grad: -0.3307854050575634
iteration: 140 loss: 1.1048371788257016 grad: -0.3098596027929431
iteration: 150 loss: 1.0327819274492123 grad: -0.2914218659976038
iteration: 160 loss: 0.9695688017743371 grad: -0.27505667683642204
iteration: 170 loss: 0.9136641306862326 grad: -0.2604352001401874
iteration: 0 loss: 64.00465651575065 grad: 232.48105376713153
iteration: 10 loss: 11.681870090468403 grad: -1.1516202094073402
iteration: 20 loss: 6.7041541782288565 grad: -1.407642471158511
iteration: 30 loss: 4.687653638588472 grad: -1.169471758966178
iteration: 40 loss: 3.5994921465845353 grad: -0.9712491039496662
iteration: 50 loss: 2.9198711515495317 grad: -0.824260999133454
iteration: 60 loss: 2.4555558279291954 grad: -0.7139618576341699
iteration: 70 loss: 2.1184169815882425 grad: -0.628955255948509
iteration: 80 loss: 1.8625855386993118 grad: -0.5617213755556574
iteration: 90 loss: 1.6618562128212282 grad: -0.5073309028346309
iteration: 100 loss: 1.5001797087224573 grad: -0.4624780561961643
iteration: 110 loss: 1.3671814924478811 grad: -0.42488285081273625
iteration: 120 loss: 1.255859378048585 grad: -0.39292950525585524
iteration: 130 loss: 1.1613172681742816 grad: -0.36544428886373626
iteration: 140 loss: 1.080029254118513 grad: -0.34155557366426903
iteration: 150 loss: 1.0093919500490798 grad: -0.32060328939806687
iteration: 160 loss: 0.947441482159444 grad: -0.30207882040626544
iteration: 170 loss: 0.8926685895065186 grad: -0.2855841500064855
iteration: 0 loss: 57.414514587141724 grad: 206.34081464579762
iteration: 10 loss: 11.335209468275707 grad: 0.06773165867498022
iteration: 20 loss: 6.538848785320666 grad: -1.0990773219311052
iteration: 30 loss: 4.582607403552629 grad: -1.014541939522711
iteration: 40 loss: 3.523688631634617 grad: -0.8734485826001422
iteration: 50 loss: 2.861071160865363 grad: -0.7552181884948347
iteration: 60 loss: 2.407768727065931 grad: -0.6618512704351707
iteration: 70 loss: 2.0782968893785934 grad: -0.5878204836384481
iteration: 80 loss: 1.8280871057902666 grad: -0.528183122223192
iteration: 90 loss: 1.6316438124859318 grad: -0.4793077403518285
iteration: 100 loss: 1.4733360548042667 grad: -0.4386079711214215
iteration: 110 loss: 1.3430510128524806 grad: -0.4042320458385549
iteration: 120 loss: 1.233958184904912 grad: -0.37483354302130395
iteration: 130 loss: 1.1412785465483302 grad: -0.3494158274566011
iteration: 140 loss: 1.0615686376156026 grad: -0.3272280084326164
iteration: 150 loss: 0.9922847408441736 grad: -0.30769486777997357
iteration: 160 loss: 0.9315071849830681 grad: -0.2903689818356663
iteration: 170 loss: 0.8777601233779767 grad: -0.27489745535761395
iteration: 0 loss: 64.81705998513227 grad: 224.866669935598
iteration: 10 loss: 11.582482613808907 grad: -1.871074352642348
iteration: 20 loss: 6.658255813514601 grad: -1.6820185098040072
iteration: 30 loss: 4.6570750495939555 grad: -1.3141824752063256
iteration: 40 loss: 3.5766292808141107 grad: -1.0623396509286895
iteration: 50 loss: 2.901686702624664 grad: -0.8878873475687489
iteration: 60 loss: 2.440500930611341 grad: -0.761506931678837
iteration: 70 loss: 2.1055948736301286 grad: -0.6661912194377976
iteration: 80 loss: 1.8514321751799798 grad: -0.5919041540720018
iteration: 90 loss: 1.6519948619572296 grad: -0.5324463834644856
iteration: 100 loss: 1.4913468804164618 grad: -0.4838120029866251
iteration: 110 loss: 1.3591860197786694 grad: -0.44330782922762535
iteration: 120 loss: 1.2485583567868574 grad: -0.4090606053027943
iteration: 130 loss: 1.154601182415387 grad: -0.3797288979144886
iteration: 140 loss: 1.0738123933706034 grad: -0.3543277575671476
iteration: 150 loss: 1.0036060174267154 grad: -0.33211812673930313
iteration: 160 loss: 0.9420312029066984 grad: -0.312534854522183
iteration: 170 loss: 0.8875886156323428 grad: -0.2951385204313032
iteration: 0 loss: 66.4575900007075 grad: 231.97277418706807
iteration: 10 loss: 11.876560902184814 grad: -1.282184210010735
iteration: 20 loss: 6.782718579884824 grad: -1.42814540550562
iteration: 30 loss: 4.7339180801736385 grad: -1.1824442339739543
iteration: 40 loss: 3.6319899671005427 grad: -0.9810074872744905
iteration: 50 loss: 2.9449265845672183 grad: -0.8319324935901188
iteration: 60 loss: 2.4759756411885148 grad: -0.7201752546035339
iteration: 70 loss: 2.1356733501060345 grad: -0.6341101787691208
iteration: 80 loss: 1.877543228865389 grad: -0.5660834591283678
iteration: 90 loss: 1.675065982979752 grad: -0.5110828418170961
iteration: 100 loss: 1.5120140891172014 grad: -0.4657493846004293
iteration: 110 loss: 1.377904404182945 grad: -0.42776794726307016
iteration: 120 loss: 1.2656647720864385 grad: -0.39549881545576343
iteration: 130 loss: 1.1703519620011569 grad: -0.3677515273891566
iteration: 140 loss: 1.0884071350959525 grad: -0.34364247474051185
iteration: 150 loss: 1.0172031893806972 grad: -0.3225028219153608
iteration: 160 loss: 0.9547587271882466 grad: -0.30381741511210003
iteration: 170 loss: 0.8995512522169933 grad: -0.2871832755648285
iteration: 0 loss: 61.54886237549352 grad: 263.72056843867676
iteration: 10 loss: 11.441344002080996 grad: -1.977250537740098
iteration: 20 loss: 6.607192204483123 grad: -1.8089210687488633
iteration: 30 loss: 4.6244583820428655 grad: -1.4208314912829638
iteration: 40 loss: 3.5510783772372387 grad: -1.1495831285226523
iteration: 50 loss: 2.880027737599655 grad: -0.9602663299269105
iteration: 60 loss: 2.421447021516873 grad: -0.8227297790007408
iteration: 70 loss: 2.0884765907575127 grad: -0.718918224346178
iteration: 80 loss: 1.8358404559496855 grad: -0.6380207984303942
iteration: 90 loss: 1.6376526244670542 grad: -0.5733090906932541
iteration: 100 loss: 1.478053380638812 grad: -0.5204177118594562
iteration: 110 loss: 1.3467890102417435 grad: -0.4764054409844941
iteration: 120 loss: 1.2369385378748452 grad: -0.43922427106451
iteration: 130 loss: 1.1436627617254502 grad: -0.40740697414730176
iteration: 140 loss: 1.0634769768038579 grad: -0.3798761758091841
iteration: 150 loss: 0.9938084578050372 grad: -0.3558236198810054
iteration: 160 loss: 0.9327166688329023 grad: -0.3346314511778029
iteration: 170 loss: 0.8787104579310753 grad: -0.31581948043832747
iteration: 0 loss: 63.533630842364076 grad: 187.2827290961668
iteration: 10 loss: 11.86721854900702 grad: 0.25119356905591156
iteration: 20 loss: 6.815246874011915 grad: -0.9882477211334546
iteration: 30 loss: 4.770446805470576 grad: -0.9342009698736656
iteration: 40 loss: 3.6663485576262826 grad: -0.8110514176495309
iteration: 50 loss: 2.9762111027036386 grad: -0.704706521725007
iteration: 60 loss: 2.504340311284812 grad: -0.6197099284733324
iteration: 70 loss: 2.161476512383284 grad: -0.5518403911471863
iteration: 80 loss: 1.9011436085325757 grad: -0.49689865660036325
iteration: 90 loss: 1.6967759277154486 grad: -0.4517040795324835
iteration: 100 loss: 1.5320947401985243 grad: -0.4139573179176651
iteration: 110 loss: 1.3965717124040318 grad: -0.3819968771100759
iteration: 120 loss: 1.2830973594915867 grad: -0.3546068520715332
iteration: 130 loss: 1.1866982903858032 grad: -0.33088279004728327
iteration: 140 loss: 1.1037914283942056 grad: -0.31014057426227926
iteration: 150 loss: 1.031730155966133 grad: -0.29185443242858844
iteration: 160 loss: 0.9685172798013727 grad: -0.2756142701817458
iteration: 170 loss: 0.9126174328302196 grad: -0.261095859874403
iteration: 0 loss: 65.7332998739283 grad: 230.5304052168381
iteration: 10 loss: 11.693114460475561 grad: -1.896344651489137
iteration: 20 loss: 6.7232907969814315 grad: -1.7145458532198918
iteration: 30 loss: 4.703715206062745 grad: -1.341533061157681
iteration: 40 loss: 3.6127486576633188 grad: -1.084913990427466
iteration: 50 loss: 2.930999565609661 grad: -0.9068303288406138
iteration: 60 loss: 2.465073919455418 grad: -0.7777132003344276
iteration: 70 loss: 2.1266920113833874 grad: -0.6802960527391932
iteration: 80 loss: 1.869879533958283 grad: -0.6043586596612472
iteration: 90 loss: 1.6683600718000973 grad: -0.543577088349772
iteration: 100 loss: 1.5060357302304923 grad: -0.49386069187114356
iteration: 110 loss: 1.3724980598613414 grad: -0.45245752899895636
iteration: 120 loss: 1.2607206624931926 grad: -0.4174526877733535
iteration: 130 loss: 1.16578977648144 grad: -0.38747455203072706
iteration: 140 loss: 1.0841663467807103 grad: -0.3615158515442121
iteration: 150 loss: 1.0132370258381178 grad: -0.3388207119671897
iteration: 160 loss: 0.9510302679102758 grad: -0.31881109683778985
iteration: 170 loss: 0.8960308167177324 grad: -0.3010375608645851
iteration: 0 loss: 62.54472313722806 grad: 200.00870679704235
iteration: 10 loss: 11.490203506365685 grad: -0.6576419780340885
iteration: 20 loss: 6.630830331819694 grad: -1.2886900847108196
iteration: 30 loss: 4.646011626431283 grad: -1.1032224699988613
iteration: 40 loss: 3.571424764053931 grad: -0.9257804756473507
iteration: 50 loss: 2.8990827156794694 grad: -0.7900445707428942
iteration: 60 loss: 2.439216776107845 grad: -0.6867959391246546
iteration: 70 loss: 2.105042701519613 grad: -0.6066024469973533
iteration: 80 loss: 1.8513113286626734 grad: -0.5428480848857689
iteration: 90 loss: 1.652138919929009 grad: -0.4910795694040682
iteration: 100 loss: 1.4916580571349518 grad: -0.4482659798334998
iteration: 110 loss: 1.3596041457967658 grad: -0.41229705290212093
iteration: 120 loss: 1.2490450459374023 grad: -0.3816673435468515
iteration: 130 loss: 1.1551312379301635 grad: -0.3552778485505922
iteration: 140 loss: 1.0743689022376863 grad: -0.33230927240072805
iteration: 150 loss: 1.004177430074957 grad: -0.31213923172330543
iteration: 160 loss: 0.9426095320693755 grad: -0.29428690198986324
iteration: 170 loss: 0.8881682914027972 grad: -0.27837518290114127
iteration: 0 loss: 62.91684393132673 grad: 181.0314055756353
iteration: 10 loss: 11.717554328479412 grad: 0.07812212684790178
iteration: 20 loss: 6.715568194744471 grad: -0.9270486225815204
iteration: 30 loss: 4.697867433700594 grad: -0.8796930859102379
iteration: 40 loss: 3.609810109625414 grad: -0.7667834823215445
iteration: 50 loss: 2.9300952311000814 grad: -0.6676748498965991
iteration: 60 loss: 2.465493511611302 grad: -0.5878436334072961
iteration: 70 loss: 2.1279710931719986 grad: -0.5238341800063727
iteration: 80 loss: 1.871720812618481 grad: -0.47189407330439137
iteration: 90 loss: 1.670570670403446 grad: -0.42910715058067483
iteration: 100 loss: 1.508487949082833 grad: -0.3933388400308075
iteration: 110 loss: 1.3751056500629024 grad: -0.36303561903086057
iteration: 120 loss: 1.263424346112722 grad: -0.33705547727197305
iteration: 130 loss: 1.1685482233586877 grad: -0.314546398269589
iteration: 140 loss: 1.0869504185414123 grad: -0.2948626098009344
iteration: 150 loss: 1.016026019224145 grad: -0.27750709341443414
iteration: 160 loss: 0.9538094103212312 grad: -0.26209177998648214
iteration: 170 loss: 0.898789563953001 grad: -0.2483096225546882
iteration: 0 loss: 62.29742046762508 grad: 185.83695446130145
iteration: 10 loss: 11.727563879473875 grad: 0.60678258113399
iteration: 20 loss: 6.712928997761041 grad: -0.81802614382496
iteration: 30 loss: 4.695524830745564 grad: -0.8481013016446639
iteration: 40 loss: 3.6081089723515603 grad: -0.7589949871996442
iteration: 50 loss: 2.928869389272801 grad: -0.669261869421536
iteration: 60 loss: 2.4645991872886372 grad: -0.5935729675341086
iteration: 70 loss: 2.1273099416571313 grad: -0.5314649814261514
iteration: 80 loss: 1.8712269514655666 grad: -0.48036322542847304
iteration: 90 loss: 1.6701993253582672 grad: -0.43787623924120767
iteration: 100 loss: 1.508208025121555 grad: -0.40212443233230577
iteration: 110 loss: 1.3748950714871584 grad: -0.371686267153009
iteration: 120 loss: 1.2632671167782312 grad: -0.3454912874784283
iteration: 130 loss: 1.1684325370168318 grad: -0.32272757187188283
iteration: 140 loss: 1.0868674172310917 grad: -0.30277230309639064
iteration: 150 loss: 1.0159689594426957 grad: -0.2851417368438074
iteration: 160 loss: 0.9537730913540062 grad: -0.26945545423280215
iteration: 170 loss: 0.8987699283983684 grad: -0.2554107211764771
iteration: 0 loss: 64.09587710585025 grad: 181.23808649926292
iteration: 10 loss: 11.642612563192229 grad: 0.4249324774043628
iteration: 20 loss: 6.690848328097342 grad: -0.9202319893621173
iteration: 30 loss: 4.688448921573746 grad: -0.9130040243025397
iteration: 40 loss: 3.6060483915733297 grad: -0.8030128482468148
iteration: 50 loss: 2.9288482248366976 grad: -0.7006652656661025
iteration: 60 loss: 2.4654992427454916 grad: -0.6169134851468157
iteration: 70 loss: 2.128645423445877 grad: -0.5493862963201013
iteration: 80 loss: 1.8727655039155335 grad: -0.4944834175375848
iteration: 90 loss: 1.6718223808484254 grad: -0.4492340676623473
iteration: 100 loss: 1.509852518188795 grad: -0.41141489189169045
iteration: 110 loss: 1.3765267391263065 grad: -0.3793910301321197
iteration: 120 loss: 1.2648673561340977 grad: -0.3519543503314156
iteration: 130 loss: 1.1699915587086749 grad: -0.3282009977118653
iteration: 140 loss: 1.0883805129776853 grad: -0.3074449247093588
iteration: 150 loss: 1.0174343919668238 grad: -0.28915776392060144
iteration: 160 loss: 0.9551908662980085 grad: -0.27292688666083464
iteration: 170 loss: 0.9001410631843679 grad: -0.2584258425357113
iteration: 0 loss: 64.64521518574362 grad: 221.87935456069036
iteration: 10 loss: 11.655188185559712 grad: -1.4976085268972321
iteration: 20 loss: 6.696469139553259 grad: -1.5119943924561814
iteration: 30 loss: 4.686143531773558 grad: -1.2111311818534842
iteration: 40 loss: 3.600624737265372 grad: -0.9903037503828336
iteration: 50 loss: 2.9222204303037373 grad: -0.8333914124475258
iteration: 60 loss: 2.4584735489294705 grad: -0.7181358938256214
iteration: 70 loss: 2.1215845577997823 grad: -0.6304319154317004
iteration: 80 loss: 1.8658378618850044 grad: -0.5616430098210097
iteration: 90 loss: 1.6651049430693092 grad: -0.5063219783989885
iteration: 100 loss: 1.5033770744400747 grad: -0.46090052450854246
iteration: 110 loss: 1.3703020691804009 grad: -0.422956119100932
iteration: 120 loss: 1.2588904279096327 grad: -0.39079124965151696
iteration: 130 loss: 1.164253398846251 grad: -0.3631832216586376
iteration: 140 loss: 1.0828693649795738 grad: -0.3392300035281888
iteration: 150 loss: 1.0121374389511237 grad: -0.31825208000172783
iteration: 160 loss: 0.9500951708470727 grad: -0.299728090784502
iteration: 170 loss: 0.895234070400668 grad: -0.2832514947548453
iteration: 0 loss: 63.69207793828052 grad: 226.3538353608555
iteration: 10 loss: 11.764551826584466 grad: -0.7513963769790872
iteration: 20 loss: 6.763676077108027 grad: -1.4054143489467328
iteration: 30 loss: 4.73420405375191 grad: -1.1912679047768553
iteration: 40 loss: 3.6377204808665984 grad: -0.9925655899637591
iteration: 50 loss: 2.9523161491346483 grad: -0.8428012152003848
iteration: 60 loss: 2.483741786001991 grad: -0.729955460954733
iteration: 70 loss: 2.14333234922337 grad: -0.6428873422327638
iteration: 80 loss: 1.8849090660339491 grad: -0.5740089534968407
iteration: 90 loss: 1.6820747084007879 grad: -0.5182949023204031
iteration: 100 loss: 1.5186544396488815 grad: -0.47236133566404814
iteration: 110 loss: 1.384188046781467 grad: -0.4338704889078714
iteration: 120 loss: 1.2716128205552195 grad: -0.4011645283755957
iteration: 130 loss: 1.1759886733038085 grad: -0.37303894713437574
iteration: 140 loss: 1.0937569446003759 grad: -0.3485993004568022
iteration: 150 loss: 1.0222893573534086 grad: -0.3271683454145719
iteration: 160 loss: 0.9596027801953411 grad: -0.3082243436515471
iteration: 170 loss: 0.9041728188989592 grad: -0.29135910394922526
iteration: 0 loss: 66.90911480795283 grad: 250.4064161832154
iteration: 10 loss: 11.985589232092831 grad: -2.061553829716461
iteration: 20 loss: 6.840561346754509 grad: -1.7279980349234665
iteration: 30 loss: 4.768280945050678 grad: -1.3408651068138062
iteration: 40 loss: 3.654701040078676 grad: -1.081524885239176
iteration: 50 loss: 2.9610980691152444 grad: -0.9028710775603697
iteration: 60 loss: 2.488118539073515 grad: -0.7737570826964821
iteration: 70 loss: 2.1451572437206115 grad: -0.6765158881592546
iteration: 80 loss: 1.885177594251135 grad: -0.6008010668939007
iteration: 90 loss: 1.6813600197863046 grad: -0.5402445962056999
iteration: 100 loss: 1.5173043516792446 grad: -0.49074027985233304
iteration: 110 loss: 1.3824224402838312 grad: -0.4495314329259257
iteration: 120 loss: 1.2695752518779955 grad: -0.4147026371050184
iteration: 130 loss: 1.1737752967169812 grad: -0.38488339124988913
iteration: 140 loss: 1.0914334812419755 grad: -0.35906807574299227
iteration: 150 loss: 1.0199014403014273 grad: -0.33650254764198806
iteration: 160 loss: 0.9571824695365815 grad: -0.3166103992339933
iteration: 170 loss: 0.9017428204679163 grad: -0.2989436470611178
iteration: 0 loss: 63.40324099360908 grad: 236.43812828781464
iteration: 10 loss: 11.948826647443246 grad: -1.010930107317317
iteration: 20 loss: 6.826003666085481 grad: -1.3249252615957892
iteration: 30 loss: 4.762718576215651 grad: -1.1244109515095722
iteration: 40 loss: 3.6532070702187 grad: -0.9427056653600716
iteration: 50 loss: 2.961618416784569 grad: -0.8040494238626847
iteration: 60 loss: 2.4896874148807497 grad: -0.698607596212225
iteration: 70 loss: 2.1472822495910475 grad: -0.616734102967093
iteration: 80 loss: 1.8875922217463001 grad: -0.5516677223617298
iteration: 90 loss: 1.683913569834286 grad: -0.498854419472763
iteration: 100 loss: 1.519908983141332 grad: -0.45519367877158784
iteration: 110 loss: 1.3850260159470054 grad: -0.41852667569981405
iteration: 120 loss: 1.2721466604280067 grad: -0.3873134934318855
iteration: 130 loss: 1.176296164924183 grad: -0.36043018400179433
iteration: 140 loss: 1.0938933215364324 grad: -0.3370390551795934
iteration: 150 loss: 1.02229472260714 grad: -0.316503911477158
iteration: 160 loss: 0.9595068093402997 grad: -0.2983333863334069
iteration: 170 loss: 0.9039978341232365 grad: -0.28214221163709674
iteration: 0 loss: 65.56197706153088 grad: 223.65846485977926
iteration: 10 loss: 11.78743533370632 grad: -1.5890372972022984
iteration: 20 loss: 6.763397230103527 grad: -1.4934126949338797
iteration: 30 loss: 4.728591549884971 grad: -1.1817298031891685
iteration: 40 loss: 3.631031570263407 grad: -0.962611069953067
iteration: 50 loss: 2.945660867785715 grad: -0.8088809189986357
iteration: 60 loss: 2.4774364470601373 grad: -0.6965820038667181
iteration: 70 loss: 2.137452388447359 grad: -0.6113673389335489
iteration: 80 loss: 1.8794499006873016 grad: -0.5446348027151087
iteration: 90 loss: 1.6770055951284453 grad: -0.4910157170666446
iteration: 100 loss: 1.513938002405625 grad: -0.44701467379258863
iteration: 110 loss: 1.3797875186015849 grad: -0.4102677775694443
iteration: 120 loss: 1.2674945709054968 grad: -0.3791228580453705
iteration: 130 loss: 1.1721228635482144 grad: -0.35239199297666857
iteration: 140 loss: 1.0901174464332721 grad: -0.3291999062820117
iteration: 150 loss: 1.0188534045779638 grad: -0.30888783621221483
iteration: 160 loss: 0.956350551003156 grad: -0.2909506799833025
iteration: 170 loss: 0.9010870292007976 grad: -0.2749947646189526
iteration: 0 loss: 63.109555649503456 grad: 236.2154753776477
iteration: 10 loss: 11.851691077834609 grad: -0.9246055793302412
iteration: 20 loss: 6.779160822562765 grad: -1.3910237451897691
iteration: 30 loss: 4.733956043107231 grad: -1.1610799620167733
iteration: 40 loss: 3.6328132382982816 grad: -0.9620791028380026
iteration: 50 loss: 2.9459476018931294 grad: -0.8144451280688263
iteration: 60 loss: 2.477032528890463 grad: -0.7040254001028319
iteration: 70 loss: 2.136713537974546 grad: -0.6192141566527717
iteration: 80 loss: 1.8785486325093437 grad: -0.5523301391306712
iteration: 90 loss: 1.676031021921269 grad: -0.4983530315716068
iteration: 100 loss: 1.5129381201198322 grad: -0.4539293063798616
iteration: 110 loss: 1.3787887969639419 grad: -0.416754868440097
iteration: 120 loss: 1.2665116822017395 grad: -0.38520241751322076
iteration: 130 loss: 1.1711637878940813 grad: -0.35809337212784764
iteration: 140 loss: 1.0891862723445807 grad: -0.3345547584566033
iteration: 150 loss: 1.0179519213958306 grad: -0.3139269646224987
iteration: 160 loss: 0.9554791809256461 grad: -0.2957027164435208
iteration: 170 loss: 0.9002453854449496 grad: -0.279485721717719
iteration: 0 loss: 64.85548324030327 grad: 235.57670078528838
iteration: 10 loss: 11.930410287360349 grad: -1.0887700498308686
iteration: 20 loss: 6.814236822851564 grad: -1.349793561619001
iteration: 30 loss: 4.75716937753503 grad: -1.1272889198545384
iteration: 40 loss: 3.650458464135392 grad: -0.9387448877575179
iteration: 50 loss: 2.9602653190471426 grad: -0.7981210390333087
iteration: 60 loss: 2.48910763371499 grad: -0.6922733101069936
iteration: 70 loss: 2.1471652834640156 grad: -0.6105297386522447
iteration: 80 loss: 1.8877675542317278 grad: -0.5457763834957489
iteration: 90 loss: 1.684280524576314 grad: -0.493326919240252
iteration: 100 loss: 1.5204047419637887 grad: -0.4500290202995573
iteration: 110 loss: 1.385609665173392 grad: -0.4137039469330954
iteration: 120 loss: 1.2727906360410914 grad: -0.3828051864934429
iteration: 130 loss: 1.1769813884609654 grad: -0.35620787140549204
iteration: 140 loss: 1.0946062981502935 grad: -0.33307575253125
iteration: 150 loss: 1.0230257389920636 grad: -0.31277495725381566
iteration: 160 loss: 0.9602487761984445 grad: -0.2948166520646003
iteration: 170 loss: 0.9047455209108027 grad: -0.2788180167614974
iteration: 0 loss: 62.27137782662024 grad: 196.6013090659326
iteration: 10 loss: 11.820233209038799 grad: 1.4319530837015733
iteration: 20 loss: 6.743638124422887 grad: -0.6122639933720948
iteration: 30 loss: 4.715215463659123 grad: -0.753661862270085
iteration: 40 loss: 3.6235275536688607 grad: -0.702881654972823
iteration: 50 loss: 2.9418886734046366 grad: -0.6310972574014517
iteration: 60 loss: 2.475998093783305 grad: -0.5654224085490738
iteration: 70 loss: 2.137503612217064 grad: -0.5095589658298533
iteration: 80 loss: 1.8804729041885366 grad: -0.4626593074510099
iteration: 90 loss: 1.6786731614607064 grad: -0.4231604342776952
iteration: 100 loss: 1.5160371123263097 grad: -0.38962411210982895
iteration: 110 loss: 1.3821758093358767 grad: -0.36088320010761293
iteration: 120 loss: 1.2700747616733101 grad: -0.33602312886627195
iteration: 130 loss: 1.1748272448653965 grad: -0.31433228558909465
iteration: 140 loss: 1.092898183834884 grad: -0.29525502989752905
iteration: 150 loss: 1.0216763082737337 grad: -0.2783541281948248
iteration: 160 loss: 0.959190972996277 grad: -0.26328228333798
iteration: 170 loss: 0.9039271016914344 grad: -0.24976092450023266
iteration: 0 loss: 58.84196069789532 grad: 244.35167509004714
iteration: 10 loss: 11.73903329092555 grad: 0.3655405091264511
iteration: 20 loss: 6.7138097313410405 grad: -1.039453490202008
iteration: 30 loss: 4.693342801296487 grad: -1.0296151468178445
iteration: 40 loss: 3.604462090126493 grad: -0.9038135433360698
iteration: 50 loss: 2.9245171534114642 grad: -0.7870101379000985
iteration: 60 loss: 2.459932121111003 grad: -0.691629261621588
iteration: 70 loss: 2.122537018686506 grad: -0.6148810184610256
iteration: 80 loss: 1.8664627870142878 grad: -0.5525988925731727
iteration: 90 loss: 1.6655072699209499 grad: -0.5013572078593466
iteration: 100 loss: 1.503622562403755 grad: -0.4585980098831313
iteration: 110 loss: 1.3704339815468218 grad: -0.42244396192462985
iteration: 120 loss: 1.2589383322153316 grad: -0.39151010073701975
iteration: 130 loss: 1.1642381296218114 grad: -0.3647618943574985
iteration: 140 loss: 1.082805975375653 grad: -0.34141524628522635
iteration: 150 loss: 1.0120370341948977 grad: -0.32086709168999394
iteration: 160 loss: 0.949966089030361 grad: -0.30264706184008117
iteration: 170 loss: 0.8950826665133966 grad: -0.28638346573758855
iteration: 0 loss: 61.10271284730588 grad: 210.57024251928777
iteration: 10 loss: 11.599034532655423 grad: 0.11043541941970356
iteration: 20 loss: 6.652767961972628 grad: -1.2292271530685746
iteration: 30 loss: 4.652981614627564 grad: -1.111469037184859
iteration: 40 loss: 3.574171148852119 grad: -0.9428001593194613
iteration: 50 loss: 2.900314865395351 grad: -0.8069122633026222
iteration: 60 loss: 2.4398284938368886 grad: -0.7019793751910104
iteration: 70 loss: 2.1053812241401912 grad: -0.6200105266985436
iteration: 80 loss: 1.8515266631981666 grad: -0.5546861570871564
iteration: 90 loss: 1.6523005168793465 grad: -0.5015874187741518
iteration: 100 loss: 1.4917994248682838 grad: -0.457656772596081
iteration: 110 loss: 1.3597416973964094 grad: -0.42074756165899047
iteration: 120 loss: 1.2491866855930724 grad: -0.3893212661349182
iteration: 130 loss: 1.1552805070141816 grad: -0.36225168230217486
iteration: 140 loss: 1.0745270729207732 grad: -0.33869785644178946
iteration: 150 loss: 1.0043445987400694 grad: -0.31802031128827724
iteration: 160 loss: 0.9427852040926982 grad: -0.29972471828427416
iteration: 170 loss: 0.8883516983002913 grad: -0.2834232527920555
iteration: 0 loss: 65.55489602618061 grad: 171.73544742302167
iteration: 10 loss: 11.871025479552806 grad: 0.6906950288731064
iteration: 20 loss: 6.802676006154632 grad: -0.9611476579433815
iteration: 30 loss: 4.760085615063252 grad: -0.9518868790467885
iteration: 40 loss: 3.6580911012764443 grad: -0.8318643596228338
iteration: 50 loss: 2.969446889242549 grad: -0.7226241206914528
iteration: 60 loss: 2.498647809769905 grad: -0.6342975223356414
iteration: 70 loss: 2.1565805149793547 grad: -0.5636122507285712
iteration: 80 loss: 1.8968592584664377 grad: -0.5064345578912558
iteration: 90 loss: 1.6929743718759378 grad: -0.45948521392199915
iteration: 100 loss: 1.5286830596363954 grad: -0.4203560921595426
iteration: 110 loss: 1.3934809117391855 grad: -0.3872968287244759
iteration: 120 loss: 1.2802749145224963 grad: -0.3590241480489987
iteration: 130 loss: 1.184103355440832 grad: -0.33458355083640234
iteration: 140 loss: 1.101391652296446 grad: -0.3132537882294611
iteration: 150 loss: 1.0294995048505122 grad: -0.29448135998036024
iteration: 160 loss: 0.966434510116125 grad: -0.2778352673773262
iteration: 170 loss: 0.9106649960798346 grad: -0.26297535364525
iteration: 0 loss: 63.62402293851935 grad: 158.59518291745
iteration: 10 loss: 11.73748698854082 grad: 0.9407296232824114
iteration: 20 loss: 6.7320240336973916 grad: -0.7234791514078233
iteration: 30 loss: 4.7127008784748545 grad: -0.7895404552111567
iteration: 40 loss: 3.6230906643231306 grad: -0.715089950258671
iteration: 50 loss: 2.942029349951967 grad: -0.6339001916373859
iteration: 60 loss: 2.476290901549295 grad: -0.5639730685069305
iteration: 70 loss: 2.137810615993079 grad: -0.5060471798616532
iteration: 80 loss: 1.8807483110931984 grad: -0.4581230125697875
iteration: 90 loss: 1.6789039319323407 grad: -0.418130289960831
iteration: 100 loss: 1.516222365768995 grad: -0.3843859849878743
iteration: 110 loss: 1.3823190490239872 grad: -0.35559639990721137
iteration: 120 loss: 1.270180796507909 grad: -0.3307779901839942
iteration: 130 loss: 1.174900961009991 grad: -0.30917999593590867
iteration: 140 loss: 1.0929440798622636 grad: -0.2902238670075389
iteration: 150 loss: 1.0216983523619092 grad: -0.2734586625728393
iteration: 160 loss: 0.9591925945805584 grad: -0.2585287681358674
iteration: 170 loss: 0.9039112356728186 grad: -0.2451505378056685
iteration: 0 loss: 63.361822846296945 grad: 180.09074894422784
iteration: 10 loss: 11.8674203033732 grad: 0.6060307279452983
iteration: 20 loss: 6.775156442493384 grad: -0.7474545621176727
iteration: 30 loss: 4.736653155645524 grad: -0.7840412030272468
iteration: 40 loss: 3.6395072129706603 grad: -0.706610258878038
iteration: 50 loss: 2.954550357111326 grad: -0.6260714891193304
iteration: 60 loss: 2.48645366583105 grad: -0.5572021288045802
iteration: 70 loss: 2.146393944791147 grad: -0.5002187672275816
iteration: 80 loss: 1.888197964024419 grad: -0.453063013863652
iteration: 90 loss: 1.6854982715597295 grad: -0.41368857832161743
iteration: 100 loss: 1.5221468851252047 grad: -0.38044507120325755
iteration: 110 loss: 1.387703693642958 grad: -0.35206607611052254
iteration: 120 loss: 1.2751202665787622 grad: -0.32758874590961673
iteration: 130 loss: 1.1794665063078933 grad: -0.30627767152133784
iteration: 140 loss: 1.09719072902784 grad: -0.2875656980194936
iteration: 150 loss: 1.0256695446637836 grad: -0.27101042415580834
iteration: 160 loss: 0.962923260108751 grad: -0.25626271604074363
iteration: 170 loss: 0.9074299097103242 grad: -0.24304391208972637
iteration: 0 loss: 64.64424632850111 grad: 215.4797169989518
iteration: 10 loss: 11.986279051501821 grad: -0.4114040777598038
iteration: 20 loss: 6.841290645871926 grad: -1.116129296862378
iteration: 30 loss: 4.779649635985813 grad: -0.9972279103971641
iteration: 40 loss: 3.6705648212563244 grad: -0.8508548431601126
iteration: 50 loss: 2.9784921079629965 grad: -0.7324432235466214
iteration: 60 loss: 2.505735288257132 grad: -0.6401259603329282
iteration: 70 loss: 2.162418016624997 grad: -0.5674244669992075
iteration: 80 loss: 1.9018334379146091 grad: -0.5091129457752426
iteration: 90 loss: 1.6973170586986106 grad: -0.4614724728836368
iteration: 100 loss: 1.5325434231413468 grad: -0.42189475056657305
iteration: 110 loss: 1.3969603632849612 grad: -0.38852928137130804
iteration: 120 loss: 1.2834454855873114 grad: -0.36003881237950697
iteration: 130 loss: 1.1870180596619817 grad: -0.33543792411608003
iteration: 140 loss: 1.1040906651122564 grad: -0.3139867388958318
iteration: 150 loss: 1.0320140254599126 grad: -0.2951200447420326
iteration: 160 loss: 0.9687892723866092 grad: -0.27839920677545127
iteration: 170 loss: 0.9128799571985424 grad: -0.2634789299401233
iteration: 0 loss: 64.47532448633052 grad: 201.68048369509887
iteration: 10 loss: 11.752932266708946 grad: -0.592262449434048
iteration: 20 loss: 6.7254641994741595 grad: -1.2199428339135094
iteration: 30 loss: 4.701481063750676 grad: -1.0649202623665592
iteration: 40 loss: 3.611032806167135 grad: -0.9008277458098684
iteration: 50 loss: 2.9302170673053793 grad: -0.7720683558695232
iteration: 60 loss: 2.465054203040836 grad: -0.6729720191204502
iteration: 70 loss: 2.12722939028357 grad: -0.5954799397236761
iteration: 80 loss: 1.8708125448826485 grad: -0.5335995507226639
iteration: 90 loss: 1.6695718289159396 grad: -0.48319537266524876
iteration: 100 loss: 1.5074430281956666 grad: -0.4414131532442066
iteration: 110 loss: 1.3740415504744719 grad: -0.40624767121055155
iteration: 120 loss: 1.2623576752349768 grad: -0.3762593835342537
iteration: 130 loss: 1.167489342114343 grad: -0.3503924888686173
iteration: 140 loss: 1.085905785635532 grad: -0.3278570666555829
iteration: 150 loss: 1.014999599418347 grad: -0.3080513333062305
iteration: 160 loss: 0.9528035464224807 grad: -0.2905092971722929
iteration: 170 loss: 0.8978055306117974 grad: -0.2748647853999285
iteration: 0 loss: 65.10181552176873 grad: 196.12222177909686
iteration: 10 loss: 11.767314635566457 grad: -0.6729263310633522
iteration: 20 loss: 6.746235175771414 grad: -1.2562951648273688
iteration: 30 loss: 4.715629790814918 grad: -1.0729085801237028
iteration: 40 loss: 3.6209018030451925 grad: -0.8988902697312886
iteration: 50 loss: 2.937454749080561 grad: -0.7663250330541524
iteration: 60 loss: 2.4705910340397335 grad: -0.6657565084852075
iteration: 70 loss: 2.1316082174091453 grad: -0.5877802433006836
iteration: 80 loss: 1.874367685313287 grad: -0.5258615003477711
iteration: 90 loss: 1.6725197084445178 grad: -0.475625152668837
iteration: 100 loss: 1.5099298370190477 grad: -0.4341037554452384
iteration: 110 loss: 1.3761696283381752 grad: -0.39923618564748287
iteration: 120 loss: 1.2642008563771014 grad: -0.3695546102925077
iteration: 130 loss: 1.1691022777757676 grad: -0.3439888846928335
iteration: 140 loss: 1.0873298275681587 grad: -0.3217420339349814
iteration: 150 loss: 1.0162666381128143 grad: -0.3022091152515902
iteration: 160 loss: 0.953938588577578 grad: -0.28492307966617475
iteration: 170 loss: 0.8988284745191364 grad: -0.26951782459157514
iteration: 0 loss: 67.62133236193604 grad: 183.36327291635888
iteration: 10 loss: 11.999652076879432 grad: -0.18562503714767936
iteration: 20 loss: 6.856125541040083 grad: -1.0776435393176982
iteration: 30 loss: 4.791128698515464 grad: -0.9652805105882183
iteration: 40 loss: 3.679494594577467 grad: -0.8224716850452274
iteration: 50 loss: 2.985710730119577 grad: -0.7075154178111421
iteration: 60 loss: 2.5117776543566337 grad: -0.618247118646047
iteration: 70 loss: 2.167617107311137 grad: -0.5481219726479447
iteration: 80 loss: 1.9064033790228594 grad: -0.4919617947665863
iteration: 90 loss: 1.7014012726205527 grad: -0.4461194635675383
iteration: 100 loss: 1.536241754806931 grad: -0.4080537184776055
iteration: 110 loss: 1.4003447381008878 grad: -0.37596961181970723
iteration: 120 loss: 1.2865692773891004 grad: -0.3485740974544347
iteration: 130 loss: 1.1899219045474803 grad: -0.3249164951141728
iteration: 140 loss: 1.1068061954128594 grad: -0.30428420246302323
iteration: 150 loss: 1.034566343762146 grad: -0.2861335183485754
iteration: 160 loss: 0.9711986372462889 grad: -0.2700428900081317
iteration: 170 loss: 0.9151629608534843 grad: -0.25568069566672347
iteration: 0 loss: 66.27607482828601 grad: 249.54281941530644
iteration: 10 loss: 11.930066428890479 grad: -1.836283596341484
iteration: 20 loss: 6.82244248203543 grad: -1.6653789213241885
iteration: 30 loss: 4.761737448236958 grad: -1.3122736979392406
iteration: 40 loss: 3.6526721192832485 grad: -1.0649563002518578
iteration: 50 loss: 2.961143242964805 grad: -0.8919465719423008
iteration: 60 loss: 2.489210389736441 grad: -0.765977839573626
iteration: 70 loss: 2.146803120933989 grad: -0.6706914512836314
iteration: 80 loss: 1.8871195423536269 grad: -0.5962846562719557
iteration: 90 loss: 1.6834544550557355 grad: -0.5366509597739519
iteration: 100 loss: 1.519467879342673 grad: -0.4878239823643955
iteration: 110 loss: 1.384605236959115 grad: -0.44712791422953146
iteration: 120 loss: 1.2717470786531273 grad: -0.41269704488002756
iteration: 130 loss: 1.1759177458501322 grad: -0.383192929751558
iteration: 140 loss: 1.0935354885437762 grad: -0.35763144523726387
iteration: 150 loss: 1.0219565941214237 grad: -0.3352733363346525
iteration: 160 loss: 0.9591873492359727 grad: -0.31555279113452445
iteration: 170 loss: 0.9036959454154136 grad: -0.29802954471122267
iteration: 0 loss: 60.82997256543612 grad: 219.00906216394395
iteration: 10 loss: 11.753008714695026 grad: -0.6706490032641459
iteration: 20 loss: 6.739472802077754 grad: -1.20340970395115
iteration: 30 loss: 4.710327957971045 grad: -1.040132217750553
iteration: 40 loss: 3.6163318499490575 grad: -0.8775832155558054
iteration: 50 loss: 2.9334004852919198 grad: -0.7515316731397864
iteration: 60 loss: 2.4669418193816868 grad: -0.6549650504815627
iteration: 70 loss: 2.128290429568551 grad: -0.5796171565577329
iteration: 80 loss: 1.8713269614640509 grad: -0.5195160968667688
iteration: 90 loss: 1.669714052154549 grad: -0.4705883314653756
iteration: 100 loss: 1.5073259884987402 grad: -0.43003958669237563
iteration: 110 loss: 1.3737407902337757 grad: -0.395913967803232
iteration: 120 loss: 1.2619251396780866 grad: -0.3668106227552914
iteration: 130 loss: 1.1669615908595832 grad: -0.34170371616775486
iteration: 140 loss: 1.0853090602721005 grad: -0.31982654925095433
iteration: 150 loss: 1.014353050958355 grad: -0.30059544491715073
iteration: 160 loss: 0.9521213485481481 grad: -0.2835586499617886
iteration: 170 loss: 0.8970982975703753 grad: -0.26836129327694747
iteration: 0 loss: 59.32214795055374 grad: 207.73414303506306
iteration: 10 loss: 11.449815347960008 grad: 0.41523627275495156
iteration: 20 loss: 6.6008401061096516 grad: -1.045756599672076
iteration: 30 loss: 4.63228945771082 grad: -1.0053256847058147
iteration: 40 loss: 3.5656629382588108 grad: -0.8723131136973683
iteration: 50 loss: 2.8973483618965474 grad: -0.7553259533823121
iteration: 60 loss: 2.439644322437785 grad: -0.6617027536414133
iteration: 70 loss: 2.1066814886460388 grad: -0.5871412356500645
iteration: 80 loss: 1.8536443071717175 grad: -0.5270030026684522
iteration: 90 loss: 1.6548698561329152 grad: -0.4777219153940558
iteration: 100 loss: 1.4946102759711886 grad: -0.4367119770025545
iteration: 110 loss: 1.3626690543030484 grad: -0.4021060439956922
iteration: 120 loss: 1.252154452698216 grad: -0.37254087928001467
iteration: 130 loss: 1.1582417492871337 grad: -0.34700539814776526
iteration: 140 loss: 1.077452769888127 grad: -0.32473721334525424
iteration: 150 loss: 1.0072169993322033 grad: -0.305152228409554
iteration: 160 loss: 0.9455937767568023 grad: -0.2877962397565427
iteration: 170 loss: 0.8910905978853159 grad: -0.2723111888476573
iteration: 0 loss: 65.51736626745195 grad: 209.39538198075724
iteration: 10 loss: 11.809583923381854 grad: -1.0571550851720648
iteration: 20 loss: 6.765183633964088 grad: -1.3922269305083108
iteration: 30 loss: 4.729463167940181 grad: -1.1481151187827572
iteration: 40 loss: 3.6323427380646702 grad: -0.9494624062820525
iteration: 50 loss: 2.947350331005716 grad: -0.8040270334834074
iteration: 60 loss: 2.4793533358802877 grad: -0.6956486444776315
iteration: 70 loss: 2.139487368401743 grad: -0.6124596535136049
iteration: 80 loss: 1.881533469054251 grad: -0.5468274218060197
iteration: 90 loss: 1.6790949361538465 grad: -0.49381778201410753
iteration: 100 loss: 1.5160070735492726 grad: -0.4501499574716121
iteration: 110 loss: 1.3818207273566259 grad: -0.4135740621420012
iteration: 120 loss: 1.2694828720492213 grad: -0.3825019722818318
iteration: 130 loss: 1.1740613478364552 grad: -0.35578336211911954
iteration: 140 loss: 1.0920038338467748 grad: -0.3325658089319489
iteration: 150 loss: 1.0206870896645341 grad: -0.31220483275265787
iteration: 160 loss: 0.958131989863556 grad: -0.2942044519654081
iteration: 170 loss: 0.9028173412543352 grad: -0.278176915777138
iteration: 0 loss: 62.03995468187777 grad: 169.22627902276065
iteration: 10 loss: 11.71480766294239 grad: 1.6088619792152012
iteration: 20 loss: 6.706912413658798 grad: -0.6845442266638284
iteration: 30 loss: 4.6973236355829195 grad: -0.8142870492093341
iteration: 40 loss: 3.612865581839742 grad: -0.7469798572072619
iteration: 50 loss: 2.9346712333129044 grad: -0.6637208903096365
iteration: 60 loss: 2.4706739767856645 grad: -0.5902899255182468
iteration: 70 loss: 2.1333307669273878 grad: -0.5290348606874211
iteration: 80 loss: 1.8770546621212403 grad: -0.47826051065151753
iteration: 90 loss: 1.6757790077009636 grad: -0.4358915319361827
iteration: 100 loss: 1.5135240675406914 grad: -0.40017301149557316
iteration: 110 loss: 1.3799504259833488 grad: -0.36973557375893606
iteration: 120 loss: 1.2680732842279048 grad: -0.3435312215169368
iteration: 130 loss: 1.1730045848769144 grad: -0.32075763968282595
iteration: 140 loss: 1.0912214677081133 grad: -0.3007958611211441
iteration: 150 loss: 1.020120932598688 grad: -0.28316337938311986
iteration: 160 loss: 0.9577381010954841 grad: -0.26747984514924716
iteration: 170 loss: 0.9025620225988032 grad: -0.253442103267138
iteration: 0 loss: 64.23903837479156 grad: 217.6631379119508
iteration: 10 loss: 11.737537570799304 grad: -1.2270261312856507
iteration: 20 loss: 6.722132826640848 grad: -1.4063181516966266
iteration: 30 loss: 4.695468198572309 grad: -1.1531808904952192
iteration: 40 loss: 3.6038288006962333 grad: -0.953190305962357
iteration: 50 loss: 2.922722197384577 grad: -0.8073528825999234
iteration: 60 loss: 2.4576599640300545 grad: -0.6987488648132906
iteration: 70 loss: 2.120096096282189 grad: -0.6153807459334065
iteration: 80 loss: 1.8639980510777356 grad: -0.5495887958605833
iteration: 90 loss: 1.6630879192080514 grad: -0.4964325955191271
iteration: 100 loss: 1.5012808408366503 grad: -0.45262964171954767
iteration: 110 loss: 1.3681834388030025 grad: -0.4159292175594988
iteration: 120 loss: 1.2567828682589275 grad: -0.3847424684353795
iteration: 130 loss: 1.162176657568869 grad: -0.35791830826013804
iteration: 140 loss: 1.0808348981158402 grad: -0.33460353962263245
iteration: 150 loss: 1.0101515860175658 grad: -0.31415292636463965
iteration: 160 loss: 0.9481610688340383 grad: -0.29606977304030696
iteration: 170 loss: 0.8933528365966843 grad: -0.2799656701789165
iteration: 0 loss: 65.28790167199048 grad: 196.26686887017968
iteration: 10 loss: 11.922089163196729 grad: -0.07818059481190825
iteration: 20 loss: 6.8235216707952615 grad: -0.9929828679207842
iteration: 30 loss: 4.771719726405123 grad: -0.9249414044210741
iteration: 40 loss: 3.6660663925391335 grad: -0.8007168910496159
iteration: 50 loss: 2.9755764307735095 grad: -0.6946593206558959
iteration: 60 loss: 2.5036851002690295 grad: -0.6101603057368439
iteration: 70 loss: 2.1608967810949995 grad: -0.5428074952266393
iteration: 80 loss: 1.9006613814635371 grad: -0.48835821197515833
iteration: 90 loss: 1.6963887946849767 grad: -0.4436207884186807
iteration: 100 loss: 1.5317927385105412 grad: -0.4062942276920625
iteration: 110 loss: 1.396343375581058 grad: -0.3747186001516435
iteration: 120 loss: 1.2829318560782832 grad: -0.3476807028437564
iteration: 130 loss: 1.186586082682838 grad: -0.3242790925711442
iteration: 140 loss: 1.1037243662956577 grad: -0.303832612975378
iteration: 150 loss: 1.0317013662878376 grad: -0.285818251467025
iteration: 160 loss: 0.9685209955755454 grad: -0.2698284170502475
iteration: 170 loss: 0.912648819446531 grad: -0.25554111961465553
iteration: 0 loss: 65.53000398359822 grad: 207.8581550363989
iteration: 10 loss: 11.610338265797653 grad: -1.4302768246560087
iteration: 20 loss: 6.689556062050625 grad: -1.5769213891711553
iteration: 30 loss: 4.685032707208723 grad: -1.2790275695955666
iteration: 40 loss: 3.600821918542765 grad: -1.0494215340190638
iteration: 50 loss: 2.9227261280897108 grad: -0.8839634301168532
iteration: 60 loss: 2.459012791462845 grad: -0.761763731890369
iteration: 70 loss: 2.122077191445359 grad: -0.6685532763248293
iteration: 80 loss: 1.8662642037064954 grad: -0.595367737182619
iteration: 90 loss: 1.6654651937348965 grad: -0.5364857428738579
iteration: 100 loss: 1.5036776665428988 grad: -0.4881358406722651
iteration: 110 loss: 1.3705508198502436 grad: -0.44774836525690376
iteration: 120 loss: 1.2590948270787357 grad: -0.41351879894656923
iteration: 130 loss: 1.164420071669445 grad: -0.3841456237060199
iteration: 140 loss: 1.083003979289639 grad: -0.35866784540398006
iteration: 150 loss: 1.012244773906125 grad: -0.33636115074184425
iteration: 160 loss: 0.9501792340194118 grad: -0.3166696328522767
iteration: 170 loss: 0.895298219180652 grad: -0.29915973758915626
iteration: 0 loss: 66.33013958308871 grad: 273.21682717854225
iteration: 10 loss: 11.654909789847496 grad: -2.617470237016754
iteration: 20 loss: 6.701909544523047 grad: -1.9500725101153797
iteration: 30 loss: 4.685738066978502 grad: -1.4777753852672064
iteration: 40 loss: 3.5971149793767836 grad: -1.178679658450426
iteration: 50 loss: 2.9172306214525388 grad: -0.9772573832782582
iteration: 60 loss: 2.452814751674065 grad: -0.8335493279592332
iteration: 70 loss: 2.1156688612031758 grad: -0.7262341539506034
iteration: 80 loss: 1.8598806064557143 grad: -0.6431876142037725
iteration: 90 loss: 1.6592203357976856 grad: -0.5770786985018472
iteration: 100 loss: 1.4976251908074119 grad: -0.5232369682516134
iteration: 110 loss: 1.364713055775362 grad: -0.4785545705544551
iteration: 120 loss: 1.2534775025116935 grad: -0.44088679280481746
iteration: 130 loss: 1.15902008709149 grad: -0.4087074458955051
iteration: 140 loss: 1.0778136473873432 grad: -0.3809017050756826
iteration: 150 loss: 1.0072541816908929 grad: -0.3566366918204551
iteration: 160 loss: 0.9453775738631355 grad: -0.33527787828774946
iteration: 170 loss: 0.8906745384600651 grad: -0.31633349433488944
iteration: 0 loss: 67.05975127089404 grad: 180.94557133451045
iteration: 10 loss: 12.005710322594682 grad: 0.0381944211922072
iteration: 20 loss: 6.858703229408878 grad: -1.0417549175610743
iteration: 30 loss: 4.7951819407180665 grad: -0.9577816008366538
iteration: 40 loss: 3.684116154650755 grad: -0.8236847960749194
iteration: 50 loss: 2.9903855163986672 grad: -0.7119582977422643
iteration: 60 loss: 2.5162912885167232 grad: -0.6239014316349696
iteration: 70 loss: 2.1718916437007465 grad: -0.5541399643046311
iteration: 80 loss: 1.910419834669979 grad: -0.4979638257901234
iteration: 90 loss: 1.7051656647303057 grad: -0.4519332672490656
iteration: 100 loss: 1.53977006111001 grad: -0.41360507996939655
iteration: 110 loss: 1.4036561433066377 grad: -0.3812325127536509
iteration: 120 loss: 1.2896831529842943 grad: -0.353546568707102
iteration: 130 loss: 1.1928565146002421 grad: -0.3296084079723687
iteration: 140 loss: 1.109578212194554 grad: -0.3087109157952016
iteration: 150 loss: 1.0371907421989073 grad: -0.29031252225523596
iteration: 160 loss: 0.9736887609420095 grad: -0.27399203290060575
iteration: 170 loss: 0.9175306588511127 grad: -0.25941726781891994
iteration: 0 loss: 65.14577396817543 grad: 174.94809752909333
iteration: 10 loss: 11.776512594433763 grad: -0.10762529431274453
iteration: 20 loss: 6.741945043266779 grad: -1.0160905625283625
iteration: 30 loss: 4.715629829465797 grad: -0.9363308472999263
iteration: 40 loss: 3.623553837016557 grad: -0.8087398620234741
iteration: 50 loss: 2.941434894630881 grad: -0.7011736970817544
iteration: 60 loss: 2.4751977327503236 grad: -0.6157541232691168
iteration: 70 loss: 2.1364751877618717 grad: -0.5477438539050921
iteration: 80 loss: 1.879299758166367 grad: -0.4927867910686387
iteration: 90 loss: 1.6774113577863234 grad: -0.44763995796327677
iteration: 100 loss: 1.5147241783178371 grad: -0.4099743001908293
iteration: 110 loss: 1.3808369939119158 grad: -0.37811246806464366
iteration: 120 loss: 1.2687271619540959 grad: -0.3508295144188809
iteration: 130 loss: 1.173482486974494 grad: -0.3272156358738677
iteration: 140 loss: 1.0915641742702642 grad: -0.3065834968126151
iteration: 150 loss: 1.0203583925705806 grad: -0.2884053900820006
iteration: 160 loss: 0.9578927135404146 grad: -0.272270081688344
iteration: 170 loss: 0.9026508075672113 grad: -0.2578527058136754
iteration: 0 loss: 64.68310863743848 grad: 185.68090462726155
iteration: 10 loss: 11.903584359818483 grad: 0.3479993374311076
iteration: 20 loss: 6.7981610915856185 grad: -0.7729937167529121
iteration: 30 loss: 4.7513551399674725 grad: -0.7918509589158338
iteration: 40 loss: 3.6498170965110974 grad: -0.7091670748481801
iteration: 50 loss: 2.962260973912883 grad: -0.6262934411730237
iteration: 60 loss: 2.492487510798624 grad: -0.5562536381662777
iteration: 70 loss: 2.151276074769281 grad: -0.49865038099644554
iteration: 80 loss: 1.892250777984414 grad: -0.45116039607127806
iteration: 90 loss: 1.6889315663986302 grad: -0.411609657684104
iteration: 100 loss: 1.525103571370809 grad: -0.37828117930542204
iteration: 110 loss: 1.3902847817496629 grad: -0.34987170074800483
iteration: 120 loss: 1.2773993587196524 grad: -0.32539702505302714
iteration: 130 loss: 1.1814986206420806 grad: -0.30410891120194655
iteration: 140 loss: 1.0990178956469043 grad: -0.2854322668175802
iteration: 150 loss: 1.0273244785169646 grad: -0.2689196702093116
iteration: 160 loss: 0.9644318440232382 grad: -0.2542187591070038
iteration: 170 loss: 0.9088129165822714 grad: -0.24104877273451375
iteration: 0 loss: 67.0956065638784 grad: 210.72960046030738
iteration: 10 loss: 11.561642008737822 grad: -1.6404530890831315
iteration: 20 loss: 6.669023826544617 grad: -1.723705223651572
iteration: 30 loss: 4.6744338579576175 grad: -1.3619969108815169
iteration: 40 loss: 3.5943553066872482 grad: -1.102756606088255
iteration: 50 loss: 2.9183379594816703 grad: -0.9215757810895946
iteration: 60 loss: 2.4558158609777574 grad: -0.7900187318085958
iteration: 70 loss: 2.119628585948786 grad: -0.6907527544996976
iteration: 80 loss: 1.8643182938990734 grad: -0.6133985738203839
iteration: 90 loss: 1.6638745833026654 grad: -0.5515086748165621
iteration: 100 loss: 1.5023483470408379 grad: -0.5009069524493287
iteration: 110 loss: 1.3694198616711384 grad: -0.4587830822235663
iteration: 120 loss: 1.2581184192152415 grad: -0.4231817205546624
iteration: 130 loss: 1.1635667083404853 grad: -0.39270267213462556
iteration: 140 loss: 1.0822503789035678 grad: -0.366318026382731
iteration: 150 loss: 1.0115733295056502 grad: -0.3432566930215269
iteration: 160 loss: 0.9495763623998424 grad: -0.32292920446952283
iteration: 170 loss: 0.8947532584387029 grad: -0.30487737906931406
iteration: 0 loss: 62.83604709265499 grad: 218.50622485675072
iteration: 10 loss: 11.82221676356159 grad: -0.6712431899453124
iteration: 20 loss: 6.747933188328633 grad: -1.2329563220850206
iteration: 30 loss: 4.7079531449007845 grad: -1.0670384752305038
iteration: 40 loss: 3.6114134109532396 grad: -0.8982066262541742
iteration: 50 loss: 2.927980862659545 grad: -0.7676835332534461
iteration: 60 loss: 2.461629493651468 grad: -0.6680758478268329
iteration: 70 loss: 2.123271759507051 grad: -0.5905865130216257
iteration: 80 loss: 1.8666459005749774 grad: -0.5289148086059394
iteration: 90 loss: 1.665363779289609 grad: -0.47879320754331733
iteration: 100 loss: 1.5032822642915975 grad: -0.43730971202258834
iteration: 110 loss: 1.369974825941385 grad: -0.4024340392986937
iteration: 120 loss: 1.2584086146841649 grad: -0.37271645244373547
iteration: 130 loss: 1.1636684902315542 grad: -0.34709784294617096
iteration: 140 loss: 1.0822161900977831 grad: -0.3247881587291937
iteration: 150 loss: 1.01144000919885 grad: -0.30518688792192117
iteration: 160 loss: 0.9493702874717035 grad: -0.2878298555008116
iteration: 170 loss: 0.8944936244195312 grad: -0.2723528357680819
iteration: 0 loss: 67.20524422871053 grad: 239.60077943269164
iteration: 10 loss: 11.749251218483757 grad: -1.802481391877958
iteration: 20 loss: 6.732845267975873 grad: -1.6826443884998032
iteration: 30 loss: 4.705833244062763 grad: -1.3316825011395237
iteration: 40 loss: 3.6131680411701628 grad: -1.0818364516318058
iteration: 50 loss: 2.931031070229024 grad: -0.906282704428605
iteration: 60 loss: 2.4650765891340765 grad: -0.7782372773520219
iteration: 70 loss: 2.1267654561897045 grad: -0.6813032661072143
iteration: 80 loss: 1.8700436874312139 grad: -0.6055833417408706
iteration: 90 loss: 1.6686097663273358 grad: -0.5448898782942344
iteration: 100 loss: 1.5063588191774628 grad: -0.4951954924705054
iteration: 110 loss: 1.3728816252331937 grad: -0.45377974986974084
iteration: 120 loss: 1.2611530360105379 grad: -0.41874427167486117
iteration: 130 loss: 1.1662610401891802 grad: -0.3887264516902673
iteration: 140 loss: 1.0846682910640661 grad: -0.36272405555051856
iteration: 150 loss: 1.013762935580334 grad: -0.3399840607054416
iteration: 160 loss: 0.9515746772211225 grad: -0.3199300445202368
iteration: 170 loss: 0.8965892824598168 grad: -0.3021134572573966
iteration: 0 loss: 64.358928853341 grad: 240.6402135697423
iteration: 10 loss: 11.447502747392242 grad: -2.171672278967148
iteration: 20 loss: 6.620855106684096 grad: -1.7971932205353685
iteration: 30 loss: 4.636603456830678 grad: -1.3980225123450505
iteration: 40 loss: 3.5614071763690265 grad: -1.1290665326588059
iteration: 50 loss: 2.888848815548003 grad: -0.9430130567880666
iteration: 60 loss: 2.4290747511250546 grad: -0.8082157877757795
iteration: 70 loss: 2.0951592110856256 grad: -0.706545436141321
iteration: 80 loss: 1.8417652425003077 grad: -0.6273147856366235
iteration: 90 loss: 1.6429606072449794 grad: -0.5639167104011953
iteration: 100 loss: 1.4828519362804813 grad: -0.5120776263607794
iteration: 110 loss: 1.3511610999974188 grad: -0.46892215510392515
iteration: 120 loss: 1.2409492392789203 grad: -0.432449231993174
iteration: 130 loss: 1.1473638704824225 grad: -0.4012254158499118
iteration: 140 loss: 1.0669102764786245 grad: -0.3741980105994731
iteration: 150 loss: 0.9970080731147225 grad: -0.35057707778660363
iteration: 160 loss: 0.9357107838090997 grad: -0.3297585767124017
iteration: 170 loss: 0.8815225885573454 grad: -0.31127289492893595
iteration: 0 loss: 64.61901652663342 grad: 223.50566353980085
iteration: 10 loss: 11.44822908942737 grad: -1.7480476967612442
iteration: 20 loss: 6.592982580213009 grad: -1.6607243082060634
iteration: 30 loss: 4.613642257447943 grad: -1.3210355286740914
iteration: 40 loss: 3.543669665007573 grad: -1.0769304771170172
iteration: 50 loss: 2.8749383928882044 grad: -0.904176156563151
iteration: 60 loss: 2.4179001770332267 grad: -0.7776026714316244
iteration: 70 loss: 2.0859775777806715 grad: -0.6814916383457319
iteration: 80 loss: 1.834072956518908 grad: -0.6062500416607755
iteration: 90 loss: 1.6364090500845287 grad: -0.5458397996713678
iteration: 100 loss: 1.477193506929573 grad: -0.49631277351760644
iteration: 110 loss: 1.3462155147387256 grad: -0.4549930798574532
iteration: 120 loss: 1.2365822174039998 grad: -0.4200085844671043
iteration: 130 loss: 1.1434733471758343 grad: -0.39001273122957636
iteration: 140 loss: 1.0634172636691388 grad: -0.36401340334571963
iteration: 150 loss: 0.993850455569786 grad: -0.34126411529536077
iteration: 160 loss: 0.9328390133146058 grad: -0.3211927533166195
iteration: 170 loss: 0.8788966340426172 grad: -0.3033536535732444
iteration: 0 loss: 63.728712144742005 grad: 208.9096793715485
iteration: 10 loss: 11.56286957258048 grad: -0.8664411300604111
iteration: 20 loss: 6.637913272943686 grad: -1.3795701321912546
iteration: 30 loss: 4.6446602787908144 grad: -1.1625858651947436
iteration: 40 loss: 3.568610467366646 grad: -0.9675057127836242
iteration: 50 loss: 2.8961799122722285 grad: -0.8211649860411792
iteration: 60 loss: 2.4365375684554818 grad: -0.7110789126384062
iteration: 70 loss: 2.1026399894918457 grad: -0.6262112161101954
iteration: 80 loss: 1.8491691283101401 grad: -0.5591082502727887
iteration: 90 loss: 1.6502251466103273 grad: -0.5048490513138854
iteration: 100 loss: 1.4899400864868022 grad: -0.4601256187386508
iteration: 110 loss: 1.3580531184577673 grad: -0.42265505609715653
iteration: 120 loss: 1.2476365992368466 grad: -0.39081969253993154
iteration: 130 loss: 1.1538451390150923 grad: -0.3634448932921761
iteration: 140 loss: 1.0731884129066367 grad: -0.3396587997173074
iteration: 150 loss: 1.0030886779158292 grad: -0.3188015153822482
iteration: 160 loss: 0.9416009678064422 grad: -0.30036481887869
iteration: 170 loss: 0.8872302444266841 grad: -0.28395119266255214
iteration: 0 loss: 66.85541477077236 grad: 246.8579343562557
iteration: 10 loss: 11.636930545003679 grad: -2.5681905158267417
iteration: 20 loss: 6.702426920654034 grad: -1.9298898173116559
iteration: 30 loss: 4.688448615641019 grad: -1.4629295127886213
iteration: 40 loss: 3.5998508462143946 grad: -1.1687460830638838
iteration: 50 loss: 2.919707136438443 grad: -0.9701521264761359
iteration: 60 loss: 2.455029751986 grad: -0.8281209273977769
iteration: 70 loss: 2.1176607877803026 grad: -0.7218908333466969
iteration: 80 loss: 1.8616873706813417 grad: -0.6395984529287687
iteration: 90 loss: 1.6608730228563218 grad: -0.5740418748672956
iteration: 100 loss: 1.499148360953396 grad: -0.5206206744995476
iteration: 110 loss: 1.366126078428682 grad: -0.4762681036798624
iteration: 120 loss: 1.254795789303113 grad: -0.4388651549509961
iteration: 130 loss: 1.1602560486454339 grad: -0.4069025370941557
iteration: 140 loss: 1.0789774030477846 grad: -0.3792769873833227
iteration: 150 loss: 1.0083540816221186 grad: -0.3551638014257773
iteration: 160 loss: 0.9464205850292522 grad: -0.33393438975852824
iteration: 170 loss: 0.8916665328758675 grad: -0.31510139679477545
iteration: 0 loss: 61.07531130016297 grad: 220.8785458311604
iteration: 10 loss: 11.813812414639456 grad: -0.32847262476470795
iteration: 20 loss: 6.762272063517437 grad: -1.0427403650499565
iteration: 30 loss: 4.724173612081441 grad: -0.9464233434626255
iteration: 40 loss: 3.62649894412793 grad: -0.8147615859105773
iteration: 50 loss: 2.941570277089051 grad: -0.7057754763112073
iteration: 60 loss: 2.4738421031266955 grad: -0.6197370471583872
iteration: 70 loss: 2.134301354566064 grad: -0.5513994406288336
iteration: 80 loss: 1.8766735497118712 grad: -0.49623550039369135
iteration: 90 loss: 1.6745417021470943 grad: -0.4509370248730051
iteration: 100 loss: 1.5117348381058533 grad: -0.4131477353467127
iteration: 110 loss: 1.3778031924117717 grad: -0.3811781060962438
iteration: 120 loss: 1.2656953451698676 grad: -0.3537974205225117
iteration: 130 loss: 1.1704814889843957 grad: -0.33009298746127647
iteration: 140 loss: 1.0886117734106102 grad: -0.3093759361635902
iteration: 150 loss: 1.01746528791372 grad: -0.29111768139371436
iteration: 160 loss: 0.9550650034830555 grad: -0.27490643977733786
iteration: 170 loss: 0.899891570745281 grad: -0.26041696541085035
iteration: 0 loss: 66.89500365391888 grad: 206.71786849833055
iteration: 10 loss: 11.393035615345712 grad: -1.837573803207682
iteration: 20 loss: 6.582220736775567 grad: -1.705987027009505
iteration: 30 loss: 4.61513518481388 grad: -1.3389163935723334
iteration: 40 loss: 3.549379958654423 grad: -1.0840731485606183
iteration: 50 loss: 2.882184938283031 grad: -0.9067187260588698
iteration: 60 loss: 2.425636433387932 grad: -0.7779533672836838
iteration: 70 loss: 2.0937555761326454 grad: -0.6807198034774053
iteration: 80 loss: 1.8416929498026464 grad: -0.6048807168039931
iteration: 90 loss: 1.6437834172939718 grad: -0.5441505996702691
iteration: 100 loss: 1.4842882211458923 grad: -0.494458570160312
iteration: 110 loss: 1.3530229933224043 grad: -0.45306360601752993
iteration: 120 loss: 1.2431080036201985 grad: -0.41805714222050716
iteration: 130 loss: 1.1497293087205767 grad: -0.38807139260728707
iteration: 140 loss: 1.069418026967593 grad: -0.3621014635331162
iteration: 150 loss: 0.9996115138804437 grad: -0.3393929860188314
iteration: 160 loss: 0.938375728149946 grad: -0.3193688909311424
iteration: 170 loss: 0.8842237241880287 grad: -0.3015803599103513
iteration: 0 loss: 63.504108056007006 grad: 206.27560878370252
iteration: 10 loss: 11.713601122027264 grad: -0.7686433150534933
iteration: 20 loss: 6.718523461410396 grad: -1.2258583190462498
iteration: 30 loss: 4.698960996170792 grad: -1.0483743068834492
iteration: 40 loss: 3.6097009909270295 grad: -0.8813491148555991
iteration: 50 loss: 2.9293664064346725 grad: -0.7533539566017176
iteration: 60 loss: 2.4644506245384203 grad: -0.6557906161900928
iteration: 70 loss: 2.126772556127228 grad: -0.5798744585888793
iteration: 80 loss: 1.8704514941733494 grad: -0.5194251812333581
iteration: 90 loss: 1.669277486240348 grad: -0.4702731820167534
iteration: 100 loss: 1.507197370858607 grad: -0.4295750924796137
iteration: 110 loss: 1.3738327014729377 grad: -0.39534782687988634
iteration: 120 loss: 1.262177433158947 grad: -0.3661744875959415
iteration: 130 loss: 1.1673318318584363 grad: -0.3410192786309122
iteration: 140 loss: 1.085766672163779 grad: -0.31910905921594346
iteration: 150 loss: 1.0148756051528174 grad: -0.2998558436141279
iteration: 160 loss: 0.9526921421088446 grad: -0.28280491542793873
iteration: 170 loss: 0.8977047301573293 grad: -0.2675993175452593
iteration: 0 loss: 61.54136426675909 grad: 202.46626249114425
iteration: 10 loss: 11.617897692177518 grad: -0.16248449515594987
iteration: 20 loss: 6.673691151497348 grad: -1.1881005957555424
iteration: 30 loss: 4.670710322312144 grad: -1.0744463397676771
iteration: 40 loss: 3.5887937201099374 grad: -0.9152464006614396
iteration: 50 loss: 2.9125209868917326 grad: -0.7855173218184713
iteration: 60 loss: 2.450199624996003 grad: -0.6845763814101997
iteration: 70 loss: 2.1143420636547656 grad: -0.6053517513416681
iteration: 80 loss: 1.8593823516863184 grad: -0.5420215301102913
iteration: 90 loss: 1.6592729644479816 grad: -0.4904379312364475
iteration: 100 loss: 1.4980531471820873 grad: -0.44769924271129813
iteration: 110 loss: 1.3654011387062714 grad: -0.41175368534654555
iteration: 120 loss: 1.254347786738022 grad: -0.3811235704476991
iteration: 130 loss: 1.1600185289400378 grad: -0.3547234662482433
iteration: 140 loss: 1.0789019838858125 grad: -0.33174079303054893
iteration: 150 loss: 1.0084048557187089 grad: -0.31155645702799484
iteration: 160 loss: 0.9465704572153713 grad: -0.2936911730151865
iteration: 170 loss: 0.8918947595114084 grad: -0.27776847700695173
iteration: 0 loss: 65.41141135316438 grad: 248.3060517061161
iteration: 10 loss: 11.867638081139313 grad: -2.1675525717745314
iteration: 20 loss: 6.8173655429946765 grad: -1.809453181850175
iteration: 30 loss: 4.759528408319966 grad: -1.4009106631451984
iteration: 40 loss: 3.649752519228514 grad: -1.1265481653094243
iteration: 50 loss: 2.957534280453935 grad: -0.9378032346036576
iteration: 60 loss: 2.485187403382828 grad: -0.8017640794129139
iteration: 70 loss: 2.142572339120513 grad: -0.6995930582907165
iteration: 80 loss: 1.8828123857021886 grad: -0.620242067715026
iteration: 90 loss: 1.6791514675649275 grad: -0.5569196925458096
iteration: 100 loss: 1.5152175800575274 grad: -0.50525593108027
iteration: 110 loss: 1.380436240851726 grad: -0.4623236832611324
iteration: 120 loss: 1.2676755835165585 grad: -0.4260936232591124
iteration: 130 loss: 1.1719521504344563 grad: -0.3951167445755678
iteration: 140 loss: 1.0896792703079385 grad: -0.3683318381549312
iteration: 150 loss: 1.0182101261682626 grad: -0.3449444504671478
iteration: 160 loss: 0.9555490621726842 grad: -0.3243483232278319
iteration: 170 loss: 0.9001630784487961 grad: -0.306072954303133
iteration: 0 loss: 64.37442586962365 grad: 204.56393402915816
iteration: 10 loss: 11.637096739744637 grad: -1.2057168841006674
iteration: 20 loss: 6.702799132773055 grad: -1.5121433224643366
iteration: 30 loss: 4.693930252407716 grad: -1.2270184485590045
iteration: 40 loss: 3.6075592720224203 grad: -1.0049349854826053
iteration: 50 loss: 2.9281498292493366 grad: -0.8450839941641107
iteration: 60 loss: 2.4635473441058324 grad: -0.7273953150978824
iteration: 70 loss: 2.1259697187828435 grad: -0.6378708538729783
iteration: 80 loss: 1.869671925355719 grad: -0.5677256908345593
iteration: 90 loss: 1.6684943520400255 grad: -0.5113778249014929
iteration: 100 loss: 1.5064033452230516 grad: -0.46516332022542156
iteration: 110 loss: 1.3730279791949547 grad: -0.4265945219296483
iteration: 120 loss: 1.2613648608618695 grad: -0.3939294066815544
iteration: 130 loss: 1.1665149038472835 grad: -0.3659143549337751
iteration: 140 loss: 1.084948729452249 grad: -0.34162542082901903
iteration: 150 loss: 1.0140595509207577 grad: -0.3203672495792318
iteration: 160 loss: 0.9518803780077344 grad: -0.3016068159686377
iteration: 170 loss: 0.8968991829540507 grad: -0.28492883883979414
iteration: 0 loss: 65.04885620929898 grad: 196.90999448944024
iteration: 10 loss: 11.89048260670031 grad: -0.34085453592510395
iteration: 20 loss: 6.807556556162539 grad: -1.1808338750791323
iteration: 30 loss: 4.759793273957547 grad: -1.0392609486259556
iteration: 40 loss: 3.6562327779174253 grad: -0.8801101601965738
iteration: 50 loss: 2.967140665272919 grad: -0.7547262065923279
iteration: 60 loss: 2.4962799195766987 grad: -0.6581941426957475
iteration: 70 loss: 2.1542910065678287 grad: -0.582704793359448
iteration: 80 loss: 1.894696214683005 grad: -0.5224163408866788
iteration: 90 loss: 1.6909493071189816 grad: -0.47329769461865845
iteration: 100 loss: 1.5267927001123924 grad: -0.4325688947712507
iteration: 110 loss: 1.391716300852336 grad: -0.3982782941747985
iteration: 120 loss: 1.2786253524038578 grad: -0.369025497249908
iteration: 130 loss: 1.1825581092089124 grad: -0.34378371699423976
iteration: 140 loss: 1.0999406662171665 grad: -0.3217848497749285
iteration: 150 loss: 1.0281336526400082 grad: -0.3024437498444895
iteration: 160 loss: 0.965145638133973 grad: -0.28530728517172593
iteration: 170 loss: 0.9094458745889824 grad: -0.270019348175655
iteration: 0 loss: 63.810946927306894 grad: 166.14134428793113
iteration: 10 loss: 11.54532879786296 grad: 0.820676054175456
iteration: 20 loss: 6.65953115784495 grad: -0.989954690922655
iteration: 30 loss: 4.674401893082301 grad: -0.9922896221927411
iteration: 40 loss: 3.5982755236603805 grad: -0.8688564306839992
iteration: 50 loss: 2.9238845566690297 grad: -0.7547559074921626
iteration: 60 loss: 2.4619874644168207 grad: -0.6622085140776578
iteration: 70 loss: 2.125966676079439 grad: -0.5881010353670969
iteration: 80 loss: 1.870604404797508 grad: -0.5281635877973837
iteration: 90 loss: 1.67000398071307 grad: -0.47896722644440104
iteration: 100 loss: 1.508273052831307 grad: -0.4379844373183671
iteration: 110 loss: 1.3751212251843918 grad: -0.40337563984859903
iteration: 120 loss: 1.2635932116858881 grad: -0.37379159816177965
iteration: 130 loss: 1.1688197865772632 grad: -0.34822874814208093
iteration: 140 loss: 1.087290700045039 grad: -0.3259289102208851
iteration: 150 loss: 1.0164117445084975 grad: -0.3063104089998794
iteration: 160 loss: 0.9542243842898146 grad: -0.2889204565864599
iteration: 170 loss: 0.8992223782405001 grad: -0.273401825038859
iteration: 0 loss: 62.13187651289441 grad: 230.49590387560897
iteration: 10 loss: 11.643118971331411 grad: -1.476794935568886
iteration: 20 loss: 6.68953922052829 grad: -1.5704982055800232
iteration: 30 loss: 4.677364648554596 grad: -1.2728897584744867
iteration: 40 loss: 3.591318103614164 grad: -1.0433489350491187
iteration: 50 loss: 2.9130223794966863 grad: -0.8779020713623441
iteration: 60 loss: 2.449633585301895 grad: -0.7559006945855848
iteration: 70 loss: 2.113186067090365 grad: -0.6629904935543331
iteration: 80 loss: 1.857893465156146 grad: -0.590137512318962
iteration: 90 loss: 1.657597573933303 grad: -0.5315849025871966
iteration: 100 loss: 1.4962781707310404 grad: -0.4835456609679384
iteration: 110 loss: 1.3635797796458229 grad: -0.4434444913804524
iteration: 120 loss: 1.2525132940082089 grad: -0.4094759212905339
iteration: 130 loss: 1.158191924907235 grad: -0.3803395675531043
iteration: 140 loss: 1.0770965873135454 grad: -0.35507640644472377
iteration: 150 loss: 1.0066290220844498 grad: -0.3329643306289527
iteration: 160 loss: 0.9448292880214963 grad: -0.31344959653277654
iteration: 170 loss: 0.8901911950636697 grad: -0.29610065106904015
iteration: 0 loss: 61.82362600673214 grad: 222.90619723857768
iteration: 10 loss: 11.66000437696909 grad: -1.0020818897633732
iteration: 20 loss: 6.708559383593183 grad: -1.4177115722249314
iteration: 30 loss: 4.694613852814985 grad: -1.1749898328960233
iteration: 40 loss: 3.6063695508109133 grad: -0.9706630606411057
iteration: 50 loss: 2.9262350615755497 grad: -0.8204123036562718
iteration: 60 loss: 2.461370022680967 grad: -0.708532545843118
iteration: 70 loss: 2.1237302311434507 grad: -0.6228185963609234
iteration: 80 loss: 1.8674605492824412 grad: -0.5553307922525736
iteration: 90 loss: 1.6663515834533096 grad: -0.5009249134489115
iteration: 100 loss: 1.5043459662823475 grad: -0.45618231161357203
iteration: 110 loss: 1.3710611132296022 grad: -0.41876196857247205
iteration: 120 loss: 1.2594878467798893 grad: -0.3870143043885934
iteration: 130 loss: 1.1647242717831845 grad: -0.35974658578256496
iteration: 140 loss: 1.083239747755463 grad: -0.33607645757828486
iteration: 150 loss: 1.0124270407123648 grad: -0.31533786147664056
iteration: 160 loss: 0.950319140609529 grad: -0.29701895632352837
iteration: 170 loss: 0.8954042181448213 grad: -0.2807201285709408
iteration: 0 loss: 62.378262878941364 grad: 215.81364384142282
iteration: 10 loss: 11.573181523797365 grad: -0.5880277261849876
iteration: 20 loss: 6.651133816513638 grad: -1.254719483826089
iteration: 30 loss: 4.65700693963167 grad: -1.0944708646167576
iteration: 40 loss: 3.5797494090318405 grad: -0.9246290267040362
iteration: 50 loss: 2.906199803429114 grad: -0.7915783930887528
iteration: 60 loss: 2.4455945367142458 grad: -0.689330230272458
iteration: 70 loss: 2.1108820124802814 grad: -0.6094723457235027
iteration: 80 loss: 1.8567210340064444 grad: -0.5457698358771668
iteration: 90 loss: 1.6571888708227893 grad: -0.4939285885132187
iteration: 100 loss: 1.4963983427965517 grad: -0.4509892565166165
iteration: 110 loss: 1.3640732534022804 grad: -0.41487527894141196
iteration: 120 loss: 1.2532738474662273 grad: -0.3840974161459936
iteration: 130 loss: 1.159145194755059 grad: -0.3575643894186888
iteration: 140 loss: 1.0781894163147543 grad: -0.33446038114705995
iteration: 150 loss: 1.0078227416761778 grad: -0.314164321775328
iteration: 160 loss: 0.9460953534598957 grad: -0.296195597372423
iteration: 170 loss: 0.8915082744618469 grad: -0.2801767692744346
iteration: 0 loss: 61.05109400806116 grad: 230.98107963837188
iteration: 10 loss: 11.622544711439456 grad: -0.3857771863626527
iteration: 20 loss: 6.671567432181746 grad: -1.2785861872944162
iteration: 30 loss: 4.667519734059831 grad: -1.1253416304459463
iteration: 40 loss: 3.58592480237307 grad: -0.950479185371952
iteration: 50 loss: 2.9101326834823094 grad: -0.8126427421939186
iteration: 60 loss: 2.4482322531855307 grad: -0.7067503354224138
iteration: 70 loss: 2.1127116608372902 grad: -0.624162429325311
iteration: 80 loss: 1.858017130968082 grad: -0.5583805957240431
iteration: 90 loss: 1.6581171349630082 grad: -0.5049204792512207
iteration: 100 loss: 1.49706429885807 grad: -0.4606938787706485
iteration: 110 loss: 1.3645470220492164 grad: -0.4235368145380596
iteration: 120 loss: 1.2536036724118689 grad: -0.3918996550534276
iteration: 130 loss: 1.1593652406602153 grad: -0.3646484758337034
iteration: 140 loss: 1.0783244733271535 grad: -0.34093668184754183
iteration: 150 loss: 1.0078911747278985 grad: -0.3201205349650589
iteration: 160 loss: 0.946111012804026 grad: -0.3017024121610157
iteration: 170 loss: 0.891481768861757 grad: -0.28529189593568405
iteration: 0 loss: 65.29968659857259 grad: 204.2970959195348
iteration: 10 loss: 11.566846105788768 grad: -0.9481618523195192
iteration: 20 loss: 6.655283866614851 grad: -1.4168676922380603
iteration: 30 loss: 4.662376620692902 grad: -1.187472883138625
iteration: 40 loss: 3.5848591995694816 grad: -0.9874661499271633
iteration: 50 loss: 2.9108154564435775 grad: -0.8382775461868642
iteration: 60 loss: 2.4497302417939038 grad: -0.7261885935307586
iteration: 70 loss: 2.1145996583040043 grad: -0.6397758288672311
iteration: 80 loss: 1.8600839835791125 grad: -0.571423798751955
iteration: 90 loss: 1.6602518046682813 grad: -0.5161267963875995
iteration: 100 loss: 1.4992062931435364 grad: -0.47052503954933683
iteration: 110 loss: 1.366662869351537 grad: -0.4323007803950807
iteration: 120 loss: 1.2556749799377114 grad: -0.3998114399018947
iteration: 130 loss: 1.1613823002444406 grad: -0.3718638718891701
iteration: 140 loss: 1.0802826887245993 grad: -0.34757207942649987
iteration: 150 loss: 1.0097889958140678 grad: -0.3262651442905565
iteration: 160 loss: 0.9479487079732446 grad: -0.30742611150844096
iteration: 170 loss: 0.893260679766544 grad: -0.2906504484986386
iteration: 0 loss: 65.63365262884044 grad: 216.3278097139673
iteration: 10 loss: 11.603817746228081 grad: -1.2906569773382128
iteration: 20 loss: 6.66500397369501 grad: -1.5929249478904115
iteration: 30 loss: 4.66463722037989 grad: -1.2873331527805676
iteration: 40 loss: 3.5843767965570392 grad: -1.0529153119889951
iteration: 50 loss: 2.909190586752588 grad: -0.8851451383688116
iteration: 60 loss: 2.447610038252647 grad: -0.7617624120411632
iteration: 70 loss: 2.112276474063327 grad: -0.6679067573230044
iteration: 80 loss: 1.8576995857542071 grad: -0.5943543809178421
iteration: 90 loss: 1.6578777652241363 grad: -0.5352600177702894
iteration: 100 loss: 1.4968791544916216 grad: -0.48678820596132444
iteration: 110 loss: 1.3644008089284563 grad: -0.446333987804145
iteration: 120 loss: 1.2534862129540754 grad: -0.4120723397998296
iteration: 130 loss: 1.159269527320496 grad: -0.38268931647610144
iteration: 140 loss: 1.0782455408425449 grad: -0.3572161315209737
iteration: 150 loss: 1.0078254177098478 grad: -0.33492344968376986
iteration: 160 loss: 0.9460557573192173 grad: -0.31525205514230514
iteration: 170 loss: 0.8914349948189407 grad: -0.2977661748730859
iteration: 0 loss: 59.84457051824426 grad: 244.75277626388942
iteration: 10 loss: 11.473752671263417 grad: -1.0807186758149825
iteration: 20 loss: 6.604193452241899 grad: -1.4938861160459416
iteration: 30 loss: 4.622920567286821 grad: -1.230506076398362
iteration: 40 loss: 3.5519137720922447 grad: -1.0139231374251083
iteration: 50 loss: 2.8824019379212587 grad: -0.8556403710990572
iteration: 60 loss: 2.4247301929040077 grad: -0.7381026515105232
iteration: 70 loss: 2.092278771688763 grad: -0.6482082076022522
iteration: 80 loss: 1.8399249402698181 grad: -0.5775192903840615
iteration: 90 loss: 1.641874313533952 grad: -0.5205919278310251
iteration: 100 loss: 1.4823221485488498 grad: -0.47381687119741345
iteration: 110 loss: 1.351048363019138 grad: -0.43472660199806484
iteration: 120 loss: 1.2411530039901002 grad: -0.40158457096456046
iteration: 130 loss: 1.1478102709047562 grad: -0.3731364469562031
iteration: 140 loss: 1.067544143626616 grad: -0.34845511929042833
iteration: 150 loss: 0.9977875976354963 grad: -0.32684128418465064
iteration: 160 loss: 0.9366038720276761 grad: -0.30775792258963025
iteration: 170 loss: 0.8825043253405883 grad: -0.2907860250468296
iteration: 0 loss: 61.38305613215614 grad: 200.26286668317118
iteration: 10 loss: 11.646640989799813 grad: 0.1758695882128155
iteration: 20 loss: 6.683281892078412 grad: -1.0697927664654134
iteration: 30 loss: 4.677805773142651 grad: -1.0096353671320217
iteration: 40 loss: 3.5951176087172496 grad: -0.8743149574908812
iteration: 50 loss: 2.9183534042724038 grad: -0.7574886578178758
iteration: 60 loss: 2.455616697704913 grad: -0.6643089196289217
iteration: 70 loss: 2.119386924272972 grad: -0.5901081026384318
iteration: 80 loss: 1.8640916777465628 grad: -0.5302107343169653
iteration: 90 loss: 1.66368042476316 grad: -0.48107294028677816
iteration: 100 loss: 1.5021893892292362 grad: -0.4401357194749081
iteration: 110 loss: 1.369293609073669 grad: -0.4055534644015769
iteration: 120 loss: 1.2580207881679102 grad: -0.37597851081743644
iteration: 130 loss: 1.1634934477392562 grad: -0.35041069139069364
iteration: 140 loss: 1.0821975927762486 grad: -0.3280952055210722
iteration: 150 loss: 1.011537631391961 grad: -0.3084532260051981
iteration: 160 loss: 0.9495548813086068 grad: -0.2910342170674477
iteration: 170 loss: 0.8947435896506332 grad: -0.2754826792408163
iteration: 0 loss: 65.65696328618684 grad: 217.59541440320842
iteration: 10 loss: 12.095842504874698 grad: -0.8345172185838861
iteration: 20 loss: 6.896856323347575 grad: -1.2188636705422748
iteration: 30 loss: 4.8112202607602 grad: -1.029668574553129
iteration: 40 loss: 3.6906890345940155 grad: -0.8621198372894493
iteration: 50 loss: 2.9924070552742705 grad: -0.735853206084208
iteration: 60 loss: 2.515933454724332 grad: -0.6402587028088587
iteration: 70 loss: 2.1702222495375945 grad: -0.5661163900605269
iteration: 80 loss: 1.9080073306587442 grad: -0.5071801122090241
iteration: 90 loss: 1.7023311371223662 grad: -0.4593024272769882
iteration: 100 loss: 1.5367030621819966 grad: -0.41967868016818155
iteration: 110 loss: 1.4004722667228138 grad: -0.38636249079768326
iteration: 120 loss: 1.2864545129170273 grad: -0.3579676013552068
iteration: 130 loss: 1.1896287313559448 grad: -0.33348277197096965
iteration: 140 loss: 1.10638028858189 grad: -0.31215430289584445
iteration: 150 loss: 1.0340410134007139 grad: -0.2934095891297042
iteration: 160 loss: 0.9705985874515218 grad: -0.2768061367011031
iteration: 170 loss: 0.9145067771581291 grad: -0.26199676669362415
iteration: 0 loss: 63.709895423957754 grad: 203.06221390184217
iteration: 10 loss: 11.658834522698209 grad: -0.7674307160045278
iteration: 20 loss: 6.707861327461242 grad: -1.4065572571804874
iteration: 30 loss: 4.697425656205227 grad: -1.1870816860687285
iteration: 40 loss: 3.610717679474874 grad: -0.9878761557826827
iteration: 50 loss: 2.931139299088665 grad: -0.8384188371074144
iteration: 60 loss: 2.4663908685769953 grad: -0.7259835609770227
iteration: 70 loss: 2.128670181048578 grad: -0.6392921094027326
iteration: 80 loss: 1.872232691282906 grad: -0.5707358746884597
iteration: 90 loss: 1.6709214193618374 grad: -0.5152939124608846
iteration: 100 loss: 1.5087044538051322 grad: -0.4695910418025327
iteration: 110 loss: 1.3752116101104548 grad: -0.43129752760673534
iteration: 120 loss: 1.2634395631182336 grad: -0.39876204150380473
iteration: 130 loss: 1.168488909023381 grad: -0.37078515515564925
iteration: 140 loss: 1.0868297570849874 grad: -0.346476360940319
iteration: 150 loss: 1.0158547286230466 grad: -0.32516148171582204
iteration: 160 loss: 0.953596226863407 grad: -0.30632118805346076
iteration: 170 loss: 0.8985416380887314 grad: -0.2895492018469012
iteration: 0 loss: 63.278821811013565 grad: 247.11495350101515
iteration: 10 loss: 11.67478986067034 grad: -1.3775919020758292
iteration: 20 loss: 6.7157811885208085 grad: -1.539817270414809
iteration: 30 loss: 4.700001193939153 grad: -1.2552721466852534
iteration: 40 loss: 3.6106123523068163 grad: -1.0325702112945305
iteration: 50 loss: 2.929671783602606 grad: -0.8711019302365448
iteration: 60 loss: 2.464216982505067 grad: -0.751443390885301
iteration: 70 loss: 2.1261326861812533 grad: -0.6599747889279524
iteration: 80 loss: 1.8695192563594984 grad: -0.5880509360089528
iteration: 90 loss: 1.668138511740918 grad: -0.530122102786327
iteration: 100 loss: 1.5059138978651225 grad: -0.48251626663631086
iteration: 110 loss: 1.3724496736836458 grad: -0.4427249650194559
iteration: 120 loss: 1.2607274017366101 grad: -0.40898327720433214
iteration: 130 loss: 1.1658385142945504 grad: -0.3800163823668256
iteration: 140 loss: 1.0842474605212622 grad: -0.3548819180373908
iteration: 150 loss: 1.0133433244632628 grad: -0.33286895837125985
iteration: 160 loss: 0.9511562885323986 grad: -0.31343145822406226
iteration: 170 loss: 0.8961723502486414 grad: -0.29614328549283964
iteration: 0 loss: 62.75281472698905 grad: 228.45599452060713
iteration: 10 loss: 11.753422962797769 grad: -0.6902911125315869
iteration: 20 loss: 6.747900620883725 grad: -1.3755420064689343
iteration: 30 loss: 4.7226188674671485 grad: -1.170759948534277
iteration: 40 loss: 3.6289142047397616 grad: -0.9754249122695072
iteration: 50 loss: 2.945289965476687 grad: -0.8276253114586091
iteration: 60 loss: 2.4779249768345255 grad: -0.7162480031142229
iteration: 70 loss: 2.138383740532473 grad: -0.6303829653268169
iteration: 80 loss: 1.8806116960483699 grad: -0.5625245181057551
iteration: 90 loss: 1.6782829407803463 grad: -0.5076888561025586
iteration: 100 loss: 1.515266079096405 grad: -0.4625191593981942
iteration: 110 loss: 1.3811287372781904 grad: -0.42469762496115965
iteration: 120 loss: 1.2688268263050717 grad: -0.39258199621860534
iteration: 130 loss: 1.1734331672634488 grad: -0.3649802230931257
iteration: 140 loss: 1.0913983238630285 grad: -0.3410079512533133
iteration: 150 loss: 1.020100782299158 grad: -0.31999618202504326
iteration: 160 loss: 0.9575624812065061 grad: -0.30142996090796786
iteration: 170 loss: 0.9022629007319499 grad: -0.28490671534825945
iteration: 0 loss: 63.574146185648644 grad: 177.65742630767085
iteration: 10 loss: 11.903451263567975 grad: 0.983162954333062
iteration: 20 loss: 6.796682919720959 grad: -0.680248058360547
iteration: 30 loss: 4.751293805400812 grad: -0.7509115540977004
iteration: 40 loss: 3.6503430348073547 grad: -0.6811012374315127
iteration: 50 loss: 2.9630571327057953 grad: -0.6039461456019959
iteration: 60 loss: 2.4934179354189094 grad: -0.5373766511982301
iteration: 70 loss: 2.1522735647583264 grad: -0.4822293066555884
iteration: 80 loss: 1.8932787714184087 grad: -0.43661726404222784
iteration: 90 loss: 1.6899689202705717 grad: -0.39856602238740446
iteration: 100 loss: 1.5261376019939235 grad: -0.3664681399534202
iteration: 110 loss: 1.391307741142596 grad: -0.3390883251626188
iteration: 120 loss: 1.2784065097955797 grad: -0.3154880563878226
iteration: 130 loss: 1.1824871235587373 grad: -0.29495146130510164
iteration: 140 loss: 1.0999861342942578 grad: -0.27692725192598777
iteration: 150 loss: 1.0282716388143276 grad: -0.2609859970364382
iteration: 160 loss: 0.9653576425046346 grad: -0.24678922194443054
iteration: 170 loss: 0.9097174218311259 grad: -0.23406707184168504
iteration: 0 loss: 63.836287212830364 grad: 170.07317139520146
iteration: 10 loss: 12.037205086375934 grad: 0.3308517408744469
iteration: 20 loss: 6.865734999791957 grad: -0.7511335491934035
iteration: 30 loss: 4.793379612210629 grad: -0.7468947958769352
iteration: 40 loss: 3.679479534457931 grad: -0.6646629562256656
iteration: 50 loss: 2.984897974719223 grad: -0.586604064409535
iteration: 60 loss: 2.5106795449540766 grad: -0.5215612432593273
iteration: 70 loss: 2.1664354214527437 grad: -0.46830640005735236
iteration: 80 loss: 1.9052230280874276 grad: -0.42444600920556563
iteration: 90 loss: 1.7002584949037471 grad: -0.3879024165355963
iteration: 100 loss: 1.535151084707443 grad: -0.3570749150384696
iteration: 110 loss: 1.3993106285352868 grad: -0.33076158877651457
iteration: 120 loss: 1.2855914362961067 grad: -0.30805946095593056
iteration: 130 loss: 1.1889978589023982 grad: -0.28828371212050385
iteration: 140 loss: 1.1059325597367538 grad: -0.2709085067959324
iteration: 150 loss: 1.033739455628744 grad: -0.2555247998138953
iteration: 160 loss: 0.9704148729730481 grad: -0.2418103347194198
iteration: 170 loss: 0.9144188858633491 grad: -0.22950816830519388
iteration: 0 loss: 67.39092109644754 grad: 225.56513062917614
iteration: 10 loss: 11.757505436012975 grad: -1.7226245358793904
iteration: 20 loss: 6.738910588636827 grad: -1.6341727503967765
iteration: 30 loss: 4.710769386656218 grad: -1.3019683930623265
iteration: 40 loss: 3.6175323577632628 grad: -1.0620771219506078
iteration: 50 loss: 2.934978642385431 grad: -0.892217030910234
iteration: 60 loss: 2.468682274247881 grad: -0.7677002351158846
iteration: 70 loss: 2.130080883322452 grad: -0.6730993870950439
iteration: 80 loss: 1.8731092929810158 grad: -0.5990027084307781
iteration: 90 loss: 1.6714584730112951 grad: -0.5394851325862685
iteration: 100 loss: 1.5090178243068626 grad: -0.4906707986527128
iteration: 110 loss: 1.3753735894959969 grad: -0.4499315723266088
iteration: 120 loss: 1.2634969772402656 grad: -0.4154278904520744
iteration: 130 loss: 1.1684730303001991 grad: -0.38583607290504807
iteration: 140 loss: 1.086762005204946 grad: -0.36018049111004735
iteration: 150 loss: 1.0157500868066232 grad: -0.3377268174507389
iteration: 160 loss: 0.9534653619623648 grad: -0.31791208436351076
iteration: 170 loss: 0.8983922564118736 grad: -0.30029763118833913
iteration: 0 loss: 64.69004058619772 grad: 165.79378198121444
iteration: 10 loss: 11.875321658321726 grad: -0.06681798118886366
iteration: 20 loss: 6.796804521443846 grad: -0.8869806414446757
iteration: 30 loss: 4.751332376525914 grad: -0.8142939081560547
iteration: 40 loss: 3.6495068004342515 grad: -0.7034439178297782
iteration: 50 loss: 2.9616652411271143 grad: -0.6107620115732811
iteration: 60 loss: 2.491719108076144 grad: -0.5373403356661429
iteration: 70 loss: 2.1504176185936887 grad: -0.4788984309173103
iteration: 80 loss: 1.8913534631240674 grad: -0.43164109244094373
iteration: 90 loss: 1.6880258773915309 grad: -0.3927759903178333
iteration: 100 loss: 1.5242073503937634 grad: -0.36030840293735206
iteration: 110 loss: 1.3894082495415783 grad: -0.3328054865016465
iteration: 120 loss: 1.2765481248330532 grad: -0.30922208191084954
iteration: 130 loss: 1.180675487651487 grad: -0.2887822023177238
iteration: 140 loss: 1.0982239520569073 grad: -0.2708995349284571
iteration: 150 loss: 1.0265597695421838 grad: -0.2551237415384239
iteration: 160 loss: 0.9636957883708832 grad: -0.2411036638923047
iteration: 170 loss: 0.9081045679877207 grad: -0.22856169022104958
iteration: 0 loss: 62.03933109000315 grad: 168.91973221903362
iteration: 10 loss: 11.452465799411998 grad: 0.9465000732841828
iteration: 20 loss: 6.583069279683886 grad: -0.9337931044602439
iteration: 30 loss: 4.614915314922283 grad: -0.945205637724005
iteration: 40 loss: 3.5507095267941455 grad: -0.8314199870116616
iteration: 50 loss: 2.884622796047097 grad: -0.7249898639781798
iteration: 60 loss: 2.42871701992904 grad: -0.6380622198690087
iteration: 70 loss: 2.0971795187292215 grad: -0.5680795684341884
iteration: 80 loss: 1.8452798987312813 grad: -0.5112329848613799
iteration: 90 loss: 1.6474255008248624 grad: -0.4644077723221447
iteration: 100 loss: 1.487920806622862 grad: -0.4252846582639157
iteration: 110 loss: 1.35660748622236 grad: -0.392163457956223
iteration: 120 loss: 1.2466217378534814 grad: -0.36379018846224287
iteration: 130 loss: 1.1531595094930212 grad: -0.33922787335718985
iteration: 140 loss: 1.0727581333984413 grad: -0.31776592397149217
iteration: 150 loss: 1.0028588950360493 grad: -0.2988573860289195
iteration: 160 loss: 0.9415302395622844 grad: -0.28207528407865423
iteration: 170 loss: 0.8872867819736299 grad: -0.2670819098061456
iteration: 0 loss: 62.955406287939084 grad: 171.26088351006496
iteration: 10 loss: 11.685708158436539 grad: 0.30703295395749586
iteration: 20 loss: 6.718075189908797 grad: -0.9354238617217134
iteration: 30 loss: 4.705198609092514 grad: -0.8940949566505536
iteration: 40 loss: 3.617439886872076 grad: -0.7783157454944867
iteration: 50 loss: 2.937131008354217 grad: -0.6767131273385486
iteration: 60 loss: 2.4718035642849 grad: -0.5950907194058759
iteration: 70 loss: 2.1336051222247905 grad: -0.5297891088676696
iteration: 80 loss: 1.8767658742348927 grad: -0.47689051916189173
iteration: 90 loss: 1.6751123027418708 grad: -0.43337204309837585
iteration: 100 loss: 1.5126005354919803 grad: -0.39703112903191085
iteration: 110 loss: 1.3788513862719727 grad: -0.3662699585430773
iteration: 120 loss: 1.2668546122777529 grad: -0.3399167901774296
iteration: 130 loss: 1.1717054148838626 grad: -0.3170990645739354
iteration: 140 loss: 1.0898696438340598 grad: -0.2971564342766143
iteration: 150 loss: 1.0187365195624394 grad: -0.2795812922160956
iteration: 160 loss: 0.9563357017441789 grad: -0.2639777138226409
iteration: 170 loss: 0.9011523419745903 grad: -0.25003271495819035
iteration: 0 loss: 64.86141464821013 grad: 237.3204182110455
iteration: 10 loss: 11.603402927105869 grad: -1.485800406126569
iteration: 20 loss: 6.661387757995974 grad: -1.6119989433593351
iteration: 30 loss: 4.657882038594835 grad: -1.2919509159989242
iteration: 40 loss: 3.5766832455079896 grad: -1.0539850485320774
iteration: 50 loss: 2.901421001702188 grad: -0.8849519334957425
iteration: 60 loss: 2.440087560969418 grad: -0.7610460810261566
iteration: 70 loss: 2.10511350205814 grad: -0.6669664364231005
iteration: 80 loss: 1.8509228507181101 grad: -0.5933270792730145
iteration: 90 loss: 1.65147867699013 grad: -0.5342130259084672
iteration: 100 loss: 1.4908351895196161 grad: -0.4857556428113032
iteration: 110 loss: 1.358684920678322 grad: -0.44533308488918905
iteration: 120 loss: 1.2480710078254809 grad: -0.4111113354544924
iteration: 130 loss: 1.1541290591528857 grad: -0.38177152570564415
iteration: 140 loss: 1.0733559955621208 grad: -0.3563421422278298
iteration: 150 loss: 1.0031652784093488 grad: -0.33409234849156294
iteration: 160 loss: 0.9416057319388694 grad: -0.31446212821253616
iteration: 170 loss: 0.8871778436437125 grad: -0.29701531469807696
iteration: 0 loss: 62.204336092505216 grad: 194.11359511270615
iteration: 10 loss: 11.6329915100885 grad: 0.2362664981547753
iteration: 20 loss: 6.6817092378501 grad: -1.1422070776811515
iteration: 30 loss: 4.6793471299688045 grad: -1.0686425669663553
iteration: 40 loss: 3.597387445273398 grad: -0.9189003938047047
iteration: 50 loss: 2.920704405740129 grad: -0.7920013933923201
iteration: 60 loss: 2.4578592114812223 grad: -0.6918612203407659
iteration: 70 loss: 2.121470356230723 grad: -0.6127013275963938
iteration: 80 loss: 1.866011494862381 grad: -0.5491502132827796
iteration: 90 loss: 1.6654473689993872 grad: -0.49723845162441505
iteration: 100 loss: 1.503818459388888 grad: -0.45414045800057146
iteration: 110 loss: 1.370800008406067 grad: -0.4178380000329526
iteration: 120 loss: 1.2594185009358048 grad: -0.38686792417469507
iteration: 130 loss: 1.1647948437402276 grad: -0.3601504652347437
iteration: 140 loss: 1.083413431776576 grad: -0.3368744891927835
iteration: 150 loss: 1.0126772117244565 grad: -0.3164203550133852
iteration: 160 loss: 0.9506262272674245 grad: -0.2983073377114158
iteration: 170 loss: 0.8957536353506673 grad: -0.28215719284759394
iteration: 0 loss: 68.76221134391695 grad: 237.54379197941086
iteration: 10 loss: 11.673376573321217 grad: -2.188903521196323
iteration: 20 loss: 6.711232617921227 grad: -1.8351900646508434
iteration: 30 loss: 4.693145625454194 grad: -1.4190100309225184
iteration: 40 loss: 3.6032590237984383 grad: -1.1420738269324189
iteration: 50 loss: 2.9224817901702163 grad: -0.9516439071199281
iteration: 60 loss: 2.4574039509672976 grad: -0.8142372233652457
iteration: 70 loss: 2.1197470976708246 grad: -0.7109221733685854
iteration: 80 loss: 1.8635519823720559 grad: -0.6306059064726717
iteration: 90 loss: 1.6625597934882783 grad: -0.5664625172992244
iteration: 100 loss: 1.5006885604291462 grad: -0.5140943500258448
iteration: 110 loss: 1.3675430887876907 grad: -0.470552434666124
iteration: 120 loss: 1.256107682633187 grad: -0.4337902710564633
iteration: 130 loss: 1.1614771397731105 grad: -0.4023453533174136
iteration: 140 loss: 1.0801192500488965 grad: -0.37514580278447457
iteration: 150 loss: 1.0094261633175847 grad: -0.35138866694975235
iteration: 160 loss: 0.9474307772813352 grad: -0.33046085827032834
iteration: 170 loss: 0.8926214534043208 grad: -0.3118863377757065
iteration: 0 loss: 64.70451406784997 grad: 235.39992800152783
iteration: 10 loss: 11.887676469980203 grad: -1.362188944890292
iteration: 20 loss: 6.818485043036575 grad: -1.5163580215892356
iteration: 30 loss: 4.76565582960765 grad: -1.2220122509169737
iteration: 40 loss: 3.658574318595046 grad: -0.9999683221695159
iteration: 50 loss: 2.9674191000838785 grad: -0.8413871627445184
iteration: 60 loss: 2.4953460680004684 grad: -0.7247754102335325
iteration: 70 loss: 2.152632879373662 grad: -0.6360355797829104
iteration: 80 loss: 1.8926019545156123 grad: -0.5664537828869376
iteration: 90 loss: 1.6885947213853465 grad: -0.510515626527149
iteration: 100 loss: 1.5242881684152962 grad: -0.4646045152755156
iteration: 110 loss: 1.3891327242157454 grad: -0.42626437155334546
iteration: 120 loss: 1.2760090482848598 grad: -0.3937742996960556
iteration: 130 loss: 1.1799396662184087 grad: -0.36589506005254935
iteration: 140 loss: 1.0973403762812846 grad: -0.34171269672564686
iteration: 150 loss: 1.0255649371037263 grad: -0.32053893805644895
iteration: 160 loss: 0.9626172615958652 grad: -0.3018458777108868
iteration: 170 loss: 0.9069634058920945 grad: -0.2852219990026023
iteration: 0 loss: 69.02112309691884 grad: 257.45233328822565
iteration: 10 loss: 11.93736054050117 grad: -2.051970054403715
iteration: 20 loss: 6.839133466432444 grad: -1.7368511108738658
iteration: 30 loss: 4.776451535131235 grad: -1.3604469434351727
iteration: 40 loss: 3.6648576932138894 grad: -1.1008848720797233
iteration: 50 loss: 2.9712805414108683 grad: -0.919979372971814
iteration: 60 loss: 2.4977686751987096 grad: -0.7886069509912057
iteration: 70 loss: 2.1541395810118047 grad: -0.6894509953313016
iteration: 80 loss: 1.89349606032771 grad: -0.6121674332347065
iteration: 90 loss: 1.6890637487933666 grad: -0.5503283087004098
iteration: 100 loss: 1.5244538597855601 grad: -0.4997672173728404
iteration: 110 loss: 1.3890772341735762 grad: -0.4576785640317613
iteration: 120 loss: 1.2757895780400759 grad: -0.42210931061956936
iteration: 130 loss: 1.1795970822384498 grad: -0.39166038576817075
iteration: 140 loss: 1.0969045101198225 grad: -0.3653044243869934
iteration: 150 loss: 1.0250579645090014 grad: -0.3422705988861218
iteration: 160 loss: 0.9620559243468382 grad: -0.3219695679009465
iteration: 170 loss: 0.9063605119651832 grad: -0.30394321226501025
iteration: 0 loss: 66.94670904077628 grad: 192.07750524015486
iteration: 10 loss: 11.95624188772647 grad: -0.2145800670204123
iteration: 20 loss: 6.826329105982165 grad: -1.082207068036392
iteration: 30 loss: 4.768269057991595 grad: -0.9745701179140878
iteration: 40 loss: 3.661039436995452 grad: -0.8330696569688075
iteration: 50 loss: 2.970253147052352 grad: -0.7180173112424857
iteration: 60 loss: 2.4984781245033116 grad: -0.6281673576354627
iteration: 70 loss: 2.1559417568409613 grad: -0.5573272424563706
iteration: 80 loss: 1.8959937936010234 grad: -0.5004523586545733
iteration: 90 loss: 1.6920058622187588 grad: -0.453943881026471
iteration: 100 loss: 1.5276773438456737 grad: -0.4152747415235334
iteration: 110 loss: 1.392473877884599 grad: -0.38265052931074756
iteration: 120 loss: 1.2792862018776843 grad: -0.3547735510596943
iteration: 130 loss: 1.1831434791590503 grad: -0.3306868729044767
iteration: 140 loss: 1.100465878169292 grad: -0.3096715301038442
iteration: 150 loss: 1.0286100338901798 grad: -0.2911779403407315
iteration: 160 loss: 0.9655817461237514 grad: -0.27477934147118077
iteration: 170 loss: 0.9098482978183711 grad: -0.2601395917779554
iteration: 0 loss: 65.68383105867696 grad: 193.88361036210733
iteration: 10 loss: 11.920850227524108 grad: -0.24966511561312807
iteration: 20 loss: 6.800727322688172 grad: -1.0537895705621587
iteration: 30 loss: 4.7483950595809885 grad: -0.9517761870603807
iteration: 40 loss: 3.6451420165240465 grad: -0.8163097991091124
iteration: 50 loss: 2.957134807970554 grad: -0.705391899351518
iteration: 60 loss: 2.4873692743492692 grad: -0.6183547652886889
iteration: 70 loss: 2.146338207948702 grad: -0.5494884386887926
iteration: 80 loss: 1.887553240955764 grad: -0.49404552079720143
iteration: 90 loss: 1.6844875065990808 grad: -0.44860738540495093
iteration: 100 loss: 1.520906340232407 grad: -0.4107590891113725
iteration: 110 loss: 1.3863197382159507 grad: -0.37877833311545767
iteration: 120 loss: 1.2736492121225638 grad: -0.35141532074647197
iteration: 130 loss: 1.1779458699275809 grad: -0.3277458788924298
iteration: 140 loss: 1.095645910381708 grad: -0.30707404552536594
iteration: 150 loss: 1.024117929250017 grad: -0.28886676914211257
iteration: 160 loss: 0.9613768191766212 grad: -0.27270941479317434
iteration: 170 loss: 0.9058968912697971 grad: -0.2582749020429095
iteration: 0 loss: 63.02340697351196 grad: 224.08320050521877
iteration: 10 loss: 11.379543186164607 grad: -1.3520712256060727
iteration: 20 loss: 6.56240684429961 grad: -1.509357910647564
iteration: 30 loss: 4.598716161259538 grad: -1.2368381612752342
iteration: 40 loss: 3.535979266169389 grad: -1.020250113290042
iteration: 50 loss: 2.871002675317248 grad: -0.8620214088521159
iteration: 60 loss: 2.416085846190361 grad: -0.7443014273298544
iteration: 70 loss: 2.0854368975342368 grad: -0.654101779446347
iteration: 80 loss: 1.834330560059243 grad: -0.5830641830459024
iteration: 90 loss: 1.6371820431934132 grad: -0.5257852921064448
iteration: 100 loss: 1.4783055362445339 grad: -0.47867461460128324
iteration: 110 loss: 1.3475525279582556 grad: -0.4392722560629222
iteration: 120 loss: 1.2380682561747562 grad: -0.40584381255581525
iteration: 130 loss: 1.1450566189416185 grad: -0.37713449071531746
iteration: 140 loss: 1.0650617842581844 grad: -0.35221555758509426
iteration: 150 loss: 0.995530853254629 grad: -0.33038565300737466
iteration: 160 loss: 0.9345371989266327 grad: -0.3111056256980701
iteration: 170 loss: 0.8805996104161667 grad: -0.29395442771316227
iteration: 0 loss: 64.13488672664607 grad: 216.05599964239664
iteration: 10 loss: 11.678128550523896 grad: -1.2697799616171142
iteration: 20 loss: 6.705087455722275 grad: -1.4035931054110533
iteration: 30 loss: 4.68826963586021 grad: -1.1495096268970042
iteration: 40 loss: 3.599995623050042 grad: -0.9497468736042898
iteration: 50 loss: 2.920357509859679 grad: -0.8039973616218044
iteration: 60 loss: 2.456046257904332 grad: -0.6954395052149549
iteration: 70 loss: 2.1189117900170955 grad: -0.6121212226779441
iteration: 80 loss: 1.863080641789979 grad: -0.5463914823944847
iteration: 90 loss: 1.6623475649460653 grad: -0.49330801027288884
iteration: 100 loss: 1.5006642496109635 grad: -0.44958455675407016
iteration: 110 loss: 1.367657148167506 grad: -0.41296709479341376
iteration: 120 loss: 1.256324852536512 grad: -0.38186431742248406
iteration: 130 loss: 1.1617718304091795 grad: -0.3551234545564182
iteration: 140 loss: 1.0804725680583778 grad: -0.331890208139518
iteration: 150 loss: 1.009823949376782 grad: -0.31151865704595905
iteration: 160 loss: 0.947862281126163 grad: -0.2935117081680423
iteration: 170 loss: 0.8930784204597496 grad: -0.2774807493776392
iteration: 0 loss: 63.84761834104998 grad: 262.6596309155835
iteration: 10 loss: 11.585843449072048 grad: -1.8790620814688528
iteration: 20 loss: 6.647814118060441 grad: -1.6467577710462786
iteration: 30 loss: 4.644498281952607 grad: -1.3121593930888942
iteration: 40 loss: 3.5642579912879557 grad: -1.0698254939906522
iteration: 50 loss: 2.890115019546887 grad: -0.898027008061184
iteration: 60 loss: 2.4298282849791906 grad: -0.7720955527958442
iteration: 70 loss: 2.095774467675545 grad: -0.67646656128676
iteration: 80 loss: 1.842378623671059 grad: -0.6016111549927495
iteration: 90 loss: 1.6436192577199025 grad: -0.5415228790844426
iteration: 100 loss: 1.4835678626529691 grad: -0.49227124287527035
iteration: 110 loss: 1.3519324790531342 grad: -0.4511912540369873
iteration: 120 loss: 1.2417692267343805 grad: -0.4164180614965818
iteration: 130 loss: 1.1482243135946275 grad: -0.3866102895408887
iteration: 140 loss: 1.0678032495500867 grad: -0.3607796805461498
iteration: 150 loss: 0.9979264574876652 grad: -0.3381827247583711
iteration: 160 loss: 0.9366484265662042 grad: -0.3182496633261356
iteration: 170 loss: 0.8824742806419553 grad: -0.3005367335183349
iteration: 0 loss: 67.15287120320941 grad: 208.45349586293077
iteration: 10 loss: 11.837624851575667 grad: -1.234127225528081
iteration: 20 loss: 6.786773794980366 grad: -1.4732400119696876
iteration: 30 loss: 4.745421307381383 grad: -1.2111536111076773
iteration: 40 loss: 3.6447443846708336 grad: -0.9998958714380869
iteration: 50 loss: 2.957403513262224 grad: -0.8453445131697704
iteration: 60 loss: 2.4877629392786234 grad: -0.7303061178294841
iteration: 70 loss: 2.1466907910064283 grad: -0.6421278140082827
iteration: 80 loss: 1.887818026809162 grad: -0.5726565669910987
iteration: 90 loss: 1.6846584216116505 grad: -0.5166202485608437
iteration: 100 loss: 1.5209909522594982 grad: -0.4705151206451026
iteration: 110 loss: 1.38632940605889 grad: -0.43194044530637965
iteration: 120 loss: 1.27359544875683 grad: -0.3992033002720594
iteration: 130 loss: 1.1778390845803932 grad: -0.3710787135090076
iteration: 140 loss: 1.095495082268082 grad: -0.3466598692615212
iteration: 150 loss: 1.0239306389083305 grad: -0.32526183372539985
iteration: 160 loss: 0.9611594033956614 grad: -0.3063579827228799
iteration: 170 loss: 0.9056546262197365 grad: -0.2895369711943863
iteration: 0 loss: 61.81044477358039 grad: 247.533534494284
iteration: 10 loss: 11.39495135158373 grad: -2.1402743699082722
iteration: 20 loss: 6.571490605802782 grad: -1.7860836590311107
iteration: 30 loss: 4.597217377063771 grad: -1.3856151152783291
iteration: 40 loss: 3.529709911138525 grad: -1.1177915461816923
iteration: 50 loss: 2.862671492624486 grad: -0.9332623910731485
iteration: 60 loss: 2.4069316250195034 grad: -0.7998382184651978
iteration: 70 loss: 2.076052630240693 grad: -0.6993070671769647
iteration: 80 loss: 1.8250095587648667 grad: -0.6210048794390741
iteration: 90 loss: 1.6280702276898493 grad: -0.558364711513605
iteration: 100 loss: 1.4694728964468182 grad: -0.5071493262866846
iteration: 110 loss: 1.3390286442315962 grad: -0.4645122997439474
iteration: 120 loss: 1.2298608097808854 grad: -0.4284747953129556
iteration: 130 loss: 1.1371614061961488 grad: -0.3976202976517032
iteration: 140 loss: 1.0574682900885983 grad: -0.3709090392005315
iteration: 150 loss: 0.9882254150446154 grad: -0.3475610340544675
iteration: 160 loss: 0.9275048218995917 grad: -0.32697997759265107
iteration: 170 loss: 0.8738250096109202 grad: -0.3087023324768454
iteration: 0 loss: 66.06160289539683 grad: 204.92063424667714
iteration: 10 loss: 12.122140906496146 grad: -0.2542380743444264
iteration: 20 loss: 6.907863870042712 grad: -1.0527323004146893
iteration: 30 loss: 4.822397742989572 grad: -0.9578269707167728
iteration: 40 loss: 3.7018275493149884 grad: -0.8235595449737216
iteration: 50 loss: 3.0030891796145625 grad: -0.7123158823125588
iteration: 60 loss: 2.5260013518573445 grad: -0.6246377974597918
iteration: 70 loss: 2.1796501450216965 grad: -0.5551239801176273
iteration: 80 loss: 1.916823651405676 grad: -0.49910235722757895
iteration: 90 loss: 1.710583463611246 grad: -0.4531650341461624
iteration: 100 loss: 1.544443148333316 grad: -0.41489005721368155
iteration: 110 loss: 1.4077498859542175 grad: -0.38254434467471155
iteration: 120 loss: 1.2933151760405714 grad: -0.3548677245236861
iteration: 130 loss: 1.1961130982444956 grad: -0.33092713899060183
iteration: 140 loss: 1.1125243004023766 grad: -0.3100193264962501
iteration: 150 loss: 1.0398763050249975 grad: -0.2916053265230814
iteration: 160 loss: 0.9761529810578974 grad: -0.2752657368799058
iteration: 170 loss: 0.9198047695584303 grad: -0.26066962322524806
iteration: 0 loss: 68.94778437410767 grad: 191.8091940297606
iteration: 10 loss: 11.963788426346769 grad: -0.8828672475881247
iteration: 20 loss: 6.839345838795328 grad: -1.2487404363552799
iteration: 30 loss: 4.781359361710036 grad: -1.052658833753299
iteration: 40 loss: 3.673240615737706 grad: -0.8804979973082357
iteration: 50 loss: 2.9814195784831345 grad: -0.7508830808349847
iteration: 60 loss: 2.5086742563826605 grad: -0.6527974898977208
iteration: 70 loss: 2.165278726455075 grad: -0.5767594523166852
iteration: 80 loss: 1.904582637693817 grad: -0.5163480854549295
iteration: 90 loss: 1.6999449817556276 grad: -0.4672988879158745
iteration: 100 loss: 1.5350505694455023 grad: -0.42672765304915805
iteration: 110 loss: 1.3993517200705439 grad: -0.3926328413455018
iteration: 120 loss: 1.2857278508320384 grad: -0.36358911312489084
iteration: 130 loss: 1.1891987626923268 grad: -0.33855694914451295
iteration: 140 loss: 1.106176952282443 grad: -0.3167617809693448
iteration: 150 loss: 1.0340127749011365 grad: -0.29761532405633623
iteration: 160 loss: 0.9707068754257968 grad: -0.2806631096322103
iteration: 170 loss: 0.9147222847689342 grad: -0.2655486770046125
iteration: 0 loss: 63.625629566308795 grad: 241.5648424546752
iteration: 10 loss: 11.979931168207797 grad: -0.8875801202197564
iteration: 20 loss: 6.841270456918292 grad: -1.2867203619015992
iteration: 30 loss: 4.776077954170102 grad: -1.0957583545199514
iteration: 40 loss: 3.664983945469513 grad: -0.918988836804938
iteration: 50 loss: 2.97200738945875 grad: -0.7840817359128684
iteration: 60 loss: 2.4989235950639257 grad: -0.681495426960214
iteration: 70 loss: 2.155572240933204 grad: -0.6018233367846488
iteration: 80 loss: 1.8951017175327665 grad: -0.5384887230070168
iteration: 90 loss: 1.690772498718533 grad: -0.4870662840405924
iteration: 100 loss: 1.5262192858982333 grad: -0.44454386316996886
iteration: 110 loss: 1.390868379777244 grad: -0.40882398635766437
iteration: 120 loss: 1.2775856897035076 grad: -0.3784101943263387
iteration: 130 loss: 1.1813842137984962 grad: -0.35220999372494055
iteration: 140 loss: 1.0986733151106902 grad: -0.32940893656685394
iteration: 150 loss: 1.02680224268426 grad: -0.3093883360141907
iteration: 160 loss: 0.9637716506030384 grad: -0.2916702444253896
iteration: 170 loss: 0.9080451780535607 grad: -0.27587983999738697
iteration: 0 loss: 61.398383273919755 grad: 191.73869233870366
iteration: 10 loss: 11.607123651988331 grad: -0.44918864312955686
iteration: 20 loss: 6.6690021150811845 grad: -1.2143853116823076
iteration: 30 loss: 4.664911377729988 grad: -1.0645406213822253
iteration: 40 loss: 3.5831087155191366 grad: -0.9005568371512886
iteration: 50 loss: 2.9072947385660552 grad: -0.7715654364507115
iteration: 60 loss: 2.4454712374509984 grad: -0.6722976121356822
iteration: 70 loss: 2.110068055512453 grad: -0.5947070483625043
iteration: 80 loss: 1.8555032754756708 grad: -0.5327774987228098
iteration: 90 loss: 1.6557325523774373 grad: -0.4823544862022192
iteration: 100 loss: 1.4948029468170552 grad: -0.44057163007873806
iteration: 110 loss: 1.3624007142107488 grad: -0.40541630331885015
iteration: 120 loss: 1.2515636941639885 grad: -0.37544445529030523
iteration: 130 loss: 1.1574230982385167 grad: -0.34959751926458354
iteration: 140 loss: 1.0764722198463128 grad: -0.3270838624259448
iteration: 150 loss: 1.0061215280272997 grad: -0.30730063426838616
iteration: 160 loss: 0.9444173717268065 grad: -0.28978117781099744
iteration: 170 loss: 0.8898581818102271 grad: -0.27415890839866275
iteration: 0 loss: 62.53444607985216 grad: 248.28657330442545
iteration: 10 loss: 11.609059107254346 grad: -1.5643536422953335
iteration: 20 loss: 6.65258305730574 grad: -1.5346895968607357
iteration: 30 loss: 4.645753657913577 grad: -1.2520331793354091
iteration: 40 loss: 3.5646416662088583 grad: -1.0314384395472325
iteration: 50 loss: 2.8902284511655125 grad: -0.870791954101028
iteration: 60 loss: 2.4298403529257318 grad: -0.7514615228827193
iteration: 70 loss: 2.0957409635980184 grad: -0.660128903783191
iteration: 80 loss: 1.8423206827321172 grad: -0.5882597859715145
iteration: 90 loss: 1.6435459972780833 grad: -0.5303490328023208
iteration: 100 loss: 1.4834837824391385 grad: -0.48274413785523124
iteration: 110 loss: 1.3518401295951994 grad: -0.4429457569318498
iteration: 120 loss: 1.2416702528431331 grad: -0.40919343175173156
iteration: 130 loss: 1.148119892874795 grad: -0.38021458812725967
iteration: 140 loss: 1.0676942900831405 grad: -0.35506800493921786
iteration: 150 loss: 0.9978136939070779 grad: -0.3330433268418329
iteration: 160 loss: 0.9365324708779057 grad: -0.3135947773764006
iteration: 170 loss: 0.8823556517395446 grad: -0.2962963316895427
iteration: 0 loss: 62.38168454661263 grad: 178.21868751595866
iteration: 10 loss: 11.717424624879827 grad: 0.3284039761086158
iteration: 20 loss: 6.737976113198701 grad: -1.049189077876738
iteration: 30 loss: 4.717340574247208 grad: -0.9981855431538503
iteration: 40 loss: 3.625273185323046 grad: -0.8642494050434453
iteration: 50 loss: 2.942445830410303 grad: -0.747955433275272
iteration: 60 loss: 2.4755490888505847 grad: -0.6552392186120265
iteration: 70 loss: 2.1363182507671725 grad: -0.5815076184736456
iteration: 80 loss: 1.8787694203608474 grad: -0.5220729075782466
iteration: 90 loss: 1.676610628670005 grad: -0.47337685044485905
iteration: 100 loss: 1.513728669133467 grad: -0.4328528249620359
iteration: 110 loss: 1.3797017770020004 grad: -0.3986526420485865
iteration: 120 loss: 1.2674924775802454 grad: -0.3694289100952863
iteration: 130 loss: 1.1721779430529915 grad: -0.3441831530516878
iteration: 140 loss: 1.090211727729721 grad: -0.32216286205210404
iteration: 150 loss: 1.0189744493002928 grad: -0.3027916642969158
iteration: 160 loss: 0.9564896124683483 grad: -0.28562141911324246
iteration: 170 loss: 0.9012378779668159 grad: -0.27029885915220936
iteration: 0 loss: 63.53925718650941 grad: 232.15063003321671
iteration: 10 loss: 11.864139710425889 grad: -0.9355174573035749
iteration: 20 loss: 6.773672210297305 grad: -1.3257246180604336
iteration: 30 loss: 4.727964382368555 grad: -1.114016169415386
iteration: 40 loss: 3.6277141015733916 grad: -0.9271768342481421
iteration: 50 loss: 2.941685004291809 grad: -0.7872138118083507
iteration: 60 loss: 2.473424355188523 grad: -0.6819350551411336
iteration: 70 loss: 2.1336086249300688 grad: -0.6007607316590147
iteration: 80 loss: 1.8758357287193492 grad: -0.5365629009218194
iteration: 90 loss: 1.6736292492409204 grad: -0.48463913120783264
iteration: 100 loss: 1.5107879348573656 grad: -0.4418292231708558
iteration: 110 loss: 1.3768455316400254 grad: -0.4059524573202997
iteration: 120 loss: 1.2647411731540243 grad: -0.3754635217215389
iteration: 130 loss: 1.1695394244042787 grad: -0.3492402196936314
iteration: 140 loss: 1.087686979980514 grad: -0.3264495059088476
iteration: 150 loss: 1.0165607593830415 grad: -0.30646076557971524
iteration: 160 loss: 0.9541823482178106 grad: -0.28878824941145476
iteration: 170 loss: 0.8990315034680779 grad: -0.27305195296719137
iteration: 0 loss: 65.43059161400944 grad: 176.48497847787436
iteration: 10 loss: 11.995385283324573 grad: 0.7853517707407532
iteration: 20 loss: 6.841396921742754 grad: -0.7748258736544165
iteration: 30 loss: 4.780384136456178 grad: -0.8279222060502808
iteration: 40 loss: 3.6718291717528784 grad: -0.7465597448396429
iteration: 50 loss: 2.980027749267926 grad: -0.6603068141234536
iteration: 60 loss: 2.5073934473887842 grad: -0.5864924753204876
iteration: 70 loss: 2.1641184847476738 grad: -0.5255331697902573
iteration: 80 loss: 1.9035327479534152 grad: -0.47520522270300347
iteration: 90 loss: 1.6989915699491989 grad: -0.43327761263663106
iteration: 100 loss: 1.5341804887342816 grad: -0.39795222324955826
iteration: 110 loss: 1.3985535903015758 grad: -0.3678525265455231
iteration: 120 loss: 1.2849921055531426 grad: -0.3419347045803501
iteration: 130 loss: 1.1885174246150945 grad: -0.3194035309401885
iteration: 140 loss: 1.105543365072704 grad: -0.29964716864341
iteration: 150 loss: 1.0334213598785853 grad: -0.28218939705284085
iteration: 160 loss: 0.9701529288561853 grad: -0.26665513633584303
iteration: 170 loss: 0.914201814381241 grad: -0.25274555705548357
iteration: 0 loss: 65.58322100465604 grad: 228.4848124303008
iteration: 10 loss: 11.815983179117625 grad: -1.6951075437944998
iteration: 20 loss: 6.783374603195295 grad: -1.5791192529836962
iteration: 30 loss: 4.742554984524097 grad: -1.250071599385866
iteration: 40 loss: 3.641561087268904 grad: -1.01846948483729
iteration: 50 loss: 2.95403538715242 grad: -0.8556995854101953
iteration: 60 loss: 2.4843494572325486 grad: -0.7366664861152284
iteration: 70 loss: 2.1433157683818123 grad: -0.6462893825798308
iteration: 80 loss: 1.8845262578640194 grad: -0.575496925116451
iteration: 90 loss: 1.681471804584486 grad: -0.518613878619431
iteration: 100 loss: 1.517918460988519 grad: -0.47193888174240595
iteration: 110 loss: 1.3833728179540066 grad: -0.4329657426991207
iteration: 120 loss: 1.270752600875064 grad: -0.3999413792320866
iteration: 130 loss: 1.1751057335596173 grad: -0.3716045979962032
iteration: 140 loss: 1.0928659734943886 grad: -0.3470257866725046
iteration: 150 loss: 1.0214001222703617 grad: -0.3255051227590293
iteration: 160 loss: 0.9587217854697304 grad: -0.30650594828685607
iteration: 170 loss: 0.9033043674251282 grad: -0.28960997201860705
iteration: 0 loss: 67.1327974876732 grad: 193.72703699812328
iteration: 10 loss: 11.749299816770769 grad: -1.1083049949705295
iteration: 20 loss: 6.75077682049017 grad: -1.5136917467201227
iteration: 30 loss: 4.724075900679798 grad: -1.2374880973776952
iteration: 40 loss: 3.6297147053424355 grad: -1.0175896956359762
iteration: 50 loss: 2.9458162357036186 grad: -0.858350496564551
iteration: 60 loss: 2.47832920857598 grad: -0.7405085384188307
iteration: 70 loss: 2.1387293963967666 grad: -0.6505015057379642
iteration: 80 loss: 1.8809268882426873 grad: -0.5797551378733334
iteration: 90 loss: 1.6785807444553442 grad: -0.5227831959086302
iteration: 100 loss: 1.5155527920614986 grad: -0.4759637984029764
iteration: 110 loss: 1.3814074629294737 grad: -0.43682645287924715
iteration: 120 loss: 1.2690991142706645 grad: -0.40363470016882136
iteration: 130 loss: 1.1736997975620975 grad: -0.3751351165080576
iteration: 140 loss: 1.091659693966366 grad: -0.3504015632061709
iteration: 150 loss: 1.0203571024133204 grad: -0.3287355488056165
iteration: 160 loss: 0.9578138738912788 grad: -0.30960066786290097
iteration: 170 loss: 0.9025094516237832 grad: -0.2925783467811176
iteration: 0 loss: 68.46216008460091 grad: 212.76055152589169
iteration: 10 loss: 11.730913499294088 grad: -1.814477566314788
iteration: 20 loss: 6.733724535506099 grad: -1.6489268238864576
iteration: 30 loss: 4.707165612388715 grad: -1.297329469415033
iteration: 40 loss: 3.6140296168757895 grad: -1.0518135759860403
iteration: 50 loss: 2.9315309993749823 grad: -0.8804711962946509
iteration: 60 loss: 2.4653433074255107 grad: -0.7559229011693428
iteration: 70 loss: 2.126883563836972 grad: -0.6618086863626556
iteration: 80 loss: 1.87006584040579 grad: -0.5883638819611192
iteration: 90 loss: 1.668569024775438 grad: -0.5295240118100322
iteration: 100 loss: 1.506276398912219 grad: -0.4813583642617078
iteration: 110 loss: 1.3727714596627514 grad: -0.4412191358081742
iteration: 120 loss: 1.2610244708296043 grad: -0.4072620785223632
iteration: 130 loss: 1.1661204587243943 grad: -0.3781651339286889
iteration: 140 loss: 1.0845201193794558 grad: -0.3529567509960896
iteration: 150 loss: 1.013610279605473 grad: -0.3309074057661368
iteration: 160 loss: 0.9514197360828637 grad: -0.31145889520851394
iteration: 170 loss: 0.8964336223594646 grad: -0.29417696709234564
iteration: 0 loss: 61.70512979531477 grad: 207.16631650084935
iteration: 10 loss: 11.550876989396846 grad: 0.06484068938817777
iteration: 20 loss: 6.617671897082676 grad: -1.1039091981680202
iteration: 30 loss: 4.6320733832115115 grad: -1.0373910072665842
iteration: 40 loss: 3.5609967012109007 grad: -0.8985546859907922
iteration: 50 loss: 2.8915434453269055 grad: -0.7787677980002414
iteration: 60 loss: 2.433742879363639 grad: -0.6830959768739195
iteration: 70 loss: 2.1010319926238235 grad: -0.6068201044041885
iteration: 80 loss: 1.84835323367712 grad: -0.5451949383044273
iteration: 90 loss: 1.6499532770261462 grad: -0.49461005111472645
iteration: 100 loss: 1.4900503204457478 grad: -0.45245084832845367
iteration: 110 loss: 1.3584363479245356 grad: -0.4168275199488539
iteration: 120 loss: 1.2482172982774749 grad: -0.3863578359619891
iteration: 130 loss: 1.1545698244877216 grad: -0.36001466464846343
iteration: 140 loss: 1.074018473680429 grad: -0.33702208460232597
iteration: 150 loss: 1.0039957793056105 grad: -0.31678460732018787
iteration: 160 loss: 0.9425640254638562 grad: -0.2988384431477232
iteration: 170 loss: 0.8882333834863775 grad: -0.28281745631013566
iteration: 0 loss: 64.89751622744782 grad: 169.4184574101384
iteration: 10 loss: 11.896285298647543 grad: 0.3512164563830167
iteration: 20 loss: 6.786363734708588 grad: -0.7998738288139733
iteration: 30 loss: 4.740994996764419 grad: -0.7989700971860426
iteration: 40 loss: 3.6412282495831607 grad: -0.7093232722423638
iteration: 50 loss: 2.9550983356015506 grad: -0.6239360144807314
iteration: 60 loss: 2.4864176661520685 grad: -0.552965210316914
iteration: 70 loss: 2.14604778507948 grad: -0.4950593095756968
iteration: 80 loss: 1.8876815812194847 grad: -0.44753519313084583
iteration: 90 loss: 1.6848882405170529 grad: -0.40806842617230643
iteration: 100 loss: 1.5214872400787582 grad: -0.3748743582536705
iteration: 110 loss: 1.3870206721899607 grad: -0.34661779383389324
iteration: 120 loss: 1.274429881974811 grad: -0.32229910024524033
iteration: 130 loss: 1.1787786644371077 grad: -0.30116260913164106
iteration: 140 loss: 1.096511573059574 grad: -0.2826298253501297
iteration: 150 loss: 1.025002842453432 grad: -0.26625198865480926
iteration: 160 loss: 0.9622712517227889 grad: -0.2516764614707528
iteration: 170 loss: 0.9067938391233177 grad: -0.23862274571353512
iteration: 0 loss: 64.16845385040693 grad: 216.08567193695762
iteration: 10 loss: 11.78642142200465 grad: -0.41871101981233083
iteration: 20 loss: 6.750975481598454 grad: -1.2386679288749978
iteration: 30 loss: 4.72079915094483 grad: -1.0884494900948525
iteration: 40 loss: 3.626159498089609 grad: -0.9193263276689749
iteration: 50 loss: 2.942489857986402 grad: -0.786301801531922
iteration: 60 loss: 2.4753024348273027 grad: -0.6841780139181101
iteration: 70 loss: 2.1359831081119656 grad: -0.604545006937962
iteration: 80 loss: 1.878425239027944 grad: -0.5411135833240666
iteration: 90 loss: 1.6762887900781607 grad: -0.4895539632874187
iteration: 100 loss: 1.513440414826701 grad: -0.44688825152766143
iteration: 110 loss: 1.3794496275230481 grad: -0.41103141040370517
iteration: 120 loss: 1.2672752001863046 grad: -0.3804910737783844
iteration: 130 loss: 1.1719928009122007 grad: -0.3541755434145246
iteration: 140 loss: 1.0900555170081299 grad: -0.331269880100466
iteration: 150 loss: 1.0188439732991024 grad: -0.3111544536300445
iteration: 160 loss: 0.9563818872739748 grad: -0.2933502711370725
iteration: 170 loss: 0.9011502086186738 grad: -0.2774815070544367
iteration: 0 loss: 61.41053145276022 grad: 221.65953318691902
iteration: 10 loss: 11.411447476169458 grad: -1.2826317723502845
iteration: 20 loss: 6.580072752812364 grad: -1.5496989814933748
iteration: 30 loss: 4.609291464127264 grad: -1.257569410628392
iteration: 40 loss: 3.543145524990574 grad: -1.03087210729753
iteration: 50 loss: 2.876276830979722 grad: -0.8678225588254308
iteration: 60 loss: 2.4201892669203433 grad: -0.7475935821528356
iteration: 70 loss: 2.0887571927057915 grad: -0.6559831879185938
iteration: 80 loss: 1.8370961679195055 grad: -0.5841046878350553
iteration: 90 loss: 1.6395373626144398 grad: -0.5263024218123953
iteration: 100 loss: 1.480346930701382 grad: -0.4788555457660573
iteration: 110 loss: 1.3493471299276611 grad: -0.4392323723040823
iteration: 120 loss: 1.2396644915485344 grad: -0.4056568429258695
iteration: 130 loss: 1.1464904384377983 grad: -0.3768489378449932
iteration: 140 loss: 1.0663605287282651 grad: -0.35186409179073613
iteration: 150 loss: 0.9967157467257576 grad: -0.3299907053747292
iteration: 160 loss: 0.9356250107625783 grad: -0.31068283456329737
iteration: 170 loss: 0.8816037955500426 grad: -0.2935148260954276
iteration: 0 loss: 72.30154051601124 grad: 135.06703953111085
iteration: 10 loss: 20.699310483343154 grad: -1.0277973164549996
iteration: 20 loss: 13.145130029780189 grad: -2.026218074076674
iteration: 30 loss: 9.6062268695498 grad: -1.749142149645918
iteration: 40 loss: 7.5489329666214955 grad: -1.4848482223774087
iteration: 50 loss: 6.2070357864644885 grad: -1.2789186379405106
iteration: 60 loss: 5.2647014953490725 grad: -1.119149474512187
iteration: 70 loss: 4.5677021622225515 grad: -0.9929859831048038
iteration: 80 loss: 4.031867069902091 grad: -0.8913790652726579
iteration: 90 loss: 3.6074330555646323 grad: -0.8080506404908582
iteration: 100 loss: 3.2631334617224392 grad: -0.7386094798327663
iteration: 110 loss: 2.9783566150263967 grad: -0.6799267969596663
iteration: 120 loss: 2.7389751490868135 grad: -0.6297275065235131
iteration: 130 loss: 2.534988624132645 grad: -0.5863244359343032
iteration: 140 loss: 2.3591229726746614 grad: -0.5484433923541613
iteration: 150 loss: 2.20596299809598 grad: -0.5151059564539292
iteration: 160 loss: 2.0713956783043943 grad: -0.485549431780002
iteration: 170 loss: 1.9522418635192893 grad: -0.45917112876231353
iteration: 180 loss: 1.8460060621904715 grad: -0.43548888462508534
iteration: 190 loss: 1.7507024440856185 grad: -0.4141126159794643
iteration: 200 loss: 1.6647313203287177 grad: -0.3947235007987982
iteration: 210 loss: 1.5867898270397 grad: -0.3770585239620994
iteration: 220 loss: 1.5158062646791306 grad: -0.36089885193489074
iteration: 230 loss: 1.450891101931244 grad: -0.34606098055279355
iteration: 240 loss: 1.3912999161644124 grad: -0.33238991801094836
iteration: 250 loss: 1.3364050142528703 grad: -0.31975388008549643
iteration: 0 loss: 75.58071939736273 grad: 117.15906817298566
iteration: 10 loss: 22.128361759315855 grad: 0.6268441721091067
iteration: 20 loss: 13.89306130856682 grad: -1.2853793184594575
iteration: 30 loss: 10.100830468657973 grad: -1.3144874082026163
iteration: 40 loss: 7.916561364323777 grad: -1.1887477281827512
iteration: 50 loss: 6.499559269073126 grad: -1.0579728752966402
iteration: 60 loss: 5.507854324419151 grad: -0.944272434326986
iteration: 70 loss: 4.775968136594781 grad: -0.8488931020393281
iteration: 80 loss: 4.214165510934518 grad: -0.7691883619684323
iteration: 90 loss: 3.769636757938811 grad: -0.7021897740583489
iteration: 100 loss: 3.4093131361111606 grad: -0.6453688462919822
iteration: 110 loss: 3.1114504684273925 grad: -0.5967187208399601
iteration: 120 loss: 2.8611743106835505 grad: -0.5546780022007498
iteration: 130 loss: 2.647971487412468 grad: -0.5180342292980571
iteration: 140 loss: 2.4642042626311524 grad: -0.48584084004580785
iteration: 150 loss: 2.3041922535315034 grad: -0.45735264382390867
iteration: 160 loss: 2.163624440593594 grad: -0.4319773893617
iteration: 170 loss: 2.039170798821198 grad: -0.40923978752704493
iteration: 180 loss: 1.928218811155437 grad: -0.38875477859261026
iteration: 190 loss: 1.8286904523371381 grad: -0.3702075633285932
iteration: 200 loss: 1.738912392640693 grad: -0.3533385716979336
iteration: 210 loss: 1.6575222211040868 grad: -0.33793204896568885
iteration: 220 loss: 1.5833995549665318 grad: -0.3238073093851492
iteration: 230 loss: 1.5156146650219027 grad: -0.3108119727885982
iteration: 240 loss: 1.4533896377585982 grad: -0.298816687905891
iteration: 250 loss: 1.396068648224958 grad: -0.2877109802559048
iteration: 260 loss: 1.3430949464863344 grad: -0.27739995814226714
iteration: 0 loss: 77.88633131609403 grad: 121.81535591244085
iteration: 10 loss: 22.272224038989137 grad: 0.12212540498253127
iteration: 20 loss: 13.918327920438804 grad: -1.6135141582452648
iteration: 30 loss: 10.100340521416104 grad: -1.523305814605573
iteration: 40 loss: 7.908892315608248 grad: -1.3288953986039114
iteration: 50 loss: 6.490065171928688 grad: -1.1580528931058944
iteration: 60 loss: 5.498272813268283 grad: -1.0196574020244482
iteration: 70 loss: 4.766876212071318 grad: -0.908166600520065
iteration: 80 loss: 4.205729891444042 grad: -0.8174036551552105
iteration: 90 loss: 3.7618720402378627 grad: -0.7424810575232594
iteration: 100 loss: 3.402178454483047 grad: -0.679773172242601
iteration: 110 loss: 3.104887624774082 grad: -0.6266150469700854
iteration: 120 loss: 2.8551230636404314 grad: -0.5810331332678291
iteration: 130 loss: 2.6423754619751723 grad: -0.5415468299642162
iteration: 140 loss: 2.4590128999637852 grad: -0.5070291998743226
iteration: 150 loss: 2.29936109035382 grad: -0.47661010218672134
iteration: 160 loss: 2.159114716324232 grad: -0.4496084627421112
iteration: 170 loss: 2.0349488316112665 grad: -0.425484390973476
iteration: 180 loss: 1.9242553356767034 grad: -0.40380487829203177
iteration: 190 loss: 1.8249599993033703 grad: -0.3842188860329607
iteration: 200 loss: 1.735392738485974 grad: -0.3664390064955052
iteration: 210 loss: 1.6541939130858985 grad: -0.35022778666172233
iteration: 220 loss: 1.5802455071305055 grad: -0.3353874033421793
iteration: 230 loss: 1.5126198168757747 grad: -0.3217517782586678
iteration: 240 loss: 1.4505406667654328 grad: -0.30918049130021735
iteration: 250 loss: 1.393353727741917 grad: -0.29755403440230843
iteration: 260 loss: 1.340503541743601 grad: -0.2867700758772126
iteration: 0 loss: 74.83448690115092 grad: 138.51409397313031
iteration: 10 loss: 21.948189381676993 grad: -0.40108476237112906
iteration: 20 loss: 13.782968357448123 grad: -1.5990992649629105
iteration: 30 loss: 10.009009841581722 grad: -1.469681680956711
iteration: 40 loss: 7.837194289657873 grad: -1.2811540493910971
iteration: 50 loss: 6.430027700244285 grad: -1.1196285625806168
iteration: 60 loss: 5.446231325300315 grad: -0.9887995287777893
iteration: 70 loss: 4.720776681200112 grad: -0.8829641512830422
iteration: 80 loss: 4.164271417181598 grad: -0.796411636061179
iteration: 90 loss: 3.724162781128226 grad: -0.7246723208428233
iteration: 100 loss: 3.3675729271141623 grad: -0.6644197602679512
iteration: 110 loss: 3.072900008526796 grad: -0.6131937691592886
iteration: 120 loss: 2.8253767337106512 grad: -0.5691607612823544
iteration: 130 loss: 2.614571584942496 grad: -0.5309370594310041
iteration: 140 loss: 2.432909860676773 grad: -0.497464055509336
iteration: 150 loss: 2.2747604569416664 grad: -0.4679207013844856
iteration: 160 loss: 2.135851347360875 grad: -0.44166185497966126
iteration: 170 loss: 2.012883712659459 grad: -0.4181743895534974
iteration: 180 loss: 1.9032702736623668 grad: -0.3970455576619184
iteration: 190 loss: 1.8049536164170272 grad: -0.37793988981830384
iteration: 200 loss: 1.7162773964600588 grad: -0.36058210698076143
iteration: 210 loss: 1.6358933170489847 grad: -0.3447443237529786
iteration: 220 loss: 1.562692814624494 grad: -0.3302363514773978
iteration: 230 loss: 1.4957561282386367 grad: -0.3168982683963234
iteration: 240 loss: 1.4343138074104487 grad: -0.30459466732878887
iteration: 250 loss: 1.3777172566342002 grad: -0.2932101585129016
iteration: 260 loss: 1.3254159371308973 grad: -0.2826458215323515
iteration: 0 loss: 75.87475877291007 grad: 113.20857870826052
iteration: 10 loss: 22.050161526235488 grad: 1.3046873688178435
iteration: 20 loss: 13.805188612794154 grad: -1.386029122829342
iteration: 30 loss: 10.026422897433493 grad: -1.4210303531123856
iteration: 40 loss: 7.855091886135762 grad: -1.2640705918165696
iteration: 50 loss: 6.448473982377808 grad: -1.1110551338593138
iteration: 60 loss: 5.464834315325416 grad: -0.983270283768892
iteration: 70 loss: 4.739226548141645 grad: -0.8788411304677667
iteration: 80 loss: 4.182372467974966 grad: -0.7930921514695347
iteration: 90 loss: 3.7418027497937643 grad: -0.7218882589550195
iteration: 100 loss: 3.3846938383144747 grad: -0.6620296311451186
iteration: 110 loss: 3.0894778752028533 grad: -0.6111115936124268
iteration: 120 loss: 2.8414084161649953 grad: -0.5673284716308926
iteration: 130 loss: 2.6300664683509156 grad: -0.5293124452966429
iteration: 140 loss: 2.4478846136024996 grad: -0.4960148755092849
iteration: 150 loss: 2.2892357336570304 grad: -0.46662149177023654
iteration: 160 loss: 2.1498497164622923 grad: -0.4404920332755639
iteration: 170 loss: 2.02642836302051 grad: -0.41711703759812424
iteration: 180 loss: 1.9163842160364353 grad: -0.3960865914175645
iteration: 190 loss: 1.817659202685966 grad: -0.37706746583067713
iteration: 200 loss: 1.728596043065822 grad: -0.35978618273336666
iteration: 210 loss: 1.647845359652861 grad: -0.3440163231825763
iteration: 220 loss: 1.5742974467387036 grad: -0.32956890522714627
iteration: 230 loss: 1.5070313929941221 grad: -0.316285008844027
iteration: 240 loss: 1.4452766222861184 grad: -0.30403006473943994
iteration: 250 loss: 1.3883834578876881 grad: -0.2926893886619277
iteration: 260 loss: 1.335800335245701 grad: -0.2821646577852731
iteration: 0 loss: 72.27872902940986 grad: 93.21284533084946
iteration: 10 loss: 21.352584173859647 grad: 2.876342255062011
iteration: 20 loss: 13.499498437649436 grad: -0.9534541085246622
iteration: 30 loss: 9.855169619139422 grad: -1.2251228088382917
iteration: 40 loss: 7.742676550651967 grad: -1.156256709902546
iteration: 50 loss: 6.366645237458794 grad: -1.0454313368523782
iteration: 60 loss: 5.400978987603842 grad: -0.940540143277611
iteration: 70 loss: 4.686922550669723 grad: -0.8495586750742948
iteration: 80 loss: 4.13800981942152 grad: -0.7722026957267925
iteration: 90 loss: 3.7031920762271344 grad: -0.7065026835377364
iteration: 100 loss: 3.350425337727467 grad: -0.6504036755355914
iteration: 110 loss: 3.058598607983227 grad: -0.6021421353144726
iteration: 120 loss: 2.813247908246963 grad: -0.5602898325393062
iteration: 130 loss: 2.6041368439367614 grad: -0.5237109941109567
iteration: 140 loss: 2.423820574206316 grad: -0.4915050854040075
iteration: 150 loss: 2.266756973026159 grad: -0.46295547834702777
iteration: 160 loss: 2.128736537406646 grad: -0.43748812476603516
iteration: 170 loss: 2.0065053638168138 grad: -0.4146395807870877
iteration: 180 loss: 1.8975090758988038 grad: -0.39403261648252086
iteration: 190 loss: 1.7997147850683177 grad: -0.37535766453186403
iteration: 200 loss: 1.7114847147349792 grad: -0.358358671827705
iteration: 210 loss: 1.6314848244459923 grad: -0.34282224933377203
iteration: 220 loss: 1.5586176366931879 grad: -0.3285692935622198
iteration: 230 loss: 1.4919721117816236 grad: -0.31544846762736956
iteration: 240 loss: 1.4307857332069447 grad: -0.3033310896959117
iteration: 250 loss: 1.3744154723231077 grad: -0.2921070939852544
iteration: 260 loss: 1.3223152998744725 grad: -0.28168181516069185
iteration: 0 loss: 76.75626770418839 grad: 114.51245655970213
iteration: 10 loss: 22.45725958799056 grad: 1.2318613511368102
iteration: 20 loss: 13.9233735940289 grad: -0.9835941266588493
iteration: 30 loss: 10.076434104094005 grad: -1.151758944583721
iteration: 40 loss: 7.882181422208697 grad: -1.086975996675801
iteration: 50 loss: 6.465689699646233 grad: -0.9874467350866756
iteration: 60 loss: 5.476997975036945 grad: -0.8920210690767507
iteration: 70 loss: 4.7484645644832115 grad: -0.8083910969444641
iteration: 80 loss: 4.189753874226096 grad: -0.7367609397565071
iteration: 90 loss: 3.7479214630509223 grad: -0.6755871262374963
iteration: 100 loss: 3.3899065897585388 grad: -0.623124950242738
iteration: 110 loss: 3.094012668480652 grad: -0.5778315131496925
iteration: 120 loss: 2.8454185004970896 grad: -0.5384362498582483
iteration: 130 loss: 2.633659195836117 grad: -0.5039175200086488
iteration: 140 loss: 2.451137816501449 grad: -0.4734586997749657
iteration: 150 loss: 2.2922074981495992 grad: -0.4464058433980649
iteration: 160 loss: 2.1525844709264508 grad: -0.42223245474472365
iteration: 170 loss: 2.0289608131005976 grad: -0.4005116909585672
iteration: 180 loss: 1.9187419708240752 grad: -0.38089487562318547
iteration: 190 loss: 1.8198645772045396 grad: -0.36309499452910077
iteration: 200 loss: 1.7306673263310999 grad: -0.34687401205647284
iteration: 210 loss: 1.6497977265026251 grad: -0.3320330842539047
iteration: 220 loss: 1.5761436264114392 grad: -0.31840496265769014
iteration: 230 loss: 1.5087821679893005 grad: -0.30584805845231744
iteration: 240 loss: 1.446941206479592 grad: -0.294241770791447
iteration: 250 loss: 1.3899697872398478 grad: -0.28348278336633126
iteration: 260 loss: 1.3373152941903341 grad: -0.2734821075022158
iteration: 0 loss: 75.922809691746 grad: 86.83824794647593
iteration: 10 loss: 22.632110253220016 grad: 3.8109068117739495
iteration: 20 loss: 14.10389916995657 grad: -0.3576922538121323
iteration: 30 loss: 10.229651485022218 grad: -0.8538965592625198
iteration: 40 loss: 8.010656290682226 grad: -0.8978547788014037
iteration: 50 loss: 6.575373931524734 grad: -0.8514256314254215
iteration: 60 loss: 5.5724433149798225 grad: -0.7875558381114091
iteration: 70 loss: 4.832866388106678 grad: -0.7247858071397516
iteration: 80 loss: 4.265374512848997 grad: -0.667876315535943
iteration: 90 loss: 3.816403847059684 grad: -0.6175668189266939
iteration: 100 loss: 3.452476057153862 grad: -0.5733943074972284
iteration: 110 loss: 3.151605562777604 grad: -0.5345934108953241
iteration: 120 loss: 2.898765725028335 grad: -0.5003924113070457
iteration: 130 loss: 2.683341940593726 grad: -0.4701037003125742
iteration: 140 loss: 2.4976257191049793 grad: -0.4431420806068906
iteration: 150 loss: 2.3358854271715987 grad: -0.4190185116229551
iteration: 160 loss: 2.193771748363071 grad: -0.39732682099133115
iteration: 170 loss: 2.0679252900350464 grad: -0.3777297688383261
iteration: 180 loss: 1.955710521418289 grad: -0.35994660177747884
iteration: 190 loss: 1.855031049745073 grad: -0.34374260848371946
iteration: 200 loss: 1.764198655732178 grad: -0.3289205947946501
iteration: 210 loss: 1.68183869548011 grad: -0.3153140106247735
iteration: 220 loss: 1.6068206184729867 grad: -0.30278143332603147
iteration: 230 loss: 1.5382061579495134 grad: -0.2912021406618436
iteration: 240 loss: 1.475210167104747 grad: -0.28047255027542184
iteration: 250 loss: 1.4171706436642457 grad: -0.27050334569404494
iteration: 260 loss: 1.3635255243996405 grad: -0.26121714626893305
iteration: 0 loss: 75.5902158829587 grad: 131.2513511033785
iteration: 10 loss: 22.048471597160766 grad: 0.611873666693851
iteration: 20 loss: 13.799921047330848 grad: -1.4406279519340481
iteration: 30 loss: 10.021339518876609 grad: -1.432071085533174
iteration: 40 loss: 7.850056455426469 grad: -1.2725954129700328
iteration: 50 loss: 6.4433322850228585 grad: -1.1204399582105147
iteration: 60 loss: 5.459551927788405 grad: -0.9931070023008113
iteration: 70 loss: 4.733828719919447 grad: -0.8886064668892614
iteration: 80 loss: 4.176902415734317 grad: -0.802487102459827
iteration: 90 loss: 3.736303268996339 grad: -0.7307783076172123
iteration: 100 loss: 3.37920103962324 grad: -0.6703726839975961
iteration: 110 loss: 3.0840200497197077 grad: -0.6189128837147477
iteration: 120 loss: 2.8360067716047195 grad: -0.5746156581040848
iteration: 130 loss: 2.6247363761697007 grad: -0.5361224459890983
iteration: 140 loss: 2.4426368479518548 grad: -0.5023870873119476
iteration: 150 loss: 2.2840775337308514 grad: -0.47259442119779577
iteration: 160 loss: 2.1447856421444325 grad: -0.44610173433333117
iteration: 170 loss: 2.0214609631521356 grad: -0.42239649946876745
iteration: 180 loss: 1.911514542018019 grad: -0.40106561634699145
iteration: 190 loss: 1.8128871998748186 grad: -0.3817727901847259
iteration: 200 loss: 1.7239208477280954 grad: -0.36424170862924665
iteration: 210 loss: 1.6432655237562854 grad: -0.3482433901967727
iteration: 220 loss: 1.5698111079022552 grad: -0.3335865655856457
iteration: 230 loss: 1.5026364028557593 grad: -0.3201102880473389
iteration: 240 loss: 1.440970643337261 grad: -0.3076781996732505
iteration: 250 loss: 1.3841640363870367 grad: -0.29617404066116776
iteration: 260 loss: 1.3316649559432003 grad: -0.28549810093292505
iteration: 0 loss: 76.2473148431484 grad: 111.88985653220288
iteration: 10 loss: 22.384516568927605 grad: 1.4806510612558919
iteration: 20 loss: 13.953959324976246 grad: -0.9330822714019787
iteration: 30 loss: 10.117668129033602 grad: -1.1521891851975237
iteration: 40 loss: 7.920308554676467 grad: -1.1047138881862464
iteration: 50 loss: 6.498982125300452 grad: -1.0109178962009657
iteration: 60 loss: 5.505883580094922 grad: -0.9166535098487321
iteration: 70 loss: 4.773678860969664 grad: -0.8323400979543737
iteration: 80 loss: 4.211964989252217 grad: -0.7593244873095041
iteration: 90 loss: 3.7676715004665975 grad: -0.6965568826212818
iteration: 100 loss: 3.4076230352318984 grad: -0.64250621426555
iteration: 110 loss: 3.110031542643686 grad: -0.5957180790942551
iteration: 120 loss: 2.860005498887941 grad: -0.5549533532766899
iteration: 130 loss: 2.647026374214916 grad: -0.5191958312824092
iteration: 140 loss: 2.4634561856974786 grad: -0.4876228610580323
iteration: 150 loss: 2.3036164141159463 grad: -0.45956991972059
iteration: 160 loss: 2.1631986389874824 grad: -0.43449866774283624
iteration: 170 loss: 2.038875544908809 grad: -0.4119706699978173
iteration: 180 loss: 1.928037164324492 grad: -0.3916265587457943
iteration: 190 loss: 1.8286077591474994 grad: -0.3731697536870947
iteration: 200 loss: 1.7389160013823461 grad: -0.3563537983933613
iteration: 210 loss: 1.6576012088380594 grad: -0.3409725009223747
iteration: 220 loss: 1.5835444813821844 grad: -0.326852228939715
iteration: 230 loss: 1.5158173571583327 grad: -0.31384585679433663
iteration: 240 loss: 1.4536430048779452 grad: -0.3018279815375534
iteration: 250 loss: 1.396366524102384 grad: -0.2906911175842178
iteration: 260 loss: 1.3434319557260428 grad: -0.28034265007319026
iteration: 0 loss: 76.17317945088156 grad: 99.64017233806263
iteration: 10 loss: 21.9033189327609 grad: 2.2949940532871835
iteration: 20 loss: 13.717929410069598 grad: -1.1904536811312305
iteration: 30 loss: 9.969835517747697 grad: -1.3360195792499758
iteration: 40 loss: 7.814467156885981 grad: -1.219111775957266
iteration: 50 loss: 6.417074074228119 grad: -1.085267060711635
iteration: 60 loss: 5.439295799370939 grad: -0.9677253408105672
iteration: 70 loss: 4.717699523877275 grad: -0.8691489344426114
iteration: 80 loss: 4.16375099102412 grad: -0.7869192187824754
iteration: 90 loss: 3.725379692466752 grad: -0.7179215122896438
iteration: 100 loss: 3.369992063480574 grad: -0.6594944181188187
iteration: 110 loss: 3.076161086660291 grad: -0.6095320303468701
iteration: 120 loss: 2.829230954731098 grad: -0.5664018403945238
iteration: 130 loss: 2.6188434735299366 grad: -0.5288403060326083
iteration: 140 loss: 2.4374734621533207 grad: -0.4958637290597466
iteration: 150 loss: 2.279524034717104 grad: -0.46669949645700903
iteration: 160 loss: 2.1407472822197633 grad: -0.4407347751091366
iteration: 170 loss: 2.0178616794465514 grad: -0.4174785817159247
iteration: 180 loss: 1.9082925367804158 grad: -0.39653370926694076
iteration: 190 loss: 1.809991726860062 grad: -0.37757582503051024
iteration: 200 loss: 1.721309835093755 grad: -0.36033778026084806
iteration: 210 loss: 1.6409037885880273 grad: -0.34459772398206
iteration: 220 loss: 1.5676689965391069 grad: -0.33017001324088574
iteration: 230 loss: 1.5006887425144504 grad: -0.31689819653303075
iteration: 240 loss: 1.439195924574423 grad: -0.3046495480608883
iteration: 250 loss: 1.382543768946231 grad: -0.2933107727001825
iteration: 260 loss: 1.3301831563943447 grad: -0.28278460270239325
iteration: 0 loss: 76.19544959700015 grad: 130.10923356075304
iteration: 10 loss: 21.407878914920914 grad: -0.6553189688563796
iteration: 20 loss: 13.534678837899078 grad: -1.9692680136460337
iteration: 30 loss: 9.877551743535605 grad: -1.7322312135714533
iteration: 40 loss: 7.759127321632357 grad: -1.4770376550013091
iteration: 50 loss: 6.379580467229268 grad: -1.2738439842450082
iteration: 60 loss: 5.4115062190949175 grad: -1.115126752431242
iteration: 70 loss: 4.695684524315973 grad: -0.9894675938531828
iteration: 80 loss: 4.145430887147375 grad: -0.8881492584280085
iteration: 90 loss: 3.709570100303939 grad: -0.8050101009852866
iteration: 100 loss: 3.355976560979178 grad: -0.7357071796753936
iteration: 110 loss: 3.0634837501881353 grad: -0.6771344709971957
iteration: 120 loss: 2.817588729071593 grad: -0.6270285587889288
iteration: 130 loss: 2.6080271393874352 grad: -0.583708556074127
iteration: 140 loss: 2.4273336869138222 grad: -0.5459037518127929
iteration: 150 loss: 2.2699510333129016 grad: -0.5126376872211931
iteration: 160 loss: 2.1316581886193986 grad: -0.48314878304727804
iteration: 170 loss: 2.009192454297714 grad: -0.4568349951742873
iteration: 180 loss: 1.8999926338342035 grad: -0.43321454240835444
iteration: 190 loss: 1.8020204875687837 grad: -0.4118975765100673
iteration: 200 loss: 1.7136339948792738 grad: -0.39256543065058314
iteration: 210 loss: 1.633495715537017 grad: -0.374955202396261
iteration: 220 loss: 1.5605054233112903 grad: -0.35884814919591207
iteration: 230 loss: 1.4937498370513822 grad: -0.34406084744353377
iteration: 240 loss: 1.4324645978616983 grad: -0.33043838132227976
iteration: 250 loss: 1.376005151469392 grad: -0.31784904083218923
iteration: 260 loss: 1.3238241965469486 grad: -0.3061801547754909
iteration: 0 loss: 74.71826450728736 grad: 97.79685463064048
iteration: 10 loss: 22.45638716300782 grad: 2.641309094058564
iteration: 20 loss: 13.942031081627771 grad: -0.349497560772397
iteration: 30 loss: 10.095532551681304 grad: -0.7783939539814415
iteration: 40 loss: 7.898734924703645 grad: -0.8302561010490201
iteration: 50 loss: 6.479780591568959 grad: -0.7950769798615338
iteration: 60 loss: 5.489108668608671 grad: -0.7398234266153089
iteration: 70 loss: 4.759026820706987 grad: -0.6833909995777563
iteration: 80 loss: 4.199097432246245 grad: -0.6312622806736883
iteration: 90 loss: 3.75629097333678 grad: -0.5846821498673737
iteration: 100 loss: 3.3974842734564845 grad: -0.5435079228697868
iteration: 110 loss: 3.100936143552231 grad: -0.5071783570767551
iteration: 120 loss: 2.8517934396483557 grad: -0.4750558185108678
iteration: 130 loss: 2.6395681078394193 grad: -0.4465439306045544
iteration: 140 loss: 2.4566461492445755 grad: -0.4211217083531815
iteration: 150 loss: 2.2973679277881938 grad: -0.39834670626833524
iteration: 160 loss: 2.1574399605303203 grad: -0.3778474805174393
iteration: 170 loss: 2.0335468279895395 grad: -0.3593132834326503
iteration: 180 loss: 1.9230881080161866 grad: -0.34248398079816134
iteration: 190 loss: 1.8239957817549608 grad: -0.327141190883249
iteration: 200 loss: 1.7346048120236282 grad: -0.31310084884031186
iteration: 210 loss: 1.6535596818740819 grad: -0.30020710108525345
iteration: 220 loss: 1.5797457627524472 grad: -0.28832734113787706
iteration: 230 loss: 1.5122381494985264 grad: -0.27734818807546735
iteration: 240 loss: 1.4502629914492735 grad: -0.2671722288040174
iteration: 250 loss: 1.3931679013008265 grad: -0.2577153737441596
iteration: 260 loss: 1.340399051133359 grad: -0.24890470345316407
iteration: 0 loss: 76.39907074647428 grad: 116.3112582453864
iteration: 10 loss: 21.679072423602335 grad: 0.14788423121754135
iteration: 20 loss: 13.617541215608071 grad: -1.6868002402840963
iteration: 30 loss: 9.903109220770727 grad: -1.5361153239986756
iteration: 40 loss: 7.764539631956268 grad: -1.326085635582217
iteration: 50 loss: 6.377358131504769 grad: -1.1521886455603179
iteration: 60 loss: 5.4064034976813256 grad: -1.0139541266426932
iteration: 70 loss: 4.689656318268259 grad: -0.9033311013650203
iteration: 80 loss: 4.139314322710231 grad: -0.8134673708194238
iteration: 90 loss: 3.7037223088625733 grad: -0.7393129940388734
iteration: 100 loss: 3.3505381963976686 grad: -0.6772248534992096
iteration: 110 loss: 3.0584952624525283 grad: -0.6245579086612179
iteration: 120 loss: 2.813044252624381 grad: -0.5793642282395671
iteration: 130 loss: 2.6039003343147753 grad: -0.5401858846929124
iteration: 140 loss: 2.423590365161958 grad: -0.5059140564123599
iteration: 150 loss: 2.2665553306516646 grad: -0.47569255690955786
iteration: 160 loss: 2.1285755028015543 grad: -0.4488509317798678
iteration: 170 loss: 2.0063907389973625 grad: -0.4248573619358735
iteration: 180 loss: 1.8974428550609859 grad: -0.4032849969582877
iteration: 190 loss: 1.7996966603297386 grad: -0.38378752625184753
iteration: 200 loss: 1.7115130190668424 grad: -0.3660811992399421
iteration: 210 loss: 1.6315571261937458 grad: -0.3499314139187629
iteration: 220 loss: 1.5587311151741894 grad: -0.3351425872383128
iteration: 230 loss: 1.4921237936699236 grad: -0.32155041465145817
iteration: 240 loss: 1.430972639579605 grad: -0.30901589089751236
iteration: 250 loss: 1.374634708015244 grad: -0.29742064447292055
iteration: 260 loss: 1.3225641054635326 grad: -0.2866632628158102
iteration: 0 loss: 76.82473461915562 grad: 121.86568381345936
iteration: 10 loss: 22.552082624006456 grad: 0.5681221044348974
iteration: 20 loss: 14.028601214595234 grad: -1.104391261845092
iteration: 30 loss: 10.15889906228139 grad: -1.1757261095497507
iteration: 40 loss: 7.947063121080873 grad: -1.0842397287101333
iteration: 50 loss: 6.518256363292204 grad: -0.9765784427699447
iteration: 60 loss: 5.520769613736899 grad: -0.8789437779842639
iteration: 70 loss: 4.785745688676136 grad: -0.795144751032862
iteration: 80 loss: 4.222094280443468 grad: -0.7240499716162603
iteration: 90 loss: 3.776398563343925 grad: -0.6636249800298097
iteration: 100 loss: 3.4152931939212396 grad: -0.6119392713099537
iteration: 110 loss: 3.1168786841424 grad: -0.567381282056072
iteration: 120 loss: 2.8661944886815616 grad: -0.5286584196626996
iteration: 130 loss: 2.652677355283603 grad: -0.49474573063365623
iteration: 140 loss: 2.468659201660847 grad: -0.46483049742760524
iteration: 150 loss: 2.308440584339306 grad: -0.43826507284064564
iteration: 160 loss: 2.1676981252254666 grad: -0.41452963612591615
iteration: 170 loss: 2.0430935538548427 grad: -0.3932035967550828
iteration: 180 loss: 1.9320087065877323 grad: -0.3739438377001476
iteration: 190 loss: 1.832361597173903 grad: -0.3564681688771736
iteration: 200 loss: 1.7424760543398903 grad: -0.3405426932634905
iteration: 210 loss: 1.6609875853035356 grad: -0.3259721010418671
iteration: 220 loss: 1.5867742515102194 grad: -0.31259215921173344
iteration: 230 loss: 1.5189051390193113 grad: -0.3002638553664602
iteration: 240 loss: 1.456601415946848 grad: -0.28886879573797697
iteration: 250 loss: 1.3992065333584172 grad: -0.2783055610719487
iteration: 260 loss: 1.346163161441175 grad: -0.26848679941019576
iteration: 0 loss: 76.80366396279392 grad: 147.19989074105962
iteration: 10 loss: 21.525399113682543 grad: -1.8118445033206427
iteration: 20 loss: 13.577250130989631 grad: -2.1298187059101275
iteration: 30 loss: 9.883199406594713 grad: -1.7878527728152434
iteration: 40 loss: 7.7488942308841615 grad: -1.5068325263559945
iteration: 50 loss: 6.362380611143305 grad: -1.293557292778229
iteration: 60 loss: 5.391318946068912 grad: -1.1297259215917523
iteration: 70 loss: 4.674398918252212 grad: -1.0010307213765572
iteration: 80 loss: 4.123984809736033 grad: -0.8977185773741758
iteration: 90 loss: 3.688439214372548 grad: -0.8131762798131962
iteration: 100 loss: 3.3353997894779157 grad: -0.7428349066952404
iteration: 110 loss: 3.0435739263646777 grad: -0.6834634823359284
iteration: 120 loss: 2.7983897905238666 grad: -0.632724222416459
iteration: 130 loss: 2.589544948311165 grad: -0.5888894243096542
iteration: 140 loss: 2.409553185774381 grad: -0.5506575662181292
iteration: 150 loss: 2.2528460263442454 grad: -0.5170311567391386
iteration: 160 loss: 2.1151970079096247 grad: -0.4872338073951676
iteration: 170 loss: 1.9933412655492164 grad: -0.46065276387849063
iteration: 180 loss: 1.8847173587486479 grad: -0.4367983154055107
iteration: 190 loss: 1.7872879020642771 grad: -0.41527461979735436
iteration: 200 loss: 1.6994123308107356 grad: -0.39575839508713545
iteration: 210 loss: 1.6197549617360727 grad: -0.37798312632608766
iteration: 220 loss: 1.5472174447821254 grad: -0.3617272012326074
iteration: 230 loss: 1.4808883853615127 grad: -0.3468048861256378
iteration: 240 loss: 1.4200052579806404 grad: -0.3330593833104507
iteration: 250 loss: 1.3639252532272457 grad: -0.3203574331396673
iteration: 260 loss: 1.3121027082431944 grad: -0.30858507585061434
iteration: 0 loss: 74.92370131874955 grad: 118.29166319790967
iteration: 10 loss: 21.81527194697163 grad: 0.4311287020102376
iteration: 20 loss: 13.716826945195697 grad: -1.527847866635148
iteration: 30 loss: 9.972477789353578 grad: -1.4561654720939166
iteration: 40 loss: 7.815393619278811 grad: -1.2776034266060021
iteration: 50 loss: 6.416375967683424 grad: -1.1188204260444958
iteration: 60 loss: 5.43746149127781 grad: -0.9890142422079193
iteration: 70 loss: 4.715111805185146 grad: -0.8836368686113061
iteration: 80 loss: 4.160672539080059 grad: -0.7973108510223508
iteration: 90 loss: 3.7219869169712596 grad: -0.7256884334366132
iteration: 100 loss: 3.3664038099572777 grad: -0.6654956479456795
iteration: 110 loss: 3.0724581813682645 grad: -0.6142975802780167
iteration: 120 loss: 2.8254689209551214 grad: -0.5702738460558228
iteration: 130 loss: 2.615060769589077 grad: -0.5320481120938395
iteration: 140 loss: 2.433696868631118 grad: -0.49856606732016784
iteration: 150 loss: 2.2757722279692727 grad: -0.46900928407618453
iteration: 160 loss: 2.1370332420358618 grad: -0.4427342736235239
iteration: 170 loss: 2.014194336434767 grad: -0.4192289815887962
iteration: 180 loss: 1.9046779160978737 grad: -0.39808137146616673
iteration: 190 loss: 1.8064337530167078 grad: -0.3789564529462215
iteration: 200 loss: 1.7178109052116186 grad: -0.3615792736249125
iteration: 210 loss: 1.637465185222647 grad: -0.3457221720821502
iteration: 220 loss: 1.5642911878648276 grad: -0.3311951132278502
iteration: 230 loss: 1.49737160246894 grad: -0.3178382798186928
iteration: 240 loss: 1.4359388954861225 grad: -0.30551633457415595
iteration: 250 loss: 1.3793459824217857 grad: -0.2941139329628931
iteration: 260 loss: 1.3270435236219618 grad: -0.2835321820886564
iteration: 0 loss: 74.80213667988869 grad: 99.29960293064232
iteration: 10 loss: 21.860339846562884 grad: 2.6411399225822088
iteration: 20 loss: 13.67178298915823 grad: -0.7457771371108396
iteration: 30 loss: 9.935946099764578 grad: -1.0834926272258598
iteration: 40 loss: 7.789231969768273 grad: -1.0600422784064325
iteration: 50 loss: 6.397605322605449 grad: -0.974503024565676
iteration: 60 loss: 5.423759591093668 grad: -0.8848486815004137
iteration: 70 loss: 4.704936172916405 grad: -0.8039120480025963
iteration: 80 loss: 4.15300896482189 grad: -0.7336469236456811
iteration: 90 loss: 3.7161563966030204 grad: -0.673207513482657
iteration: 100 loss: 3.3619405770145105 grad: -0.6211595072378406
iteration: 110 loss: 3.0690348092908524 grad: -0.576109460552671
iteration: 120 loss: 2.822849908247173 grad: -0.5368631517874003
iteration: 130 loss: 2.6130731855737395 grad: -0.5024397435801983
iteration: 140 loss: 2.432211350638268 grad: -0.47204519754448926
iteration: 150 loss: 2.2746903893581805 grad: -0.4450384117990043
iteration: 160 loss: 2.1362791413800384 grad: -0.4209003440441594
iteration: 170 loss: 2.013708577929058 grad: -0.39920853939342815
iteration: 180 loss: 1.9044134917178532 grad: -0.37961692642299283
iteration: 190 loss: 1.8063530610450222 grad: -0.36184004081732934
iteration: 200 loss: 1.7178835781979118 grad: -0.34564076434851215
iteration: 210 loss: 1.637666484564017 grad: -0.3308207880430606
iteration: 220 loss: 1.564600801356226 grad: -0.3172131663259027
iteration: 230 loss: 1.4977727326585077 grad: -0.3046764726549546
iteration: 240 loss: 1.4364175607303713 grad: -0.2930901840075385
iteration: 250 loss: 1.379890475469231 grad: -0.2823510121261496
iteration: 260 loss: 1.327643988205306 grad: -0.2723699680609392
iteration: 0 loss: 74.63651525396762 grad: 145.19210033151847
iteration: 10 loss: 21.75287657637924 grad: -0.2771079440148628
iteration: 20 loss: 13.70543331812312 grad: -1.5231981954012008
iteration: 30 loss: 9.97535009389528 grad: -1.4671385000821702
iteration: 40 loss: 7.820331539822953 grad: -1.3012505823021496
iteration: 50 loss: 6.420466795876977 grad: -1.1464079871064783
iteration: 60 loss: 5.4402344985340445 grad: -1.0169052377156218
iteration: 70 loss: 4.716690731430886 grad: -0.9104218920811369
iteration: 80 loss: 4.1612888404435004 grad: -0.8224962026241233
iteration: 90 loss: 3.721856295260254 grad: -0.7491666351811035
iteration: 100 loss: 3.36570152901901 grad: -0.687321298741852
iteration: 110 loss: 3.0713205589218804 grad: -0.63458845025007
iteration: 120 loss: 2.8240005150899132 grad: -0.5891664232035536
iteration: 130 loss: 2.6133415971932084 grad: -0.5496778443882078
iteration: 140 loss: 2.431788330641625 grad: -0.5150590947330466
iteration: 150 loss: 2.2737216641470366 grad: -0.4844795625081844
iteration: 160 loss: 2.1348773488502544 grad: -0.4572831960404069
iteration: 170 loss: 2.0119617132013032 grad: -0.43294609959230645
iteration: 180 loss: 1.9023909625809459 grad: -0.4110455406470188
iteration: 190 loss: 1.8041100897106512 grad: -0.3912370784606781
iteration: 200 loss: 1.7154644428713832 grad: -0.37323750757408014
iteration: 210 loss: 1.635106935878949 grad: -0.35681200128699775
iteration: 220 loss: 1.5619298842786768 grad: -0.34176431869945667
iteration: 230 loss: 1.4950141747004553 grad: -0.3279292694595637
iteration: 240 loss: 1.4335908401268087 grad: -0.31516685948285206
iteration: 250 loss: 1.377011650600583 grad: -0.3033577008316836
iteration: 260 loss: 1.3247263471015354 grad: -0.2923993815174261
iteration: 0 loss: 75.74733819501705 grad: 103.06839016552811
iteration: 10 loss: 22.136861381879974 grad: 2.0807257954055776
iteration: 20 loss: 13.825196239367973 grad: -0.7685444923533054
iteration: 30 loss: 10.045591276711134 grad: -1.0465851682955205
iteration: 40 loss: 7.876016523178185 grad: -1.0174329313723653
iteration: 50 loss: 6.4699445195072425 grad: -0.9350841936917359
iteration: 60 loss: 5.4859814263869255 grad: -0.8499758800959379
iteration: 70 loss: 4.759615126950641 grad: -0.7733187125673486
iteration: 80 loss: 4.201822272780233 grad: -0.7067380533814489
iteration: 90 loss: 3.7602660931546037 grad: -0.6493903245437949
iteration: 100 loss: 3.402188544361624 grad: -0.5999234487852481
iteration: 110 loss: 3.10605220557632 grad: -0.5570346524876961
iteration: 120 loss: 2.8571229523700326 grad: -0.5196093251310798
iteration: 130 loss: 2.644984856487223 grad: -0.48673171400825815
iteration: 140 loss: 2.4620689569638587 grad: -0.4576596196160275
iteration: 150 loss: 2.302744436301096 grad: -0.4317928955759702
iteration: 160 loss: 2.1627366194814965 grad: -0.4086448602556342
iteration: 170 loss: 2.0387425550642218 grad: -0.3878187338149131
iteration: 180 loss: 1.9281701845997388 grad: -0.3689889399249624
iteration: 190 loss: 1.8289571455466516 grad: -0.35188648949711654
iteration: 200 loss: 1.7394422424256184 grad: -0.336287605806588
iteration: 210 loss: 1.6582725673054648 grad: -0.3220048636917274
iteration: 220 loss: 1.584335253891988 grad: -0.3088802608050442
iteration: 230 loss: 1.5167065724004218 grad: -0.29677977069013073
iteration: 240 loss: 1.4546134380186893 grad: -0.28558903456080303
iteration: 250 loss: 1.397403942225744 grad: -0.27520993169727537
iteration: 260 loss: 1.3445245342321537 grad: -0.26555783137547806
iteration: 0 loss: 76.97023581826566 grad: 126.40786214466948
iteration: 10 loss: 22.036240506154215 grad: -0.1866237829710829
iteration: 20 loss: 13.758546436850441 grad: -1.4958050061746395
iteration: 30 loss: 9.98506916030442 grad: -1.4229283174012977
iteration: 40 loss: 7.821680156315964 grad: -1.257358872673103
iteration: 50 loss: 6.4211955285181 grad: -1.1058785658782124
iteration: 60 loss: 5.442010798657066 grad: -0.9799774026858272
iteration: 70 loss: 4.719682740009293 grad: -0.8768072123526515
iteration: 80 loss: 4.165310010872459 grad: -0.791819306446966
iteration: 90 loss: 3.726670226716413 grad: -0.7210657994041247
iteration: 100 loss: 3.371099947883067 grad: -0.6614738331949784
iteration: 110 loss: 3.077137114907559 grad: -0.6107148211845652
iteration: 120 loss: 2.830107052687419 grad: -0.5670274175128445
iteration: 130 loss: 2.6196407808450783 grad: -0.529069610297461
iteration: 140 loss: 2.438206625990543 grad: -0.49580791050343553
iteration: 150 loss: 2.2802036137101243 grad: -0.466437034892776
iteration: 160 loss: 2.1413811550867816 grad: -0.44032210642138686
iteration: 170 loss: 2.018455904208457 grad: -0.416956912241097
iteration: 180 loss: 1.9088518940048247 grad: -0.3959335089314089
iteration: 190 loss: 1.810520074433994 grad: -0.3769198618261509
iteration: 200 loss: 1.7218103468249368 grad: -0.359643212809875
iteration: 210 loss: 1.6413791190486997 grad: -0.3438775712290886
iteration: 220 loss: 1.5681213977893342 grad: -0.3294342035147957
iteration: 230 loss: 1.5011201486435395 grad: -0.3161543272125731
iteration: 240 loss: 1.4396080142126018 grad: -0.30390344279112697
iteration: 250 loss: 1.3829380123823862 grad: -0.2925668948495682
iteration: 260 loss: 1.3305608517105776 grad: -0.28204636534792393
iteration: 0 loss: 76.9863402152055 grad: 136.18763658014754
iteration: 10 loss: 21.418262936475085 grad: -1.6471103339287039
iteration: 20 loss: 13.530013560419464 grad: -2.1188225704586077
iteration: 30 loss: 9.861213351704794 grad: -1.7531221058281532
iteration: 40 loss: 7.738618813334721 grad: -1.4673349941033706
iteration: 50 loss: 6.358158635519472 grad: -1.2562230802242444
iteration: 60 loss: 5.390505625930928 grad: -1.0960827602309577
iteration: 70 loss: 4.675625022602204 grad: -0.9710112309956929
iteration: 80 loss: 4.126481754253911 grad: -0.8708757809608146
iteration: 90 loss: 3.6917478675618027 grad: -0.7890339083064153
iteration: 100 loss: 3.3392324381901 grad: -0.720976021054945
iteration: 110 loss: 3.0477434883036123 grad: -0.6635428284193359
iteration: 120 loss: 2.8027710842403724 grad: -0.6144602630166585
iteration: 130 loss: 2.5940523420051083 grad: -0.5720527190047734
iteration: 140 loss: 2.4141271154306776 grad: -0.5350601455792787
iteration: 150 loss: 2.2574445576958833 grad: -0.5025178629688223
iteration: 160 loss: 2.119790388118872 grad: -0.47367550826453053
iteration: 170 loss: 1.9979083108241453 grad: -0.4479410992956104
iteration: 180 loss: 1.889243004173047 grad: -0.4248416337717816
iteration: 190 loss: 1.7917615067747399 grad: -0.4039948208246864
iteration: 200 loss: 1.7038264855116474 grad: -0.38508846013261533
iteration: 210 loss: 1.6241046370915186 grad: -0.3678651713506586
iteration: 220 loss: 1.551499375290074 grad: -0.3521109292341644
iteration: 230 loss: 1.48510061850092 grad: -0.3376463470863128
iteration: 240 loss: 1.4241468209977277 grad: -0.32431997271190216
iteration: 250 loss: 1.3679959047350911 grad: -0.3120030770364622
iteration: 260 loss: 1.3161027516513804 grad: -0.3005855629849689
iteration: 0 loss: 73.3044366696343 grad: 95.88171484577411
iteration: 10 loss: 21.830042411850915 grad: 3.2180594673820684
iteration: 20 loss: 13.65480419555609 grad: -0.8021126358476726
iteration: 30 loss: 9.922782776637508 grad: -1.161017701466742
iteration: 40 loss: 7.778939551881939 grad: -1.1191644380992296
iteration: 50 loss: 6.389469284223238 grad: -1.018744095192938
iteration: 60 loss: 5.417210807834181 grad: -0.9190098689750135
iteration: 70 loss: 4.699564895475842 grad: -0.8311933428390672
iteration: 80 loss: 4.148525979846041 grad: -0.7560676494857044
iteration: 90 loss: 3.7123559184651227 grad: -0.6920692412107787
iteration: 100 loss: 3.3586737890041976 grad: -0.6373309255904348
iteration: 110 loss: 3.066192141534512 grad: -0.5901909391702768
iteration: 120 loss: 2.8203492580188056 grad: -0.549282969434389
iteration: 130 loss: 2.6108520121203003 grad: -0.5135120606060701
iteration: 140 loss: 2.4302212882188625 grad: -0.4820061510685649
iteration: 150 loss: 2.2728935136640924 grad: -0.4540693137683529
iteration: 160 loss: 2.1346453378158583 grad: -0.42914303231158224
iteration: 170 loss: 2.012213656324796 grad: -0.4067758085196632
iteration: 180 loss: 1.90303781504781 grad: -0.386599786219907
iteration: 190 loss: 1.8050805337515654 grad: -0.3683128769514983
iteration: 200 loss: 1.7167008899102918 grad: -0.3516650790308532
iteration: 210 loss: 1.6365625377708801 grad: -0.3364479592883449
iteration: 220 loss: 1.563566273345011 grad: -0.322486515761752
iteration: 230 loss: 1.4967997366629375 grad: -0.3096328376155723
iteration: 240 loss: 1.435499381241822 grad: -0.2977611285341601
iteration: 250 loss: 1.3790213595726444 grad: -0.286763771071898
iteration: 260 loss: 1.326818979683594 grad: -0.27654819126423585
iteration: 0 loss: 76.59587839804753 grad: 112.98377908851478
iteration: 10 loss: 21.520469026471968 grad: 0.24198108711814784
iteration: 20 loss: 13.53804604100035 grad: -1.7044353465477446
iteration: 30 loss: 9.857619162089225 grad: -1.5774629022837598
iteration: 40 loss: 7.734801845232472 grad: -1.3724234927850272
iteration: 50 loss: 6.35578449613393 grad: -1.1968528634298292
iteration: 60 loss: 5.389508587126701 grad: -1.055093572442586
iteration: 70 loss: 4.675681522341744 grad: -0.9407020344879093
iteration: 80 loss: 4.127297381927705 grad: -0.8473301652087736
iteration: 90 loss: 3.6930987357267524 grad: -0.7700575899471532
iteration: 100 loss: 3.340956351544264 grad: -0.7052433236394856
iteration: 110 loss: 3.0497243990646647 grad: -0.6502039676461067
iteration: 120 loss: 2.804925951784381 grad: -0.6029441497478228
iteration: 130 loss: 2.5963214711915827 grad: -0.5619605472802179
iteration: 140 loss: 2.416467347939736 grad: -0.526104451035873
iteration: 150 loss: 2.259824527178601 grad: -0.49448585080042806
iteration: 160 loss: 2.1221872091425045 grad: -0.4664060482278472
iteration: 170 loss: 2.000305252683864 grad: -0.4413097752763091
iteration: 180 loss: 1.8916278417672898 grad: -0.4187507285975002
iteration: 190 loss: 1.794125339910765 grad: -0.39836642832167424
iteration: 200 loss: 1.7061628851853874 grad: -0.379859637576183
iteration: 210 loss: 1.626409022272872 grad: -0.3629844578073463
iteration: 220 loss: 1.5537685538695933 grad: -0.34753579927681166
iteration: 230 loss: 1.487332446323142 grad: -0.33334131816060397
iteration: 240 loss: 1.4263399466601847 grad: -0.32025517765545086
iteration: 250 loss: 1.3701495772490324 grad: -0.3081531730800654
iteration: 260 loss: 1.3182166745813944 grad: -0.29692888780437704
iteration: 0 loss: 75.44305708609376 grad: 120.0203046178734
iteration: 10 loss: 21.774831420922077 grad: 0.13582229502863294
iteration: 20 loss: 13.73049653988586 grad: -1.667805018684669
iteration: 30 loss: 9.996154322369854 grad: -1.5223051328040336
iteration: 40 loss: 7.837950723393575 grad: -1.3131538157362335
iteration: 50 loss: 6.435745541324153 grad: -1.13930733528388
iteration: 60 loss: 5.453688814620281 grad: -1.001203156565511
iteration: 70 loss: 4.728673101216662 grad: -0.8908699212460613
iteration: 80 loss: 4.172060475047767 grad: -0.8014014058353891
iteration: 90 loss: 3.731618446151846 grad: -0.7276946974458283
iteration: 100 loss: 3.3746123734753466 grad: -0.6660715909194268
iteration: 110 loss: 3.079506111782848 grad: -0.6138663049399111
iteration: 120 loss: 2.8315623232515876 grad: -0.569119443356134
iteration: 130 loss: 2.620362360090729 grad: -0.5303672263418183
iteration: 140 loss: 2.438336073483821 grad: -0.49649828188216016
iteration: 150 loss: 2.279852870336733 grad: -0.466655778458957
iteration: 160 loss: 2.140639333784954 grad: -0.44016968957767
iteration: 170 loss: 2.0173944401676223 grad: -0.41650920280873766
iteration: 180 loss: 1.907528444966002 grad: -0.39524876049593444
iteration: 190 loss: 1.80898148386636 grad: -0.3760434586373238
iteration: 200 loss: 1.720094907626679 grad: -0.35861096704304746
iteration: 210 loss: 1.6395183144716363 grad: -0.3427180613992045
iteration: 220 loss: 1.5661412499474947 grad: -0.32817046346008705
iteration: 230 loss: 1.499042270700515 grad: -0.3148050862554447
iteration: 240 loss: 1.4374504372986765 grad: -0.3024840499933452
iteration: 250 loss: 1.3807158400382336 grad: -0.2910900171886991
iteration: 260 loss: 1.3282867813301726 grad: -0.2805225216284089
iteration: 0 loss: 71.82321712808219 grad: 112.30748376269571
iteration: 10 loss: 21.216215764508558 grad: 1.8624562035953203
iteration: 20 loss: 13.445561859914877 grad: -1.1638087889163877
iteration: 30 loss: 9.82186075966919 grad: -1.3189422606691443
iteration: 40 loss: 7.718592229113049 grad: -1.2116726614033912
iteration: 50 loss: 6.347686616389844 grad: -1.0830435432289207
iteration: 60 loss: 5.385227226864944 grad: -0.9684781352513389
iteration: 70 loss: 4.673356803329377 grad: -0.8717027837940801
iteration: 80 loss: 4.1260363725828855 grad: -0.7905998122447108
iteration: 90 loss: 3.6924387450974128 grad: -0.7223139764147052
iteration: 100 loss: 3.3406434919079815 grad: -0.6643314153171672
iteration: 110 loss: 3.0496131931753223 grad: -0.6146361163168845
iteration: 120 loss: 2.8049302918213717 grad: -0.5716529760612352
iteration: 130 loss: 2.5963891792663905 grad: -0.5341563142843486
iteration: 140 loss: 2.4165663155004453 grad: -0.5011879933662524
iteration: 150 loss: 2.259934849977154 grad: -0.4719929633772789
iteration: 160 loss: 2.122296570273234 grad: -0.4459706270148299
iteration: 170 loss: 2.0004061339410657 grad: -0.4226386192565085
iteration: 180 loss: 1.8917157983826407 grad: -0.4016058555365014
iteration: 190 loss: 1.79419791143755 grad: -0.3825523802877827
iteration: 200 loss: 1.7062188963227527 grad: -0.3652141872707424
iteration: 210 loss: 1.6264481278161922 grad: -0.34937168578572064
iteration: 220 loss: 1.5537909393198623 grad: -0.33484085728336477
iteration: 230 loss: 1.4873386293343174 grad: -0.321466412968292
iteration: 240 loss: 1.426330644701755 grad: -0.3091164524636302
iteration: 250 loss: 1.3701256192587772 grad: -0.2976782584540125
iteration: 260 loss: 1.3181789421160333 grad: -0.28705495856385266
iteration: 0 loss: 73.90778983234408 grad: 145.24611251440894
iteration: 10 loss: 21.540955097087217 grad: 0.009598657796589971
iteration: 20 loss: 13.531849263883549 grad: -1.3323414019607882
iteration: 30 loss: 9.844650544660992 grad: -1.3473656741648754
iteration: 40 loss: 7.7194361593011225 grad: -1.2230707748671876
iteration: 50 loss: 6.339685979146783 grad: -1.0922601873091609
iteration: 60 loss: 5.373440959092435 grad: -0.9773646122182477
iteration: 70 loss: 4.660003840662077 grad: -0.8802889990895593
iteration: 80 loss: 4.112170613631489 grad: -0.798740290609482
iteration: 90 loss: 3.6785838847682415 grad: -0.7299211394818006
iteration: 100 loss: 3.327063490904369 grad: -0.6713798226915273
iteration: 110 loss: 3.036437943044017 grad: -0.6211387452917249
iteration: 120 loss: 2.7922179743565967 grad: -0.5776424364615036
iteration: 130 loss: 2.5841588203358943 grad: -0.5396733891945836
iteration: 140 loss: 2.4048154547778555 grad: -0.5062753063983967
iteration: 150 loss: 2.248649416974992 grad: -0.476691722261699
iteration: 160 loss: 2.1114565193586468 grad: -0.4503191050967982
iteration: 170 loss: 1.9899886985288255 grad: -0.42667158389731186
iteration: 180 loss: 1.881697374885789 grad: -0.4053544939882578
iteration: 190 loss: 1.7845551392427461 grad: -0.38604447278177845
iteration: 200 loss: 1.6969292594621534 grad: -0.36847439017621075
iteration: 210 loss: 1.6174902697318265 grad: -0.35242185121951
iteration: 220 loss: 1.5451448052122032 grad: -0.33770035099287676
iteration: 230 loss: 1.4789855055503474 grad: -0.32415241194200456
iteration: 240 loss: 1.418253137735023 grad: -0.3116442144655975
iteration: 250 loss: 1.3623076006069823 grad: -0.30006136142562745
iteration: 260 loss: 1.3106054752225562 grad: -0.2893055107839335
iteration: 0 loss: 72.69436657522353 grad: 139.89360098994544
iteration: 10 loss: 21.248053771308893 grad: -0.35465750922891004
iteration: 20 loss: 13.489862668410938 grad: -1.6581358186868935
iteration: 30 loss: 9.850798486846191 grad: -1.5350129434887783
iteration: 40 loss: 7.735876706567027 grad: -1.3461295498601076
iteration: 50 loss: 6.357005271070235 grad: -1.1821220226631595
iteration: 60 loss: 5.3891123341903615 grad: -1.0478414131727214
iteration: 70 loss: 4.673499628289846 grad: -0.9382160590124183
iteration: 80 loss: 4.123576820191829 grad: -0.8478897565162782
iteration: 90 loss: 3.6881563439859417 grad: -0.772570138975256
iteration: 100 loss: 3.3350794482156 grad: -0.7090060736004894
iteration: 110 loss: 3.0431487712904857 grad: -0.6547585502650826
iteration: 120 loss: 2.7978377276584236 grad: -0.6079878633024991
iteration: 130 loss: 2.588865215838847 grad: -0.5672914176743199
iteration: 140 loss: 2.4087546301403893 grad: -0.5315866370317588
iteration: 150 loss: 2.251941287551827 grad: -0.500027518327455
iteration: 160 loss: 2.1141996490962978 grad: -0.471945012356043
iteration: 170 loss: 1.992264397983945 grad: -0.4468040525974863
iteration: 180 loss: 1.8835730337336827 grad: -0.4241722372217146
iteration: 190 loss: 1.7860868951816387 grad: -0.4036967331093622
iteration: 200 loss: 1.6981641229032325 grad: -0.38508704214356826
iteration: 210 loss: 1.618467813581116 grad: -0.3681019949579113
iteration: 220 loss: 1.5458985112404249 grad: -0.3525398282965503
iteration: 230 loss: 1.4795438408488157 grad: -0.3382305369235249
iteration: 240 loss: 1.418640419135086 grad: -0.32502992143982434
iteration: 250 loss: 1.3625446922164883 grad: -0.31281491361692315
iteration: 260 loss: 1.310710354168598 grad: -0.30147987350244554
iteration: 0 loss: 77.40033595881589 grad: 122.70026072812101
iteration: 10 loss: 21.613183701684378 grad: -0.5270781285476525
iteration: 20 loss: 13.598428402424155 grad: -2.0033912854198523
iteration: 30 loss: 9.89781462654142 grad: -1.7385543671952803
iteration: 40 loss: 7.764142734799572 grad: -1.4686928436052356
iteration: 50 loss: 6.378919680247898 grad: -1.2593552975663647
iteration: 60 loss: 5.408773900128588 grad: -1.0983946442874437
iteration: 70 loss: 4.692336038280246 grad: -0.9722652110817744
iteration: 80 loss: 4.142072487371272 grad: -0.8712867268019087
iteration: 90 loss: 3.706449256918528 grad: -0.7888456675261124
iteration: 100 loss: 3.3531824036887716 grad: -0.7203800277694723
iteration: 110 loss: 3.0610342680188296 grad: -0.6626758784882762
iteration: 120 loss: 2.8154704751331834 grad: -0.6134171386069419
iteration: 130 loss: 2.606213848259482 grad: -0.5708984923534441
iteration: 140 loss: 2.425795103528344 grad: -0.5338394450571625
iteration: 150 loss: 2.2686570463404014 grad: -0.5012613402760379
iteration: 160 loss: 2.1305806632706417 grad: -0.47240423896400946
iteration: 170 loss: 2.0083059290109304 grad: -0.4466695598016984
iteration: 180 loss: 1.8992744613211803 grad: -0.42357972101287356
iteration: 190 loss: 1.8014507143074794 grad: -0.4027492317092239
iteration: 200 loss: 1.7131951289217218 grad: -0.3838636416922719
iteration: 210 loss: 1.6331724584982277 grad: -0.36666398100452213
iteration: 220 loss: 1.560284403857333 grad: -0.35093509762022346
iteration: 230 loss: 1.493619363097299 grad: -0.33649680515412705
iteration: 240 loss: 1.4324144342103038 grad: -0.3231970846019823
iteration: 250 loss: 1.3760263242848767 grad: -0.3109068069617962
iteration: 260 loss: 1.323908823465362 grad: -0.2995155954730419
iteration: 0 loss: 74.1907025857624 grad: 102.5787963627943
iteration: 10 loss: 21.689246877758727 grad: 2.0610609942646176
iteration: 20 loss: 13.612383707429423 grad: -1.112297677707591
iteration: 30 loss: 9.902293627203479 grad: -1.2770192971098608
iteration: 40 loss: 7.765566564009615 grad: -1.174990891414089
iteration: 50 loss: 6.378976538103953 grad: -1.0497438790554505
iteration: 60 loss: 5.408140590159539 grad: -0.9376530285533782
iteration: 70 loss: 4.69134089620039 grad: -0.8429008947103562
iteration: 80 loss: 4.140890650251591 grad: -0.7635586421992447
iteration: 90 loss: 3.705179301851372 grad: -0.6968492283202482
iteration: 100 loss: 3.351881570539676 grad: -0.6402952096331043
iteration: 110 loss: 3.0597362919196383 grad: -0.5919013397133335
iteration: 120 loss: 2.814195227004389 grad: -0.5501071190142924
iteration: 130 loss: 2.6049728022308045 grad: -0.5136987054136901
iteration: 140 loss: 2.4245945533858007 grad: -0.4817282411324246
iteration: 150 loss: 2.267500053053275 grad: -0.4534498326376252
iteration: 160 loss: 2.129468261536162 grad: -0.4282710891100137
iteration: 170 loss: 2.007237888404523 grad: -0.4057169949664452
iteration: 180 loss: 1.8982497732004049 grad: -0.3854030304442984
iteration: 190 loss: 1.8004679064055464 grad: -0.3670150893011629
iteration: 200 loss: 1.7122524698298052 grad: -0.35029436787452617
iteration: 210 loss: 1.6322680907789866 grad: -0.33502589837851343
iteration: 220 loss: 1.5594164303708402 grad: -0.3210297691974373
iteration: 230 loss: 1.4927859023840606 grad: -0.3081543414883375
iteration: 240 loss: 1.4316136552599594 grad: -0.2962709614855157
iteration: 250 loss: 1.3752564675460235 grad: -0.2852698032403307
iteration: 260 loss: 1.3231682126460411 grad: -0.27505657319934734
iteration: 0 loss: 73.78834504112025 grad: 159.1362609499754
iteration: 10 loss: 21.80188958850309 grad: -1.249669253443069
iteration: 20 loss: 13.764708202717635 grad: -1.9917898134921943
iteration: 30 loss: 10.016033145445569 grad: -1.728040277333316
iteration: 40 loss: 7.848137130724312 grad: -1.4737434620989238
iteration: 50 loss: 6.439855401559684 grad: -1.2725120604020395
iteration: 60 loss: 5.453971882848443 grad: -1.1148328813445576
iteration: 70 loss: 4.72651105265438 grad: -0.9895631562055032
iteration: 80 loss: 4.168316622712692 grad: -0.888311010263514
iteration: 90 loss: 3.7268451230654382 grad: -0.8051013503260168
iteration: 100 loss: 3.3691713070563556 grad: -0.7356823426856246
iteration: 110 loss: 3.073639185721566 grad: -0.6769886878070555
iteration: 120 loss: 2.8254341498596163 grad: -0.626773345333701
iteration: 130 loss: 2.6140865600056546 grad: -0.5833606140794885
iteration: 140 loss: 2.431991911654305 grad: -0.5454800972645002
iteration: 150 loss: 2.273496037470977 grad: -0.5121537286204383
iteration: 160 loss: 2.1343090849236845 grad: -0.4826177380605936
iteration: 170 loss: 2.0111184135225812 grad: -0.45626788600294427
iteration: 180 loss: 1.901325971755837 grad: -0.4326204167001512
iteration: 190 loss: 1.8028658972238274 grad: -0.41128379355045935
iteration: 200 loss: 1.7140751757031631 grad: -0.3919379436637816
iteration: 210 loss: 1.6336002098389806 grad: -0.37431881032635284
iteration: 220 loss: 1.5603281961435145 grad: -0.35820671070646587
iteration: 230 loss: 1.4933359609735817 grad: -0.3434174582205066
iteration: 240 loss: 1.4318512906682124 grad: -0.32979551893091347
iteration: 250 loss: 1.375223339897771 grad: -0.31720868217767284
iteration: 260 loss: 1.3228997283977977 grad: -0.305543870999378
iteration: 0 loss: 77.21202240763677 grad: 122.2065971604929
iteration: 10 loss: 22.38036143587161 grad: -0.033676529299047034
iteration: 20 loss: 13.977411125243345 grad: -1.450192200766522
iteration: 30 loss: 10.138346333289416 grad: -1.3633725198280517
iteration: 40 loss: 7.937595545547558 grad: -1.2020244524779033
iteration: 50 loss: 6.513761705970989 grad: -1.0582094933683557
iteration: 60 loss: 5.51882840635273 grad: -0.9394115473998492
iteration: 70 loss: 4.7852348614250415 grad: -0.8421074710135596
iteration: 80 loss: 4.222431713241886 grad: -0.7618385715383443
iteration: 90 loss: 3.777257652207688 grad: -0.6948783449487712
iteration: 100 loss: 3.4164796107442355 grad: -0.6383594453028438
iteration: 110 loss: 3.1182715389370306 grad: -0.5901166998436953
iteration: 120 loss: 2.8677159613405987 grad: -0.5485130238391231
iteration: 130 loss: 2.654276017996298 grad: -0.5122998488534208
iteration: 140 loss: 2.470300386187029 grad: -0.4805140843019346
iteration: 150 loss: 2.3101005815213864 grad: -0.45240387442706936
iteration: 160 loss: 2.16936056807303 grad: -0.4273752831613496
iteration: 170 loss: 2.044747094555491 grad: -0.40495380084547206
iteration: 180 loss: 1.933645486398663 grad: -0.38475629489858554
iteration: 190 loss: 1.8339762164499172 grad: -0.36647034881560786
iteration: 200 loss: 1.7440648660708988 grad: -0.3498388705960637
iteration: 210 loss: 1.6625482025932465 grad: -0.3346484973015577
iteration: 220 loss: 1.588305199307301 grad: -0.32072076399765886
iteration: 230 loss: 1.5204056048110033 grad: -0.30790530782956765
iteration: 240 loss: 1.4580710693828058 grad: -0.2960745864963878
iteration: 250 loss: 1.4006453945535213 grad: -0.2851197353530389
iteration: 260 loss: 1.3475715040733782 grad: -0.274947289126735
iteration: 0 loss: 75.21579460126856 grad: 118.01621147018434
iteration: 10 loss: 21.72441714591481 grad: 0.5673413639473652
iteration: 20 loss: 13.685723588702082 grad: -1.4210882834889844
iteration: 30 loss: 9.9664443584098 grad: -1.3885924555426126
iteration: 40 loss: 7.8196407259742955 grad: -1.2353113966928977
iteration: 50 loss: 6.424976874171659 grad: -1.091645331172343
iteration: 60 loss: 5.447850640396651 grad: -0.9710312986490304
iteration: 70 loss: 4.7261025678217266 grad: -0.8714312882573837
iteration: 80 loss: 4.171693690574929 grad: -0.7888667078648283
iteration: 90 loss: 3.732760928433932 grad: -0.7197746320733198
iteration: 100 loss: 3.376800322051291 grad: -0.661333384422709
iteration: 110 loss: 3.082422996833057 grad: -0.6113776004972602
iteration: 120 loss: 2.83498789597012 grad: -0.5682531319048787
iteration: 130 loss: 2.6241407660768346 grad: -0.5306895205493964
iteration: 140 loss: 2.4423554350366263 grad: -0.4977018855800397
iteration: 150 loss: 2.2840319125756734 grad: -0.4685185331735887
iteration: 160 loss: 2.144918438208937 grad: -0.4425281682708697
iteration: 170 loss: 2.021729543615369 grad: -0.4192413740707037
iteration: 180 loss: 1.911886805614441 grad: -0.39826232525159205
iteration: 190 loss: 1.813338696018284 grad: -0.3792678309582388
iteration: 200 loss: 1.724432766596444 grad: -0.361991652889315
iteration: 210 loss: 1.6438232695648944 grad: -0.34621264960492865
iteration: 220 loss: 1.570403270335274 grad: -0.33174572189585105
iteration: 230 loss: 1.50325400511842 grad: -0.3184348289300676
iteration: 240 loss: 1.4416065854527667 grad: -0.30614755051767834
iteration: 250 loss: 1.3848126782831143 grad: -0.2947708150536149
iteration: 260 loss: 1.3323218019520275 grad: -0.28420751463072524
iteration: 0 loss: 75.99361059520216 grad: 127.7292610389758
iteration: 10 loss: 22.100970494734216 grad: -0.02551135886656891
iteration: 20 loss: 13.823409661146032 grad: -1.550253835292442
iteration: 30 loss: 10.031821919622457 grad: -1.459204269699335
iteration: 40 loss: 7.855066837374166 grad: -1.2810274173472234
iteration: 50 loss: 6.445511403535213 grad: -1.123250457703846
iteration: 60 loss: 5.460097075237255 grad: -0.9938592582716355
iteration: 70 loss: 4.733376543039743 grad: -0.8884726593423905
iteration: 80 loss: 4.1758225480196165 grad: -0.8019158802310422
iteration: 90 loss: 3.7348222479520037 grad: -0.7299670061246161
iteration: 100 loss: 3.3774630334523037 grad: -0.6694199526545368
iteration: 110 loss: 3.0821194373526684 grad: -0.6178737572770165
iteration: 120 loss: 2.8340067022646838 grad: -0.5735236041551123
iteration: 130 loss: 2.6226798193823595 grad: -0.5349991395467757
iteration: 140 loss: 2.4405534077104263 grad: -0.5012471919330435
iteration: 150 loss: 2.2819877753726856 grad: -0.47144814851063344
iteration: 160 loss: 2.142703909515397 grad: -0.44495632582791855
iteration: 170 loss: 2.01939725316569 grad: -0.4212571414238356
iteration: 180 loss: 1.9094757759904 grad: -0.3999360343110275
iteration: 190 loss: 1.8108781003418037 grad: -0.38065565081692726
iteration: 200 loss: 1.7219445521900525 grad: -0.3631388997167081
iteration: 210 loss: 1.6413240199224426 grad: -0.3471562204283548
iteration: 220 loss: 1.56790554592139 grad: -0.33251590972879663
iteration: 230 loss: 1.5007673215725597 grad: -0.3190566940056074
iteration: 240 loss: 1.439138135688278 grad: -0.3066419683714219
iteration: 250 loss: 1.382367871216874 grad: -0.29515528623163945
iteration: 260 loss: 1.3299046682966698 grad: -0.284496796420905
iteration: 0 loss: 76.81479619680394 grad: 131.57688830206143
iteration: 10 loss: 22.404886381145676 grad: 0.6364211648388809
iteration: 20 loss: 14.01258554927848 grad: -1.6648525489348802
iteration: 30 loss: 10.167925472455552 grad: -1.6010559143975287
iteration: 40 loss: 7.958702822042452 grad: -1.398658124189252
iteration: 50 loss: 6.527986493041722 grad: -1.2185689807943643
iteration: 60 loss: 5.528025685505781 grad: -1.0722078361049219
iteration: 70 loss: 4.790834669091882 grad: -0.9540939382931675
iteration: 80 loss: 4.22545075113542 grad: -0.8578608872840671
iteration: 90 loss: 3.7784085331930495 grad: -0.778411868732562
iteration: 100 loss: 3.4162647043073195 grad: -0.7119361632606283
iteration: 110 loss: 3.117049358736288 grad: -0.6556171273525259
iteration: 120 loss: 2.865745934337817 grad: -0.6073608968783137
iteration: 130 loss: 2.6517484510787157 grad: -0.5655926572007726
iteration: 140 loss: 2.4673566656157906 grad: -0.5291118840104312
iteration: 150 loss: 2.3068469601025754 grad: -0.49699079102284094
iteration: 160 loss: 2.1658777120362007 grad: -0.4685028829115332
iteration: 170 loss: 2.0410967920225134 grad: -0.4430722184479437
iteration: 180 loss: 1.9298754289696014 grad: -0.4202369481717547
iteration: 190 loss: 1.8301234536628395 grad: -0.3996227729168752
iteration: 200 loss: 1.7401583327759496 grad: -0.38092337441397406
iteration: 210 loss: 1.6586105898493935 grad: -0.36388580579949176
iteration: 220 loss: 1.5843543544305332 grad: -0.3482994547022299
iteration: 230 loss: 1.5164555896211906 grad: -0.3339876112406125
iteration: 240 loss: 1.4541329675343646 grad: -0.3208009578525306
iteration: 250 loss: 1.396727932715335 grad: -0.30861249300288895
iteration: 260 loss: 1.3436815336614762 grad: -0.2973135361433535
iteration: 0 loss: 74.03812637792144 grad: 127.97162102424677
iteration: 10 loss: 21.392809240776863 grad: -0.37169038276786
iteration: 20 loss: 13.5636792430085 grad: -1.814303342249549
iteration: 30 loss: 9.89753067124647 grad: -1.6422391891070462
iteration: 40 loss: 7.770236194662175 grad: -1.4154081510596481
iteration: 50 loss: 6.384924058644463 grad: -1.2274779961257924
iteration: 60 loss: 5.413261206362801 grad: -1.0782770674956765
iteration: 70 loss: 4.69519313081016 grad: -0.9591261040615693
iteration: 80 loss: 4.143516612176923 grad: -0.8625373240155638
iteration: 90 loss: 3.7067479576932616 grad: -0.7829840587573806
iteration: 100 loss: 3.3525761495114406 grad: -0.7164861508291622
iteration: 110 loss: 3.059720578956041 grad: -0.6601609342561287
iteration: 120 loss: 2.8136063033178043 grad: -0.6118905917661437
iteration: 130 loss: 2.6039224221257924 grad: -0.5700934760189207
iteration: 140 loss: 2.4231729034480107 grad: -0.533568956482156
iteration: 150 loss: 2.265779721929448 grad: -0.5013915006198856
iteration: 160 loss: 2.1275077352922214 grad: -0.472837418661373
iteration: 170 loss: 2.005084427011711 grad: -0.4473334189744238
iteration: 180 loss: 1.895941709232912 grad: -0.42441991017232017
iteration: 190 loss: 1.798036458800983 grad: -0.40372442117388296
iteration: 200 loss: 1.709723168679499 grad: -0.38494206936908393
iteration: 210 loss: 1.6296618965546423 grad: -0.367821012069543
iteration: 220 loss: 1.5567506152744335 grad: -0.3521514719379192
iteration: 230 loss: 1.4900747470320643 grad: -0.33775736046925203
iteration: 240 loss: 1.4288690019212398 grad: -0.3244898141928684
iteration: 250 loss: 1.372488161640211 grad: -0.3122221558761201
iteration: 260 loss: 1.3203844568683136 grad: -0.3008459292403486
iteration: 0 loss: 73.64081032785963 grad: 120.55540616325177
iteration: 10 loss: 21.469330810504207 grad: 0.7247487277484426
iteration: 20 loss: 13.539785064901274 grad: -1.4474801750535138
iteration: 30 loss: 9.868115269404473 grad: -1.450577225311693
iteration: 40 loss: 7.744888228470824 grad: -1.2924996544466483
iteration: 50 loss: 6.3639844831955505 grad: -1.140214090501692
iteration: 60 loss: 5.395942285578754 grad: -1.0123104461680765
iteration: 70 loss: 4.680716984839654 grad: -0.906982601257075
iteration: 80 loss: 4.131265464026538 grad: -0.8199091882219186
iteration: 90 loss: 3.696253218991718 grad: -0.7472151751051808
iteration: 100 loss: 3.3434848535166566 grad: -0.6858494203202636
iteration: 110 loss: 3.0517652372377184 grad: -0.6334826707903598
iteration: 120 loss: 2.8065819868601136 grad: -0.5883430945600815
iteration: 130 loss: 2.5976701577288126 grad: -0.5490745177253447
iteration: 140 loss: 2.4175677953095476 grad: -0.514628591298421
iteration: 150 loss: 2.260722439400648 grad: -0.48418584753914806
iteration: 160 loss: 2.1229183643451752 grad: -0.45709846674162474
iteration: 170 loss: 2.0008979814650214 grad: -0.4328487028090642
iteration: 180 loss: 1.8921048151288977 grad: -0.4110184657725422
iteration: 190 loss: 1.7945048760431994 grad: -0.39126685438878317
iteration: 200 loss: 1.7064599168260925 grad: -0.3733133868402233
iteration: 210 loss: 1.626635821569752 grad: -0.3569253505962559
iteration: 220 loss: 1.5539352818084147 grad: -0.34190815931905083
iteration: 230 loss: 1.4874475736850483 grad: -0.3280979275105481
iteration: 240 loss: 1.4264105798127027 grad: -0.3153556975847799
iteration: 250 loss: 1.3701817126664702 grad: -0.30356291051232115
iteration: 260 loss: 1.3182153994319543 grad: -0.2926178214008836
iteration: 0 loss: 77.3828117898697 grad: 148.13487335486272
iteration: 10 loss: 21.71597818485942 grad: -1.9798911927382066
iteration: 20 loss: 13.6763765881851 grad: -2.1728177692799537
iteration: 30 loss: 9.951254803668462 grad: -1.7980361492889285
iteration: 40 loss: 7.801138960865501 grad: -1.5057723559968161
iteration: 50 loss: 6.40480071945992 grad: -1.2884602587977865
iteration: 60 loss: 5.427020657130139 grad: -1.1231848137335132
iteration: 70 loss: 4.7052317587111 grad: -0.9940306367247418
iteration: 80 loss: 4.151136497169443 grad: -0.8906634178644499
iteration: 90 loss: 3.712713311548751 grad: -0.8062438567224743
iteration: 100 loss: 3.3573628427482785 grad: -0.7361059586171101
iteration: 110 loss: 3.0636393187965867 grad: -0.6769729022730918
iteration: 120 loss: 2.8168680383711777 grad: -0.6264836489639335
iteration: 130 loss: 2.6066751481498613 grad: -0.5828982204272624
iteration: 140 loss: 2.4255233928784863 grad: -0.5449085745486333
iteration: 150 loss: 2.2678068835514917 grad: -0.5115138772597814
iteration: 160 loss: 2.1292711194410003 grad: -0.48193620674537335
iteration: 170 loss: 2.0066298071451456 grad: -0.4555623189137774
iteration: 180 loss: 1.8973048439173992 grad: -0.43190261243735384
iteration: 190 loss: 1.7992456879459164 grad: -0.41056168999596626
iteration: 200 loss: 1.7108012434348645 grad: -0.391216891182085
iteration: 210 loss: 1.6306272937034587 grad: -0.3736024034795433
iteration: 220 loss: 1.55761849758185 grad: -0.35749734038089587
iteration: 230 loss: 1.4908576758379737 grad: -0.3427166833966542
iteration: 240 loss: 1.4295774733839772 grad: -0.329104320166681
iteration: 250 loss: 1.3731310154828613 grad: -0.31652763634961667
iteration: 260 loss: 1.3209691916299562 grad: -0.30487327291795585
iteration: 0 loss: 73.33811125330865 grad: 108.2775108927608
iteration: 10 loss: 21.398562814522805 grad: 1.0365678857894127
iteration: 20 loss: 13.531896345703382 grad: -1.4449624979966744
iteration: 30 loss: 9.867969787124737 grad: -1.4214663668265803
iteration: 40 loss: 7.746701765745456 grad: -1.2578756249154537
iteration: 50 loss: 6.366673870136352 grad: -1.1066574870143333
iteration: 60 loss: 5.3990726790485 grad: -0.9813008419050261
iteration: 70 loss: 4.684049554559269 grad: -0.8787322748145782
iteration: 80 loss: 4.134663001223419 grad: -0.7942644994860464
iteration: 90 loss: 3.6996370041987245 grad: -0.7239155080781843
iteration: 100 loss: 3.3468102232132795 grad: -0.6646194607481011
iteration: 110 loss: 3.0550075445160227 grad: -0.614066329555109
iteration: 120 loss: 2.8097284888606073 grad: -0.5705139876858298
iteration: 130 loss: 2.6007152592075147 grad: -0.5326369580113673
iteration: 140 loss: 2.4205102086518555 grad: -0.49941501449521275
iteration: 150 loss: 2.2635634586333917 grad: -0.4700531845342339
iteration: 160 loss: 2.1256608002835162 grad: -0.44392448991654715
iteration: 170 loss: 2.003545497305161 grad: -0.4205287290317154
iteration: 180 loss: 1.894661509125647 grad: -0.3994625214281405
iteration: 190 loss: 1.796975019099591 grad: -0.38039729403695854
iteration: 200 loss: 1.708847788502267 grad: -0.3630629154098889
iteration: 210 loss: 1.6289456092615018 grad: -0.34723538864109404
iteration: 220 loss: 1.556171020761419 grad: -0.33272749333339524
iteration: 230 loss: 1.489613113498432 grad: -0.3193815943857309
iteration: 240 loss: 1.4285095681180295 grad: -0.30706406035563966
iteration: 250 loss: 1.3722175898691271 grad: -0.295660890121229
iteration: 260 loss: 1.3201914004867334 grad: -0.2850742557792978
iteration: 0 loss: 75.25909379614858 grad: 114.45280406090934
iteration: 10 loss: 21.58807470221708 grad: 0.9450431078578538
iteration: 20 loss: 13.542521186868271 grad: -1.2484508465646427
iteration: 30 loss: 9.854221474049899 grad: -1.3222291826329589
iteration: 40 loss: 7.73169457201307 grad: -1.2053794531897428
iteration: 50 loss: 6.354305276257326 grad: -1.0759446062289757
iteration: 60 loss: 5.389601526652768 grad: -0.9617468186083895
iteration: 70 loss: 4.677031007866265 grad: -0.8654134256993188
iteration: 80 loss: 4.129592798339962 grad: -0.784677380296275
iteration: 90 loss: 3.6960882936397286 grad: -0.7166854475652311
iteration: 100 loss: 3.3444476056243233 grad: -0.65894397347778
iteration: 110 loss: 3.053573172071579 grad: -0.6094534693562095
iteration: 120 loss: 2.8090248126033903 grad: -0.5666499290837295
iteration: 130 loss: 2.600590328590732 grad: -0.5293147680783519
iteration: 140 loss: 2.4208463275273804 grad: -0.49649436175222295
iteration: 150 loss: 2.2642687226684344 grad: -0.467436570792939
iteration: 160 loss: 2.12666284720064 grad: -0.44154272274197703
iteration: 170 loss: 2.0047868902796506 grad: -0.41833176810429407
iteration: 180 loss: 1.89609630931566 grad: -0.39741355673969136
iteration: 190 loss: 1.7985662223138041 grad: -0.3784688251484028
iteration: 200 loss: 1.7105653921225068 grad: -0.36123410179099985
iteration: 210 loss: 1.6307651417921327 grad: -0.3454902258955297
iteration: 220 loss: 1.5580724136180337 grad: -0.33105353700324847
iteration: 230 loss: 1.4915798280988135 grad: -0.3177690534766074
iteration: 240 loss: 1.430527915045993 grad: -0.30550514463761846
iteration: 250 loss: 1.3742761937657468 grad: -0.2941493342518887
iteration: 260 loss: 1.3222807764146969 grad: -0.2836049683312687
iteration: 0 loss: 71.63523919565645 grad: 121.05365893320965
iteration: 10 loss: 21.24838456345558 grad: 1.682103364482153
iteration: 20 loss: 13.387428457026855 grad: -1.1582082173916983
iteration: 30 loss: 9.754335112056673 grad: -1.3168135994421424
iteration: 40 loss: 7.655664547964204 grad: -1.2160532817090535
iteration: 50 loss: 6.291424834448993 grad: -1.090354024625242
iteration: 60 loss: 5.335228273647607 grad: -0.9765891591653508
iteration: 70 loss: 4.628755623870637 grad: -0.8796351003637407
iteration: 80 loss: 4.085986841044484 grad: -0.7979617533730591
iteration: 90 loss: 3.65621891781699 grad: -0.7289867371031056
iteration: 100 loss: 3.3076615279261534 grad: -0.6703161248934835
iteration: 110 loss: 3.0193889388368587 grad: -0.6199824149425993
iteration: 120 loss: 2.77707385759314 grad: -0.5764263501332951
iteration: 130 loss: 2.5705821966057556 grad: -0.5384236055301622
iteration: 140 loss: 2.3925468581309857 grad: -0.5050113944015171
iteration: 150 loss: 2.2374856637034926 grad: -0.475427943052528
iteration: 160 loss: 2.1012359727384053 grad: -0.4490656309897435
iteration: 170 loss: 1.980580980783171 grad: -0.42543552429077364
iteration: 180 loss: 1.8729960238995937 grad: -0.4041406961613515
iteration: 190 loss: 1.7764722643209594 grad: -0.38485612633115607
iteration: 200 loss: 1.6893915842369593 grad: -0.36731347607499654
iteration: 210 loss: 1.610436156558287 grad: -0.35128947305120684
iteration: 220 loss: 1.5385219862664186 grad: -0.3365969786074156
iteration: 230 loss: 1.4727493308412043 grad: -0.32307806060523403
iteration: 240 loss: 1.4123652066429135 grad: -0.31059857665300983
iteration: 250 loss: 1.3567346817168817 grad: -0.299043903856106
iteration: 260 loss: 1.3053186454473524 grad: -0.2883155458982277
iteration: 0 loss: 76.01222217248146 grad: 95.37515592993984
iteration: 10 loss: 21.977555306320276 grad: 3.352292092622825
iteration: 20 loss: 13.775392441093903 grad: -0.8145037692821376
iteration: 30 loss: 10.025935996243954 grad: -1.1831373479942924
iteration: 40 loss: 7.866918121019932 grad: -1.1418553055573257
iteration: 50 loss: 6.465147868946133 grad: -1.039656666399604
iteration: 60 loss: 5.483100158726591 grad: -0.9376403643748042
iteration: 70 loss: 4.757630124112356 grad: -0.847631930579338
iteration: 80 loss: 4.200264017808712 grad: -0.7705677102821089
iteration: 90 loss: 3.758907115101134 grad: -0.7049029820585047
iteration: 100 loss: 3.4009153745590663 grad: -0.6487448263312375
iteration: 110 loss: 3.1048077953571256 grad: -0.6003956025663456
iteration: 120 loss: 2.855879176715255 grad: -0.5584544308271325
iteration: 130 loss: 2.643728752355318 grad: -0.5217964046996812
iteration: 140 loss: 2.4607955798249104 grad: -0.4895245621818827
iteration: 150 loss: 2.30145304918795 grad: -0.4609226193634114
iteration: 160 loss: 2.161428621286632 grad: -0.4354155116073624
iteration: 170 loss: 2.0374203431164957 grad: -0.41253829004295905
iteration: 180 loss: 1.9268365305665198 grad: -0.3919121348366588
iteration: 190 loss: 1.827614858087641 grad: -0.3732259820771915
iteration: 200 loss: 1.73809399170531 grad: -0.3562224443915159
iteration: 210 loss: 1.6569208011647292 grad: -0.34068697753719956
iteration: 220 loss: 1.5829821656382344 grad: -0.3264394946385165
iteration: 230 loss: 1.5153540971384494 grad: -0.31332783012261506
iteration: 240 loss: 1.453263264138348 grad: -0.3012226081079403
iteration: 250 loss: 1.3960575302985903 grad: -0.29001318367787393
iteration: 260 loss: 1.3431831388695834 grad: -0.2796044093120158
iteration: 0 loss: 76.15212731694984 grad: 167.40871219278986
iteration: 10 loss: 21.57415291644086 grad: -2.3118749662862053
iteration: 20 loss: 13.631481819774887 grad: -2.1147356069156635
iteration: 30 loss: 9.921817970943142 grad: -1.7408560920356546
iteration: 40 loss: 7.775806450616941 grad: -1.4647750280235572
iteration: 50 loss: 6.381634118439376 grad: -1.2593167973713153
iteration: 60 loss: 5.405568707556085 grad: -1.1017606608387458
iteration: 70 loss: 4.685288351454264 grad: -0.9776899727379824
iteration: 80 loss: 4.132542011931988 grad: -0.8777941057563359
iteration: 90 loss: 3.6953259141578614 grad: -0.7958365461005088
iteration: 100 loss: 3.341055652171731 grad: -0.7275049384706616
iteration: 110 loss: 3.04830009590482 grad: -0.6697362418814689
iteration: 120 loss: 2.802398324865573 grad: -0.6203034545270052
iteration: 130 loss: 2.592988917652995 grad: -0.577553647211122
iteration: 140 loss: 2.412545541239524 grad: -0.5402371405206634
iteration: 150 loss: 2.255471763074597 grad: -0.5073933531801613
iteration: 160 loss: 2.1175212103818755 grad: -0.4782728304044851
iteration: 170 loss: 1.9954145560190724 grad: -0.4522829132142743
iteration: 180 loss: 1.8865796669361274 grad: -0.42894918409849137
iteration: 190 loss: 1.7889711245981992 grad: -0.40788764626846696
iteration: 200 loss: 1.7009422409312884 grad: -0.3887843371186061
iteration: 210 loss: 1.6211526029169776 grad: -0.3713801764012292
iteration: 220 loss: 1.5485001628481685 grad: -0.35545955713077515
iteration: 230 loss: 1.4820706032709163 grad: -0.3408416506403028
iteration: 240 loss: 1.4210990646919195 grad: -0.32737370591331605
iteration: 250 loss: 1.3649408564145398 grad: -0.31492583223475423
iteration: 260 loss: 1.313048786033133 grad: -0.3033868977078692
iteration: 0 loss: 71.38786979181045 grad: 106.96723442807914
iteration: 10 loss: 21.186196733076237 grad: 2.8218830592850646
iteration: 20 loss: 13.329600243815714 grad: -0.7292728461861304
iteration: 30 loss: 9.721328532057173 grad: -1.118004771466858
iteration: 40 loss: 7.637256039083737 grad: -1.1001729965672287
iteration: 50 loss: 6.281429979521652 grad: -1.0117256639848633
iteration: 60 loss: 5.330273280625037 grad: -0.9179054288419981
iteration: 70 loss: 4.626937461229897 grad: -0.8330854410537404
iteration: 80 loss: 4.08618226088075 grad: -0.7594980205021478
iteration: 90 loss: 3.6577367444232634 grad: -0.6962755591260317
iteration: 100 loss: 3.3100610310441634 grad: -0.6418991618060887
iteration: 110 loss: 3.022380565122975 grad: -0.5948898982030928
iteration: 120 loss: 2.780462591494556 grad: -0.5539815312797809
iteration: 130 loss: 2.574233973757228 grad: -0.5181358142460294
iteration: 140 loss: 2.3963679599433734 grad: -0.4865137125392177
iteration: 150 loss: 2.2414096927286664 grad: -0.45843892679672615
iteration: 160 loss: 2.1052154008427184 grad: -0.4333647566425016
iteration: 170 loss: 1.9845815362848704 grad: -0.4108468515940978
iteration: 180 loss: 1.87699288700254 grad: -0.39052165704053093
iteration: 190 loss: 1.7804474410373918 grad: -0.3720896239318047
iteration: 200 loss: 1.693332064244673 grad: -0.35530218708936534
iteration: 210 loss: 1.6143326022411202 grad: -0.3399516554276439
iteration: 220 loss: 1.5423677877002278 grad: -0.32586333170206805
iteration: 230 loss: 1.4765399167165676 grad: -0.31288933621155673
iteration: 240 loss: 1.4160975366284303 grad: -0.30090373552986316
iteration: 250 loss: 1.3604068690605013 grad: -0.28979867502504064
iteration: 260 loss: 1.30892967412344 grad: -0.2794812877022358
iteration: 0 loss: 77.54965383514337 grad: 140.15150000829397
iteration: 10 loss: 21.84735729151711 grad: -1.9484527312397144
iteration: 20 loss: 13.764677327563133 grad: -2.1795290270522267
iteration: 30 loss: 10.00879992235704 grad: -1.7863192326809858
iteration: 40 loss: 7.842329071510366 grad: -1.4907968936572242
iteration: 50 loss: 6.43672638962934 grad: -1.273670300780842
iteration: 60 loss: 5.453179307602485 grad: -1.1094398301340203
iteration: 70 loss: 4.727489377431126 grad: -0.9815316501374514
iteration: 80 loss: 4.170578269794698 grad: -0.8793893618997033
iteration: 90 loss: 3.7300201694950954 grad: -0.7960925993741828
iteration: 100 loss: 3.372989546185884 grad: -0.7269523819282286
iteration: 110 loss: 3.0779051569759095 grad: -0.6686937027406191
iteration: 120 loss: 2.8300063533829416 grad: -0.6189669413476728
iteration: 130 loss: 2.6188620848373914 grad: -0.5760462220299117
iteration: 140 loss: 2.4368954908465885 grad: -0.5386373156526989
iteration: 150 loss: 2.2784723348919544 grad: -0.5057517848138318
iteration: 160 loss: 2.1393172529172015 grad: -0.4766223939652029
iteration: 170 loss: 2.016128228450229 grad: -0.45064493263806876
iteration: 180 loss: 1.9063150663345452 grad: -0.42733736770249625
iteration: 190 loss: 1.8078177596611236 grad: -0.40631062214047675
iteration: 200 loss: 1.7189776893830475 grad: -0.38724731449882643
iteration: 210 loss: 1.638444577660266 grad: -0.3698860507507766
iteration: 220 loss: 1.5651081412381906 grad: -0.35400965473678425
iteration: 230 loss: 1.4980471279274175 grad: -0.3394362358827726
iteration: 240 loss: 1.4364907935616678 grad: -0.32601232999849605
iteration: 250 loss: 1.3797894188076663 grad: -0.3136075746652229
iteration: 260 loss: 1.32739148670275 grad: -0.3021105343468893
iteration: 0 loss: 73.32566336807702 grad: 123.20635804385128
iteration: 10 loss: 21.834758080152906 grad: 1.5253474721804958
iteration: 20 loss: 13.690703249108394 grad: -0.9725010068297909
iteration: 30 loss: 9.952087887301849 grad: -1.180519523489974
iteration: 40 loss: 7.800388838227553 grad: -1.1208614490181419
iteration: 50 loss: 6.404906911007154 grad: -1.0207186950131937
iteration: 60 loss: 5.428322111249517 grad: -0.923329014959561
iteration: 70 loss: 4.707566661433537 grad: -0.8374169629915015
iteration: 80 loss: 4.154262869751082 grad: -0.763530098552054
iteration: 90 loss: 3.716416110841072 grad: -0.700251449157381
iteration: 100 loss: 3.3614730148704606 grad: -0.6458763467916114
iteration: 110 loss: 3.0680295057492333 grad: -0.598864955774844
iteration: 120 loss: 2.8214437736859592 grad: -0.5579343942732138
iteration: 130 loss: 2.6113666985104995 grad: -0.52204519850684
iteration: 140 loss: 2.4302793117915162 grad: -0.4903619266415782
iteration: 150 loss: 2.2725891913997573 grad: -0.462212795391289
iteration: 160 loss: 2.1340517721717087 grad: -0.43705521463248176
iteration: 170 loss: 2.011388123532139 grad: -0.41444818163895747
iteration: 180 loss: 1.9020256287301642 grad: -0.3940307253481635
iteration: 190 loss: 1.8039178563812626 grad: -0.37550522721191837
iteration: 200 loss: 1.715416806620231 grad: -0.35862453090583774
iteration: 210 loss: 1.635180605328432 grad: -0.3431819515942008
iteration: 220 loss: 1.5621056905297337 grad: -0.32900349357704983
iteration: 230 loss: 1.4952762380369842 grad: -0.31594175091025667
iteration: 240 loss: 1.4339259264200088 grad: -0.3038710951338619
iteration: 250 loss: 1.3774086696289487 grad: -0.2926838524423341
iteration: 260 loss: 1.3251759581188376 grad: -0.2822872460628335
iteration: 0 loss: 72.66133814723311 grad: 100.20432307170668
iteration: 10 loss: 21.97580999854356 grad: 3.3291432277515622
iteration: 20 loss: 13.788900328188818 grad: -0.6252653042303258
iteration: 30 loss: 10.036328060272725 grad: -1.0590823623497463
iteration: 40 loss: 7.872837485280095 grad: -1.054675336360115
iteration: 50 loss: 6.46776862368597 grad: -0.9737124022050032
iteration: 60 loss: 5.483521212340747 grad: -0.8852029681129183
iteration: 70 loss: 4.756610254738255 grad: -0.8044776425045418
iteration: 80 loss: 4.198299892287196 grad: -0.7341603539508516
iteration: 90 loss: 3.756323396738935 grad: -0.6736042759684795
iteration: 100 loss: 3.397927018276236 grad: -0.62143618844488
iteration: 110 loss: 3.10155938685495 grad: -0.5762800725801756
iteration: 120 loss: 2.8524694417113676 grad: -0.5369448719251227
iteration: 130 loss: 2.6402259726877344 grad: -0.5024484392023567
iteration: 140 loss: 2.4572474570230622 grad: -0.4719943014304804
iteration: 150 loss: 2.2978931368578412 grad: -0.44493892052546735
iteration: 160 loss: 2.157880599762145 grad: -0.4207611477153673
iteration: 170 loss: 2.033900911145834 grad: -0.39903680409084946
iteration: 180 loss: 1.9233573937513442 grad: -0.37941843878750336
iteration: 190 loss: 1.8241841167331254 grad: -0.36161949288091577
iteration: 200 loss: 1.7347171225836642 grad: -0.34540198058596644
iteration: 210 loss: 1.653601360435914 grad: -0.33056690358179647
iteration: 220 loss: 1.5797222975620158 grad: -0.31694676607098815
iteration: 230 loss: 1.5121549057602435 grad: -0.3043996998999129
iteration: 240 loss: 1.4501250871360505 grad: -0.29280482547369463
iteration: 250 loss: 1.3929801422172288 grad: -0.28205856485281866
iteration: 260 loss: 1.340165902883448 grad: -0.2720716923095434
iteration: 0 loss: 75.80712533604496 grad: 112.84343671236695
iteration: 10 loss: 21.6467861763207 grad: 1.5851842220686754
iteration: 20 loss: 13.599718827818668 grad: -1.3117125816943327
iteration: 30 loss: 9.89713337938319 grad: -1.4023531789019108
iteration: 40 loss: 7.7626937622788486 grad: -1.266834730217202
iteration: 50 loss: 6.377035129169929 grad: -1.1222657789864932
iteration: 60 loss: 5.40668403386272 grad: -0.9975819105147721
iteration: 70 loss: 4.690187853821209 grad: -0.8939696168963058
iteration: 80 loss: 4.13995210756461 grad: -0.8080477562105375
iteration: 90 loss: 3.704403857398533 grad: -0.7362527298165487
iteration: 100 loss: 3.3512356269175143 grad: -0.6756454591340534
iteration: 110 loss: 3.059195933727882 grad: -0.623942289975809
iteration: 120 loss: 2.813742458423886 grad: -0.5793932829608699
iteration: 130 loss: 2.604593586220496 grad: -0.5406549529374733
iteration: 140 loss: 2.424277675063794 grad: -0.5066876415017193
iteration: 150 loss: 2.2672363999890464 grad: -0.47667855595688774
iteration: 160 loss: 2.129250335924677 grad: -0.44998534690550196
iteration: 170 loss: 2.0070594594461166 grad: -0.42609490016220963
iteration: 180 loss: 1.8981056203158666 grad: -0.4045931096110891
iteration: 190 loss: 1.8003536245154133 grad: -0.38514252636288615
iteration: 200 loss: 1.7121643188118634 grad: -0.3674656721433451
iteration: 210 loss: 1.6322028773224289 grad: -0.3513324539663528
iteration: 220 loss: 1.5593714141482038 grad: -0.3365505746641908
iteration: 230 loss: 1.4927587207862416 grad: -0.3229581530148727
iteration: 240 loss: 1.4316022625171585 grad: -0.31041798975826657
iteration: 250 loss: 1.3752590851052198 grad: -0.29881307167801285
iteration: 260 loss: 1.3231832884630834 grad: -0.2880430158935588
iteration: 0 loss: 74.48444625192624 grad: 134.6530128029602
iteration: 10 loss: 21.54017932058954 grad: -0.20321901547265556
iteration: 20 loss: 13.541792676058714 grad: -1.613947074778119
iteration: 30 loss: 9.847970860825315 grad: -1.5206693660362072
iteration: 40 loss: 7.7193703276720536 grad: -1.3377902702290605
iteration: 50 loss: 6.338318678052516 grad: -1.1738084362184753
iteration: 60 loss: 5.371713262140722 grad: -1.0385446482112344
iteration: 70 loss: 4.658310313130724 grad: -0.9281180041440218
iteration: 80 loss: 4.110664127370414 grad: -0.837355726121132
iteration: 90 loss: 3.677311711801229 grad: -0.7619091077472258
iteration: 100 loss: 3.3260274750759065 grad: -0.6984354034037371
iteration: 110 loss: 3.035621431938249 grad: -0.6444178070568307
iteration: 120 loss: 2.791597973312698 grad: -0.5979599563822313
iteration: 130 loss: 2.583711424294589 grad: -0.5576206511501217
iteration: 140 loss: 2.4045181651681973 grad: -0.5222920126052865
iteration: 150 loss: 2.248482003905296 grad: -0.4911119440509932
iteration: 160 loss: 2.111401211183916 grad: -0.4634014754455628
iteration: 170 loss: 1.9900300730070575 grad: -0.43861968007636565
iteration: 180 loss: 1.8818221318765196 grad: -0.4163309324140888
iteration: 190 loss: 1.7847518405818514 grad: -0.3961808672522182
iteration: 200 loss: 1.6971880752735986 grad: -0.37787852437680103
iteration: 210 loss: 1.6178027475940657 grad: -0.36118293530959483
iteration: 220 loss: 1.5455036674553329 grad: -0.3458929351373727
iteration: 230 loss: 1.4793844747109688 grad: -0.33183934191577036
iteration: 240 loss: 1.4186867878973894 grad: -0.3188788931383008
iteration: 250 loss: 1.3627712315567557 grad: -0.30688949995521936
iteration: 260 loss: 1.3110950062095155 grad: -0.29576649964501833
iteration: 0 loss: 75.25366030374812 grad: 135.75990650913937
iteration: 10 loss: 21.766154964000922 grad: -0.08403996504844786
iteration: 20 loss: 13.722399276835983 grad: -1.6003009600566342
iteration: 30 loss: 9.99021720007206 grad: -1.509631522246171
iteration: 40 loss: 7.833110800062431 grad: -1.328283833834095
iteration: 50 loss: 6.431486087151412 grad: -1.1670801582403216
iteration: 60 loss: 5.449812545007675 grad: -1.034399981647204
iteration: 70 loss: 4.725100279369227 grad: -0.9259437675519305
iteration: 80 loss: 4.168750069466279 grad: -0.8365784198342675
iteration: 90 loss: 3.7285421367167726 grad: -0.7620957956155096
iteration: 100 loss: 3.3717467084411923 grad: -0.699281330522627
iteration: 110 loss: 3.0768303169199744 grad: -0.6457129412964417
iteration: 120 loss: 2.8290577259332084 grad: -0.5995597557884167
iteration: 130 loss: 2.618012208081051 grad: -0.5594249872525142
iteration: 140 loss: 2.4361254121619655 grad: -0.5242308761616687
iteration: 150 loss: 2.2777684053657636 grad: -0.493135919920729
iteration: 160 loss: 2.138669276939626 grad: -0.4654753924312993
iteration: 170 loss: 2.015528349715706 grad: -0.4407183414334044
iteration: 180 loss: 1.9057570706976632 grad: -0.4184362110119314
iteration: 190 loss: 1.807296623006509 grad: -0.39827970329583895
iteration: 200 loss: 1.7184892745874756 grad: -0.3799615271173023
iteration: 210 loss: 1.637985425262617 grad: -0.363243394243569
iteration: 220 loss: 1.5646753207946527 grad: -0.3479261125304809
iteration: 230 loss: 1.4976381297840156 grad: -0.33384196111010966
iteration: 240 loss: 1.4361034482562074 grad: -0.3208487647969832
iteration: 250 loss: 1.3794218358127843 grad: -0.30882524661667166
iteration: 260 loss: 1.3270420070214806 grad: -0.29766735110070197
iteration: 0 loss: 74.76204796593488 grad: 145.83187469263112
iteration: 10 loss: 21.10107485019545 grad: -1.1217335788749574
iteration: 20 loss: 13.358204744475808 grad: -1.928798375111421
iteration: 30 loss: 9.743051458046715 grad: -1.6696933052466614
iteration: 40 loss: 7.646785992251187 grad: -1.4216446573474013
iteration: 50 loss: 6.282060182689375 grad: -1.227188604798464
iteration: 60 loss: 5.325021504974798 grad: -1.0756235164524108
iteration: 70 loss: 4.6178859866528335 grad: -0.9554977416604521
iteration: 80 loss: 4.0747024641338445 grad: -0.8584704295643355
iteration: 90 loss: 3.6447295966336655 grad: -0.7787165707859016
iteration: 100 loss: 3.2961236560766918 grad: -0.7121381128374324
iteration: 110 loss: 3.007912986173613 grad: -0.6557996657148581
iteration: 120 loss: 2.7657353354884022 grad: -0.6075567801103702
iteration: 130 loss: 2.5594312128485566 grad: -0.565812809576153
iteration: 140 loss: 2.3816154326714654 grad: -0.5293579157351265
iteration: 150 loss: 2.2267930648293013 grad: -0.4972605570562158
iteration: 160 loss: 2.090792532842856 grad: -0.46879296349611077
iteration: 170 loss: 1.970390804548267 grad: -0.44337900621167353
iteration: 180 loss: 1.8630589069568322 grad: -0.4205570963166565
iteration: 190 loss: 1.766785053617613 grad: -0.39995335072944566
iteration: 200 loss: 1.6799491441281382 grad: -0.38126189169568786
iteration: 210 loss: 1.6012320558966993 grad: -0.3642301822191965
iteration: 220 loss: 1.5295489873399426 grad: -0.34864796962743094
iteration: 230 loss: 1.4639997369646898 grad: -0.33433885025840704
iteration: 240 loss: 1.4038311085034614 grad: -0.3211537629031623
iteration: 250 loss: 1.348408130893197 grad: -0.30896591862800915
iteration: 0 loss: 73.37030295048834 grad: 120.87953142571288
iteration: 10 loss: 21.25399929114333 grad: -0.378803304831678
iteration: 20 loss: 13.465967953753783 grad: -1.7040483667000026
iteration: 30 loss: 9.822688437846079 grad: -1.520041737916984
iteration: 40 loss: 7.711540539513532 grad: -1.311944860905164
iteration: 50 loss: 6.337609469682196 grad: -1.1422829125889382
iteration: 60 loss: 5.374150016731867 grad: -1.0072250969258647
iteration: 70 loss: 4.6621568891232945 grad: -0.8987374313493973
iteration: 80 loss: 4.115098939774146 grad: -0.8103011077943703
iteration: 90 loss: 3.6819266541815523 grad: -0.7371187323382526
iteration: 100 loss: 3.330615218480737 grad: -0.6757080334012572
iteration: 110 loss: 3.0400772669338703 grad: -0.6235239223628588
iteration: 120 loss: 2.7958715555935836 grad: -0.5786813306677985
iteration: 130 loss: 2.587781863623935 grad: -0.5397627549979114
iteration: 140 loss: 2.408380667264117 grad: -0.5056860051938008
iteration: 150 loss: 2.2521404970668963 grad: -0.4756128549342814
iteration: 160 loss: 2.1148641908450205 grad: -0.4488852281076967
iteration: 170 loss: 1.9933082298814704 grad: -0.42498000788724627
iteration: 180 loss: 1.8849269931154347 grad: -0.4034765730903943
iteration: 190 loss: 1.7876949973114775 grad: -0.3840331456620145
iteration: 200 loss: 1.6999807542314225 grad: -0.36636932233927383
iteration: 210 loss: 1.6204555863352488 grad: -0.3502530067204824
iteration: 220 loss: 1.5480266101943594 grad: -0.3354905144919354
iteration: 230 loss: 1.4817867391858603 grad: -0.3219189961830531
iteration: 240 loss: 1.4209768732851609 grad: -0.3094005730984627
iteration: 250 loss: 1.3649569492271936 grad: -0.2978177541741479
iteration: 260 loss: 1.3131835222551755 grad: -0.2870698208740941
iteration: 0 loss: 77.15316092170765 grad: 147.74553927797027
iteration: 10 loss: 21.3798897214628 grad: -1.6778403486648497
iteration: 20 loss: 13.46955247195861 grad: -2.091490837193407
iteration: 30 loss: 9.806466823223156 grad: -1.7546522757446232
iteration: 40 loss: 7.691878830533948 grad: -1.4769024501845638
iteration: 50 loss: 6.318332560978226 grad: -1.2665865521721908
iteration: 60 loss: 5.356209672268878 grad: -1.1053108201910429
iteration: 70 loss: 4.64570713209638 grad: -0.9788148714925307
iteration: 80 loss: 4.100056853577329 grad: -0.8774095553086476
iteration: 90 loss: 3.6681464351638833 grad: -0.7945314517502098
iteration: 100 loss: 3.3179457615480548 grad: -0.7256494445090974
iteration: 110 loss: 3.0283806192227614 grad: -0.6675627631939149
iteration: 120 loss: 2.7850274635519865 grad: -0.6179586946961075
iteration: 130 loss: 2.577687389643622 grad: -0.5751306966499378
iteration: 140 loss: 2.3989482301501748 grad: -0.5377951890582386
iteration: 150 loss: 2.243295603156515 grad: -0.5049698943814741
iteration: 160 loss: 2.106543313058823 grad: -0.4758913148876917
iteration: 170 loss: 1.9854569727313405 grad: -0.4499576163721837
iteration: 180 loss: 1.8774985580503611 grad: -0.42668834539097866
iteration: 190 loss: 1.7806489187540302 grad: -0.40569552002746617
iteration: 200 loss: 1.6932818649175632 grad: -0.38666254769093833
iteration: 210 loss: 1.6140731713646594 grad: -0.3693286224015526
iteration: 220 loss: 1.5419337142760179 grad: -0.35347701962175726
iteration: 230 loss: 1.4759595975399613 grad: -0.33892620458182987
iteration: 240 loss: 1.415394442218795 grad: -0.32552299949824015
iteration: 250 loss: 1.3596005171664067 grad: -0.3131372766775435
iteration: 260 loss: 1.308036385893171 grad: -0.3016577958555264
iteration: 0 loss: 74.6806024008239 grad: 121.71035899301145
iteration: 10 loss: 21.59757909525731 grad: 0.9950287442112916
iteration: 20 loss: 13.584047460242676 grad: -1.4004975627744636
iteration: 30 loss: 9.886843186784517 grad: -1.4433264272847373
iteration: 40 loss: 7.754341401976862 grad: -1.292184887223014
iteration: 50 loss: 6.36990128949926 grad: -1.139833812658688
iteration: 60 loss: 5.400451961894015 grad: -1.0106666702989886
iteration: 70 loss: 4.68464933774265 grad: -0.9042402957926534
iteration: 80 loss: 4.1349566017316155 grad: -0.8164320160601278
iteration: 90 loss: 3.69984053716404 grad: -0.7433030044776773
iteration: 100 loss: 3.347022172699574 grad: -0.6817102318575708
iteration: 110 loss: 3.055270515235613 grad: -0.629252480320071
iteration: 120 loss: 2.8100580045564296 grad: -0.5841084961451184
iteration: 130 loss: 2.6011137321470827 grad: -0.5448895538410794
iteration: 140 loss: 2.420973904260708 grad: -0.5105261444694289
iteration: 150 loss: 2.264086021097269 grad: -0.48018508775716817
iteration: 160 loss: 2.1262350387437436 grad: -0.453209681650637
iteration: 170 loss: 2.004164273279085 grad: -0.42907643539256757
iteration: 180 loss: 1.8953181536169061 grad: -0.40736356315671673
iteration: 190 loss: 1.7976635030349 grad: -0.3877278136535308
iteration: 200 loss: 1.7095627653041612 grad: -0.3698872437333981
iteration: 210 loss: 1.629682393356523 grad: -0.35360826853454524
iteration: 220 loss: 1.556925536414176 grad: -0.3386958202050037
iteration: 230 loss: 1.490381832370779 grad: -0.3249857904744162
iteration: 240 loss: 1.4292894452005385 grad: -0.31233916911251497
iteration: 250 loss: 1.3730060025914925 grad: -0.3006374548011417
iteration: 260 loss: 1.320986093181215 grad: -0.28977903025045065
iteration: 0 loss: 78.89243672017942 grad: 105.76852427770842
iteration: 10 loss: 22.75095451433737 grad: 1.419593131062204
iteration: 20 loss: 14.0919940694686 grad: -1.106723082874365
iteration: 30 loss: 10.195458852226444 grad: -1.230862173529656
iteration: 40 loss: 7.973839981357969 grad: -1.1307856505540665
iteration: 50 loss: 6.540130624365909 grad: -1.0108250047034217
iteration: 60 loss: 5.539653620376033 grad: -0.9035769430165027
iteration: 70 loss: 4.802562137994548 grad: -0.8128689435191059
iteration: 80 loss: 4.237359819720454 grad: -0.7368587963010316
iteration: 90 loss: 3.7904359612762577 grad: -0.672905234221986
iteration: 100 loss: 3.428321198382119 grad: -0.6186501292289722
iteration: 110 loss: 3.129054848608038 grad: -0.5721928407332413
iteration: 120 loss: 2.8776376173597615 grad: -0.5320459162953141
iteration: 130 loss: 2.6634802233127797 grad: -0.49705170383064556
iteration: 140 loss: 2.478896249182857 grad: -0.46630576995724904
iteration: 150 loss: 2.31817283134102 grad: -0.43909604864767854
iteration: 160 loss: 2.1769764581601354 grad: -0.4148567447936778
iteration: 170 loss: 2.051961009721832 grad: -0.39313395330222783
iteration: 180 loss: 1.9405021126372461 grad: -0.3735600696286676
iteration: 190 loss: 1.8405127807828565 grad: -0.3558346649477489
iteration: 200 loss: 1.75031275679211 grad: -0.33971009110622086
iteration: 210 loss: 1.6685341658327906 grad: -0.3249805536753587
iteration: 220 loss: 1.5940522391245973 grad: -0.31147374277834466
iteration: 230 loss: 1.5259336714944682 grad: -0.2990443647026753
iteration: 240 loss: 1.463397593610187 grad: -0.287569098018419
iteration: 250 loss: 1.4057857074767526 grad: -0.2769426266177784
iteration: 260 loss: 1.3525391716814077 grad: -0.2670744940292495
iteration: 0 loss: 75.85968879795509 grad: 120.58716499883927
iteration: 10 loss: 21.949830775473277 grad: 0.0760414200605747
iteration: 20 loss: 13.811998678330404 grad: -1.5832596058296606
iteration: 30 loss: 10.037226588469593 grad: -1.4705439176806587
iteration: 40 loss: 7.861131930378881 grad: -1.2854811598778504
iteration: 50 loss: 6.450046498541641 grad: -1.1264528550789208
iteration: 60 loss: 5.46317535947748 grad: -0.9972941835263118
iteration: 70 loss: 4.73537022843088 grad: -0.8922765620648397
iteration: 80 loss: 4.1770509280100585 grad: -0.8059363000004278
iteration: 90 loss: 3.735514971984933 grad: -0.7340604378462547
iteration: 100 loss: 3.3777789284691524 grad: -0.6734953466378137
iteration: 110 loss: 3.082168081853276 grad: -0.6218800857280872
iteration: 120 loss: 2.833864403386997 grad: -0.5774341973380845
iteration: 130 loss: 2.622400399138438 grad: -0.5388012388638769
iteration: 140 loss: 2.4401753337371948 grad: -0.5049358638302628
iteration: 150 loss: 2.281538885515684 grad: -0.4750228575086881
iteration: 160 loss: 2.1422045760133517 grad: -0.44841900233057175
iteration: 170 loss: 2.0188625312225668 grad: -0.4246110762075264
iteration: 180 loss: 1.9089168841287574 grad: -0.4031852343697806
iteration: 190 loss: 1.810303455061206 grad: -0.3838044577982954
iteration: 200 loss: 1.721360501047374 grad: -0.36619176136201326
iteration: 210 loss: 1.640735367620106 grad: -0.3501175532860911
iteration: 220 loss: 1.567315936354796 grad: -0.3353900174961746
iteration: 230 loss: 1.5001795183303246 grad: -0.32184772038845594
iteration: 240 loss: 1.4385542300905896 grad: -0.3093538716541351
iteration: 250 loss: 1.381789438045623 grad: -0.2977918276116445
iteration: 260 loss: 1.3293328833707496 grad: -0.2870615370819687
iteration: 0 loss: 78.90209048605108 grad: 112.76217935824275
iteration: 10 loss: 22.102606390955543 grad: 0.4772234014182481
iteration: 20 loss: 13.829563026106536 grad: -1.5863541918388404
iteration: 30 loss: 10.052101221368007 grad: -1.4881555860481708
iteration: 40 loss: 7.881346747586928 grad: -1.2943388825781144
iteration: 50 loss: 6.473969336070158 grad: -1.1273508763724829
iteration: 60 loss: 5.488968552175348 grad: -0.9930128202199646
iteration: 70 loss: 4.761824222018029 grad: -0.8850354313624769
iteration: 80 loss: 4.203452291664152 grad: -0.7971645616691038
iteration: 90 loss: 3.7614620524076683 grad: -0.724598521934102
iteration: 100 loss: 3.4030554431435784 grad: -0.6638186102975161
iteration: 110 loss: 3.1066666151529962 grad: -0.6122523650192606
iteration: 120 loss: 2.857541379208752 grad: -0.5679990254015573
iteration: 130 loss: 2.645249552491309 grad: -0.5296336108403834
iteration: 140 loss: 2.462211931281135 grad: -0.4960714368559039
iteration: 150 loss: 2.302790241599475 grad: -0.46647458849157797
iteration: 160 loss: 2.1627043078871626 grad: -0.4401867745763001
iteration: 170 loss: 2.038647068552936 grad: -0.41668735612267993
iteration: 180 loss: 1.928023356308131 grad: -0.39555843827792314
iteration: 190 loss: 1.8287684301826217 grad: -0.37646097278829416
iteration: 200 loss: 1.7392192556686 grad: -0.3591171611518674
iteration: 210 loss: 1.658021488820726 grad: -0.34329732544673275
iteration: 220 loss: 1.5840611317626512 grad: -0.32880999056262555
iteration: 230 loss: 1.5164135554703286 grad: -0.3154943051574357
iteration: 240 loss: 1.4543049549269695 grad: -0.3032141870123843
iteration: 250 loss: 1.3970828406060554 grad: -0.29185375473810954
iteration: 260 loss: 1.344193189855725 grad: -0.2813137296264151
iteration: 0 loss: 75.98892905163675 grad: 123.03096628857958
iteration: 10 loss: 22.149921999637794 grad: 0.1459287465593526
iteration: 20 loss: 13.851106064497303 grad: -1.3651392481807867
iteration: 30 loss: 10.048176182757432 grad: -1.3410627790458847
iteration: 40 loss: 7.86566992974721 grad: -1.200266488237014
iteration: 50 loss: 6.453178196544989 grad: -1.0638488483046595
iteration: 60 loss: 5.466178504743724 grad: -0.9478148692422323
iteration: 70 loss: 4.738540979272991 grad: -0.851430555922158
iteration: 80 loss: 4.180417214711022 grad: -0.771285699017933
iteration: 90 loss: 3.739036869685606 grad: -0.7040948021929283
iteration: 100 loss: 3.3814064179985674 grad: -0.647191390444883
iteration: 110 loss: 3.0858569450856934 grad: -0.5985064959338293
iteration: 120 loss: 2.8375794069484073 grad: -0.5564502141909394
iteration: 130 loss: 2.626114712514705 grad: -0.5197969900949664
iteration: 140 loss: 2.443869034839984 grad: -0.487594419232821
iteration: 150 loss: 2.2851974525986574 grad: -0.45909492400255375
iteration: 160 loss: 2.1458176138376075 grad: -0.43370550648424483
iteration: 170 loss: 2.0224227636054737 grad: -0.4109508869670557
iteration: 180 loss: 1.91241937974494 grad: -0.3904463367898552
iteration: 190 loss: 1.8137450401150503 grad: -0.37187749720022456
iteration: 200 loss: 1.7247393160273532 grad: -0.35498524715323054
iteration: 210 loss: 1.644050533218433 grad: -0.3395542446225751
iteration: 220 loss: 1.5705673014821182 grad: -0.3254041635774715
iteration: 230 loss: 1.5033674697314878 grad: -0.312382927591328
iteration: 240 loss: 1.4416795482195333 grad: -0.3003614365141319
iteration: 250 loss: 1.384853187751353 grad: -0.2892294202735446
iteration: 260 loss: 1.3323363308666722 grad: -0.27889215144362256
iteration: 0 loss: 75.89359111066352 grad: 145.92546474841564
iteration: 10 loss: 22.225590352213842 grad: -0.7723438182161363
iteration: 20 loss: 13.921044795854662 grad: -1.5788166360526374
iteration: 30 loss: 10.098989121537215 grad: -1.4477862594846003
iteration: 40 loss: 7.903586025385556 grad: -1.271362878574172
iteration: 50 loss: 6.482502219646449 grad: -1.1182482246015701
iteration: 60 loss: 5.4895594196827915 grad: -0.9924070953183133
iteration: 70 loss: 4.757649992924681 grad: -0.8893884798596077
iteration: 80 loss: 4.196351498364782 grad: -0.8043679179476468
iteration: 90 loss: 3.7525448205774823 grad: -0.733408558668964
iteration: 100 loss: 3.393015967511184 grad: -0.6734934773607191
iteration: 110 loss: 3.0959514012741405 grad: -0.6223428647932673
iteration: 120 loss: 2.846444039587037 grad: -0.5782298590387629
iteration: 130 loss: 2.633966401383449 grad: -0.5398350512289435
iteration: 140 loss: 2.450875746102983 grad: -0.5061391577384212
iteration: 150 loss: 2.2914913959814176 grad: -0.47634549120265796
iteration: 160 loss: 2.151504353035555 grad: -0.44982411350338836
iteration: 170 loss: 2.0275876640527546 grad: -0.4260713984402965
iteration: 180 loss: 1.917132261639505 grad: -0.40468049309664234
iteration: 190 loss: 1.8180635952836715 grad: -0.38531951524049485
iteration: 200 loss: 1.728711656683448 grad: -0.3677152840657464
iteration: 210 loss: 1.6477171203105172 grad: -0.3516410468052529
iteration: 220 loss: 1.5739624225595683 grad: -0.3369071210088738
iteration: 230 loss: 1.5065203849504045 grad: -0.32335368687982097
iteration: 240 loss: 1.4446153886191164 grad: -0.3108451817152934
iteration: 250 loss: 1.3875936663569683 grad: -0.29926590027862426
iteration: 260 loss: 1.3349003107746285 grad: -0.28851651174811993
iteration: 0 loss: 77.30317329352083 grad: 114.78316014155985
iteration: 10 loss: 22.325968654900965 grad: 1.0160058945025332
iteration: 20 loss: 13.935781497993359 grad: -1.2436772773638056
iteration: 30 loss: 10.114121770118489 grad: -1.311353219578737
iteration: 40 loss: 7.922025145594737 grad: -1.1881924878553778
iteration: 50 loss: 6.502656870047056 grad: -1.0557469154359762
iteration: 60 loss: 5.510235422084489 grad: -0.9405088244738922
iteration: 70 loss: 4.778176526829901 grad: -0.8441680717086325
iteration: 80 loss: 4.216381344520827 grad: -0.7639487645691725
iteration: 90 loss: 3.771911659901889 grad: -0.6967253997027572
iteration: 100 loss: 3.4116523832963552 grad: -0.6398575760769818
iteration: 110 loss: 3.113843565561679 grad: -0.5912666671614508
iteration: 120 loss: 2.8636066879976996 grad: -0.5493465938381218
iteration: 130 loss: 2.650428897929363 grad: -0.5128572915626568
iteration: 140 loss: 2.466674267471097 grad: -0.48083518043707774
iteration: 150 loss: 2.3066645435584094 grad: -0.45252460921620286
iteration: 160 loss: 2.1660907135024816 grad: -0.42732694931626247
iteration: 170 loss: 2.041624490917043 grad: -0.4047630936805337
iteration: 180 loss: 1.9306548062846476 grad: -0.38444577758975673
iteration: 190 loss: 1.8311048147435363 grad: -0.36605901617488507
iteration: 200 loss: 1.741302138168728 grad: -0.34934269723501143
iteration: 210 loss: 1.6598851277912714 grad: -0.3340809272797065
iteration: 220 loss: 1.5857340090205154 grad: -0.32009313083388546
iteration: 230 loss: 1.5179195367566287 grad: -0.3072271872922695
iteration: 240 loss: 1.4556641821392409 grad: -0.2953540896919704
iteration: 250 loss: 1.3983124253093344 grad: -0.28436375091221244
iteration: 260 loss: 1.3453077578074457 grad: -0.27416168292406323
iteration: 0 loss: 73.86359059047965 grad: 137.36203979690112
iteration: 10 loss: 21.40877661292517 grad: -1.436340592674426
iteration: 20 loss: 13.519532330805346 grad: -1.8967894487151917
iteration: 30 loss: 9.841657969684114 grad: -1.6377984214535033
iteration: 40 loss: 7.715819728125635 grad: -1.4050163086314762
iteration: 50 loss: 6.334724077094341 grad: -1.2202553705175145
iteration: 60 loss: 5.367508769776205 grad: -1.0741546797372399
iteration: 70 loss: 4.653496272489189 grad: -0.9570909077437457
iteration: 80 loss: 4.105370849688535 grad: -0.8617959505875974
iteration: 90 loss: 3.6716779344809023 grad: -0.7830235449313463
iteration: 100 loss: 3.3201696268955114 grad: -0.7169897272405582
iteration: 110 loss: 3.0296298357457503 grad: -0.6609344890417768
iteration: 120 loss: 2.7855403835014427 grad: -0.6128146820708851
iteration: 130 loss: 2.5776378123112575 grad: -0.5710941289807501
iteration: 140 loss: 2.398464995594333 grad: -0.5346001626183621
iteration: 150 loss: 2.242475637486429 grad: -0.5024246669632717
iteration: 160 loss: 2.105460493394193 grad: -0.4738549362110934
iteration: 170 loss: 1.984168266518819 grad: -0.4483246809602781
iteration: 180 loss: 1.8760483494359257 grad: -0.42537881005528805
iteration: 190 loss: 1.7790721064636108 grad: -0.40464775811555775
iteration: 200 loss: 1.6916061132009501 grad: -0.3858285191335205
iteration: 210 loss: 1.6123205676622918 grad: -0.36867045566481005
iteration: 220 loss: 1.540122005132823 grad: -0.3529645538337809
iteration: 230 loss: 1.4741031205639956 grad: -0.33853519605692983
iteration: 240 loss: 1.4135048362301281 grad: -0.325233795382566
iteration: 250 loss: 1.3576872685535322 grad: -0.3129338218723857
iteration: 260 loss: 1.3061072527171662 grad: -0.3015268809678181
iteration: 0 loss: 79.20854954386893 grad: 146.19337214839558
iteration: 10 loss: 22.450837262025868 grad: -1.761868313116956
iteration: 20 loss: 14.053165257827004 grad: -1.927849574292635
iteration: 30 loss: 10.194768788730967 grad: -1.6341200736026587
iteration: 40 loss: 7.979875561698216 grad: -1.3873393829698037
iteration: 50 loss: 6.5463105720036765 grad: -1.1958034314043422
iteration: 60 loss: 5.544530744792692 grad: -1.046715984926804
iteration: 70 loss: 4.805972328077257 grad: -0.9287375603245005
iteration: 80 loss: 4.23946630308625 grad: -0.833633728305025
iteration: 90 loss: 3.791462198249354 grad: -0.7556187536425412
iteration: 100 loss: 3.4284752433045056 grad: -0.690611439152041
iteration: 110 loss: 3.12851159221877 grad: -0.6356893505669515
iteration: 120 loss: 2.8765387262124023 grad: -0.5887215042701139
iteration: 130 loss: 2.661938895111358 grad: -0.5481252791561066
iteration: 140 loss: 2.477002608109491 grad: -0.5127046514530618
iteration: 150 loss: 2.3159987083767275 grad: -0.481541058458884
iteration: 160 loss: 2.174579293050101 grad: -0.4539185354398559
iteration: 170 loss: 2.0493869313037396 grad: -0.4292714699786644
iteration: 180 loss: 1.9377883376568703 grad: -0.40714752119095765
iteration: 190 loss: 1.8376894741168144 grad: -0.38718087693343706
iteration: 200 loss: 1.7474044749077289 grad: -0.36907267468031524
iteration: 210 loss: 1.6655609804577745 grad: -0.35257646495182016
iteration: 220 loss: 1.5910306160024612 grad: -0.33748727733152684
iteration: 230 loss: 1.522877161262758 grad: -0.323633296456934
iteration: 240 loss: 1.4603173780331995 grad: -0.31086945369368557
iteration: 250 loss: 1.4026910337785343 grad: -0.29907244213107875
iteration: 260 loss: 1.3494376998655753 grad: -0.28813680117616286
iteration: 0 loss: 76.36760283307267 grad: 93.6471280677992
iteration: 10 loss: 22.610662229999082 grad: 2.0258074715705616
iteration: 20 loss: 14.044554380674878 grad: -0.74989975015185
iteration: 30 loss: 10.170465494930362 grad: -0.9575679470447996
iteration: 40 loss: 7.957019818939836 grad: -0.9114597156606383
iteration: 50 loss: 6.527269402262515 grad: -0.8300710603683215
iteration: 60 loss: 5.529115100171649 grad: -0.7512607273064305
iteration: 70 loss: 4.79356846553099 grad: -0.6820874521077464
iteration: 80 loss: 4.229482357277985 grad: -0.6227943536536162
iteration: 90 loss: 3.783411664753864 grad: -0.5721089889976112
iteration: 100 loss: 3.4219753445560688 grad: -0.5285913952870909
iteration: 110 loss: 3.1232643213461 grad: -0.49097180246744987
iteration: 120 loss: 2.8723116916845686 grad: -0.458206700520679
iteration: 130 loss: 2.6585496891801665 grad: -0.4294579454229989
iteration: 140 loss: 2.4743068491092015 grad: -0.40405586239243513
iteration: 150 loss: 2.313881086814449 grad: -0.3814641498130854
iteration: 160 loss: 2.1729468968071823 grad: -0.3612508672408088
iteration: 170 loss: 2.0481642546881385 grad: -0.3430656280876434
iteration: 180 loss: 1.9369135250560083 grad: -0.3266220072535301
iteration: 190 loss: 1.8371114697979465 grad: -0.3116840448742594
iteration: 200 loss: 1.7470808413860057 grad: -0.29805588093741914
iteration: 210 loss: 1.665456213759047 grad: -0.2855737590343571
iteration: 220 loss: 1.5911148332857423 grad: -0.27409981977338144
iteration: 230 loss: 1.5231250698918137 grad: -0.2635172496736179
iteration: 240 loss: 1.460707459217164 grad: -0.2537264617929256
iteration: 250 loss: 1.4032048911799393 grad: -0.24464206654291773
iteration: 260 loss: 1.350059536074861 grad: -0.23619045182429788
iteration: 0 loss: 77.43041389975326 grad: 109.61892181642665
iteration: 10 loss: 22.301210805319897 grad: 0.29011610949561756
iteration: 20 loss: 13.994654163841192 grad: -1.5198913852839597
iteration: 30 loss: 10.168084625794501 grad: -1.4175958837256193
iteration: 40 loss: 7.966433419256274 grad: -1.241082195889954
iteration: 50 loss: 6.539404208448229 grad: -1.0883285229521267
iteration: 60 loss: 5.5412589845387545 grad: -0.963886594953737
iteration: 70 loss: 4.8048905022818955 grad: -0.8626872995877065
iteration: 80 loss: 4.239776974391713 grad: -0.779552161239122
iteration: 90 loss: 3.792691002394349 grad: -0.7103893823752054
iteration: 100 loss: 3.430323941868582 grad: -0.6521249329516833
iteration: 110 loss: 3.1307846397113197 grad: -0.6024660460420881
iteration: 120 loss: 2.879103667220575 grad: -0.559691785550684
iteration: 130 loss: 2.6647036781817985 grad: -0.522495935272027
iteration: 140 loss: 2.479901943366961 grad: -0.48987439502573915
iteration: 150 loss: 2.3189855304102904 grad: -0.46104533108521667
iteration: 160 loss: 2.177619238404118 grad: -0.43539239362897564
iteration: 170 loss: 2.0524546645610826 grad: -0.4124239893265469
iteration: 180 loss: 1.9408650480511576 grad: -0.39174374489753083
iteration: 190 loss: 1.8407611340687915 grad: -0.3730288292901494
iteration: 200 loss: 1.75046060654255 grad: -0.3560138507705275
iteration: 210 loss: 1.6685937679886924 grad: -0.34047875383076465
iteration: 220 loss: 1.5940342580197466 grad: -0.3262396193966385
iteration: 230 loss: 1.5258473921931 grad: -0.3131415968036457
iteration: 240 loss: 1.4632511106716182 grad: -0.3010534185922563
iteration: 250 loss: 1.4055860896205972 grad: -0.28986310315276215
iteration: 260 loss: 1.352292603885334 grad: -0.2794745579114433
iteration: 0 loss: 75.96343811358264 grad: 120.42504043630345
iteration: 10 loss: 21.956800242635175 grad: 0.1324680096771324
iteration: 20 loss: 13.751944743706293 grad: -1.2228842659180998
iteration: 30 loss: 9.985939675109504 grad: -1.202799764006222
iteration: 40 loss: 7.823308362605034 grad: -1.0861910029352972
iteration: 50 loss: 6.422772309149298 grad: -0.9705705391714745
iteration: 60 loss: 5.44345947023302 grad: -0.870369934617338
iteration: 70 loss: 4.7210158091595735 grad: -0.7859724484063613
iteration: 80 loss: 4.166544277785843 grad: -0.7150542899860761
iteration: 90 loss: 3.7278182047905584 grad: -0.6551095257808085
iteration: 100 loss: 3.372170877047996 grad: -0.6040060749724354
iteration: 110 loss: 3.0781383000396754 grad: -0.5600431183183517
iteration: 120 loss: 2.8310446406976966 grad: -0.5218889818846393
iteration: 130 loss: 2.620520152867233 grad: -0.48850302944654933
iteration: 140 loss: 2.4390325921789224 grad: -0.4590679725814875
iteration: 150 loss: 2.2809805158544805 grad: -0.43293681735667244
iteration: 160 loss: 2.1421129279596327 grad: -0.40959272202247976
iteration: 170 loss: 2.0191461187082083 grad: -0.3886189578931998
iteration: 180 loss: 1.9095037920300124 grad: -0.36967643977215014
iteration: 190 loss: 1.81113659953677 grad: -0.3524868396847973
iteration: 200 loss: 1.722394172177122 grad: -0.33681980555184554
iteration: 210 loss: 1.6419326732158714 grad: -0.3224832064362646
iteration: 220 loss: 1.5686468884690508 grad: -0.3093156225350152
iteration: 230 loss: 1.5016195844051998 grad: -0.29718051254470823
iteration: 240 loss: 1.4400832242901165 grad: -0.285961644754545
iteration: 250 loss: 1.3833906645970522 grad: -0.27555948830412125
iteration: 260 loss: 1.3309924686259778 grad: -0.26588834013655527
iteration: 0 loss: 75.86648250615225 grad: 116.22902390698495
iteration: 10 loss: 22.292335518322798 grad: 1.063875069292829
iteration: 20 loss: 13.9051833458947 grad: -1.0861645493760204
iteration: 30 loss: 10.083521851203221 grad: -1.2282410445722989
iteration: 40 loss: 7.8934571611957836 grad: -1.1414830484529699
iteration: 50 loss: 6.476503386544044 grad: -1.0263048322011041
iteration: 60 loss: 5.486371375397581 grad: -0.9200325675166514
iteration: 70 loss: 4.75634999320354 grad: -0.8287983168270264
iteration: 80 loss: 4.196331151672604 grad: -0.7517269876160069
iteration: 90 loss: 3.7534024749683423 grad: -0.6865766911054085
iteration: 100 loss: 3.394482211988237 grad: -0.6311514543463197
iteration: 110 loss: 3.0978420884446844 grad: -0.583611889237718
iteration: 120 loss: 2.848630989313196 grad: -0.5424883692483966
iteration: 130 loss: 2.636358916760152 grad: -0.5066226203314574
iteration: 140 loss: 2.45340873220911 grad: -0.4751022487751486
iteration: 150 loss: 2.294117566794036 grad: -0.44720487153186017
iteration: 160 loss: 2.1541890175588545 grad: -0.42235421796678246
iteration: 170 loss: 2.0303051724470773 grad: -0.4000866607619007
iteration: 180 loss: 1.9198635407991456 grad: -0.38002594198607786
iteration: 190 loss: 1.8207944127906641 grad: -0.36186409250553725
iteration: 200 loss: 1.7314313801963925 grad: -0.34534696471359433
iteration: 210 loss: 1.6504178193770878 grad: -0.330263190600314
iteration: 220 loss: 1.576638210593625 grad: -0.3164356892243318
iteration: 230 loss: 1.509166931957018 grad: -0.3037150817890847
iteration: 240 loss: 1.447229556826435 grad: -0.2919745438987027
iteration: 250 loss: 1.3901732353457523 grad: -0.2811057488139883
iteration: 260 loss: 1.3374437684592522 grad: -0.2710156454521845
iteration: 0 loss: 75.88602919080564 grad: 96.07549915265108
iteration: 10 loss: 22.57500236266337 grad: 3.789337299609698
iteration: 20 loss: 14.018873770670304 grad: -0.44998551175117707
iteration: 30 loss: 10.162220848606426 grad: -0.98836220101177
iteration: 40 loss: 7.956510618196304 grad: -1.0214711714541622
iteration: 50 loss: 6.530228573019444 grad: -0.9553229808431695
iteration: 60 loss: 5.5336559050072704 grad: -0.8734012319061546
iteration: 70 loss: 4.798793938499619 grad: -0.7959448083185393
iteration: 80 loss: 4.234942334788229 grad: -0.7274169226403626
iteration: 90 loss: 3.7888733601437043 grad: -0.6679307338098033
iteration: 100 loss: 3.4273178424408055 grad: -0.6164550765550432
iteration: 110 loss: 3.1284266489294414 grad: -0.5717796312427154
iteration: 120 loss: 2.877265875186202 grad: -0.5327988863743336
iteration: 130 loss: 2.663286187861765 grad: -0.4985774003521304
iteration: 140 loss: 2.478826426481109 grad: -0.4683455089637426
iteration: 150 loss: 2.31819016464041 grad: -0.44147577838755825
iteration: 160 loss: 2.1770548554329086 grad: -0.4174571479489914
iteration: 170 loss: 2.0520818501372147 grad: -0.395872009324006
iteration: 180 loss: 1.9406519606247705 grad: -0.37637734977346843
iteration: 190 loss: 1.8406818575456947 grad: -0.3586897115785409
iteration: 200 loss: 1.7504938985956537 grad: -0.3425733452661577
iteration: 210 loss: 1.668722101925637 grad: -0.32783091253343877
iteration: 220 loss: 1.594243083610226 grad: -0.3142961827585026
iteration: 230 loss: 1.5261245609084737 grad: -0.30182827523799427
iteration: 240 loss: 1.4635864252793247 grad: -0.2903070975914756
iteration: 250 loss: 1.4059709485764373 grad: -0.27962971134736214
iteration: 260 loss: 1.3527197185282025 grad: -0.26970741885734145
iteration: 0 loss: 75.69954905810434 grad: 135.89838187676196
iteration: 10 loss: 21.683367273809175 grad: -0.6122509459718364
iteration: 20 loss: 13.641871467419234 grad: -1.606009106566045
iteration: 30 loss: 9.923419175779394 grad: -1.474773488474265
iteration: 40 loss: 7.778965065585783 grad: -1.2917332875184018
iteration: 50 loss: 6.387065767754186 grad: -1.1333054058249425
iteration: 60 loss: 5.41267707339214 grad: -1.00378391828418
iteration: 70 loss: 4.693472947508161 grad: -0.8982599744897113
iteration: 80 loss: 4.14136639979072 grad: -0.8115000172899774
iteration: 90 loss: 3.70449498157115 grad: -0.7392961581293768
iteration: 100 loss: 3.350373735635003 grad: -0.678462803443804
iteration: 110 loss: 3.057638211090844 grad: -0.6266151680928778
iteration: 120 loss: 2.811671799577239 grad: -0.5819596716857509
iteration: 130 loss: 2.602142640183295 grad: -0.543133331940143
iteration: 140 loss: 2.421544270340043 grad: -0.5090876023619311
iteration: 150 loss: 2.2642937196403015 grad: -0.4790056156225319
iteration: 160 loss: 2.126153794566521 grad: -0.4522431736402963
iteration: 170 loss: 2.0038514519980954 grad: -0.42828634765769946
iteration: 180 loss: 1.89481886412254 grad: -0.40672069057938265
iteration: 190 loss: 1.797013551725451 grad: -0.3872086200118028
iteration: 200 loss: 1.7087908151206648 grad: -0.3694726059063406
iteration: 210 loss: 1.628811563049856 grad: -0.3532825273270218
iteration: 220 loss: 1.5559745998419368 grad: -0.3384460579673934
iteration: 230 loss: 1.489366128062143 grad: -0.32480127704462225
iteration: 240 loss: 1.4282215733721222 grad: -0.3122109334124117
iteration: 250 loss: 1.3718963644204534 grad: -0.30055795090096915
iteration: 260 loss: 1.3198433117340342 grad: -0.289741875000749
iteration: 0 loss: 70.29284670918715 grad: 117.90972546456547
iteration: 10 loss: 20.917246547989862 grad: 1.8168483389782883
iteration: 20 loss: 13.236020431921691 grad: -1.0641492668839148
iteration: 30 loss: 9.665273378897837 grad: -1.2535504439229819
iteration: 40 loss: 7.595163620174472 grad: -1.165421280696607
iteration: 50 loss: 6.246591269915618 grad: -1.0483013329585058
iteration: 60 loss: 5.300031271069326 grad: -0.9412589308847349
iteration: 70 loss: 4.599974299090594 grad: -0.8496680389419139
iteration: 80 loss: 4.0617338153497675 grad: -0.77228720300459
iteration: 90 loss: 3.6353089284992333 grad: -0.7067676005291366
iteration: 100 loss: 3.2893088720002197 grad: -0.6509024422256597
iteration: 110 loss: 3.003049595582318 grad: -0.6028691567492088
iteration: 120 loss: 2.7623574065142664 grad: -0.5612187878863283
iteration: 130 loss: 2.557199928623628 grad: -0.5248105723480396
iteration: 140 loss: 2.3802797908763504 grad: -0.492745205998622
iteration: 150 loss: 2.226164052980742 grad: -0.46430946872286366
iteration: 160 loss: 2.0907257467568923 grad: -0.43893319772087586
iteration: 170 loss: 1.97077449596823 grad: -0.416156630913353
iteration: 180 loss: 1.8638056045928848 grad: -0.3956057802626792
iteration: 190 loss: 1.7678255861530736 grad: -0.376973834694436
iteration: 200 loss: 1.681228308868826 grad: -0.36000704292635527
iteration: 210 loss: 1.6027054346321743 grad: -0.3444939208448917
iteration: 220 loss: 1.5311805747164726 grad: -0.33025693477292384
iteration: 230 loss: 1.465760152756502 grad: -0.3171460396653252
iteration: 240 loss: 1.4056962351168791 grad: -0.305033617073204
iteration: 250 loss: 1.3503580643902173 grad: -0.2938104776565662
iteration: 0 loss: 75.84286308836302 grad: 130.60372302327045
iteration: 10 loss: 21.483158624810667 grad: -0.952646616599096
iteration: 20 loss: 13.56760426041718 grad: -2.032724923285687
iteration: 30 loss: 9.878862040841073 grad: -1.7339841383679477
iteration: 40 loss: 7.746066782833995 grad: -1.4626874827550869
iteration: 50 loss: 6.3604382155302694 grad: -1.2547175392877077
iteration: 60 loss: 5.390084854251944 grad: -1.0948363882649907
iteration: 70 loss: 4.6737584095550035 grad: -0.9693773252047375
iteration: 80 loss: 4.123836282047788 grad: -0.868810382171822
iteration: 90 loss: 3.688692484320613 grad: -0.7866335317411027
iteration: 100 loss: 3.33597772401634 grad: -0.7183486521171574
iteration: 110 loss: 3.0444123394068314 grad: -0.6607767450962415
iteration: 120 loss: 2.7994361252229862 grad: -0.6116209113464546
iteration: 130 loss: 2.590756532344544 grad: -0.5691865982268743
iteration: 140 loss: 2.4108954999264 grad: -0.5321995240111052
iteration: 150 loss: 2.25429117009195 grad: -0.49968473321435236
iteration: 160 loss: 2.116722411887953 grad: -0.4708845662761893
iteration: 170 loss: 1.994928641439773 grad: -0.44520189603361715
iteration: 180 loss: 1.88635185544027 grad: -0.4221601031157867
iteration: 190 loss: 1.788957432461509 grad: -0.4013743583595645
iteration: 200 loss: 1.7011070362586906 grad: -0.38253068457060185
iteration: 210 loss: 1.6214667859272511 grad: -0.3653704631315057
iteration: 220 loss: 1.5489397941978496 grad: -0.34967881271750406
iteration: 230 loss: 1.482615857936294 grad: -0.335275762631232
iteration: 240 loss: 1.4217334255512553 grad: -0.32200947088510445
iteration: 250 loss: 1.365650486461246 grad: -0.3097509574645453
iteration: 260 loss: 1.313822035198634 grad: -0.298389973648297
iteration: 0 loss: 77.38804315797998 grad: 136.56456329909219
iteration: 10 loss: 22.10044215394522 grad: -0.9539752740396543
iteration: 20 loss: 13.87090877973992 grad: -1.6834502805744411
iteration: 30 loss: 10.06909667375457 grad: -1.5046665187493222
iteration: 40 loss: 7.8823624407773005 grad: -1.309425167872449
iteration: 50 loss: 6.4660025304877475 grad: -1.146129935855215
iteration: 60 loss: 5.476011601838681 grad: -1.0138853099493186
iteration: 70 loss: 4.746118020658245 grad: -0.9065223033281576
iteration: 80 loss: 4.186285284317861 grad: -0.818401580026303
iteration: 90 loss: 3.743594998743943 grad: -0.7451432353484992
iteration: 100 loss: 3.384947042185641 grad: -0.6834705683892071
iteration: 110 loss: 3.088596999399876 grad: -0.6309427303049333
iteration: 120 loss: 2.8396820977704507 grad: -0.5857283695780332
iteration: 130 loss: 2.6277045596438753 grad: -0.546437649536477
iteration: 140 loss: 2.445042311916082 grad: -0.5120022761060169
iteration: 150 loss: 2.2860295151405228 grad: -0.48159052177379313
iteration: 160 loss: 2.14636811252839 grad: -0.45454681140468506
iteration: 170 loss: 2.022739405316646 grad: -0.43034838376336704
iteration: 180 loss: 1.9125406782503556 grad: -0.408573852988533
iteration: 190 loss: 1.8137023543274815 grad: -0.3888801283586807
iteration: 200 loss: 1.7245583789766565 grad: -0.37098526565901674
iteration: 210 loss: 1.6437526122624397 grad: -0.3546555760038327
iteration: 220 loss: 1.5701700881668743 grad: -0.3396958261142564
iteration: 230 loss: 1.5028857691362314 grad: -0.3259417092215715
iteration: 240 loss: 1.4411258179401225 grad: -0.3132540022898153
iteration: 250 loss: 1.384237962978618 grad: -0.30151398896879245
iteration: 260 loss: 1.3316685623369928 grad: -0.29061984222357806
iteration: 0 loss: 73.62660887246078 grad: 158.04544408913392
iteration: 10 loss: 20.988465401432407 grad: -1.3006301701726806
iteration: 20 loss: 13.34648866190882 grad: -2.0825043759177104
iteration: 30 loss: 9.750087598895975 grad: -1.7961527833001392
iteration: 40 loss: 7.6577298651504035 grad: -1.5255314180249313
iteration: 50 loss: 6.293172029167843 grad: -1.314323130829153
iteration: 60 loss: 5.335314739123863 grad: -1.1502336798312678
iteration: 70 loss: 4.627165568249273 grad: -1.0205075392918221
iteration: 80 loss: 4.083010094778894 grad: -0.9159318304976224
iteration: 90 loss: 3.6521707967795836 grad: -0.8301083421162205
iteration: 100 loss: 3.3028116789875472 grad: -0.7585547736114393
iteration: 110 loss: 3.013951310570478 grad: -0.6980710848625649
iteration: 120 loss: 2.771213419191926 grad: -0.6463256388221006
iteration: 130 loss: 2.564424766265223 grad: -0.6015861360122676
iteration: 140 loss: 2.386188071272913 grad: -0.562542266674845
iteration: 150 loss: 2.2309982045063736 grad: -0.5281866140205331
iteration: 160 loss: 2.09467513814467 grad: -0.49773310519976605
iteration: 170 loss: 1.9739888816516213 grad: -0.47056011226945876
iteration: 180 loss: 1.8664047372114345 grad: -0.4461700437525832
iteration: 190 loss: 1.769906198079339 grad: -0.4241601717592636
iteration: 200 loss: 1.6828692571404893 grad: -0.40420124787586537
iteration: 210 loss: 1.6039715435759634 grad: -0.38602160662914636
iteration: 220 loss: 1.5321255415820731 grad: -0.36939519404538335
iteration: 230 loss: 1.4664287700138217 grad: -0.35413244341408073
iteration: 240 loss: 1.4061261081189762 grad: -0.340073243508481
iteration: 250 loss: 1.35058095200574 grad: -0.3270814633694573
iteration: 0 loss: 74.89400608492838 grad: 105.22851771833663
iteration: 10 loss: 22.24183333349292 grad: 2.3943903572763787
iteration: 20 loss: 13.938492169707615 grad: -0.8642128424978255
iteration: 30 loss: 10.135124486356517 grad: -1.1440914676001304
iteration: 40 loss: 7.9459702222941155 grad: -1.0910631657493828
iteration: 50 loss: 6.525820663149633 grad: -0.9907681058513134
iteration: 60 loss: 5.5317172881913885 grad: -0.8935029191028823
iteration: 70 loss: 4.797868400985615 grad: -0.8084723761042563
iteration: 80 loss: 4.234405411916627 grad: -0.7359232658369861
iteration: 90 loss: 3.7884460779306126 grad: -0.6741746758922308
iteration: 100 loss: 3.42687495211265 grad: -0.6213639472451764
iteration: 110 loss: 3.1279145766291716 grad: -0.5758678202677414
iteration: 120 loss: 2.8766653643138356 grad: -0.5363637884168421
iteration: 130 loss: 2.662594477347532 grad: -0.5017969361544041
iteration: 140 loss: 2.478048462925761 grad: -0.47132921090036206
iteration: 150 loss: 2.3173341826664826 grad: -0.4442927323914591
iteration: 160 loss: 2.1761301446034107 grad: -0.42015184853043763
iteration: 170 loss: 2.0510976444571 grad: -0.3984736436931824
iteration: 180 loss: 1.9396169136483132 grad: -0.37890538369552307
iteration: 190 loss: 1.8396038246155513 grad: -0.3611573299721236
iteration: 200 loss: 1.7493798788102801 grad: -0.34498961510101556
iteration: 210 loss: 1.6675782607396534 grad: -0.330202166862522
iteration: 220 loss: 1.593074813375014 grad: -0.3166269201387557
iteration: 230 loss: 1.5249365563968653 grad: -0.3041217521097714
iteration: 240 loss: 1.4623827620723866 grad: -0.2925657229724525
iteration: 250 loss: 1.4047551581663604 grad: -0.28185531242475137
iteration: 260 loss: 1.3514948572944625 grad: -0.2719014212004536
iteration: 0 loss: 76.32407141890748 grad: 135.80960885658249
iteration: 10 loss: 21.76217672229694 grad: -1.11929847703399
iteration: 20 loss: 13.713740564189989 grad: -2.027202238606691
iteration: 30 loss: 9.97872592488689 grad: -1.7404810501162655
iteration: 40 loss: 7.822627440174818 grad: -1.4738177601985076
iteration: 50 loss: 6.422644877850706 grad: -1.2671440376107346
iteration: 60 loss: 5.442430305918523 grad: -1.1073438763478627
iteration: 70 loss: 4.718873052754299 grad: -0.9814970330006144
iteration: 80 loss: 4.16341419200204 grad: -0.8803648172942222
iteration: 90 loss: 3.7238958853761854 grad: -0.7975699632494464
iteration: 100 loss: 3.3676406540599104 grad: -0.728670445691496
iteration: 110 loss: 3.0731540213597657 grad: -0.6705123379161145
iteration: 120 loss: 2.825728826245516 grad: -0.6208089586452672
iteration: 130 loss: 2.614968452997387 grad: -0.5778686686114802
iteration: 140 loss: 2.4333190540395746 grad: -0.5404165218216608
iteration: 150 loss: 2.275162277790183 grad: -0.5074752248934885
iteration: 160 loss: 2.1362340376657896 grad: -0.4782840949203835
iteration: 170 loss: 2.0132405229650514 grad: -0.4522428076613249
iteration: 180 loss: 1.9035976397992933 grad: -0.4288716312419256
iteration: 190 loss: 1.8052500053401508 grad: -0.4077828306115695
iteration: 200 loss: 1.7165425651412327 grad: -0.38865977809979246
iteration: 210 loss: 1.6361278312565446 grad: -0.3712414700236386
iteration: 220 loss: 1.5628977339271264 grad: -0.35531089560680496
iteration: 230 loss: 1.4959327988360862 grad: -0.34068619121417043
iteration: 240 loss: 1.4344637257496204 grad: -0.32721383577539875
iteration: 250 loss: 1.3778419799496078 grad: -0.31476336090713425
iteration: 260 loss: 1.325517025334772 grad: -0.3032231981818799
iteration: 0 loss: 74.13054647058563 grad: 113.59438942074397
iteration: 10 loss: 21.197066282270644 grad: 1.1620888943331504
iteration: 20 loss: 13.413360650855877 grad: -1.4165814004777073
iteration: 30 loss: 9.795009759015597 grad: -1.430115767021559
iteration: 40 loss: 7.696594543882094 grad: -1.2702514499180573
iteration: 50 loss: 6.329282483951829 grad: -1.1175808299484533
iteration: 60 loss: 5.3694949204923015 grad: -0.9903982497461028
iteration: 70 loss: 4.659666720771425 grad: -0.8863170526020262
iteration: 80 loss: 4.113949717358992 grad: -0.8006622372490562
iteration: 90 loss: 3.68164007890081 grad: -0.72937871072735
iteration: 100 loss: 3.3308994466213915 grad: -0.6693361291521913
iteration: 110 loss: 3.0407467860009505 grad: -0.6181773950489087
iteration: 120 loss: 2.7968044233698905 grad: -0.5741269175625738
iteration: 130 loss: 2.588895686038954 grad: -0.5358351741971702
iteration: 140 loss: 2.4096184953183513 grad: -0.5022643270404288
iteration: 150 loss: 2.253462227970968 grad: -0.47260624790878647
iteration: 160 loss: 2.1162411514402595 grad: -0.44622395444836993
iteration: 170 loss: 1.9947196668039064 grad: -0.42260951677623404
iteration: 180 loss: 1.8863577416822492 grad: -0.40135350155163796
iteration: 190 loss: 1.7891338993703854 grad: -0.3821225372785959
iteration: 200 loss: 1.7014195636128617 grad: -0.3646426475048885
iteration: 210 loss: 1.621888198678417 grad: -0.34868672455088645
iteration: 220 loss: 1.5494485127767554 grad: -0.3340650093271794
iteration: 230 loss: 1.4831946125757234 grad: -0.32061777847753303
iteration: 240 loss: 1.4223682994147566 grad: -0.3082096703361514
iteration: 250 loss: 1.366330195133659 grad: -0.2967252406046311
iteration: 260 loss: 1.3145373783084902 grad: -0.286065450164732
iteration: 0 loss: 74.37878764256699 grad: 101.10043720883857
iteration: 10 loss: 22.014530153954887 grad: 1.8418691392026738
iteration: 20 loss: 13.760155823497701 grad: -0.9224223870308322
iteration: 30 loss: 9.990815548779397 grad: -1.1045725313558616
iteration: 40 loss: 7.82695613361503 grad: -1.0387081585505338
iteration: 50 loss: 6.425412909276267 grad: -0.9408345624606365
iteration: 60 loss: 5.44529718192259 grad: -0.8483852887794967
iteration: 70 loss: 4.72224469071712 grad: -0.7679967059279341
iteration: 80 loss: 4.167320852367291 grad: -0.6994572334397615
iteration: 90 loss: 3.72826047231117 grad: -0.6410916399359541
iteration: 100 loss: 3.3723657164558642 grad: -0.5911342892782475
iteration: 110 loss: 3.078149370821889 grad: -0.5480615939445077
iteration: 120 loss: 2.8309187503026476 grad: -0.5106345036401982
iteration: 130 loss: 2.620291958774115 grad: -0.47786418660477925
iteration: 140 loss: 2.4387279680726848 grad: -0.44896415591795247
iteration: 150 loss: 2.280618940663565 grad: -0.4233067867750673
iteration: 160 loss: 2.1417091838748092 grad: -0.4003880611708881
iteration: 170 loss: 2.018711505639699 grad: -0.3798001489126927
iteration: 180 loss: 1.9090470036423788 grad: -0.361210405476746
iteration: 190 loss: 1.8106643611673794 grad: -0.3443453409856402
iteration: 200 loss: 1.7219117098780274 grad: -0.32897835610171006
iteration: 210 loss: 1.6414440619386037 grad: -0.3149203111282937
iteration: 220 loss: 1.5681553129120402 grad: -0.3020122255667137
iteration: 230 loss: 1.5011275361522498 grad: -0.29011958539996463
iteration: 240 loss: 1.4395926520326363 grad: -0.279127870416403
iteration: 250 loss: 1.3829030894863867 grad: -0.2689390135297002
iteration: 260 loss: 1.3305090734989706 grad: -0.2594685771466134
iteration: 0 loss: 73.09178562583539 grad: 106.03084018958077
iteration: 10 loss: 22.002252164389656 grad: 2.522238780136466
iteration: 20 loss: 13.756126498761411 grad: -0.705022628586154
iteration: 30 loss: 9.992246836944167 grad: -1.0272328099635957
iteration: 40 loss: 7.830027737774757 grad: -1.0096339409448016
iteration: 50 loss: 6.428771411069317 grad: -0.9323483006245972
iteration: 60 loss: 5.4485021339069135 grad: -0.8498293145988072
iteration: 70 loss: 4.72517545266667 grad: -0.7745602547128735
iteration: 80 loss: 4.1699659992806035 grad: -0.7087385059246323
iteration: 90 loss: 3.7306430751808133 grad: -0.6518052469019222
iteration: 100 loss: 3.374517029411601 grad: -0.6025570301801326
iteration: 110 loss: 3.080099876798282 grad: -0.5597719692108012
iteration: 120 loss: 2.8326954693157362 grad: -0.5223812963049188
iteration: 130 loss: 2.6219179956819145 grad: -0.4894962174338108
iteration: 140 loss: 2.4402228156695442 grad: -0.46039089353221724
iteration: 150 loss: 2.281998987176807 grad: -0.4344753630495664
iteration: 160 loss: 2.142988225589058 grad: -0.4112694100186545
iteration: 170 loss: 2.0199011998704672 grad: -0.39038041671053225
iteration: 180 loss: 1.910157248312027 grad: -0.37148550833053784
iteration: 190 loss: 1.8117036055494034 grad: -0.3543174532458163
iteration: 200 loss: 1.7228872065189038 grad: -0.33865361242391245
iteration: 210 loss: 1.642362070312753 grad: -0.3243072865850892
iteration: 220 loss: 1.5690212642051515 grad: -0.31112092319624624
iteration: 230 loss: 1.5019461669871816 grad: -0.2989607595068966
iteration: 240 loss: 1.4403681134480062 grad: -0.2877125747098439
iteration: 250 loss: 1.3836390361759434 grad: -0.27727830129589137
iteration: 260 loss: 1.3312087372553585 grad: -0.26757330500336046
iteration: 0 loss: 75.90376835940054 grad: 101.11026759684822
iteration: 10 loss: 21.866358612097326 grad: 2.2292340070882424
iteration: 20 loss: 13.659209295303732 grad: -0.7866563246914091
iteration: 30 loss: 9.93037622687852 grad: -1.073288034401734
iteration: 40 loss: 7.788964146920514 grad: -1.0416435326993883
iteration: 50 loss: 6.400253782241138 grad: -0.9557640846343158
iteration: 60 loss: 5.427940052482096 grad: -0.8674458597321828
iteration: 70 loss: 4.709906613727608 grad: -0.7880616064273952
iteration: 80 loss: 4.15836530254633 grad: -0.7192125725834121
iteration: 90 loss: 3.7216717552324883 grad: -0.6599952917937093
iteration: 100 loss: 3.367484333687447 grad: -0.6089914497258052
iteration: 110 loss: 3.074530722853069 grad: -0.5648367158460266
iteration: 120 loss: 2.8282535434098217 grad: -0.5263639025231943
iteration: 130 loss: 2.618359233971081 grad: -0.4926143173319315
iteration: 140 loss: 2.437366249237708 grad: -0.4628114805235341
iteration: 150 loss: 2.279707889583609 grad: -0.4363281509338848
iteration: 160 loss: 2.141157580883125 grad: -0.41265628669094984
iteration: 170 loss: 2.0184491654268144 grad: -0.3913822394028128
iteration: 180 loss: 1.9090192107360053 grad: -0.37216705521547044
iteration: 190 loss: 1.8108279604299158 grad: -0.3547310766934335
iteration: 200 loss: 1.7222323090526184 grad: -0.33884196562006064
iteration: 210 loss: 1.6418939967797583 grad: -0.32430538069244186
iteration: 220 loss: 1.5687121449201518 grad: -0.31095769532337
iteration: 230 loss: 1.5017729285078447 grad: -0.2986602793487069
iteration: 240 loss: 1.4403115181746877 grad: -0.28729498156995636
iteration: 250 loss: 1.383682940959013 grad: -0.27676053796982847
iteration: 260 loss: 1.3313395152800611 grad: -0.26696969719984043
iteration: 0 loss: 76.27620654577534 grad: 128.63980073021662
iteration: 10 loss: 21.650699164084575 grad: -0.8161998817644529
iteration: 20 loss: 13.62532635288713 grad: -1.8131010120411775
iteration: 30 loss: 9.916459123845842 grad: -1.5917849550160466
iteration: 40 loss: 7.7775282689977265 grad: -1.3619016465045621
iteration: 50 loss: 6.388893070735197 grad: -1.1776828730981515
iteration: 60 loss: 5.416453045244163 grad: -1.0330200627309276
iteration: 70 loss: 4.698411152803394 grad: -0.9181130262933963
iteration: 80 loss: 4.146985792758559 grad: -0.8252684278766544
iteration: 90 loss: 3.7104951942504325 grad: -0.7489662656469969
iteration: 100 loss: 3.3565640054287296 grad: -0.6852837929612664
iteration: 110 loss: 3.063895892380763 grad: -0.6314025951161562
iteration: 120 loss: 2.8179175226463156 grad: -0.5852634254605746
iteration: 130 loss: 2.608325064672499 grad: -0.5453346134013064
iteration: 140 loss: 2.427630508898553 grad: -0.5104570861475862
iteration: 150 loss: 2.2702631956006125 grad: -0.4797395345365913
iteration: 160 loss: 2.1319942278122324 grad: -0.45248639109566713
iteration: 170 loss: 2.00955619203759 grad: -0.4281475114749947
iteration: 180 loss: 1.9003850926228552 grad: -0.4062824269648702
iteration: 190 loss: 1.802441056089637 grad: -0.38653453942291083
iteration: 200 loss: 1.7140811430414364 grad: -0.3686122112326552
iteration: 210 loss: 1.6339674348436843 grad: -0.35227471244600495
iteration: 220 loss: 1.5609994973998171 grad: -0.33732164078268656
iteration: 230 loss: 1.4942640061549768 grad: -0.3235848596265911
iteration: 240 loss: 1.4329966565383487 grad: -0.3109222857071078
iteration: 250 loss: 1.3765530042184186 grad: -0.29921305220541755
iteration: 260 loss: 1.324385886868847 grad: -0.2883537063279507
iteration: 0 loss: 75.0302885129902 grad: 132.69132145256532
iteration: 10 loss: 21.872075041681477 grad: 0.548859255021049
iteration: 20 loss: 13.767286819905435 grad: -1.468052147016923
iteration: 30 loss: 10.027998703796728 grad: -1.4882219430606165
iteration: 40 loss: 7.868202343853475 grad: -1.331176170482387
iteration: 50 loss: 6.46432184839399 grad: -1.1743267742684795
iteration: 60 loss: 5.480485307087705 grad: -1.0412330573356958
iteration: 70 loss: 4.753731097127539 grad: -0.931426024498127
iteration: 80 loss: 4.195495935781806 grad: -0.840747529362959
iteration: 90 loss: 3.753571866920662 grad: -0.7651934751250873
iteration: 100 loss: 3.3952247117239165 grad: -0.7015489688493968
iteration: 110 loss: 3.098908784396785 grad: -0.6473469493257538
iteration: 120 loss: 2.8498738522902882 grad: -0.600710438880262
iteration: 130 loss: 2.637687986555606 grad: -0.5602052315162748
iteration: 140 loss: 2.454768606783215 grad: -0.5247252728956369
iteration: 150 loss: 2.2954739739213683 grad: -0.49340816046939084
iteration: 160 loss: 2.155520886666264 grad: -0.46557380507922697
iteration: 170 loss: 2.031599820582373 grad: -0.4406799233858243
iteration: 180 loss: 1.9211136792264782 grad: -0.41828954249143646
iteration: 190 loss: 1.8219962479515743 grad: -0.3980470572368999
iteration: 200 loss: 1.7325833885808855 grad: -0.37966040582227417
iteration: 210 loss: 1.651519946204396 grad: -0.36288765782817045
iteration: 220 loss: 1.5776913399584125 grad: -0.34752681513975436
iteration: 230 loss: 1.5101725336398886 grad: -0.3334079763792852
iteration: 240 loss: 1.4481894496716037 grad: -0.3203872580067999
iteration: 250 loss: 1.3910894284488016 grad: -0.30834203431911583
iteration: 260 loss: 1.338318354814791 grad: -0.29716717738913623
iteration: 0 loss: 77.64718766503573 grad: 149.05953407448874
iteration: 10 loss: 22.296534299416944 grad: -1.7816074194993174
iteration: 20 loss: 13.98460201029157 grad: -2.0582675803033217
iteration: 30 loss: 10.143303855311656 grad: -1.7318502617115148
iteration: 40 loss: 7.934849749797629 grad: -1.4611497368788273
iteration: 50 loss: 6.505183589122018 grad: -1.2550509280061968
iteration: 60 loss: 5.5064378655995165 grad: -1.0965211586572818
iteration: 70 loss: 4.770473500622426 grad: -0.971901558105682
iteration: 80 loss: 4.206255331768674 grad: -0.8718175966713331
iteration: 90 loss: 3.7602912400457584 grad: -0.7898932278428041
iteration: 100 loss: 3.399132547805245 grad: -0.7217155926416084
iteration: 110 loss: 3.1008131928095035 grad: -0.6641606617832224
iteration: 120 loss: 2.8503239798615967 grad: -0.6149664841156253
iteration: 130 loss: 2.6370672248814446 grad: -0.5724606859871412
iteration: 140 loss: 2.4533507700398745 grad: -0.5353830904285973
iteration: 150 loss: 2.293458415293406 grad: -0.502767730301868
iteration: 160 loss: 2.1530551479161413 grad: -0.4738626712073566
iteration: 170 loss: 2.0287946429865853 grad: -0.448074413678464
iteration: 180 loss: 1.9180531911826868 grad: -0.4249286078156621
iteration: 190 loss: 1.8187450253091646 grad: -0.40404180907267817
iteration: 200 loss: 1.729191439320733 grad: -0.385100846181349
iteration: 210 loss: 1.6480262884190944 grad: -0.3678475276597314
iteration: 220 loss: 1.5741266085203114 grad: -0.3520671521513006
iteration: 230 loss: 1.5065609047963087 grad: -0.33757976904019654
iteration: 240 loss: 1.4445500793359647 grad: -0.32423345472078346
iteration: 250 loss: 1.3874375389304574 grad: -0.3118990847815548
iteration: 260 loss: 1.3346660641693895 grad: -0.3004662293810077
iteration: 0 loss: 75.1205801297183 grad: 139.50838416018186
iteration: 10 loss: 22.170600752546452 grad: -0.500474573637107
iteration: 20 loss: 13.935075604282826 grad: -1.551077786354948
iteration: 30 loss: 10.124628658261331 grad: -1.4305107082488273
iteration: 40 loss: 7.929088365381673 grad: -1.257960346955162
iteration: 50 loss: 6.505436064884386 grad: -1.1081762169894636
iteration: 60 loss: 5.509727826695325 grad: -0.984933534649318
iteration: 70 loss: 4.775369939009 grad: -0.8837692723483527
iteration: 80 loss: 4.212007794575832 grad: -0.8000333564924018
iteration: 90 loss: 3.766480327088671 grad: -0.7299680187762603
iteration: 100 loss: 3.405513379697905 grad: -0.6706875300518174
iteration: 110 loss: 3.1072387502776193 grad: -0.6199987320276408
iteration: 120 loss: 2.8567047390560303 grad: -0.5762304638486808
iteration: 130 loss: 2.6433486539570534 grad: -0.5380992519635837
iteration: 140 loss: 2.4595002049012775 grad: -0.5046093757560214
iteration: 150 loss: 2.29945712528698 grad: -0.47497992569715874
iteration: 160 loss: 2.1588933981636895 grad: -0.448591764910932
iteration: 170 loss: 2.0344685485470935 grad: -0.42494882347603946
iteration: 180 loss: 1.9235626743165581 grad: -0.403649644140742
iteration: 190 loss: 1.82409246871529 grad: -0.38436627173222493
iteration: 200 loss: 1.7343807877024036 grad: -0.36682843440672114
iteration: 210 loss: 1.6530624440213366 grad: -0.3508115693893691
iteration: 220 loss: 1.5790150223271502 grad: -0.336127667661545
iteration: 230 loss: 1.5113072996311365 grad: -0.3226182057368301
iteration: 240 loss: 1.4491602628094453 grad: -0.31014863779004864
iteration: 250 loss: 1.3919172781177374 grad: -0.2986040655513133
iteration: 260 loss: 1.3390210029631568 grad: -0.28788580546158415
iteration: 0 loss: 76.79678984662134 grad: 130.9597359067921
iteration: 10 loss: 21.9903392665517 grad: -0.6126769844290257
iteration: 20 loss: 13.804197478668522 grad: -1.7965549829993075
iteration: 30 loss: 10.033028183742523 grad: -1.5722719156715423
iteration: 40 loss: 7.861675833437465 grad: -1.3390617048007183
iteration: 50 loss: 6.4535918633070075 grad: -1.1549570200076007
iteration: 60 loss: 5.468405020642587 grad: -1.0116132839615508
iteration: 70 loss: 4.741476281458455 grad: -0.898290673957111
iteration: 80 loss: 4.183562207287898 grad: -0.8069822084108793
iteration: 90 loss: 3.742159950740188 grad: -0.732076965653561
iteration: 100 loss: 3.3844015877160185 grad: -0.6696372972261598
iteration: 110 loss: 3.088679562841166 grad: -0.6168547321442672
iteration: 120 loss: 2.8402154327723936 grad: -0.5716867632361546
iteration: 130 loss: 2.6285653541365885 grad: -0.5326189128208312
iteration: 140 loss: 2.446142855509937 grad: -0.49850767641533145
iteration: 150 loss: 2.2873062418291905 grad: -0.4684751859007559
iteration: 160 loss: 2.1477742170661505 grad: -0.4418372523385121
iteration: 170 loss: 2.0242399560047075 grad: -0.41805329304076333
iteration: 180 loss: 1.9141093053790015 grad: -0.3966908629405752
iteration: 190 loss: 1.8153189593555743 grad: -0.37740010821576153
iteration: 200 loss: 1.726207520419228 grad: -0.3598950789296697
iteration: 210 loss: 1.6454223495623987 grad: -0.3439398615497396
iteration: 220 loss: 1.5718511413177276 grad: -0.3293381508887897
iteration: 230 loss: 1.5045708996618403 grad: -0.31592531181981753
iteration: 240 loss: 1.442809367422466 grad: -0.30356226751687176
iteration: 250 loss: 1.3859155052803709 grad: -0.29213074437661757
iteration: 260 loss: 1.3333366388938388 grad: -0.28152953632775624
iteration: 0 loss: 75.08457853510838 grad: 138.66479324901763
iteration: 10 loss: 22.155502944363 grad: 0.08457237782409656
iteration: 20 loss: 13.877619369679595 grad: -1.453154471985484
iteration: 30 loss: 10.073221378682279 grad: -1.4289353529611755
iteration: 40 loss: 7.885557732270937 grad: -1.2707920356723041
iteration: 50 loss: 6.468447457269783 grad: -1.1192409287567402
iteration: 60 loss: 5.477892800059098 grad: -0.9918939556153473
iteration: 70 loss: 4.747598543785568 grad: -0.8871854114594028
iteration: 80 loss: 4.187485820479693 grad: -0.8008365756057784
iteration: 90 loss: 3.744599660832385 grad: -0.728932510970241
iteration: 100 loss: 3.3858134134154416 grad: -0.668377739668429
iteration: 110 loss: 3.089364480864705 grad: -0.6168118667339362
iteration: 120 loss: 2.840377833786492 grad: -0.5724444224680848
iteration: 130 loss: 2.628347433836676 grad: -0.5339093438418729
iteration: 140 loss: 2.445645602260108 grad: -0.5001539397640893
iteration: 150 loss: 2.2866026581914825 grad: -0.4703577465958159
iteration: 160 loss: 2.146917883766946 grad: -0.4438738848609961
iteration: 170 loss: 2.023270716629833 grad: -0.420186608069574
iteration: 180 loss: 1.913057122677525 grad: -0.3988803495530898
iteration: 190 loss: 1.8142065823487676 grad: -0.37961693305288025
iteration: 200 loss: 1.7250523612925794 grad: -0.3621186140998707
iteration: 210 loss: 1.6442378252212289 grad: -0.3461553225305929
iteration: 220 loss: 1.5706476458966045 grad: -0.33153496240935104
iteration: 230 loss: 1.5033565185567155 grad: -0.31809596035701065
iteration: 240 loss: 1.441590407658623 grad: -0.3057014846939844
iteration: 250 loss: 1.3846968936321569 grad: -0.2942349188951039
iteration: 260 loss: 1.3321222236402883 grad: -0.28359628596652664
iteration: 0 loss: 76.66744527345239 grad: 137.48168647135745
iteration: 10 loss: 22.342761191939747 grad: -0.6006781254822862
iteration: 20 loss: 13.970844857268297 grad: -1.554258621395745
iteration: 30 loss: 10.131071871013814 grad: -1.4359567089463177
iteration: 40 loss: 7.927932841035065 grad: -1.2606781917809222
iteration: 50 loss: 6.502356558911867 grad: -1.107189885053408
iteration: 60 loss: 5.506387701080092 grad: -0.9811525465696517
iteration: 70 loss: 4.772270347727086 grad: -0.8782648516832995
iteration: 80 loss: 4.209281114738463 grad: -0.7935949722643935
iteration: 90 loss: 3.7641363727096757 grad: -0.7231009632869231
iteration: 100 loss: 3.4035222033697377 grad: -0.6636973077095469
iteration: 110 loss: 3.1055600207933325 grad: -0.6130644977528035
iteration: 120 loss: 2.855298301899562 grad: -0.5694542261540628
iteration: 130 loss: 2.642178089432615 grad: -0.531536378797638
iteration: 140 loss: 2.458533706878825 grad: -0.4982870900874206
iteration: 150 loss: 2.298667369220642 grad: -0.4689085010901204
iteration: 160 loss: 2.158257064501744 grad: -0.4427713434847985
iteration: 170 loss: 2.033965769322549 grad: -0.4193736010112394
iteration: 180 loss: 1.923176506707064 grad: -0.3983104566683767
iteration: 190 loss: 1.8238084307169051 grad: -0.3792522003695976
iteration: 200 loss: 1.7341864625693069 grad: -0.361927800338109
iteration: 210 loss: 1.6529471489408356 grad: -0.34611254618526777
iteration: 220 loss: 1.5789695330937652 grad: -0.33161865156987047
iteration: 230 loss: 1.5113236225462805 grad: -0.31828803208606044
iteration: 240 loss: 1.449231445741134 grad: -0.3059866993632655
iteration: 250 loss: 1.3920372537617036 grad: -0.2946003686709002
iteration: 260 loss: 1.3391844584963442 grad: -0.2840309868275006
iteration: 0 loss: 73.99747291685406 grad: 110.80733252036825
iteration: 10 loss: 22.395699149035842 grad: 3.352029369693316
iteration: 20 loss: 13.901999898120163 grad: -0.23526256703428508
iteration: 30 loss: 10.07483400181191 grad: -0.8069121463401574
iteration: 40 loss: 7.886889235979404 grad: -0.8859386923368535
iteration: 50 loss: 6.472369141282467 grad: -0.8524196706991367
iteration: 60 loss: 5.484136925267152 grad: -0.7930424543725023
iteration: 70 loss: 4.755501981517765 grad: -0.731534547083071
iteration: 80 loss: 4.196482886850018 grad: -0.6746261661853274
iteration: 90 loss: 3.7542767989959263 grad: -0.6238429006056385
iteration: 100 loss: 3.3958816340651867 grad: -0.579047212432142
iteration: 110 loss: 3.0996248328280918 grad: -0.5396095296113572
iteration: 120 loss: 2.8506941016894416 grad: -0.504812538550055
iteration: 130 loss: 2.6386267180777065 grad: -0.4739873092555692
iteration: 140 loss: 2.455824945321927 grad: -0.44655167362670756
iteration: 150 loss: 2.2966399025582507 grad: -0.42201304819610796
iteration: 160 loss: 2.1567854059083156 grad: -0.3999593023096995
iteration: 170 loss: 2.0329511027798617 grad: -0.3800467114003739
iteration: 180 loss: 1.922540159495301 grad: -0.3619883345232253
iteration: 190 loss: 1.8234871444159249 grad: -0.3455438896047922
iteration: 200 loss: 1.7341289158638657 grad: -0.3305113124909455
iteration: 210 loss: 1.6531113659252283 grad: -0.31671986183006134
iteration: 220 loss: 1.5793209273892899 grad: -0.3040245373726973
iteration: 230 loss: 1.5118335042667157 grad: -0.29230157501315246
iteration: 240 loss: 1.449875869710013 grad: -0.28144480936742333
iteration: 250 loss: 1.392796122302653 grad: -0.2713627297959089
iteration: 260 loss: 1.3400408162068411 grad: -0.2619760892039794
iteration: 0 loss: 72.30767945748998 grad: 144.69294898978518
iteration: 10 loss: 21.8673054685613 grad: 1.0974959130153046
iteration: 20 loss: 13.73017287444236 grad: -0.8634222881569794
iteration: 30 loss: 9.983439015583372 grad: -1.1365603409709166
iteration: 40 loss: 7.824854809633797 grad: -1.1177181816316306
iteration: 50 loss: 6.42409198032936 grad: -1.0354283229841135
iteration: 60 loss: 5.443558484100157 grad: -0.9450874326362146
iteration: 70 loss: 4.719846753450136 grad: -0.8614230689257504
iteration: 80 loss: 4.1643029067311295 grad: -0.7876656836806009
iteration: 90 loss: 3.7247322171641084 grad: -0.7236033009450951
iteration: 100 loss: 3.368441047903244 grad: -0.6680830910865716
iteration: 110 loss: 3.073927278646889 grad: -0.6198211889675288
iteration: 120 loss: 2.8264799340026086 grad: -0.5776526994619514
iteration: 130 loss: 2.6157004162057755 grad: -0.5405902011408648
iteration: 140 loss: 2.434033812720124 grad: -0.5078183140939461
iteration: 150 loss: 2.27586118195706 grad: -0.47866981575716605
iteration: 160 loss: 2.136918099986691 grad: -0.45259935047355876
iteration: 170 loss: 2.013910556121697 grad: -0.4291599343056006
iteration: 180 loss: 1.904254332535443 grad: -0.4079834701852268
iteration: 190 loss: 1.8058939652539514 grad: -0.3887650962844642
iteration: 200 loss: 1.7171743431621176 grad: -0.37125078306922654
iteration: 210 loss: 1.6367479359050363 grad: -0.3552275493720642
iteration: 220 loss: 1.5635066398326258 grad: -0.3405157425033427
iteration: 230 loss: 1.4965309519992098 grad: -0.32696292920250825
iteration: 240 loss: 1.4350515469580138 grad: -0.31443903999650025
iteration: 250 loss: 1.3784198671399033 grad: -0.3028324896396249
iteration: 260 loss: 1.3260853553455574 grad: -0.29204705995828945
iteration: 0 loss: 72.97012064590768 grad: 120.95015704178766
iteration: 10 loss: 21.704916785452557 grad: 2.314558837198388
iteration: 20 loss: 13.625976642931432 grad: -0.991867065813087
iteration: 30 loss: 9.906324874058555 grad: -1.2887954924787326
iteration: 40 loss: 7.762103030699646 grad: -1.2202353128359356
iteration: 50 loss: 6.371008011987752 grad: -1.1011228614742268
iteration: 60 loss: 5.397667085615219 grad: -0.9874224715852207
iteration: 70 loss: 4.679553588173059 grad: -0.8890064856286906
iteration: 80 loss: 4.12848607364578 grad: -0.8057238453718417
iteration: 90 loss: 3.6925665371522056 grad: -0.7353446880387985
iteration: 100 loss: 3.339302193030025 grad: -0.6755279366426784
iteration: 110 loss: 3.047332602905995 grad: -0.6242775047676712
iteration: 120 loss: 2.802049572544382 grad: -0.5799903420034862
iteration: 130 loss: 2.593130721596191 grad: -0.5414019763418239
iteration: 140 loss: 2.4130787172665475 grad: -0.5075171630883198
iteration: 150 loss: 2.2563188326604093 grad: -0.4775490213257044
iteration: 160 loss: 2.118621093299556 grad: -0.45087079983418843
iteration: 170 loss: 1.9967186835063908 grad: -0.42697902272124943
iteration: 180 loss: 1.8880491101101804 grad: -0.405465669091541
iteration: 190 loss: 1.7905744523606122 grad: -0.3859971991292679
iteration: 200 loss: 1.702653906270453 grad: -0.3682986793682162
iteration: 210 loss: 1.6229517154739286 grad: -0.35214168900388926
iteration: 220 loss: 1.550369546564277 grad: -0.337335035228968
iteration: 230 loss: 1.4839960664713048 grad: -0.3237175663062602
iteration: 240 loss: 1.423068829962052 grad: -0.3111525619880139
iteration: 250 loss: 1.3669451117559468 grad: -0.29952331911857055
iteration: 260 loss: 1.3150793288682507 grad: -0.28872965011227186
iteration: 0 loss: 76.05917582210978 grad: 95.53465683230087
iteration: 10 loss: 22.300758517168934 grad: 3.1088127595511077
iteration: 20 loss: 13.928191502383015 grad: -0.733114320546978
iteration: 30 loss: 10.118365637741197 grad: -1.1209855816451642
iteration: 40 loss: 7.929706556570037 grad: -1.0955656682128527
iteration: 50 loss: 6.511087971021838 grad: -1.0022433144922034
iteration: 60 loss: 5.518538781938704 grad: -0.9058458952092137
iteration: 70 loss: 4.786069215974459 grad: -0.8198409177554092
iteration: 80 loss: 4.223790102129235 grad: -0.7458540527805773
iteration: 90 loss: 3.778839922089529 grad: -0.6826654442147637
iteration: 100 loss: 3.4181308213530675 grad: -0.6285573982279533
iteration: 110 loss: 3.1199107701815425 grad: -0.5819385818309686
iteration: 120 loss: 2.869301514953101 grad: -0.5414786590855729
iteration: 130 loss: 2.655787559049837 grad: -0.5061026153992472
iteration: 140 loss: 2.471729409603995 grad: -0.4749503746859842
iteration: 150 loss: 2.3114451607999316 grad: -0.44733379114191973
iteration: 160 loss: 2.1706223923968704 grad: -0.4226997628033089
iteration: 170 loss: 2.0459297682998776 grad: -0.4006007866633439
iteration: 180 loss: 1.9347535435423941 grad: -0.38067209651050765
iteration: 190 loss: 1.8350145484957714 grad: -0.3626140819960209
iteration: 200 loss: 1.7450383931070725 grad: -0.34617878384593753
iteration: 210 loss: 1.663461688985039 grad: -0.33115948683049173
iteration: 220 loss: 1.589163155984393 grad: -0.3173826562069276
iteration: 230 loss: 1.5212122434215665 grad: -0.3047016488160251
iteration: 240 loss: 1.4588302872866734 grad: -0.29299177347667577
iteration: 250 loss: 1.4013607778106647 grad: -0.2821463830592723
iteration: 260 loss: 1.3482463405489398 grad: -0.27207376050156284
iteration: 0 loss: 74.60063066852204 grad: 86.79242344847407
iteration: 10 loss: 22.0856812899681 grad: 3.388094953342949
iteration: 20 loss: 13.801604119755439 grad: -0.4708608497211484
iteration: 30 loss: 10.02793789327738 grad: -0.913046153472242
iteration: 40 loss: 7.860001285984853 grad: -0.9334574203743412
iteration: 50 loss: 6.454923067093122 grad: -0.8738182974577173
iteration: 60 loss: 5.471867095511408 grad: -0.8017613481681632
iteration: 70 loss: 4.746374123400032 grad: -0.733632883725829
iteration: 80 loss: 4.1894059259391065 grad: -0.6730866041008744
iteration: 90 loss: 3.748613021669867 grad: -0.6202416785957209
iteration: 100 loss: 3.3912332304498705 grad: -0.5742647315401506
iteration: 110 loss: 3.0957306953757158 grad: -0.5341590626043382
iteration: 120 loss: 2.847375712114378 grad: -0.4990032656971974
iteration: 130 loss: 2.635757872598705 grad: -0.4680099909479029
iteration: 140 loss: 2.4533138968479515 grad: -0.4405258977855835
iteration: 150 loss: 2.2944183542430108 grad: -0.4160143705487077
iteration: 160 loss: 2.154801471570615 grad: -0.3940353858719404
iteration: 170 loss: 2.0311646597337214 grad: -0.37422721202946774
iteration: 180 loss: 1.9209196956381178 grad: -0.35629111091986154
iteration: 190 loss: 1.822007567395386 grad: -0.3399789896118534
iteration: 200 loss: 1.7327699983572777 grad: -0.3250835795077345
iteration: 210 loss: 1.6518566215272998 grad: -0.3114306657005154
iteration: 220 loss: 1.5781567841299875 grad: -0.29887293879607846
iteration: 230 loss: 1.510748687752146 grad: -0.2872851170892443
iteration: 240 loss: 1.44886093640699 grad: -0.2765600599931628
iteration: 250 loss: 1.391843102021498 grad: -0.26660565537980424
iteration: 260 loss: 1.3391429351213033 grad: -0.2573423128708802
iteration: 0 loss: 75.01470733918933 grad: 99.25329996835514
iteration: 10 loss: 22.437051797124425 grad: 2.451555040686003
iteration: 20 loss: 13.948857108113708 grad: -0.6198396220073163
iteration: 30 loss: 10.108211697835879 grad: -0.9509524591972248
iteration: 40 loss: 7.912373596477024 grad: -0.9441042713690081
iteration: 50 loss: 6.493029380693852 grad: -0.8755389908780629
iteration: 60 loss: 5.501579060278304 grad: -0.8001939285430641
iteration: 70 loss: 4.770643095889806 grad: -0.7308368380946967
iteration: 80 loss: 4.209891533984815 grad: -0.6698864217037884
iteration: 90 loss: 3.766329493644042 grad: -0.6169833235516202
iteration: 100 loss: 3.4068410052138707 grad: -0.5710942394451697
iteration: 110 loss: 3.109681777466159 grad: -0.5311346047724594
iteration: 120 loss: 2.859992059305651 grad: -0.4961432549159695
iteration: 130 loss: 2.6472764347127766 grad: -0.46531476634125835
iteration: 140 loss: 2.4639138736603274 grad: -0.43798784781477273
iteration: 150 loss: 2.304238422401092 grad: -0.4136227583631812
iteration: 160 loss: 2.1639510781336124 grad: -0.3917786585536721
iteration: 170 loss: 2.0397316618703125 grad: -0.37209408304845526
iteration: 180 loss: 1.9289756709187962 grad: -0.35427103480616556
iteration: 190 loss: 1.8296115935989044 grad: -0.33806233443230693
iteration: 200 loss: 1.73997140079721 grad: -0.32326165115740046
iteration: 210 loss: 1.6586970073490606 grad: -0.309695662628675
iteration: 220 loss: 1.5846715738576593 grad: -0.2972178774369492
iteration: 230 loss: 1.516968285657516 grad: -0.28570374842823243
iteration: 240 loss: 1.4548116370085402 grad: -0.27504678732562143
iteration: 250 loss: 1.3975478008428581 grad: -0.2651554578771148
iteration: 260 loss: 1.3446216922271477 grad: -0.25595067673262
iteration: 0 loss: 77.17757291290131 grad: 122.4555363683948
iteration: 10 loss: 22.516655856572573 grad: 0.6054943459011879
iteration: 20 loss: 14.031805803065089 grad: -1.1825439020702535
iteration: 30 loss: 10.172557157198526 grad: -1.2443458142585753
iteration: 40 loss: 7.962857831729542 grad: -1.1383296233833884
iteration: 50 loss: 6.53390975190041 grad: -1.0192084657554528
iteration: 60 loss: 5.535633152218865 grad: -0.9130426960620077
iteration: 70 loss: 4.799667029878699 grad: -0.8229052757307775
iteration: 80 loss: 4.2350849063816645 grad: -0.7470434910387318
iteration: 90 loss: 3.788523480067769 grad: -0.6829730568468189
iteration: 100 loss: 3.426631188862385 grad: -0.6284503190249353
iteration: 110 loss: 3.127507563789426 grad: -0.5816468857785067
iteration: 120 loss: 2.8761858512589655 grad: -0.5411188101384747
iteration: 130 loss: 2.6620951582712307 grad: -0.5057340288703829
iteration: 140 loss: 2.477559841683821 grad: -0.47460282022658656
iteration: 150 loss: 2.316873540939551 grad: -0.44702127564200406
iteration: 160 loss: 2.1757067668008445 grad: -0.42242781804475316
iteration: 170 loss: 2.050715938121977 grad: -0.4003704014697754
iteration: 180 loss: 1.9392783107196836 grad: -0.3804818811399058
iteration: 190 loss: 1.8393079582030458 grad: -0.36246147418418195
iteration: 200 loss: 1.7491253199798187 grad: -0.34606072319393455
iteration: 210 loss: 1.6673629831636638 grad: -0.3310727889713887
iteration: 220 loss: 1.5928964870645772 grad: -0.31732421532959787
iteration: 230 loss: 1.5247927327629276 grad: -0.30466854141570526
iteration: 240 loss: 1.4622709892274046 grad: -0.2929813052959938
iteration: 250 loss: 1.404673051148331 grad: -0.2821561036792535
iteration: 260 loss: 1.351440139065744 grad: -0.2721014599525825
iteration: 0 loss: 75.41832291202292 grad: 116.41300155661884
iteration: 10 loss: 22.063576355384633 grad: 0.4935769523439609
iteration: 20 loss: 13.803979758850552 grad: -1.374541630145294
iteration: 30 loss: 10.018024195582814 grad: -1.368475220936437
iteration: 40 loss: 7.844016005271772 grad: -1.2256930956331913
iteration: 50 loss: 6.436422066500819 grad: -1.0855362431514308
iteration: 60 loss: 5.452535927215277 grad: -0.9661382114154475
iteration: 70 loss: 4.727024089736171 grad: -0.8670243440455403
iteration: 80 loss: 4.170433214314099 grad: -0.7847139034849424
iteration: 90 loss: 3.7302071644407575 grad: -0.7158031526518904
iteration: 100 loss: 3.3734767039460496 grad: -0.6575210098377539
iteration: 110 loss: 3.0786490918453637 grad: -0.6077172021898427
iteration: 120 loss: 2.8309641551094584 grad: -0.5647410913411426
iteration: 130 loss: 2.619995407201507 grad: -0.5273221870383764
iteration: 140 loss: 2.4381715215483215 grad: -0.4944747424224518
iteration: 150 loss: 2.2798635687796707 grad: -0.46542622978394665
iteration: 160 loss: 2.140800860613953 grad: -0.43956477763436763
iteration: 170 loss: 2.017685344763694 grad: -0.4164006612855025
iteration: 180 loss: 1.907930124420504 grad: -0.39553796554485654
iteration: 190 loss: 1.8094779122264877 grad: -0.3766535687048427
iteration: 200 loss: 1.720672322879099 grad: -0.3594814097489247
iteration: 210 loss: 1.6401649116848438 grad: -0.3438005933047355
iteration: 220 loss: 1.5668469011010613 grad: -0.3294263062108555
iteration: 230 loss: 1.4997982780510855 grad: -0.3162028133132704
iteration: 240 loss: 1.4382493206021123 grad: -0.3039980057522059
iteration: 250 loss: 1.3815511550789614 grad: -0.29269911957382
iteration: 260 loss: 1.3291529661319559 grad: -0.2822093448313171
iteration: 0 loss: 75.88719482378573 grad: 112.89035647320196
iteration: 10 loss: 21.964555669345497 grad: 0.877220838105323
iteration: 20 loss: 13.782457839731086 grad: -1.3345795421478037
iteration: 30 loss: 10.016619268511352 grad: -1.3598054171813987
iteration: 40 loss: 7.8483061195626185 grad: -1.2175246118508174
iteration: 50 loss: 6.442226064596195 grad: -1.075614522103173
iteration: 60 loss: 5.458507099478869 grad: -0.9552001349972439
iteration: 70 loss: 4.732716202192478 grad: -0.8557625551941944
iteration: 80 loss: 4.1757141979266885 grad: -0.7735404902097474
iteration: 90 loss: 3.7350599265251234 grad: -0.7049346003927099
iteration: 100 loss: 3.3779256813023824 grad: -0.6470599796946253
iteration: 110 loss: 3.0827317506541476 grad: -0.5977033998410526
iteration: 120 loss: 2.8347197511991244 grad: -0.5551802646831905
iteration: 130 loss: 2.623460711651288 grad: -0.518202305482176
iteration: 140 loss: 2.441379433342966 grad: -0.4857749893371014
iteration: 150 loss: 2.282842921499609 grad: -0.45712199173568463
iteration: 160 loss: 2.143576698528622 grad: -0.4316303919549025
iteration: 170 loss: 2.020279354429683 grad: -0.4088108985134832
iteration: 180 loss: 1.9103610876801544 grad: -0.3882687935474322
iteration: 190 loss: 1.811762126700449 grad: -0.36968250837490524
iteration: 200 loss: 1.7228239718617095 grad: -0.35278765990633165
iteration: 210 loss: 1.642196381034079 grad: -0.3373650275325119
iteration: 220 loss: 1.5687690471137847 grad: -0.3232314013600605
iteration: 230 loss: 1.5016206524577351 grad: -0.3102325444226539
iteration: 240 loss: 1.4399803591239684 grad: -0.29823772746110877
iteration: 250 loss: 1.3831983354463608 grad: -0.28713544543464226
iteration: 260 loss: 1.3307229407317787 grad: -0.27683003078164303
iteration: 0 loss: 77.62467249392667 grad: 101.9719319224285
iteration: 10 loss: 22.66935408573173 grad: 2.0578822947361415
iteration: 20 loss: 14.102235291347494 grad: -1.0458994671149995
iteration: 30 loss: 10.218297287887316 grad: -1.2188399696334722
iteration: 40 loss: 7.996434149808878 grad: -1.1260144245118582
iteration: 50 loss: 6.560207039919533 grad: -1.0079221846375614
iteration: 60 loss: 5.557101726349794 grad: -0.9014789596920894
iteration: 70 loss: 4.817720110644628 grad: -0.8112543038935629
iteration: 80 loss: 4.250610648024948 grad: -0.7355835873603974
iteration: 90 loss: 3.8021139090343983 grad: -0.6718856585526569
iteration: 100 loss: 3.4386986565768893 grad: -0.6178302256195793
iteration: 110 loss: 3.1383494070501 grad: -0.5715325881399166
iteration: 120 loss: 2.8860226496768435 grad: -0.531515561320812
iteration: 130 loss: 2.6710945022351056 grad: -0.4966285387763697
iteration: 140 loss: 2.4858516894365823 grad: -0.4659721088637037
iteration: 150 loss: 2.324560469980621 grad: -0.43883788122797024
iteration: 160 loss: 2.18287100960081 grad: -0.41466280865206706
iteration: 170 loss: 2.057424328319154 grad: -0.3929950777730447
iteration: 180 loss: 1.9455858345146557 grad: -0.373468701931804
iteration: 190 loss: 1.8452603524564934 grad: -0.35578452174238784
iteration: 200 loss: 1.7547610050035638 grad: -0.3396958980425314
iteration: 210 loss: 1.6727145224179119 grad: -0.3249978477989686
iteration: 220 loss: 1.5979917047372758 grad: -0.31151872055608865
iteration: 230 loss: 1.5296555801158485 grad: -0.2991137636593287
iteration: 240 loss: 1.4669222238829784 grad: -0.28766010348168636
iteration: 250 loss: 1.409130775292934 grad: -0.2770527974635806
iteration: 260 loss: 1.355720229998083 grad: -0.26720170297904383
iteration: 0 loss: 76.99092077828725 grad: 147.4752861187231
iteration: 10 loss: 22.213164027045877 grad: -1.3845619327585799
iteration: 20 loss: 13.933806633824505 grad: -1.9672440816623937
iteration: 30 loss: 10.116055980811304 grad: -1.69281506621468
iteration: 40 loss: 7.9196928436245715 grad: -1.4403350379799935
iteration: 50 loss: 6.496634307302387 grad: -1.2425015801824872
iteration: 60 loss: 5.50176436834243 grad: -1.0882618356329825
iteration: 70 loss: 4.768207868666526 grad: -0.9660955732202011
iteration: 80 loss: 4.205552668427985 grad: -0.8675306877238819
iteration: 90 loss: 3.760636796003713 grad: -0.7866117013548355
iteration: 100 loss: 3.400197947471993 grad: -0.7191371115230385
iteration: 110 loss: 3.1023810122524496 grad: -0.6620962685574721
iteration: 120 loss: 2.8522458360024427 grad: -0.6132917322962608
iteration: 130 loss: 2.639239314903989 grad: -0.5710898459120458
iteration: 140 loss: 2.455698986748385 grad: -0.5342548087322446
iteration: 150 loss: 2.295928960821584 grad: -0.5018366678866166
iteration: 160 loss: 2.1556084305805765 grad: -0.47309441047097933
iteration: 170 loss: 2.0314012023423773 grad: -0.44744223212311945
iteration: 180 loss: 1.9206909137578392 grad: -0.424411360277127
iteration: 190 loss: 1.8213972033675996 grad: -0.4036224928746389
iteration: 200 loss: 1.7318453930637256 grad: -0.3847656011965743
iteration: 210 loss: 1.650672372535775 grad: -0.36758492182671854
iteration: 220 loss: 1.5767574857329485 grad: -0.35186765954755106
iteration: 230 loss: 1.509171007943802 grad: -0.3374353810927689
iteration: 240 loss: 1.4471352085526468 grad: -0.3241373855398803
iteration: 250 loss: 1.3899945569354797 grad: -0.31184554437944795
iteration: 260 loss: 1.3371926636641134 grad: -0.30045024674074194
iteration: 0 loss: 73.4219165604081 grad: 126.49333636487489
iteration: 10 loss: 21.79112444223568 grad: 0.16919596777817353
iteration: 20 loss: 13.7315203171659 grad: -1.3566525979166661
iteration: 30 loss: 9.992182344644705 grad: -1.3278192536828453
iteration: 40 loss: 7.833179432617519 grad: -1.1858635175479946
iteration: 50 loss: 6.431300467893792 grad: -1.050637113720874
iteration: 60 loss: 5.449831117812457 grad: -0.9362027737510007
iteration: 70 loss: 4.725407288670877 grad: -0.8412571622441152
iteration: 80 loss: 4.169319117196417 grad: -0.7623089065405309
iteration: 90 loss: 3.729321642799195 grad: -0.6961042783090766
iteration: 100 loss: 3.3726863991409615 grad: -0.6400208779471964
iteration: 110 loss: 3.077888134625482 grad: -0.5920260379701494
iteration: 120 loss: 2.830200421657413 grad: -0.550557159071424
iteration: 130 loss: 2.619214122738785 grad: -0.5144090336205429
iteration: 140 loss: 2.437366952556617 grad: -0.48264461424600374
iteration: 150 loss: 2.2790346778935433 grad: -0.4545281195638482
iteration: 160 loss: 2.1399489825192757 grad: -0.42947576828482764
iteration: 170 loss: 2.016812926808846 grad: -0.40701959099209417
iteration: 180 loss: 1.9070400426361298 grad: -0.38678073302906874
iteration: 190 loss: 1.8085731043087931 grad: -0.3684496087670962
iteration: 200 loss: 1.719755597961797 grad: -0.3517710138729828
iteration: 210 loss: 1.639238859233749 grad: -0.33653284822458895
iteration: 220 loss: 1.5659138551814893 grad: -0.32255749003969414
iteration: 230 loss: 1.4988603119999921 grad: -0.3096951345524515
iteration: 240 loss: 1.4373082577968406 grad: -0.2978186021765115
iteration: 250 loss: 1.380608587563777 grad: -0.2868192561992219
iteration: 260 loss: 1.3282102764176835 grad: -0.2766037659298149
iteration: 0 loss: 72.16096518299797 grad: 118.00710791793291
iteration: 10 loss: 21.350550576435726 grad: 2.2977223164616207
iteration: 20 loss: 13.414033087901378 grad: -0.8461494985897657
iteration: 30 loss: 9.781006789120614 grad: -1.1721015290445305
iteration: 40 loss: 7.684454258695124 grad: -1.13198501397414
iteration: 50 loss: 6.320851248036681 grad: -1.0325228641422328
iteration: 60 loss: 5.364262853028818 grad: -0.9324536148660343
iteration: 70 loss: 4.656865741538778 grad: -0.8437828236021674
iteration: 80 loss: 4.112936468123817 grad: -0.7676701773822324
iteration: 90 loss: 3.6819322344231296 grad: -0.702707036782708
iteration: 100 loss: 3.332145748040024 grad: -0.6470810677025399
iteration: 110 loss: 3.0426923998658655 grad: -0.5991444988760424
iteration: 120 loss: 2.7992634855254024 grad: -0.557528887107988
iteration: 130 loss: 2.5917314655069856 grad: -0.5211313375651254
iteration: 140 loss: 2.412729453131264 grad: -0.48907034013608724
iteration: 150 loss: 2.2567722646357735 grad: -0.46064075498385626
iteration: 160 loss: 2.119692751793826 grad: -0.435275733012163
iteration: 170 loss: 1.9982690385486834 grad: -0.41251649360249554
iteration: 180 loss: 1.8899713313331963 grad: -0.3919889207360624
iteration: 190 loss: 1.792785867749072 grad: -0.37338558644836656
iteration: 200 loss: 1.7050899366096763 grad: -0.3564519476852662
iteration: 210 loss: 1.6255614984065274 grad: -0.3409757091752934
iteration: 220 loss: 1.5531127342257027 grad: -0.32677857953471806
iteration: 230 loss: 1.4868404514020932 grad: -0.3137098392913739
iteration: 240 loss: 1.4259885641711827 grad: -0.30164128666372775
iteration: 250 loss: 1.3699193562504826 grad: -0.2904632370683038
iteration: 260 loss: 1.3180912194259855 grad: -0.28008133384886386
iteration: 0 loss: 76.06992464176246 grad: 120.20641296518923
iteration: 10 loss: 22.0719568073342 grad: 0.301651995786156
iteration: 20 loss: 13.863245751342033 grad: -1.5813499424081758
iteration: 30 loss: 10.073470039679849 grad: -1.4945893760917905
iteration: 40 loss: 7.89094527555277 grad: -1.3053575097522854
iteration: 50 loss: 6.476089737417417 grad: -1.1392021620626975
iteration: 60 loss: 5.486554627512388 grad: -1.0044114896706382
iteration: 70 loss: 4.756662273524646 grad: -0.8956167541814836
iteration: 80 loss: 4.196621870157949 grad: -0.8068844879545636
iteration: 90 loss: 3.753628193643098 grad: -0.7335174292016493
iteration: 100 loss: 3.3946370365717318 grad: -0.6720239070056726
iteration: 110 loss: 3.0979327529599603 grad: -0.6198313485761342
iteration: 120 loss: 2.8486675953095073 grad: -0.5750302206072049
iteration: 130 loss: 2.6363515249744607 grad: -0.5361849158594582
iteration: 140 loss: 2.453366224628429 grad: -0.5022006752714097
iteration: 150 loss: 2.2940473942233193 grad: -0.4722307865473865
iteration: 160 loss: 2.15409727783241 grad: -0.4456115317250336
iteration: 170 loss: 2.030196794003628 grad: -0.4218160889779743
iteration: 180 loss: 1.9197424817615767 grad: -0.40042143954646536
iteration: 190 loss: 1.8206638422875998 grad: -0.38108428761151547
iteration: 200 loss: 1.7312938322497757 grad: -0.36352330348571193
iteration: 210 loss: 1.650275319140581 grad: -0.3475058611885962
iteration: 220 loss: 1.5764923761239928 grad: -0.3328380122451715
iteration: 230 loss: 1.5090190555497016 grad: -0.31935681933614474
iteration: 240 loss: 1.4470806697679357 grad: -0.30692443162446165
iteration: 250 loss: 1.3900241594059977 grad: -0.29542346029042116
iteration: 260 loss: 1.3372951568735103 grad: -0.28475333522701196
iteration: 0 loss: 73.55210138269624 grad: 92.49738386537423
iteration: 10 loss: 22.241113778090536 grad: 4.225996347729268
iteration: 20 loss: 13.825678021294669 grad: -0.16225176405924663
iteration: 30 loss: 10.030577538366577 grad: -0.8386819881033656
iteration: 40 loss: 7.85779487748108 grad: -0.9313034269130644
iteration: 50 loss: 6.451473236004693 grad: -0.8944581358167419
iteration: 60 loss: 5.468153262218535 grad: -0.8289472379885117
iteration: 70 loss: 4.742699430190896 grad: -0.7616212739408914
iteration: 80 loss: 4.185867614429878 grad: -0.6998164128872377
iteration: 90 loss: 3.745238169199183 grad: -0.645044215005242
iteration: 100 loss: 3.3880234195622836 grad: -0.5970189487716047
iteration: 110 loss: 3.092678202738915 grad: -0.5549573476125872
iteration: 120 loss: 2.8444697765453384 grad: -0.5180131278834801
iteration: 130 loss: 2.632987207132663 grad: -0.4854158261036961
iteration: 140 loss: 2.450667687448913 grad: -0.4565049900390692
iteration: 150 loss: 2.291886622549803 grad: -0.43072792872225757
iteration: 160 loss: 2.1523751652073106 grad: -0.4076262138689584
iteration: 170 loss: 2.028835627443972 grad: -0.38682022687192863
iteration: 180 loss: 1.918680617225453 grad: -0.3679949747757942
iteration: 190 loss: 1.8198518699302275 grad: -0.35088806391189065
iteration: 200 loss: 1.730691772606447 grad: -0.33527985027383134
iteration: 210 loss: 1.6498505443959899 grad: -0.32098549756232003
iteration: 220 loss: 1.5762180491324629 grad: -0.30784861293419324
iteration: 230 loss: 1.5088729437607071 grad: -0.29573615279103826
iteration: 240 loss: 1.4470442341783134 grad: -0.2845343381395713
iteration: 250 loss: 1.3900818477057906 grad: -0.2741453684359061
iteration: 260 loss: 1.3374338498984302 grad: -0.2644847664572561
iteration: 0 loss: 74.58014612298616 grad: 126.64542439343313
iteration: 10 loss: 21.75481036814784 grad: -0.5922182093761573
iteration: 20 loss: 13.715569050209627 grad: -1.6734614731327009
iteration: 30 loss: 9.975756496599926 grad: -1.509825938406349
iteration: 40 loss: 7.816978918115307 grad: -1.3086360881919665
iteration: 50 loss: 6.4159609865261125 grad: -1.140461040001187
iteration: 60 loss: 5.4355288647331115 grad: -1.0055513172867148
iteration: 70 loss: 4.712125625189566 grad: -0.8969752448577655
iteration: 80 loss: 4.156977058845562 grad: -0.8084650308753244
iteration: 90 loss: 3.717823514143261 grad: -0.7352615649538985
iteration: 100 loss: 3.361939855211817 grad: -0.6738738952296491
iteration: 110 loss: 3.0678099822255884 grad: -0.6217421142145799
iteration: 120 loss: 2.8207177670881474 grad: -0.5769690703312613
iteration: 130 loss: 2.6102637971586193 grad: -0.5381288601367225
iteration: 140 loss: 2.428894390150907 grad: -0.504133826071462
iteration: 150 loss: 2.2709927115282964 grad: -0.47414232408917467
iteration: 160 loss: 2.1322967302601437 grad: -0.44749422193085164
iteration: 170 loss: 2.0095148223008437 grad: -0.4236652293881471
iteration: 180 loss: 1.9000650136885766 grad: -0.4022341157435913
iteration: 190 loss: 1.80189388588265 grad: -0.38285884876781495
iteration: 200 loss: 1.7133481605772596 grad: -0.3652589912194856
iteration: 210 loss: 1.6330819339251237 grad: -0.3492025456540665
iteration: 220 loss: 1.5599885381765572 grad: -0.3344960035120288
iteration: 230 loss: 1.493149734856946 grad: -0.3209767319344764
iteration: 240 loss: 1.431797311198476 grad: -0.3085070868785742
iteration: 250 loss: 1.3752836891241962 grad: -0.2969698156933574
iteration: 260 loss: 1.32305917467052 grad: -0.28626443329116386
iteration: 0 loss: 76.43075832900341 grad: 111.60773034414964
iteration: 10 loss: 22.376478591095022 grad: 0.9576739036305328
iteration: 20 loss: 13.986001179965466 grad: -1.0319606179012526
iteration: 30 loss: 10.154390050571733 grad: -1.1531656049263446
iteration: 40 loss: 7.954885093313971 grad: -1.0740591358466707
iteration: 50 loss: 6.530298826544905 grad: -0.9696804798684533
iteration: 60 loss: 5.534086465689184 grad: -0.8728438050187564
iteration: 70 loss: 4.799166059556311 grad: -0.7891895640780089
iteration: 80 loss: 4.235136859906064 grad: -0.7181140358959458
iteration: 90 loss: 3.7888737592470947 grad: -0.6577247445460441
iteration: 100 loss: 3.427141613484723 grad: -0.6061180276089203
iteration: 110 loss: 3.1281003660576308 grad: -0.5616772452450072
iteration: 120 loss: 2.876816148595067 grad: -0.5230983488023763
iteration: 130 loss: 2.6627366678905506 grad: -0.4893458469458417
iteration: 140 loss: 2.478197126499171 grad: -0.459598967039209
iteration: 150 loss: 2.317497640279808 grad: -0.43320436760567127
iteration: 160 loss: 2.1763126562914707 grad: -0.4096383858269942
iteration: 170 loss: 2.051301013971955 grad: -0.3884779387958714
iteration: 180 loss: 1.939841465575021 grad: -0.3693783660522434
iteration: 190 loss: 1.8398490077195013 grad: -0.352056587622979
iteration: 200 loss: 1.7496446421078788 grad: -0.3362782600196268
iteration: 210 loss: 1.6678612889604252 grad: -0.32184792355364406
iteration: 220 loss: 1.59337467420809 grad: -0.30860139031001593
iteration: 230 loss: 1.5252517917794897 grad: -0.29639981796960196
iteration: 240 loss: 1.4627119437094818 grad: -0.28512505992144443
iteration: 250 loss: 1.4050969200777466 grad: -0.2746759884582196
iteration: 260 loss: 1.351847913600192 grad: -0.26496556543024685
iteration: 0 loss: 75.92312269486729 grad: 120.32079578250182
iteration: 10 loss: 21.455738636405943 grad: -0.31169872647195035
iteration: 20 loss: 13.572963269643072 grad: -1.8293103091217593
iteration: 30 loss: 9.895644404100638 grad: -1.6376425485493826
iteration: 40 loss: 7.767478387550496 grad: -1.4125453018927079
iteration: 50 loss: 6.383334251305686 grad: -1.227279759179329
iteration: 60 loss: 5.413008581069459 grad: -1.0795843175653044
iteration: 70 loss: 4.696051544913272 grad: -0.9611275945606794
iteration: 80 loss: 4.145221493120599 grad: -0.864802759428265
iteration: 90 loss: 3.709076398979009 grad: -0.7853021825169209
iteration: 100 loss: 3.355355946842172 grad: -0.7187580369503695
iteration: 110 loss: 3.062822500449329 grad: -0.6623434923014503
iteration: 120 loss: 2.8169342128981456 grad: -0.6139681451810087
iteration: 130 loss: 2.6074048540948835 grad: -0.5720635957557181
iteration: 140 loss: 2.426756600797241 grad: -0.5354353839428108
iteration: 150 loss: 2.2694248635785614 grad: -0.5031606050052913
iteration: 160 loss: 2.131184454634794 grad: -0.4745164334719504
iteration: 170 loss: 2.008770276383943 grad: -0.4489295829666899
iteration: 180 loss: 1.89961980666108 grad: -0.42594007088623953
iteration: 190 loss: 1.8016941252134198 grad: -0.4051748747023399
iteration: 200 loss: 1.7133509185960891 grad: -0.38632851912148053
iteration: 210 loss: 1.6332526849230569 grad: -0.3691485853612924
iteration: 220 loss: 1.5602992712269539 grad: -0.3534247624773646
iteration: 230 loss: 1.4935775450864062 grad: -0.33898048022971383
iteration: 240 loss: 1.4323233350310973 grad: -0.32566644634307
iteration: 250 loss: 1.37589229030825 grad: -0.313355604795158
iteration: 260 loss: 1.3237373154879193 grad: -0.3019391659435521
iteration: 0 loss: 77.83760575801237 grad: 165.0360349029249
iteration: 10 loss: 21.457403218916376 grad: -2.817621240372441
iteration: 20 loss: 13.565249550985216 grad: -2.3829043024914776
iteration: 30 loss: 9.883921356945745 grad: -1.9162455929700637
iteration: 40 loss: 7.7541012931764355 grad: -1.5897294935027992
iteration: 50 loss: 6.369429724550131 grad: -1.3535918785881351
iteration: 60 loss: 5.399136130472346 grad: -1.1761486488881507
iteration: 70 loss: 4.682484392865883 grad: -1.038486337583746
iteration: 80 loss: 4.132086490527896 grad: -0.9288666285658618
iteration: 90 loss: 3.6964247314656196 grad: -0.8396785921289966
iteration: 100 loss: 3.343199361587019 grad: -0.7657950842539414
iteration: 110 loss: 3.051152281061687 grad: -0.7036481513535814
iteration: 120 loss: 2.805731346028634 grad: -0.6506849310229819
iteration: 130 loss: 2.596645573074346 grad: -0.6050344455700469
iteration: 140 loss: 2.4164154347460927 grad: -0.5652964013316367
iteration: 150 loss: 2.2594763075483324 grad: -0.530403251822751
iteration: 160 loss: 2.121603862932261 grad: -0.49952770573688754
iteration: 170 loss: 1.9995343127770453 grad: -0.47201923988939154
iteration: 180 loss: 1.8907066405804058 grad: -0.44735960012154297
iteration: 190 loss: 1.7930834830783418 grad: -0.4251310186044519
iteration: 200 loss: 1.7050240540861523 grad: -0.40499312404327537
iteration: 210 loss: 1.6251923058317432 grad: -0.38666590599399064
iteration: 220 loss: 1.5524894451475368 grad: -0.36991696763577897
iteration: 230 loss: 1.4860035970123961 grad: -0.3545518637547486
iteration: 240 loss: 1.424971744451713 grad: -0.3404066900958685
iteration: 250 loss: 1.368750592038313 grad: -0.32734233725195816
iteration: 260 loss: 1.3167940064626733 grad: -0.31523999017965026
iteration: 0 loss: 76.90352877293789 grad: 101.09838170621595
iteration: 10 loss: 22.751458390810534 grad: 2.586424219334254
iteration: 20 loss: 14.119958144238312 grad: -0.948171212406288
iteration: 30 loss: 10.223682261096428 grad: -1.191967746950176
iteration: 40 loss: 7.99872666297322 grad: -1.1165446623526405
iteration: 50 loss: 6.561785935485535 grad: -1.0044759746799592
iteration: 60 loss: 5.558643166496123 grad: -0.9006606395506673
iteration: 70 loss: 4.81940029299395 grad: -0.811764313569172
iteration: 80 loss: 4.2524503494295125 grad: -0.7368250313297067
iteration: 90 loss: 3.804088157307433 grad: -0.6735462248588316
iteration: 100 loss: 3.4407731210478896 grad: -0.6197311212799257
iteration: 110 loss: 3.140492110474249 grad: -0.57356508412291
iteration: 120 loss: 2.8882070766437042 grad: -0.5336108816809191
iteration: 130 loss: 2.6732997526731364 grad: -0.4987421183568101
iteration: 140 loss: 2.4880617241987735 grad: -0.46807471758074204
iteration: 150 loss: 2.326763186689773 grad: -0.44091032049531415
iteration: 160 loss: 2.1850573975895546 grad: -0.4166925978472516
iteration: 170 loss: 2.059587768263803 grad: -0.3949743116525139
iteration: 180 loss: 1.9477215425301686 grad: -0.37539262765819004
iteration: 190 loss: 1.8473649479029435 grad: -0.35765057377235066
iteration: 200 loss: 1.7568321777794154 grad: -0.3415030325864796
iteration: 210 loss: 1.6747507779088913 grad: -0.32674607779328274
iteration: 220 loss: 1.5999921685062295 grad: -0.31320878737858315
iteration: 230 loss: 1.5316198482468364 grad: -0.30074690356043576
iteration: 240 loss: 1.4688502481315233 grad: -0.28923788048021615
iteration: 250 loss: 1.41102277478438 grad: -0.27857698339508224
iteration: 260 loss: 1.3575766232087185 grad: -0.26867419130406744
iteration: 0 loss: 75.50838961784645 grad: 98.29754634035169
iteration: 10 loss: 22.031170611310078 grad: 1.8167342716191526
iteration: 20 loss: 13.787964566706375 grad: -1.0558616376866305
iteration: 30 loss: 10.016737813726381 grad: -1.1989472969560326
iteration: 40 loss: 7.850307086892082 grad: -1.1074939317331383
iteration: 50 loss: 6.446460318071828 grad: -0.9941402225017864
iteration: 60 loss: 5.464364503670037 grad: -0.8917546987426697
iteration: 70 loss: 4.739622806858733 grad: -0.8045471419593939
iteration: 80 loss: 4.183255458239893 grad: -0.7310534696003063
iteration: 90 loss: 3.7429547706329602 grad: -0.668922357487078
iteration: 100 loss: 3.3859869106452773 grad: -0.6160008581277387
iteration: 110 loss: 3.090835341810444 grad: -0.5705301126020972
iteration: 120 loss: 2.8427838298915216 grad: -0.5311204456834718
iteration: 130 loss: 2.6314316161577733 grad: -0.4966821307408739
iteration: 140 loss: 2.4492225195617308 grad: -0.46635845917757146
iteration: 150 loss: 2.29053646112247 grad: -0.4394713298767217
iteration: 160 loss: 2.151107790496057 grad: -0.4154794392985362
iteration: 170 loss: 2.027641155830018 grad: -0.39394677025394437
iteration: 180 loss: 1.9175509184944366 grad: -0.37451894289634335
iteration: 190 loss: 1.818780160101059 grad: -0.3569054176785512
iteration: 200 loss: 1.7296723215458356 grad: -0.3408660203306588
iteration: 210 loss: 1.6488784608248517 grad: -0.32620066047477025
iteration: 220 loss: 1.5752891192413367 grad: -0.3127414210284606
iteration: 230 loss: 1.507983507856352 grad: -0.30034641943350704
iteration: 240 loss: 1.4461910908809676 grad: -0.28889500337072294
iteration: 250 loss: 1.3892621784922001 grad: -0.2782839598198322
iteration: 260 loss: 1.3366451589129362 grad: -0.2684244999876724
iteration: 0 loss: 76.34376883855863 grad: 103.27686529486161
iteration: 10 loss: 22.470472277846138 grad: 1.5879218845706617
iteration: 20 loss: 13.978712747177042 grad: -0.7646628644747646
iteration: 30 loss: 10.12783826018139 grad: -0.9766392842437981
iteration: 40 loss: 7.92580498766907 grad: -0.9452814186225476
iteration: 50 loss: 6.5027108528028785 grad: -0.8706772308283934
iteration: 60 loss: 5.508876376950411 grad: -0.7936541406506081
iteration: 70 loss: 4.776345186307977 grad: -0.7238447263721516
iteration: 80 loss: 4.214477523467022 grad: -0.6628545937699759
iteration: 90 loss: 3.770104955340106 grad: -0.6100742573760963
iteration: 100 loss: 3.4100094097742097 grad: -0.5643781253584612
iteration: 110 loss: 3.1123836839192336 grad: -0.5246415198203604
iteration: 120 loss: 2.8623276428628 grad: -0.4898835970032053
iteration: 130 loss: 2.6493190109334304 grad: -0.45928855723081485
iteration: 140 loss: 2.4657183372732603 grad: -0.43218948296767656
iteration: 150 loss: 2.3058466946626637 grad: -0.4080436263127642
iteration: 160 loss: 2.1653957292518995 grad: -0.3864086543293325
iteration: 170 loss: 2.041038383608793 grad: -0.36692250385060254
iteration: 180 loss: 1.9301650037068199 grad: -0.3492871133212962
iteration: 190 loss: 1.8307001578647364 grad: -0.33325555278538377
iteration: 200 loss: 1.740972791012568 grad: -0.3186219194900068
iteration: 210 loss: 1.6596224519134921 grad: -0.3052134133043899
iteration: 220 loss: 1.5855304294044468 grad: -0.29288410657193265
iteration: 230 loss: 1.5177684123271897 grad: -0.2815100247660309
iteration: 240 loss: 1.4555596868575031 grad: -0.27098524123319084
iteration: 250 loss: 1.398249441981658 grad: -0.26121875866474376
iteration: 260 loss: 1.3452817848011696 grad: -0.2521320035706458
iteration: 0 loss: 77.19513183966284 grad: 121.88230147566443
iteration: 10 loss: 21.575328884042246 grad: 0.08620337303773579
iteration: 20 loss: 13.573627515275575 grad: -1.9108480773855354
iteration: 30 loss: 9.884544153589475 grad: -1.7403827215973056
iteration: 40 loss: 7.7559993383511445 grad: -1.4934895134244908
iteration: 50 loss: 6.373146799988552 grad: -1.2887090219417372
iteration: 60 loss: 5.40420520936087 grad: -1.127019879148957
iteration: 70 loss: 4.688444363083198 grad: -0.9987204546309941
iteration: 80 loss: 4.138601373471126 grad: -0.8953426118074039
iteration: 90 loss: 3.7032647785037045 grad: -0.8106529318924377
iteration: 100 loss: 3.3502096930551075 grad: -0.7401897421475628
iteration: 110 loss: 3.0582286009666033 grad: -0.6807441678555108
iteration: 120 loss: 2.8128033426736776 grad: -0.6299749749684438
iteration: 130 loss: 2.6036659049978383 grad: -0.5861449209212191
iteration: 140 loss: 2.423352434420022 grad: -0.5479430014225929
iteration: 150 loss: 2.266309107945 grad: -0.51436394054945
iteration: 160 loss: 2.1283191044073067 grad: -0.48462541750305965
iteration: 170 loss: 2.006123890195423 grad: -0.4581103198899916
iteration: 180 loss: 1.89716614362665 grad: -0.4343258032858926
iteration: 190 loss: 1.7994111045697785 grad: -0.41287380666827006
iteration: 200 loss: 1.7112198183607357 grad: -0.39342949894373047
iteration: 210 loss: 1.6312575187484981 grad: -0.3757253010043745
iteration: 220 loss: 1.5584262992764375 grad: -0.35953888535810485
iteration: 230 loss: 1.4918148866439547 grad: -0.34468405308744937
iteration: 240 loss: 1.4306606590531705 grad: -0.3310037196155095
iteration: 250 loss: 1.3743205662261226 grad: -0.31836446505981925
iteration: 260 loss: 1.322248610988936 grad: -0.3066522587617919
iteration: 0 loss: 73.444111769939 grad: 126.35608415171856
iteration: 10 loss: 21.899896752459572 grad: 0.09236887893150729
iteration: 20 loss: 13.759260496364318 grad: -1.3760705197331489
iteration: 30 loss: 9.995360764575025 grad: -1.3473123241663492
iteration: 40 loss: 7.828040920506241 grad: -1.206874670582305
iteration: 50 loss: 6.423227771440948 grad: -1.0704376239341498
iteration: 60 loss: 5.440841134972619 grad: -0.9537968619326002
iteration: 70 loss: 4.716313362859224 grad: -0.8566236657159201
iteration: 80 loss: 4.160459711227346 grad: -0.7757341861206256
iteration: 90 loss: 3.720833132185734 grad: -0.7079154662156388
iteration: 100 loss: 3.3646144197285306 grad: -0.6505057609807672
iteration: 110 loss: 3.070236444025299 grad: -0.6014188301738634
iteration: 120 loss: 2.822953588313257 grad: -0.5590438033737191
iteration: 130 loss: 2.6123484908840013 grad: -0.5221365011660736
iteration: 140 loss: 2.4308560613582233 grad: -0.48972957371424697
iteration: 150 loss: 2.272851944285404 grad: -0.46106393638359944
iteration: 160 loss: 2.1340689921575673 grad: -0.43553787809162736
iteration: 170 loss: 2.0112120036298866 grad: -0.41266955315754006
iteration: 180 loss: 1.9016964424800706 grad: -0.3920692782349968
iteration: 190 loss: 1.803467014995311 grad: -0.3734189381021766
iteration: 200 loss: 1.7148690436759104 grad: -0.35645654502025326
iteration: 210 loss: 1.6345555621151933 grad: -0.34096455282818733
iteration: 220 loss: 1.561419082576239 grad: -0.326760927249249
iteration: 230 loss: 1.4945407243894393 grad: -0.3136922571401912
iteration: 240 loss: 1.4331517649440932 grad: -0.3016283909990005
iteration: 250 loss: 1.3766042162711032 grad: -0.29045822396573895
iteration: 260 loss: 1.3243480510000154 grad: -0.28008636058631914
iteration: 0 loss: 78.12541304884668 grad: 140.65212008173503
iteration: 10 loss: 21.906232637505934 grad: -1.0996537271081315
iteration: 20 loss: 13.749844990330963 grad: -1.9741884726283954
iteration: 30 loss: 9.991133312842521 grad: -1.7115068637557642
iteration: 40 loss: 7.827130308873083 grad: -1.4589723810436983
iteration: 50 loss: 6.424098457473272 grad: -1.2594417709230756
iteration: 60 loss: 5.442671239320299 grad: -1.1032732467599153
iteration: 70 loss: 4.718668029844924 grad: -0.9793429255422179
iteration: 80 loss: 4.163098558209723 grad: -0.8792664746902419
iteration: 90 loss: 3.7236169632000466 grad: -0.79707727868618
iteration: 100 loss: 3.3674596824608587 grad: -0.7285376778679635
iteration: 110 loss: 3.0730917870874195 grad: -0.6705997108977021
iteration: 120 loss: 2.825787081591319 grad: -0.6210339494831207
iteration: 130 loss: 2.6151402997041426 grad: -0.5781809153919168
iteration: 140 loss: 2.4335940696699008 grad: -0.5407843862158241
iteration: 150 loss: 2.27552905062368 grad: -0.5078783277193964
iteration: 160 loss: 2.136681393081083 grad: -0.478709027179399
iteration: 170 loss: 2.0137580797297896 grad: -0.4526805888934843
iteration: 180 loss: 1.904176022388831 grad: -0.4293161604915819
iteration: 190 loss: 1.8058808820957728 grad: -0.40822991803719966
iteration: 200 loss: 1.7172186017499949 grad: -0.3891065254789483
iteration: 210 loss: 1.6368426068271136 grad: -0.371685866108356
iteration: 220 loss: 1.5636456444498519 grad: -0.3557515467858205
iteration: 230 loss: 1.4967089606648187 grad: -0.3411221391538043
iteration: 240 loss: 1.4352638851948376 grad: -0.32764443203233395
iteration: 250 loss: 1.3786624315295575 grad: -0.3151881795057713
iteration: 260 loss: 1.326354539363985 grad: -0.3036419738924151
iteration: 0 loss: 74.49573082402327 grad: 145.46643361494142
iteration: 10 loss: 20.952396390661782 grad: -1.9364853397897444
iteration: 20 loss: 13.351812566170793 grad: -2.1375892805829984
iteration: 30 loss: 9.76118247661721 grad: -1.780581749181254
iteration: 40 loss: 7.670113642183341 grad: -1.5009523339805109
iteration: 50 loss: 6.3055616985729595 grad: -1.290663996989597
iteration: 60 loss: 5.347236409723874 grad: -1.1291163250539455
iteration: 70 loss: 4.638444222657364 grad: -1.0019226654707678
iteration: 80 loss: 4.093600336528611 grad: -0.8995552098951551
iteration: 90 loss: 3.6620860111245914 grad: -0.8155936095682104
iteration: 100 loss: 3.312090933488848 grad: -0.7455986400734183
iteration: 110 loss: 3.0226432425661187 grad: -0.686422291472133
iteration: 120 loss: 2.7793683748673175 grad: -0.635779652168379
iteration: 130 loss: 2.5720910863446576 grad: -0.591977053335245
iteration: 140 loss: 2.3934104868610784 grad: -0.5537351802273476
iteration: 150 loss: 2.237817303412612 grad: -0.5200712283850848
iteration: 160 loss: 2.1011273622719853 grad: -0.4902186394231631
iteration: 170 loss: 1.980106773520509 grad: -0.463571302740039
iteration: 180 loss: 1.8722172885568398 grad: -0.4396440298543385
iteration: 190 loss: 1.775439224937269 grad: -0.41804406966631225
iteration: 200 loss: 1.6881457639960487 grad: -0.3984502543831592
iteration: 210 loss: 1.6090120594901485 grad: -0.3805975097434922
iteration: 220 loss: 1.5369484219522334 grad: -0.3642651960008916
iteration: 230 loss: 1.4710504631642503 grad: -0.34926822448225847
iteration: 240 loss: 1.4105613886134216 grad: -0.33545021231364414
iteration: 250 loss: 1.35484312374245 grad: -0.3226781524968662
iteration: 260 loss: 1.3033539533038576 grad: -0.31083822366294256
iteration: 0 loss: 74.62406359711389 grad: 131.18943283517788
iteration: 10 loss: 21.183448022003446 grad: -0.9082842081348741
iteration: 20 loss: 13.385308423378154 grad: -2.0015619028597422
iteration: 30 loss: 9.757300834741356 grad: -1.7196629486281911
iteration: 40 loss: 7.657269881559924 grad: -1.458455339864225
iteration: 50 loss: 6.290999200736968 grad: -1.2569085254310766
iteration: 60 loss: 5.333105635590445 grad: -1.100943259129806
iteration: 70 loss: 4.625377900219271 grad: -0.9777573087715417
iteration: 80 loss: 4.081723077452228 grad: -0.8784364601074695
iteration: 90 loss: 3.6513463993244177 grad: -0.7968771985834526
iteration: 100 loss: 3.3023816282916414 grad: -0.7288279652384191
iteration: 110 loss: 3.013846391478265 grad: -0.6712604042769554
iteration: 120 loss: 2.7713725304531427 grad: -0.6219699204566976
iteration: 130 loss: 2.5647967447843754 grad: -0.5793190211990563
iteration: 140 loss: 2.3867310376853808 grad: -0.5420690424990885
iteration: 150 loss: 2.2316782038730953 grad: -0.5092674352313286
iteration: 160 loss: 2.095464725800825 grad: -0.4801706898274245
iteration: 170 loss: 1.9748658705105464 grad: -0.4541905982545935
iteration: 180 loss: 1.8673511562528802 grad: -0.4308561060615399
iteration: 190 loss: 1.7709074510897318 grad: -0.409785777141504
iteration: 200 loss: 1.6839134522259138 grad: -0.3906676113738472
iteration: 210 loss: 1.6050489616807433 grad: -0.3732440408748735
iteration: 220 loss: 1.5332282156441992 grad: -0.35730062954862646
iteration: 230 loss: 1.4675501512523292 grad: -0.34265745866895014
iteration: 240 loss: 1.4072608006543057 grad: -0.329162486432874
iteration: 250 loss: 1.351724501040611 grad: -0.316686376016055
iteration: 0 loss: 75.07674729744784 grad: 120.28884270913156
iteration: 10 loss: 21.56126012222878 grad: 0.35951402450201486
iteration: 20 loss: 13.551205008827496 grad: -1.5548277183303516
iteration: 30 loss: 9.857219826556296 grad: -1.4941383846852487
iteration: 40 loss: 7.7284522512466705 grad: -1.316691915545785
iteration: 50 loss: 6.346877203288138 grad: -1.1547337534284656
iteration: 60 loss: 5.379615646920229 grad: -1.0209249449840334
iteration: 70 loss: 4.665553584589568 grad: -0.9117946322568422
iteration: 80 loss: 4.117297191121659 grad: -0.822217674017339
iteration: 90 loss: 3.6833984405954485 grad: -0.7478469265715894
iteration: 100 loss: 3.331631516966726 grad: -0.6853423900618649
iteration: 110 loss: 3.0408007046955636 grad: -0.6321945397235855
iteration: 120 loss: 2.796403225021756 grad: -0.5865165920997973
iteration: 130 loss: 2.5881864062233615 grad: -0.5468773954120776
iteration: 140 loss: 2.4087003704473564 grad: -0.5121786198293125
iteration: 150 loss: 2.252403536493024 grad: -0.481566808463558
iteration: 160 loss: 2.1150896135987667 grad: -0.4543706099512792
iteration: 170 loss: 1.9935090545924057 grad: -0.43005572533034403
iteration: 180 loss: 1.885112186756239 grad: -0.4081922559756477
iteration: 190 loss: 1.7878707630151107 grad: -0.38843077153057226
iteration: 200 loss: 1.7001513861092794 grad: -0.37048456263369506
iteration: 210 loss: 1.6206240462554227 grad: -0.3541163266251066
iteration: 220 loss: 1.5481949225926324 grad: -0.3391280663873051
iteration: 230 loss: 1.4819562647319964 grad: -0.32535334457132903
iteration: 240 loss: 1.4211485006769842 grad: -0.3126512836154848
iteration: 250 loss: 1.3651312308833936 grad: -0.3009018735806283
iteration: 260 loss: 1.3133607711018735 grad: -0.2900022697132804
iteration: 0 loss: 75.57643874758278 grad: 147.76380250839574
iteration: 10 loss: 21.487349267405907 grad: -1.877002973840896
iteration: 20 loss: 13.589608348170817 grad: -2.354045825041644
iteration: 30 loss: 9.905032393936422 grad: -1.911798377362738
iteration: 40 loss: 7.771980452628378 grad: -1.5836435911197158
iteration: 50 loss: 6.384434003505083 grad: -1.347123482314482
iteration: 60 loss: 5.4117592844207305 grad: -1.170450135670928
iteration: 70 loss: 4.693198115371615 grad: -1.0338383242792253
iteration: 80 loss: 4.141283420135096 grad: -0.9251657906318205
iteration: 90 loss: 3.704413407808674 grad: -0.8367275901252174
iteration: 100 loss: 3.350217159691924 grad: -0.7634114974332648
iteration: 110 loss: 3.0573812124090924 grad: -0.7016900211276058
iteration: 120 loss: 2.8113117124693088 grad: -0.6490481275778918
iteration: 130 loss: 2.6016865362899724 grad: -0.6036438420623153
iteration: 140 loss: 2.421002861795635 grad: -0.5640976686266752
iteration: 150 loss: 2.263678512730456 grad: -0.5293566648293202
iteration: 160 loss: 2.1254758051974765 grad: -0.49860378394546245
iteration: 170 loss: 2.0031206802156536 grad: -0.47119574967150435
iteration: 180 loss: 1.8940441361452363 grad: -0.4466194999700437
iteration: 190 loss: 1.7962025351139346 grad: -0.4244610416594646
iteration: 200 loss: 1.7079501097418044 grad: -0.4043827893705081
iteration: 210 loss: 1.6279468192949273 grad: -0.3861068197539482
iteration: 220 loss: 1.5550906404093274 grad: -0.36940232217145147
iteration: 230 loss: 1.4884670623878264 grad: -0.3540760734432083
iteration: 240 loss: 1.4273109003020594 grad: -0.33996512294714576
iteration: 250 loss: 1.3709770620026906 grad: -0.32693111443890654
iteration: 260 loss: 1.3189179145776755 grad: -0.3148558344029341
iteration: 0 loss: 74.28859809810618 grad: 127.87222967193361
iteration: 10 loss: 21.929972291836233 grad: 0.2815527609225559
iteration: 20 loss: 13.787902873710003 grad: -1.1673401889685202
iteration: 30 loss: 10.026435878122832 grad: -1.2063029228824442
iteration: 40 loss: 7.857935915769221 grad: -1.1027685205528317
iteration: 50 loss: 6.450967478830452 grad: -0.9889096293927351
iteration: 60 loss: 5.466327972617364 grad: -0.887643495542389
iteration: 70 loss: 4.739717345793741 grad: -0.8015929475692091
iteration: 80 loss: 4.182012949894877 grad: -0.7290484265107691
iteration: 90 loss: 3.7407621856606386 grad: -0.6676590238228169
iteration: 100 loss: 3.3831206933722653 grad: -0.6153122163809686
iteration: 110 loss: 3.0874931088894964 grad: -0.5702881842497153
iteration: 120 loss: 2.8391078474094438 grad: -0.5312280492588911
iteration: 130 loss: 2.627525215065113 grad: -0.49706516778573495
iteration: 140 loss: 2.4451614143051965 grad: -0.466959942434816
iteration: 150 loss: 2.2863765687732833 grad: -0.4402469948113306
iteration: 160 loss: 2.146890677316027 grad: -0.41639456053804846
iteration: 170 loss: 2.0233978724462016 grad: -0.3949738483627519
iteration: 180 loss: 1.9133047319245302 grad: -0.3756360107998115
iteration: 190 loss: 1.8145485225466638 grad: -0.3580947883761467
iteration: 200 loss: 1.7254683050069533 grad: -0.34211335095673445
iteration: 210 loss: 1.6447118134630563 grad: -0.3274942452690492
iteration: 220 loss: 1.5711670508387463 grad: -0.31407165167908513
iteration: 230 loss: 1.5039112791224314 grad: -0.3017053691075303
iteration: 240 loss: 1.442172459278936 grad: -0.29027610310934004
iteration: 250 loss: 1.3852997380654415 grad: -0.2796817446048637
iteration: 260 loss: 1.3327406011198946 grad: -0.2698344078663598
iteration: 0 loss: 76.8348152274287 grad: 119.35393698853419
iteration: 10 loss: 21.18732877482522 grad: -0.5369340453825979
iteration: 20 loss: 13.371753493717444 grad: -2.022817172548457
iteration: 30 loss: 9.747615643273486 grad: -1.747151155204639
iteration: 40 loss: 7.652620641669645 grad: -1.4786303244927625
iteration: 50 loss: 6.290313194652382 grad: -1.2705804470522573
iteration: 60 loss: 5.33524367936653 grad: -1.109938139653271
iteration: 70 loss: 4.629464821426485 grad: -0.983537757300811
iteration: 80 loss: 4.087136550408889 grad: -0.882020199819068
iteration: 90 loss: 3.6576529992664537 grad: -0.7989477278065741
iteration: 100 loss: 3.3092799706864753 grad: -0.7298433823539607
iteration: 110 loss: 3.0211257711259596 grad: -0.6715310725189036
iteration: 120 loss: 2.778884540375574 grad: -0.6217093810305775
iteration: 130 loss: 2.5724361063335395 grad: -0.5786766314592453
iteration: 140 loss: 2.3944226412784455 grad: -0.5411509313859022
iteration: 150 loss: 2.2393682783614084 grad: -0.5081501148918253
iteration: 160 loss: 2.103114755198724 grad: -0.47891003684790123
iteration: 170 loss: 1.9824483531796928 grad: -0.4528278810545877
iteration: 180 loss: 1.8748466130722674 grad: -0.4294220988488448
iteration: 190 loss: 1.7783022904700374 grad: -0.40830361366504986
iteration: 200 loss: 1.6911984301353764 grad: -0.38915479479404425
iteration: 210 loss: 1.6122180598078806 grad: -0.37171387938511596
iteration: 220 loss: 1.5402778167553648 grad: -0.35576327523211415
iteration: 230 loss: 1.4744784280673726 grad: -0.3411206682753472
iteration: 240 loss: 1.4140672596639556 grad: -0.32763218464440175
iteration: 250 loss: 1.3584096398070125 grad: -0.3151670766825314
iteration: 260 loss: 1.3069666511284017 grad: -0.3036135526379218
iteration: 0 loss: 74.69555363628923 grad: 117.95760756895112
iteration: 10 loss: 21.85328317669655 grad: 0.2524522854067614
iteration: 20 loss: 13.717552752738543 grad: -1.3899014171716166
iteration: 30 loss: 9.972166226223965 grad: -1.3469776433456655
iteration: 40 loss: 7.816096011942715 grad: -1.196514911372799
iteration: 50 loss: 6.417849327663424 grad: -1.0562296855291016
iteration: 60 loss: 5.439411109157845 grad: -0.9387513314511026
iteration: 70 loss: 4.717339846141923 grad: -0.8419662303907447
iteration: 80 loss: 4.163054313206555 grad: -0.7618977204787885
iteration: 90 loss: 3.724444095576308 grad: -0.6950058627847792
iteration: 100 loss: 3.3688868732911446 grad: -0.6385003017502009
iteration: 110 loss: 3.0749356671854002 grad: -0.5902487386345755
iteration: 120 loss: 2.827920855197631 grad: -0.5486283312138911
iteration: 130 loss: 2.6174745992475437 grad: -0.512396639871686
iteration: 140 loss: 2.4360648998786845 grad: -0.48059310354775264
iteration: 150 loss: 2.278089983611856 grad: -0.45246679093477415
iteration: 160 loss: 2.139298390608639 grad: -0.4274239844755462
iteration: 170 loss: 2.0164059833207584 grad: -0.4049900970819449
iteration: 180 loss: 1.9068361279917305 grad: -0.38478180961042263
iteration: 190 loss: 1.8085392352836194 grad: -0.36648649332226835
iteration: 200 loss: 1.7198647809475667 grad: -0.3498468536338789
iteration: 210 loss: 1.6394688428873367 grad: -0.33464934769814414
iteration: 220 loss: 1.5662461757132902 grad: -0.32071535629468517
iteration: 230 loss: 1.499279555260769 grad: -0.3078943865687762
iteration: 240 loss: 1.437801483669029 grad: -0.29605878759973003
iteration: 250 loss: 1.381164877078721 grad: -0.28509960426478914
iteration: 260 loss: 1.3288203725474803 grad: -0.27492329591227443
iteration: 0 loss: 73.8565626321017 grad: 116.52248403302755
iteration: 10 loss: 21.57715331473787 grad: 1.0459951762358937
iteration: 20 loss: 13.599912734816474 grad: -1.1806739666034112
iteration: 30 loss: 9.906100030105005 grad: -1.3032213898722764
iteration: 40 loss: 7.771388999881558 grad: -1.2030325517750828
iteration: 50 loss: 6.383846429117842 grad: -1.0788787861275213
iteration: 60 loss: 5.411596765891028 grad: -0.966121544356669
iteration: 70 loss: 4.693508450102937 grad: -0.8699095919055684
iteration: 80 loss: 4.142003319803941 grad: -0.7888573951050304
iteration: 90 loss: 3.7054579185199312 grad: -0.7204323435247957
iteration: 100 loss: 3.35150834416349 grad: -0.6622577939328796
iteration: 110 loss: 3.0588551575967604 grad: -0.6123739936623482
iteration: 120 loss: 2.8129182994340143 grad: -0.5692267927642043
iteration: 130 loss: 2.6033872090200187 grad: -0.5315961675770301
iteration: 140 loss: 2.422768191677278 grad: -0.4985232328280351
iteration: 150 loss: 2.2654860968993322 grad: -0.469249769556051
iteration: 160 loss: 2.1273085955652857 grad: -0.4431713592841107
iteration: 170 loss: 2.0049656998236802 grad: -0.41980191968316977
iteration: 180 loss: 1.8958915071915394 grad: -0.39874704294809193
iteration: 190 loss: 1.7980447464716567 grad: -0.3796839230998636
iteration: 200 loss: 1.7097814648099385 grad: -0.3623461631475327
iteration: 210 loss: 1.629763018552446 grad: -0.34651219236147834
iteration: 220 loss: 1.556888464427216 grad: -0.33199636411875927
iteration: 230 loss: 1.4902441298931164 grad: -0.3186420563608701
iteration: 240 loss: 1.4290654825452458 grad: -0.3063162792396299
iteration: 250 loss: 1.3727079395566761 grad: -0.2949054261244245
iteration: 260 loss: 1.3206242662985543 grad: -0.28431189903102827
iteration: 0 loss: 76.33378082465185 grad: 149.17512320281958
iteration: 10 loss: 21.759799715154426 grad: -1.8889666582015008
iteration: 20 loss: 13.804569690984636 grad: -2.123601481759393
iteration: 30 loss: 10.061324021279317 grad: -1.77522871263031
iteration: 40 loss: 7.88926352596069 grad: -1.4964466429621548
iteration: 50 loss: 6.475959191888044 grad: -1.2856290454076542
iteration: 60 loss: 5.485664726027938 grad: -1.1234247571189822
iteration: 70 loss: 4.75455823679431 grad: -0.9956891106160962
iteration: 80 loss: 4.193378968691419 grad: -0.8929344740700446
iteration: 90 loss: 3.7494496296144875 grad: -0.8087294093690982
iteration: 100 loss: 3.3897306826894567 grad: -0.738608768413749
iteration: 110 loss: 3.0924771700777622 grad: -0.6793970175251527
iteration: 120 loss: 2.84280695076024 grad: -0.6287851989533737
iteration: 130 loss: 2.63019924565581 grad: -0.5850604101031385
iteration: 140 loss: 2.4470105656519907 grad: -0.5469286591932301
iteration: 150 loss: 2.287556719062691 grad: -0.5133962418559127
iteration: 160 loss: 2.147524463342961 grad: -0.4836886396280453
iteration: 170 loss: 2.023582723919183 grad: -0.45719399559422724
iteration: 180 loss: 1.9131187571571735 grad: -0.43342301228184776
iteration: 190 loss: 1.8140548658857782 grad: -0.4119800287741644
iteration: 200 loss: 1.7247184075350064 grad: -0.3925418411122172
iteration: 210 loss: 1.6437478786925899 grad: -0.3748419735224718
iteration: 220 loss: 1.5700239280812307 grad: -0.35865884492644495
iteration: 230 loss: 1.50261791587499 grad: -0.34380675839842345
iteration: 240 loss: 1.4407530313937555 grad: -0.3301289633086789
iteration: 250 loss: 1.3837745366963645 grad: -0.31749225789352015
iteration: 260 loss: 1.3311267343487998 grad: -0.305782749714967
iteration: 0 loss: 75.72514984673222 grad: 115.74790504745289
iteration: 10 loss: 21.582547992993877 grad: 0.7838714752620721
iteration: 20 loss: 13.60765617268922 grad: -1.6395395766031022
iteration: 30 loss: 9.914339085052461 grad: -1.5623658008601773
iteration: 40 loss: 7.780620077994792 grad: -1.3577005284659658
iteration: 50 loss: 6.393775305414113 grad: -1.1795893611125248
iteration: 60 loss: 5.421840210062865 grad: -1.0365462134841428
iteration: 70 loss: 4.703790432684772 grad: -0.9218679487922582
iteration: 80 loss: 4.152151956054819 grad: -0.8287729022442742
iteration: 90 loss: 3.7153737174259978 grad: -0.75207217732729
iteration: 100 loss: 3.361137901592139 grad: -0.6879718756829053
iteration: 110 loss: 3.068173475892102 grad: -0.6337020198393553
iteration: 120 loss: 2.8219175995377968 grad: -0.5872185108746593
iteration: 130 loss: 2.6120699813769863 grad: -0.5469912015684891
iteration: 140 loss: 2.431142897123333 grad: -0.5118573455658774
iteration: 150 loss: 2.2735645056548246 grad: -0.48092037370329344
iteration: 160 loss: 2.1351041399913644 grad: -0.4534793737181042
iteration: 170 loss: 2.0124924582663963 grad: -0.4289793441608245
iteration: 180 loss: 1.9031635834242644 grad: -0.4069756142616261
iteration: 190 loss: 1.8050758986554352 grad: -0.3871080363528365
iteration: 200 loss: 1.7165848924615934 grad: -0.36908200908872657
iteration: 210 loss: 1.6363512496593764 grad: -0.35265434023257813
iteration: 220 loss: 1.5632733057908064 grad: -0.33762258429193354
iteration: 230 loss: 1.4964366570064322 grad: -0.323816907447247
iteration: 240 loss: 1.4350760536844973 grad: -0.311093813291901
iteration: 250 loss: 1.3785462241903588 grad: -0.29933125465271954
iteration: 260 loss: 1.3262992813997285 grad: -0.28842478922390136
iteration: 0 loss: 75.60798763040476 grad: 112.87477170636708
iteration: 10 loss: 22.241340247367795 grad: 1.4690293176117757
iteration: 20 loss: 13.931528538624042 grad: -1.1871777873932121
iteration: 30 loss: 10.121501191661848 grad: -1.3047922409514068
iteration: 40 loss: 7.930817849447894 grad: -1.191890988232891
iteration: 50 loss: 6.510937662940003 grad: -1.0624635757557563
iteration: 60 loss: 5.517686847239392 grad: -0.9482783147621637
iteration: 70 loss: 4.784834626960963 grad: -0.8522816549871352
iteration: 80 loss: 4.222351407370876 grad: -0.7720994335086702
iteration: 90 loss: 3.7772990273319507 grad: -0.7047664210944726
iteration: 100 loss: 3.41654723094937 grad: -0.647716423850567
iteration: 110 loss: 3.118319828487198 grad: -0.5989085477010931
iteration: 120 loss: 2.867724376561951 grad: -0.5567570249805625
iteration: 130 loss: 2.654236818771427 grad: -0.5200330770799896
iteration: 140 loss: 2.470212384510846 grad: -0.48777946405257055
iteration: 150 loss: 2.3099658593510783 grad: -0.45924405557703374
iteration: 160 loss: 2.1691827258198164 grad: -0.4338300931170512
iteration: 170 loss: 2.0445303118716436 grad: -0.41105935617849915
iteration: 180 loss: 1.9333940235667961 grad: -0.39054487620731554
iteration: 190 loss: 1.8336941582870168 grad: -0.37197061113251806
iteration: 200 loss: 1.7437560002649661 grad: -0.35507618366127597
iteration: 210 loss: 1.6622159723623637 grad: -0.3396453177163147
iteration: 220 loss: 1.587952696520047 grad: -0.32549699430174184
iteration: 230 loss: 1.5200355843210425 grad: -0.31247862378160596
iteration: 240 loss: 1.4576859736373329 grad: -0.30046072669355905
iteration: 250 loss: 1.400247382284595 grad: -0.289332753430353
iteration: 260 loss: 1.3471624796501684 grad: -0.27899977146079685
iteration: 0 loss: 74.55910205707507 grad: 91.18514451004873
iteration: 10 loss: 21.70933848490747 grad: 3.7319856117142742
iteration: 20 loss: 13.617167496512923 grad: -0.6867671409705547
iteration: 30 loss: 9.919909886253105 grad: -1.1308559422433744
iteration: 40 loss: 7.788157056263737 grad: -1.1171658968169034
iteration: 50 loss: 6.402624517780086 grad: -1.0264283704475368
iteration: 60 loss: 5.4312586146720605 grad: -0.9297664990161735
iteration: 70 loss: 4.713334854227386 grad: -0.8425468314663296
iteration: 80 loss: 4.161583244859767 grad: -0.7670901253774972
iteration: 90 loss: 3.724568607544494 grad: -0.7024398908767661
iteration: 100 loss: 3.3700370739191965 grad: -0.6469723449941464
iteration: 110 loss: 3.0767529007147956 grad: -0.5991221160056892
iteration: 120 loss: 2.830172739943092 grad: -0.5575580831195653
iteration: 130 loss: 2.6200069601961578 grad: -0.5211947436443571
iteration: 140 loss: 2.438773479786381 grad: -0.48915887777681466
iteration: 150 loss: 2.2809032167420313 grad: -0.46074937023996365
iteration: 160 loss: 2.142166597728812 grad: -0.4354013571388787
iteration: 170 loss: 2.019294417003245 grad: -0.4126570544512643
iteration: 180 loss: 1.9097203936502078 grad: -0.3921428597859017
iteration: 190 loss: 1.8114022157162915 grad: -0.3735516225242407
iteration: 200 loss: 1.7226945340592217 grad: -0.3566289619299817
iteration: 210 loss: 1.6422571421266268 grad: -0.3411626896239168
iteration: 220 loss: 1.5689874826914754 grad: -0.32697459471312873
iteration: 230 loss: 1.5019702877322578 grad: -0.31391402563280046
iteration: 240 loss: 1.4404394883383496 grad: -0.30185284222450504
iteration: 250 loss: 1.3837490463270115 grad: -0.2906814178895108
iteration: 260 loss: 1.3313503635044568 grad: -0.2803054512519155
iteration: 0 loss: 73.60039822433747 grad: 133.07183004632972
iteration: 10 loss: 21.48882316909821 grad: -0.39317382750913943
iteration: 20 loss: 13.575908768044593 grad: -1.833043360358348
iteration: 30 loss: 9.88849377731675 grad: -1.6340925004050475
iteration: 40 loss: 7.7559145226140656 grad: -1.4064198824058882
iteration: 50 loss: 6.3698936947122835 grad: -1.2212463890086214
iteration: 60 loss: 5.39893874881165 grad: -1.074133850999774
iteration: 70 loss: 4.681968780001253 grad: -0.9561742635578561
iteration: 80 loss: 4.1314273959186805 grad: -0.8602137679969499
iteration: 90 loss: 3.6957131037769284 grad: -0.7809862900979916
iteration: 100 loss: 3.3424830762350295 grad: -0.7146598306350234
iteration: 110 loss: 3.0504563483168567 grad: -0.6584291297151317
iteration: 120 loss: 2.8050683199846222 grad: -0.6102157484093998
iteration: 130 loss: 2.5960211045319492 grad: -0.5684578630027379
iteration: 140 loss: 2.41583132287513 grad: -0.5319647146405575
iteration: 150 loss: 2.2589322172668536 grad: -0.4998155796497789
iteration: 160 loss: 2.1210983085745116 grad: -0.47128900317881395
iteration: 170 loss: 1.9990652278157044 grad: -0.44581262936102534
iteration: 180 loss: 1.8902717088181733 grad: -0.42292716026453114
iteration: 190 loss: 1.792680330056018 grad: -0.4022601150310265
iteration: 200 loss: 1.7046503555491406 grad: -0.38350647207717725
iteration: 210 loss: 1.62484584660648 grad: -0.3664142082378995
iteration: 220 loss: 1.5521681482945164 grad: -0.35077336660906455
iteration: 230 loss: 1.4857055352512445 grad: -0.33640769881229693
iteration: 240 loss: 1.424695141166591 grad: -0.3231682078454726
iteration: 250 loss: 1.3684938163354756 grad: -0.3109281099180032
iteration: 260 loss: 1.3165555650259855 grad: -0.29957886702374037
iteration: 0 loss: 74.51840885544055 grad: 129.52256591369735
iteration: 10 loss: 21.616230730945063 grad: 0.15470353022113173
iteration: 20 loss: 13.629993655121098 grad: -1.5167304106964623
iteration: 30 loss: 9.929173447036668 grad: -1.4707357477134049
iteration: 40 loss: 7.790219531201604 grad: -1.2994369775407977
iteration: 50 loss: 6.3998874503660605 grad: -1.1403217282058957
iteration: 60 loss: 5.4256362317300955 grad: -1.008201737632457
iteration: 70 loss: 4.706024136814821 grad: -0.900276990623073
iteration: 80 loss: 4.153311773313445 grad: -0.8116583664650212
iteration: 90 loss: 3.7157822272664487 grad: -0.738092560377198
iteration: 100 loss: 3.3610128067904785 grad: -0.6762829259980709
iteration: 110 loss: 3.0676648358523626 grad: -0.6237438293621415
iteration: 120 loss: 2.821130886052494 grad: -0.5786039853773267
iteration: 130 loss: 2.6110806388528545 grad: -0.5394435385821787
iteration: 140 loss: 2.4300057352742526 grad: -0.5051729784278641
iteration: 150 loss: 2.2723198876425488 grad: -0.4749459914814753
iteration: 160 loss: 2.1337821419085476 grad: -0.44809712653717293
iteration: 170 loss: 2.0111157178231966 grad: -0.42409701237021125
iteration: 180 loss: 1.9017492849025075 grad: -0.4025198958384306
iteration: 190 loss: 1.803637178130099 grad: -0.38301985324824994
iteration: 200 loss: 1.7151318471563337 grad: -0.365313155646088
iteration: 210 loss: 1.6348916726458562 grad: -0.3491650447174886
iteration: 220 loss: 1.5618132273093166 grad: -0.33437970469829054
iteration: 230 loss: 1.494980747759407 grad: -0.32079257618243545
iteration: 240 loss: 1.4336279283702837 grad: -0.30826440494001683
iteration: 250 loss: 1.377108672092163 grad: -0.2966765898673291
iteration: 260 loss: 1.3248744430634773 grad: -0.2859275136389643
iteration: 0 loss: 74.48175098166352 grad: 123.76470128817272
iteration: 10 loss: 21.582093254973312 grad: 0.5698281446125308
iteration: 20 loss: 13.56469326090068 grad: -1.3258943086826176
iteration: 30 loss: 9.871949288769304 grad: -1.3600579432791937
iteration: 40 loss: 7.743137042546137 grad: -1.2289181329338714
iteration: 50 loss: 6.361052063041701 grad: -1.0924272196116593
iteration: 60 loss: 5.393140596044304 grad: -0.9741610242697899
iteration: 70 loss: 4.67840229623384 grad: -0.8752625076294033
iteration: 80 loss: 4.129487647636531 grad: -0.792808172404616
iteration: 90 loss: 3.6949666281050835 grad: -0.7236094575932279
iteration: 100 loss: 3.3426198959300564 grad: -0.6649862777389672
iteration: 110 loss: 3.0512527561036635 grad: -0.6148294026071763
iteration: 120 loss: 2.806360844745913 grad: -0.5715072898920676
iteration: 130 loss: 2.59768873247334 grad: -0.533758206949135
iteration: 140 loss: 2.417783352827469 grad: -0.5006000482168929
iteration: 150 loss: 2.2610998877930384 grad: -0.4712613236057429
iteration: 160 loss: 2.123428952377956 grad: -0.44512986903164237
iteration: 170 loss: 2.0015181347016964 grad: -0.4217150070581389
iteration: 180 loss: 1.8928151578276646 grad: -0.40061957059836106
iteration: 190 loss: 1.795289432346594 grad: -0.38151908325080564
iteration: 200 loss: 1.7073054670717367 grad: -0.36414613250613004
iteration: 210 loss: 1.627531386496478 grad: -0.3482785301187571
iteration: 220 loss: 1.5548717097668818 grad: -0.33373025545623225
iteration: 230 loss: 1.4884172098831683 grad: -0.320344461848804
iteration: 240 loss: 1.427407000581504 grad: -0.3079880263377947
iteration: 250 loss: 1.3711995112053046 grad: -0.2965472648110112
iteration: 260 loss: 1.3192500123693869 grad: -0.2859245351188274
iteration: 0 loss: 72.7520086862151 grad: 136.45149194500283
iteration: 10 loss: 21.559807153562403 grad: 1.171012630673609
iteration: 20 loss: 13.564477732368976 grad: -1.2193368948221321
iteration: 30 loss: 9.878567758466572 grad: -1.3492913514618257
iteration: 40 loss: 7.750357011997411 grad: -1.2401873420843357
iteration: 50 loss: 6.3674780559311905 grad: -1.1090728388488826
iteration: 60 loss: 5.39858201269875 grad: -0.9914180909431496
iteration: 70 loss: 4.682953822954005 grad: -0.891653397542508
iteration: 80 loss: 4.133294246509402 grad: -0.8079239993153421
iteration: 90 loss: 3.698163210105618 grad: -0.7374136883124396
iteration: 100 loss: 3.3453187092768117 grad: -0.6775696085513434
iteration: 110 loss: 3.053543921516145 grad: -0.6263183223383355
iteration: 120 loss: 2.80831599642178 grad: -0.5820296566864678
iteration: 130 loss: 2.599364833630385 grad: -0.5434310666644462
iteration: 140 loss: 2.4192259268050047 grad: -0.5095263080298911
iteration: 150 loss: 2.2623455705050386 grad: -0.47952988975651967
iteration: 160 loss: 2.1245074568142956 grad: -0.45281702229046833
iteration: 170 loss: 2.0024537503364543 grad: -0.4288860949380885
iteration: 180 loss: 1.893627894922495 grad: -0.40733064862867774
iteration: 190 loss: 1.7959958987637699 grad: -0.3878183747434043
iteration: 200 loss: 1.7079195442283064 grad: -0.3700752767294782
iteration: 210 loss: 1.6280647541786328 grad: -0.35387362927584853
iteration: 220 loss: 1.5553342566807837 grad: -0.3390227449706802
iteration: 230 loss: 1.4888173572634598 grad: -0.32536183122453677
iteration: 240 loss: 1.4277519591892087 grad: -0.3127544160442742
iteration: 250 loss: 1.3714954869157159 grad: -0.30108396128664894
iteration: 260 loss: 1.319502371790639 grad: -0.29025038239449474
iteration: 0 loss: 76.16883188890087 grad: 117.58985436460439
iteration: 10 loss: 21.643760528578014 grad: 0.37417101222580695
iteration: 20 loss: 13.578685361365098 grad: -1.5697384820271447
iteration: 30 loss: 9.875823180309101 grad: -1.5121980282855236
iteration: 40 loss: 7.744920642971217 grad: -1.3330214820286108
iteration: 50 loss: 6.362601088517956 grad: -1.1693454504730036
iteration: 60 loss: 5.394850286171856 grad: -1.0342291076635117
iteration: 70 loss: 4.680309871334719 grad: -0.9240917961396455
iteration: 80 loss: 4.131550148787855 grad: -0.8336998740777475
iteration: 90 loss: 3.6971325736406224 grad: -0.758639945297209
iteration: 100 loss: 3.3448461960748648 grad: -0.6955343432880448
iteration: 110 loss: 3.0535068356727852 grad: -0.6418517105724669
iteration: 120 loss: 2.808619097263669 grad: -0.5956918554676003
iteration: 130 loss: 2.599934439450325 grad: -0.555614754292707
iteration: 140 loss: 2.420004880752954 grad: -0.5205157473071246
iteration: 150 loss: 2.2632892971956524 grad: -0.48953655832200094
iteration: 160 loss: 2.1255809703371926 grad: -0.46200197974601637
iteration: 170 loss: 2.003629408639013 grad: -0.4373745168017737
iteration: 180 loss: 1.8948837165383916 grad: -0.4152215590203565
iteration: 190 loss: 1.7973142967615454 grad: -0.3951913385924
iteration: 200 loss: 1.7092863684708226 grad: -0.3769951102520531
iteration: 210 loss: 1.6294685618136926 grad: -0.3603937852897109
iteration: 220 loss: 1.556765752363183 grad: -0.34518779157643387
iteration: 230 loss: 1.4902689604149268 grad: -0.3312092973185455
iteration: 240 loss: 1.4292174669949418 grad: -0.31831618636446257
iteration: 250 loss: 1.372969810005667 grad: -0.3063873455385321
iteration: 260 loss: 1.3209813255091996 grad: -0.2953189449307011
iteration: 0 loss: 75.63443040834183 grad: 125.61602254435113
iteration: 10 loss: 21.656261747943702 grad: 0.6225030785039313
iteration: 20 loss: 13.591753920919162 grad: -1.7515443183160273
iteration: 30 loss: 9.887184290324202 grad: -1.6531478783398943
iteration: 40 loss: 7.75308981626135 grad: -1.4316122331775158
iteration: 50 loss: 6.368013971214788 grad: -1.2408935396434064
iteration: 60 loss: 5.398196421237737 grad: -1.0884468912564595
iteration: 70 loss: 4.682161919583698 grad: -0.9666870570767786
iteration: 80 loss: 4.132328466770435 grad: -0.8681451996841518
iteration: 90 loss: 3.6971361418316686 grad: -0.7871493687453782
iteration: 100 loss: 3.3442870608278925 grad: -0.7195816575451659
iteration: 110 loss: 3.0525365171152274 grad: -0.6624550814360939
iteration: 120 loss: 2.8073470717145854 grad: -0.6135773149568693
iteration: 130 loss: 2.598440751799899 grad: -0.571314249156626
iteration: 140 loss: 2.4183487336218934 grad: -0.5344282420857267
iteration: 150 loss: 2.2615149436607513 grad: -0.5019674216193885
iteration: 160 loss: 2.123721802668156 grad: -0.47318913644609806
iteration: 170 loss: 2.0017108316389813 grad: -0.4475062735158077
iteration: 180 loss: 1.892925193124649 grad: -0.42444903949881974
iteration: 190 loss: 1.7953308227700868 grad: -0.40363734060417517
iteration: 200 loss: 1.7072895488205508 grad: -0.3847605320784757
iteration: 210 loss: 1.6274674047251045 grad: -0.3675623675228117
iteration: 220 loss: 1.5547672617205783 grad: -0.35182966932683624
iteration: 230 loss: 1.4882785819276747 grad: -0.33738369820783076
iteration: 240 loss: 1.4272394273117575 grad: -0.3240735056591644
iteration: 250 loss: 1.371007376637904 grad: -0.31177076071049853
iteration: 260 loss: 1.3190370076075106 grad: -0.3003656852331428
iteration: 0 loss: 72.82632759828834 grad: 143.46023269525278
iteration: 10 loss: 21.240400756648672 grad: -0.056906547825566034
iteration: 20 loss: 13.393885313779368 grad: -1.6350992054464406
iteration: 30 loss: 9.761462895644813 grad: -1.5516371192725136
iteration: 40 loss: 7.661503722713323 grad: -1.3587272530079644
iteration: 50 loss: 6.295938950797915 grad: -1.1867567471905718
iteration: 60 loss: 5.338666001117252 grad: -1.0464451671105814
iteration: 70 loss: 4.6313638938241555 grad: -0.9328958332671745
iteration: 80 loss: 4.087963941828713 grad: -0.8401610244908314
iteration: 90 loss: 3.6577154273691947 grad: -0.7634316870098352
iteration: 100 loss: 3.308790001942725 grad: -0.6991006982299804
iteration: 110 loss: 3.0202334284271357 grad: -0.6444969368694944
iteration: 120 loss: 2.7776974624422026 grad: -0.5976311859688892
iteration: 130 loss: 2.57103266997435 grad: -0.5570044979762104
iteration: 140 loss: 2.3928606687085945 grad: -0.5214719340256071
iteration: 150 loss: 2.2376909214280287 grad: -0.49014692702016494
iteration: 160 loss: 2.1013545409411396 grad: -0.4623339365515482
iteration: 170 loss: 1.9806300104971177 grad: -0.43748056361795856
iteration: 180 loss: 1.8729890776128493 grad: -0.41514308425272545
iteration: 190 loss: 1.776420146550663 grad: -0.3949613184099752
iteration: 200 loss: 1.6893029611758885 grad: -0.37664006801631156
iteration: 210 loss: 1.6103180226787117 grad: -0.35993523540699296
iteration: 220 loss: 1.538380018854241 grad: -0.344643318479377
iteration: 230 loss: 1.4725881617230863 grad: -0.3305933720325295
iteration: 240 loss: 1.4121886321721087 grad: -0.31764079159180814
iteration: 250 loss: 1.3565458263122474 grad: -0.30566245917662155
iteration: 260 loss: 1.3051200899106912 grad: -0.29455291767195374
iteration: 0 loss: 73.58516055368428 grad: 114.87149375991547
iteration: 10 loss: 21.708990126591765 grad: 1.5852633543214116
iteration: 20 loss: 13.64412211265339 grad: -1.0105641814680935
iteration: 30 loss: 9.928262641767809 grad: -1.2211452087375454
iteration: 40 loss: 7.786204706307754 grad: -1.1527942520581145
iteration: 50 loss: 6.39562270410211 grad: -1.0446148062644147
iteration: 60 loss: 5.421826570800104 grad: -0.9413262778416137
iteration: 70 loss: 4.702792641771504 grad: -0.8512615566870045
iteration: 80 loss: 4.150618792921085 grad: -0.7744388213882567
iteration: 90 loss: 3.713549729863354 grad: -0.7090478431091112
iteration: 100 loss: 3.359162401803339 grad: -0.6531209846170115
iteration: 110 loss: 3.0661283518058386 grad: -0.6049457790562368
iteration: 120 loss: 2.819851985368297 grad: -0.5631251799610728
iteration: 130 loss: 2.610013663145103 grad: -0.5265430039006008
iteration: 140 loss: 2.429113908258924 grad: -0.4943112810520758
iteration: 150 loss: 2.2715735824187337 grad: -0.4657215757006471
iteration: 160 loss: 2.1331574119762577 grad: -0.44020525170987557
iteration: 170 loss: 2.0105931133579857 grad: -0.41730247676510707
iteration: 180 loss: 1.9013129195446625 grad: -0.3966384477569762
iteration: 190 loss: 1.8032739996017633 grad: -0.3779052335904608
iteration: 200 loss: 1.7148310682858787 grad: -0.36084788334749973
iteration: 210 loss: 1.634644326751806 grad: -0.3452537456988146
iteration: 220 loss: 1.5616118194568365 grad: -0.33094420399120766
iteration: 230 loss: 1.4948189798780567 grad: -0.31776823437771445
iteration: 240 loss: 1.4335004814068022 grad: -0.3055973471425889
iteration: 250 loss: 1.3770110323727998 grad: -0.2943215843294744
iteration: 260 loss: 1.3248027630812347 grad: -0.28384632973150836
iteration: 0 loss: 76.97297288119088 grad: 127.00874397253217
iteration: 10 loss: 22.67691014137226 grad: -0.04874612799932537
iteration: 20 loss: 14.166934711057758 grad: -1.446360185800286
iteration: 30 loss: 10.268011242359451 grad: -1.3720778676504635
iteration: 40 loss: 8.03185409216332 grad: -1.2078358515078174
iteration: 50 loss: 6.585678755541019 grad: -1.0605103589394027
iteration: 60 loss: 5.5758025356156145 grad: -0.9392553712747373
iteration: 70 loss: 4.831720077211354 grad: -0.8403952084790802
iteration: 80 loss: 4.2612582131283 grad: -0.7591774185719977
iteration: 90 loss: 3.8103082029430713 grad: -0.6916573191628897
iteration: 100 loss: 3.445055983512661 grad: -0.6348254407605358
iteration: 110 loss: 3.1433035787130477 grad: -0.586426684885879
iteration: 120 loss: 2.8898864706965965 grad: -0.5447671075880216
iteration: 130 loss: 2.674098408833288 grad: -0.508561877243796
iteration: 140 loss: 2.488168683787039 grad: -0.4768245152121907
iteration: 150 loss: 2.326322626186621 grad: -0.44878791610734825
iteration: 160 loss: 2.18418066131988 grad: -0.4238481087204403
iteration: 170 loss: 2.058361816862626 grad: -0.40152393566224603
iteration: 180 loss: 1.946214980895754 grad: -0.3814278458700359
iteration: 190 loss: 1.845632391139051 grad: -0.363244488586556
iteration: 200 loss: 1.754917457672036 grad: -0.3467148356816793
iteration: 210 loss: 1.672689328311497 grad: -0.3316242647968819
iteration: 220 loss: 1.5978128210695517 grad: -0.31779351317408083
iteration: 230 loss: 1.5293461991287918 grad: -0.3050717361852642
iteration: 240 loss: 1.4665017087526477 grad: -0.29333112638163716
iteration: 250 loss: 1.408615387259529 grad: -0.28246270211536223
iteration: 260 loss: 1.3551236986621171 grad: -0.2723729817670754
iteration: 0 loss: 74.7289085493297 grad: 115.64531579910974
iteration: 10 loss: 21.6822802390448 grad: 1.170441580886963
iteration: 20 loss: 13.683699385317823 grad: -1.4768135935782345
iteration: 30 loss: 9.970507564526686 grad: -1.5002195151499849
iteration: 40 loss: 7.823077529608781 grad: -1.3332008001781726
iteration: 50 loss: 6.426980702324685 grad: -1.1712626220108286
iteration: 60 loss: 5.448642690361381 grad: -1.0359490402066762
iteration: 70 loss: 4.726005938595955 grad: -0.9252867750358778
iteration: 80 loss: 4.1709706391013075 grad: -0.8343922869215274
iteration: 90 loss: 3.7316021232888734 grad: -0.7589205647291724
iteration: 100 loss: 3.3753405200693787 grad: -0.6954934055872768
iteration: 110 loss: 3.0807571028978846 grad: -0.6415638745136178
iteration: 120 loss: 2.8331830859837366 grad: -0.5952154945812433
iteration: 130 loss: 2.622244955504853 grad: -0.5549946581739621
iteration: 140 loss: 2.4404030699242454 grad: -0.5197861407724307
iteration: 150 loss: 2.282047924828393 grad: -0.4887236513749218
iteration: 160 loss: 2.142920965970792 grad: -0.4611259448512587
iteration: 170 loss: 2.019731821748792 grad: -0.4364509738973819
iteration: 180 loss: 1.9098984946682238 grad: -0.41426267874198686
iteration: 190 loss: 1.8113668273994419 grad: -0.3942066608714805
iteration: 200 loss: 1.722482422386127 grad: -0.37599215389559915
iteration: 210 loss: 1.6418980766862992 grad: -0.359378504639281
iteration: 220 loss: 1.568505763323091 grad: -0.3441649210664296
iteration: 230 loss: 1.501385894848888 grad: -0.3301826134466511
iteration: 240 loss: 1.4397689598481107 grad: -0.31728870841639045
iteration: 250 loss: 1.383006153335166 grad: -0.30536149055946205
iteration: 260 loss: 1.3305466361137983 grad: -0.29429664825448903
iteration: 0 loss: 75.59495399112296 grad: 146.5508566276484
iteration: 10 loss: 21.69754518299607 grad: -0.8388515568209584
iteration: 20 loss: 13.66181810658364 grad: -1.7572119639956374
iteration: 30 loss: 9.945651716411392 grad: -1.5948538176272358
iteration: 40 loss: 7.800281632186728 grad: -1.3860963904956072
iteration: 50 loss: 6.406654316159017 grad: -1.2087818709523628
iteration: 60 loss: 5.430483912256188 grad: -1.065563085972864
iteration: 70 loss: 4.709656754798206 grad: -0.9499334902119508
iteration: 80 loss: 4.156127059064891 grad: -0.8555397222315337
iteration: 90 loss: 3.718021496958789 grad: -0.7774325657416719
iteration: 100 loss: 3.3628307989697945 grad: -0.7119354002095224
iteration: 110 loss: 3.0691652884228957 grad: -0.656332591126171
iteration: 120 loss: 2.8223859455992253 grad: -0.6086027073990389
iteration: 130 loss: 2.6121420495640204 grad: -0.5672222848501582
iteration: 140 loss: 2.4309115917244526 grad: -0.5310272912400283
iteration: 150 loss: 2.273098872201527 grad: -0.49911608226663673
iteration: 160 loss: 2.1344562745508813 grad: -0.4707810826407292
iteration: 170 loss: 2.011702193533438 grad: -0.4454601940534429
iteration: 180 loss: 1.9022617333692076 grad: -0.42270180485822495
iteration: 190 loss: 1.8040865472105245 grad: -0.4021392657314591
iteration: 200 loss: 1.7155270351445737 grad: -0.38347202978939365
iteration: 210 loss: 1.6352399890345375 grad: -0.3664515433071293
iteration: 220 loss: 1.5621207340865857 grad: -0.3508705653332572
iteration: 230 loss: 1.4952525159623788 grad: -0.3365549925725694
iteration: 240 loss: 1.4338682335441346 grad: -0.32335753625981
iteration: 250 loss: 1.3773211450225475 grad: -0.3111527834322578
iteration: 260 loss: 1.325062187326859 grad: -0.2998333040330178
iteration: 0 loss: 74.8722800969818 grad: 131.66188670021074
iteration: 10 loss: 21.944446844345933 grad: 0.6916623201687366
iteration: 20 loss: 13.762573578677946 grad: -1.374340314782873
iteration: 30 loss: 10.01128995336568 grad: -1.4364642382611112
iteration: 40 loss: 7.850863969977664 grad: -1.297191596994737
iteration: 50 loss: 6.448648030160959 grad: -1.1486531685771504
iteration: 60 loss: 5.466737245434285 grad: -1.0200091630428725
iteration: 70 loss: 4.7417048415980965 grad: -0.912936704188884
iteration: 80 loss: 4.184916330787101 grad: -0.8241451461741716
iteration: 90 loss: 3.744190040621933 grad: -0.7500106725253392
iteration: 100 loss: 3.386835802129967 grad: -0.6875007717768756
iteration: 110 loss: 3.0913487347304245 grad: -0.6342438726334673
iteration: 120 loss: 2.843011921453622 grad: -0.5884172413688253
iteration: 130 loss: 2.631419555922052 grad: -0.5486198556398414
iteration: 140 loss: 2.4490093086678413 grad: -0.5137673448113711
iteration: 150 loss: 2.290155148398588 grad: -0.48301231019213275
iteration: 160 loss: 2.1505861287158208 grad: -0.45568555919864717
iteration: 170 loss: 2.0270023643632737 grad: -0.43125304465869585
iteration: 180 loss: 1.9168144094870845 grad: -0.409284223833045
iteration: 190 loss: 1.817962177952433 grad: -0.38942864375690683
iteration: 200 loss: 1.728786485431349 grad: -0.37139845736064464
iteration: 210 loss: 1.6479362162695823 grad: -0.35495523983588195
iteration: 220 loss: 1.5743001103683127 grad: -0.33989994825946124
iteration: 230 loss: 1.5069558828906173 grad: -0.32606519991338856
iteration: 240 loss: 1.4451317525475298 grad: -0.3133092774602705
iteration: 250 loss: 1.3881769893662088 grad: -0.3015114325590445
iteration: 260 loss: 1.335539110128469 grad: -0.29056817497714876
iteration: 0 loss: 75.16112568631326 grad: 98.17125860012696
iteration: 10 loss: 22.52121246673129 grad: 3.1441610817136385
iteration: 20 loss: 14.002735224697373 grad: -0.41466150767857757
iteration: 30 loss: 10.147247776764702 grad: -0.8649349883943818
iteration: 40 loss: 7.941740190146344 grad: -0.89551170392056
iteration: 50 loss: 6.516001722701193 grad: -0.8417824416072659
iteration: 60 loss: 5.520164877760098 grad: -0.7734904538874181
iteration: 70 loss: 4.7861009379445765 grad: -0.7080640748965652
iteration: 80 loss: 4.223039817942787 grad: -0.6496296909465173
iteration: 90 loss: 3.7777205422623243 grad: -0.598526603928247
iteration: 100 loss: 3.4168603780187614 grad: -0.5540345689482211
iteration: 110 loss: 3.118606766896309 grad: -0.5152213911386748
iteration: 120 loss: 2.8680273560123 grad: -0.481206216775807
iteration: 130 loss: 2.65457661371118 grad: -0.4512294045169567
iteration: 140 loss: 2.470598015358229 grad: -0.4246578254579987
iteration: 150 loss: 2.3103998882812364 grad: -0.40097033743896504
iteration: 160 loss: 2.1696642125293693 grad: -0.3797391810488061
iteration: 170 loss: 2.0450564907970477 grad: -0.3606125880404727
iteration: 180 loss: 1.9339612637761754 grad: -0.3433000610484215
iteration: 190 loss: 1.8342985124273499 grad: -0.32756041450991935
iteration: 200 loss: 1.7443935143612428 grad: -0.31319222950013687
iteration: 210 loss: 1.662882855005054 grad: -0.3000262861338824
iteration: 220 loss: 1.5886454049965655 grad: -0.2879195696355758
iteration: 230 loss: 1.5207508618053576 grad: -0.27675051252680205
iteration: 240 loss: 1.4584208578982973 grad: -0.26641520317249756
iteration: 250 loss: 1.4009991982455754 grad: -0.25682434960743905
iteration: 260 loss: 1.3479288232659778 grad: -0.24790083504708205
iteration: 0 loss: 75.3284790973721 grad: 93.45602626910438
iteration: 10 loss: 22.66430297038187 grad: 2.4205402730131236
iteration: 20 loss: 14.141705792975204 grad: -0.7299121302647777
iteration: 30 loss: 10.247363974475114 grad: -0.976315906714869
iteration: 40 loss: 8.016172400658517 grad: -0.9348449569978745
iteration: 50 loss: 6.573885867386459 grad: -0.853401603554334
iteration: 60 loss: 5.5668769047442765 grad: -0.7736380206280371
iteration: 70 loss: 4.824900838357817 grad: -0.7033122893029233
iteration: 80 loss: 4.2560058074044385 grad: -0.6428509487247649
iteration: 90 loss: 3.8062406537746525 grad: -0.5910438758808132
iteration: 100 loss: 3.441899357376025 grad: -0.5464742244163162
iteration: 110 loss: 3.140858032212732 grad: -0.5078793193062408
iteration: 120 loss: 2.888003623939436 grad: -0.47421537356790167
iteration: 130 loss: 2.6726662375220425 grad: -0.4446405011773625
iteration: 140 loss: 2.4871013028068774 grad: -0.41847977622073007
iteration: 150 loss: 2.3255532666520264 grad: -0.39519115663847315
iteration: 160 loss: 2.1836567588035325 grad: -0.3743370031811897
iteration: 170 loss: 2.058041517675106 grad: -0.3555615371989525
iteration: 180 loss: 1.9460646169463458 grad: -0.33857336680685485
iteration: 190 loss: 1.8456246274209165 grad: -0.32313202877841307
iteration: 200 loss: 1.7550299142954744 grad: -0.309037618304765
iteration: 210 loss: 1.6729035423669218 grad: -0.29612276613038047
iteration: 220 loss: 1.5981134552567213 grad: -0.28424639561165366
iteration: 230 loss: 1.5297204319146906 grad: -0.27328883221781136
iteration: 240 loss: 1.4669387596805707 grad: -0.2631479453453186
iteration: 250 loss: 1.4091061440214452 grad: -0.25373608273496284
iteration: 260 loss: 1.3556604215785806 grad: -0.24497761744049323
iteration: 0 loss: 77.6709687314208 grad: 130.7888142019733
iteration: 10 loss: 21.94261616026911 grad: -0.8540629336844083
iteration: 20 loss: 13.77086482083037 grad: -1.9467118048204914
iteration: 30 loss: 10.001623070868039 grad: -1.6895334182813537
iteration: 40 loss: 7.833634640710608 grad: -1.4401686788903127
iteration: 50 loss: 6.429040459327635 grad: -1.2436062254580622
iteration: 60 loss: 5.4469324064707925 grad: -1.090002643287287
iteration: 70 loss: 4.72258653349321 grad: -0.9681822856783416
iteration: 80 loss: 4.166811418126062 grad: -0.8698077109543872
iteration: 90 loss: 3.7271825524754933 grad: -0.7889860879994394
iteration: 100 loss: 3.3709042960305684 grad: -0.7215500263240644
iteration: 110 loss: 3.0764282921549735 grad: -0.6645095785310913
iteration: 120 loss: 2.8290226822645392 grad: -0.6156806442501378
iteration: 130 loss: 2.6182798445173088 grad: -0.5734384252673761
iteration: 140 loss: 2.4366414655183193 grad: -0.5365530920696988
iteration: 150 loss: 2.2784878529175994 grad: -0.5040787454843512
iteration: 160 loss: 2.139555034615279 grad: -0.4752771312363517
iteration: 170 loss: 2.0165499431995655 grad: -0.44956431948047776
iteration: 180 loss: 1.9068894484897678 grad: -0.42647280402755583
iteration: 190 loss: 1.808519154327562 grad: -0.4056241285260337
iteration: 200 loss: 1.719784925825934 grad: -0.38670881779180355
iteration: 210 loss: 1.6393400919516188 grad: -0.36947145852238295
iteration: 220 loss: 1.5660772888936663 grad: -0.3536994639035493
iteration: 230 loss: 1.4990776419721525 grad: -0.3392145105257078
iteration: 240 loss: 1.4375723545147758 grad: -0.32586593912149764
iteration: 250 loss: 1.3809133113074743 grad: -0.313525616056933
iteration: 260 loss: 1.3285503236262146 grad: -0.30208389372622735
iteration: 0 loss: 75.52755889638378 grad: 92.9391848817523
iteration: 10 loss: 22.222079328883183 grad: 1.9550249052962825
iteration: 20 loss: 13.893163763963601 grad: -0.9015651515596955
iteration: 30 loss: 10.086209964120867 grad: -1.0585518588517344
iteration: 40 loss: 7.901582380341489 grad: -0.9815750373808518
iteration: 50 loss: 6.487019766472837 grad: -0.8822566901636959
iteration: 60 loss: 5.4979470998908 grad: -0.7922438025017873
iteration: 70 loss: 4.768320313826693 grad: -0.7155388023025482
iteration: 80 loss: 4.208342949078625 grad: -0.6508777524802809
iteration: 90 loss: 3.765263473472508 grad: -0.5961919310274431
iteration: 100 loss: 3.4060882088336837 grad: -0.5495882661546283
iteration: 110 loss: 3.1091389539019474 grad: -0.5095220596615408
iteration: 120 loss: 2.8595937486687615 grad: -0.47477396003278427
iteration: 130 loss: 2.6469797864209053 grad: -0.4443884054578444
iteration: 140 loss: 2.4636900762092906 grad: -0.41761447231928445
iteration: 150 loss: 2.304067658179106 grad: -0.3938579322251018
iteration: 160 loss: 2.1638194467863783 grad: -0.37264448231952646
iteration: 170 loss: 2.0396292467218946 grad: -0.35359207063964637
iteration: 180 loss: 1.9288952916286193 grad: -0.33639014785540444
iteration: 190 loss: 1.829547982909348 grad: -0.3207840655850489
iteration: 200 loss: 1.7399206500899296 grad: -0.3065632708297111
iteration: 210 loss: 1.6586561864726057 grad: -0.29355230258578024
iteration: 220 loss: 1.5846384662149418 grad: -0.2816038669543005
iteration: 230 loss: 1.5169412009074057 grad: -0.27059346461637246
iteration: 240 loss: 1.4547892768961324 grad: -0.26041518689672505
iteration: 250 loss: 1.397529161938523 grad: -0.25097839883454254
iteration: 260 loss: 1.3446059946373183 grad: -0.24220510117830837
iteration: 0 loss: 71.57889297070966 grad: 95.0917894711452
iteration: 10 loss: 21.531369365670983 grad: 3.6276236165835543
iteration: 20 loss: 13.494674000623304 grad: -0.6054633709884427
iteration: 30 loss: 9.818879848665182 grad: -1.0826110181213662
iteration: 40 loss: 7.702372105450525 grad: -1.082767746783139
iteration: 50 loss: 6.328688626420114 grad: -0.9991255637772363
iteration: 60 loss: 5.36665750979794 grad: -0.9071563861827641
iteration: 70 loss: 4.656178230513433 grad: -0.8234253204418512
iteration: 80 loss: 4.110450554146362 grad: -0.7506772640605375
iteration: 90 loss: 3.678383527191975 grad: -0.6881728711119861
iteration: 100 loss: 3.3279732634431722 grad: -0.6344295876110538
iteration: 110 loss: 3.0381664719933394 grad: -0.5879826465658509
iteration: 120 loss: 2.7945545149280573 grad: -0.5475746483652748
iteration: 130 loss: 2.586948624351614 grad: -0.5121747274451738
iteration: 140 loss: 2.407943344881005 grad: -0.48095050887519786
iteration: 150 loss: 2.2520286373691802 grad: -0.4532316948371161
iteration: 160 loss: 2.1150210356298556 grad: -0.42847690904483926
iteration: 170 loss: 1.9936877913281188 grad: -0.4062464590132481
iteration: 180 loss: 1.885491785385718 grad: -0.38618082708260365
iteration: 190 loss: 1.7884142816106676 grad: -0.3679839435585597
iteration: 200 loss: 1.7008291783754843 grad: -0.3514102340876213
iteration: 210 loss: 1.6214121256461609 grad: -0.33625457687241056
iteration: 220 loss: 1.5490737384914361 grad: -0.32234448353537726
iteration: 230 loss: 1.4829097759690988 grad: -0.3095339766331673
iteration: 240 loss: 1.4221634664554585 grad: -0.29769876477337986
iteration: 250 loss: 1.3661966626246298 grad: -0.28673241460942994
iteration: 260 loss: 1.3144675047502792 grad: -0.2765432930266072
iteration: 0 loss: 73.70459816645243 grad: 94.62707910750649
iteration: 10 loss: 21.876685266774135 grad: 2.7849006660524385
iteration: 20 loss: 13.73961551362039 grad: -0.8294847023374238
iteration: 30 loss: 9.997698404776719 grad: -1.1026362676003143
iteration: 40 loss: 7.840868039524594 grad: -1.047735130342002
iteration: 50 loss: 6.440756108586026 grad: -0.9499219813807654
iteration: 60 loss: 5.460299670690471 grad: -0.8560405247337364
iteration: 70 loss: 4.736339834530389 grad: -0.7742673441442196
iteration: 80 loss: 4.180369093693695 grad: -0.7046097709118258
iteration: 90 loss: 3.7402807291521807 grad: -0.6453713959713873
iteration: 100 loss: 3.383433406153348 grad: -0.5947323065542197
iteration: 110 loss: 3.088355625350852 grad: -0.5511213070842509
iteration: 120 loss: 2.8403539653163534 grad: -0.5132633815769821
iteration: 130 loss: 2.6290395240317923 grad: -0.4801435034183853
iteration: 140 loss: 2.4468623618226544 grad: -0.4509561820852366
iteration: 150 loss: 2.288205679512926 grad: -0.42505989886454976
iteration: 160 loss: 2.1488055140299696 grad: -0.4019403541152135
iteration: 170 loss: 2.025367335679303 grad: -0.38118200619083786
iteration: 180 loss: 1.915305858201753 grad: -0.36244634820519245
iteration: 190 loss: 1.8165642641195683 grad: -0.34545537534444754
iteration: 200 loss: 1.7274859654832075 grad: -0.32997896819597655
iteration: 210 loss: 1.6467219296569469 grad: -0.3158252110617302
iteration: 220 loss: 1.5731625831849256 grad: -0.30283291084914365
iteration: 230 loss: 1.505887018643493 grad: -0.2908657725536563
iteration: 240 loss: 1.4441245885883622 grad: -0.2798078292490169
iteration: 250 loss: 1.3872255033138159 grad: -0.26955982868556794
iteration: 260 loss: 1.3346380647248308 grad: -0.26003635472196185
iteration: 0 loss: 75.88326346151392 grad: 140.41269180098408
iteration: 10 loss: 21.558306013190304 grad: -0.46640127666468284
iteration: 20 loss: 13.551123269912193 grad: -1.7933845792439382
iteration: 30 loss: 9.859061289201753 grad: -1.6356391201603249
iteration: 40 loss: 7.730162453669933 grad: -1.414253846385673
iteration: 50 loss: 6.348056205779358 grad: -1.2278037692501973
iteration: 60 loss: 5.380274202850656 grad: -1.0787723566422922
iteration: 70 loss: 4.665788500242963 grad: -0.9594093419278179
iteration: 80 loss: 4.117207101198966 grad: -0.8625347730977153
iteration: 90 loss: 3.6830646192503 grad: -0.7827181166327168
iteration: 100 loss: 3.3311166815673543 grad: -0.716003330039116
iteration: 110 loss: 3.0401521676583965 grad: -0.6595080403599378
iteration: 120 loss: 2.795656461046912 grad: -0.6111078005716627
iteration: 130 loss: 2.5873680409706803 grad: -0.5692132626608237
iteration: 140 loss: 2.4078304685933123 grad: -0.5326168924516173
iteration: 150 loss: 2.251497295880654 grad: -0.5003874875337181
iteration: 160 loss: 2.114158605551893 grad: -0.4717968711800179
iteration: 170 loss: 1.992562130846264 grad: -0.4462682585802454
iteration: 180 loss: 1.8841561450519129 grad: -0.4233393563592588
iteration: 190 loss: 1.7869108385698382 grad: -0.4026356062614014
iteration: 200 loss: 1.6991916171481618 grad: -0.3838505112324979
iteration: 210 loss: 1.6196675479730154 grad: -0.36673097634263907
iteration: 220 loss: 1.5472440940719603 grad: -0.3510662496205975
iteration: 230 loss: 1.4810129464127717 grad: -0.33667948122659375
iteration: 240 loss: 1.4202140950560533 grad: -0.3234212108461546
iteration: 250 loss: 1.3642067956703945 grad: -0.31116429180040756
iteration: 260 loss: 1.3124470915784265 grad: -0.29979989748499286
iteration: 0 loss: 74.23053820582665 grad: 110.27619287622092
iteration: 10 loss: 21.753522661524087 grad: 2.148201618447074
iteration: 20 loss: 13.644256764841058 grad: -0.9979771898948008
iteration: 30 loss: 9.926145944974968 grad: -1.2648934614818346
iteration: 40 loss: 7.784581839464739 grad: -1.1991988842104586
iteration: 50 loss: 6.394573586546192 grad: -1.085584382799988
iteration: 60 loss: 5.421207637014959 grad: -0.9762358247209685
iteration: 70 loss: 4.702477972045347 grad: -0.8809085982867004
iteration: 80 loss: 4.1505183518082625 grad: -0.7997726922083642
iteration: 90 loss: 3.7136012953293505 grad: -0.7308913003226778
iteration: 100 loss: 3.359322862242979 grad: -0.6721335581041289
iteration: 110 loss: 3.0663674474542195 grad: -0.621644686252401
iteration: 120 loss: 2.820148147425863 grad: -0.5779150214804374
iteration: 130 loss: 2.6103512839466188 grad: -0.539741948017348
iteration: 140 loss: 2.429481537047815 grad: -0.50617163941647
iteration: 150 loss: 2.271962715390807 grad: -0.47644535637400437
iteration: 160 loss: 2.13356166719483 grad: -0.4499557769084192
iteration: 170 loss: 2.011007658602921 grad: -0.42621305796965003
iteration: 180 loss: 1.901734069151065 grad: -0.40481889606413585
iteration: 190 loss: 1.803698926106666 grad: -0.38544678256863507
iteration: 200 loss: 1.7152575933321879 grad: -0.36782694404010763
iteration: 210 loss: 1.6350707675381437 grad: -0.3517347970418864
iteration: 220 loss: 1.5620368746698328 grad: -0.3369820380970826
iteration: 230 loss: 1.4952416440549479 grad: -0.3234097162864022
iteration: 240 loss: 1.4339199800211977 grad: -0.3108828059272838
iteration: 250 loss: 1.3774267721873086 grad: -0.2992859218430955
iteration: 260 loss: 1.3252142938766733 grad: -0.288519911228026
iteration: 0 loss: 77.76148846183791 grad: 143.06531274104452
iteration: 10 loss: 21.809775098323165 grad: -1.6923037695278322
iteration: 20 loss: 13.7163126870461 grad: -2.201165864655439
iteration: 30 loss: 9.971237857745894 grad: -1.8400540249623243
iteration: 40 loss: 7.812285433917747 grad: -1.5425046517861194
iteration: 50 loss: 6.411574371363878 grad: -1.3194189102134182
iteration: 60 loss: 5.431394427643516 grad: -1.1496248110402254
iteration: 70 loss: 4.708171391848489 grad: -1.0170515759112733
iteration: 80 loss: 4.1531618264349905 grad: -0.9110478772239817
iteration: 90 loss: 3.7141269013690033 grad: -0.8245382302530635
iteration: 100 loss: 3.3583519280836795 grad: -0.7527012748944855
iteration: 110 loss: 3.0643254369192205 grad: -0.6921582147906181
iteration: 120 loss: 2.81733311678549 grad: -0.640479060399803
iteration: 130 loss: 2.6069761181882893 grad: -0.5958755522467576
iteration: 140 loss: 2.4257008147737493 grad: -0.557004740426469
iteration: 150 loss: 2.2678902609712748 grad: -0.5228398539432915
iteration: 160 loss: 2.1292822937069684 grad: -0.4925832535747562
iteration: 170 loss: 2.0065851915774147 grad: -0.46560641450028406
iteration: 180 loss: 1.897216933158895 grad: -0.44140769721465456
iteration: 190 loss: 1.7991241005549703 grad: -0.41958208549547593
iteration: 200 loss: 1.7106534562759323 grad: -0.3997991373881222
iteration: 210 loss: 1.6304591682114733 grad: -0.3817866759322699
iteration: 220 loss: 1.5574346627009534 grad: -0.3653185580722583
iteration: 230 loss: 1.490661810511483 grad: -0.35020538542126134
iteration: 240 loss: 1.4293725175220144 grad: -0.33628736692162164
iteration: 250 loss: 1.3729193292896609 grad: -0.32342877589293706
iteration: 260 loss: 1.320752677158742 grad: -0.3115136024907723
iteration: 0 loss: 75.60580619140646 grad: 138.44295895504374
iteration: 10 loss: 22.1024139535804 grad: -0.35628031453314934
iteration: 20 loss: 13.900875944229732 grad: -1.7129413657579076
iteration: 30 loss: 10.112733622883235 grad: -1.5712741579012541
iteration: 40 loss: 7.927223136466389 grad: -1.3609780077349165
iteration: 50 loss: 6.50833134474781 grad: -1.1826999410556085
iteration: 60 loss: 5.514944253524349 grad: -1.0398687002270992
iteration: 70 loss: 4.7816983076928326 grad: -0.9253377877640677
iteration: 80 loss: 4.218821446709998 grad: -0.8323181729523517
iteration: 90 loss: 3.7734451374568296 grad: -0.7556377665367427
iteration: 100 loss: 3.4124486588131564 grad: -0.6915169479562544
iteration: 110 loss: 3.114047130456625 grad: -0.6371975543071444
iteration: 120 loss: 2.8633353580773306 grad: -0.5906449382320984
iteration: 130 loss: 2.649777130168869 grad: -0.5503361448532327
iteration: 140 loss: 2.4657174112594475 grad: -0.5151136789916564
iteration: 150 loss: 2.3054627347953764 grad: -0.4840846882069055
iteration: 160 loss: 2.1646920964455023 grad: -0.4565508260588639
iteration: 170 loss: 2.0400677774044365 grad: -0.43195882084736675
iteration: 180 loss: 1.9289712744219865 grad: -0.40986513606925107
iteration: 190 loss: 1.8293198606048795 grad: -0.3899103396717035
iteration: 200 loss: 1.7394364801463615 grad: -0.3718002544435923
iteration: 210 loss: 1.6579557438963506 grad: -0.3552919111533482
iteration: 220 loss: 1.583754869541198 grad: -0.34018294993975534
iteration: 230 loss: 1.5159021799415975 grad: -0.32630353005316015
iteration: 240 loss: 1.4536181690849512 grad: -0.31351008701044886
iteration: 250 loss: 1.396245701471881 grad: -0.30168046638402746
iteration: 260 loss: 1.3432269419539062 grad: -0.290710094749006
iteration: 0 loss: 79.22114025034018 grad: 153.9568781237047
iteration: 10 loss: 22.24035397719647 grad: -2.0920441432385597
iteration: 20 loss: 13.961321809879047 grad: -2.1085456184477716
iteration: 30 loss: 10.143246200678835 grad: -1.754665405119986
iteration: 40 loss: 7.944172426078674 grad: -1.4804264707362425
iteration: 50 loss: 6.518121622939692 grad: -1.2735064468047625
iteration: 60 loss: 5.520600309152129 grad: -1.113957460542426
iteration: 70 loss: 4.78481841390571 grad: -0.9880201185157141
iteration: 80 loss: 4.220314902141871 grad: -0.8865381479256549
iteration: 90 loss: 3.7738588865916594 grad: -0.8032790852944849
iteration: 100 loss: 3.412126735844471 grad: -0.7338908087743485
iteration: 110 loss: 3.113214051372779 grad: -0.6752637566418733
iteration: 120 loss: 2.862142200221983 grad: -0.6251296105886772
iteration: 130 loss: 2.648328196753482 grad: -0.581801970727438
iteration: 140 loss: 2.4640862839057096 grad: -0.5440050213067068
iteration: 150 loss: 2.303702274477928 grad: -0.5107580470087788
iteration: 160 loss: 2.1628408633298686 grad: -0.4812961195639425
iteration: 170 loss: 2.0381542614784602 grad: -0.45501464787755863
iteration: 180 loss: 1.9270167487566554 grad: -0.43142995420601926
iteration: 190 loss: 1.827340345608594 grad: -0.410150798145856
iteration: 200 loss: 1.7374441207643527 grad: -0.390857502229803
iteration: 210 loss: 1.655959791146126 grad: -0.373286437897389
iteration: 220 loss: 1.5817623904088742 grad: -0.35721834688599363
iteration: 230 loss: 1.513918577653385 grad: -0.3424694447247992
iteration: 240 loss: 1.4516475689977761 grad: -0.32888456831673063
iteration: 250 loss: 1.3942912403940269 grad: -0.31633184348325183
iteration: 260 loss: 1.34129098721492 grad: -0.3046984954801235
iteration: 0 loss: 77.50590584263168 grad: 109.6790717195644
iteration: 10 loss: 22.510469332076187 grad: 1.3202771704468979
iteration: 20 loss: 14.038226939219074 grad: -1.1132817771449306
iteration: 30 loss: 10.176746936211492 grad: -1.2338422453769478
iteration: 40 loss: 7.964235167748608 grad: -1.1353115710364003
iteration: 50 loss: 6.533342490033192 grad: -1.016435463105831
iteration: 60 loss: 5.533820166261194 grad: -0.9097046239228401
iteration: 70 loss: 4.797068852672427 grad: -0.8191677702389477
iteration: 80 loss: 4.231997190203898 grad: -0.7431329948996688
iteration: 90 loss: 3.7851373455149027 grad: -0.6790466511216069
iteration: 100 loss: 3.4230718844126553 grad: -0.6246013470922751
iteration: 110 loss: 3.123858258547733 grad: -0.5779261101430051
iteration: 120 loss: 2.8725023384464623 grad: -0.537550921688678
iteration: 130 loss: 2.658415093159006 grad: -0.5023283227313103
iteration: 140 loss: 2.473908660735188 grad: -0.4713597247312564
iteration: 150 loss: 2.3132683293059597 grad: -0.4439362707972556
iteration: 160 loss: 2.172158831907754 grad: -0.4194937710198492
iteration: 170 loss: 2.047232550936025 grad: -0.3975789517504838
iteration: 180 loss: 1.9358639015292944 grad: -0.3778242550644184
iteration: 190 loss: 1.8359649486756098 grad: -0.35992895570743066
iteration: 200 loss: 1.7458547088313852 grad: -0.343644916909735
iteration: 210 loss: 1.6641647625162725 grad: -0.3287657575287999
iteration: 220 loss: 1.5897699409420882 grad: -0.31511854121122984
iteration: 230 loss: 1.5217366525690026 grad: -0.3025573436765605
iteration: 240 loss: 1.4592838301405209 grad: -0.29095823008756594
iteration: 250 loss: 1.4017530460646825 grad: -0.28021530016062335
iteration: 260 loss: 1.3485853817944025 grad: -0.2702375487227837
iteration: 0 loss: 76.59249078773418 grad: 109.91888494073712
iteration: 10 loss: 22.406415111795695 grad: 1.2619565554745833
iteration: 20 loss: 13.981242152063942 grad: -1.0940168459443944
iteration: 30 loss: 10.132001903945179 grad: -1.200327462984564
iteration: 40 loss: 7.926819411680299 grad: -1.104509649347988
iteration: 50 loss: 6.5012913374286905 grad: -0.9910133442778546
iteration: 60 loss: 5.505907093480056 grad: -0.8890772313797533
iteration: 70 loss: 4.7724300931071495 grad: -0.8023239996437361
iteration: 80 loss: 4.210000070205379 grad: -0.7292065709820138
iteration: 90 loss: 3.765307565900333 grad: -0.6673777237212961
iteration: 100 loss: 3.405046895528234 grad: -0.6146999904025972
iteration: 110 loss: 3.107356000917372 grad: -0.5694280976263297
iteration: 120 loss: 2.857299815217175 grad: -0.5301826399779644
iteration: 130 loss: 2.644333411414581 grad: -0.4958814561571432
iteration: 140 loss: 2.4608024042262597 grad: -0.46567344052638315
iteration: 150 loss: 2.3010178792829636 grad: -0.43888470047118033
iteration: 160 loss: 2.160664727832558 grad: -0.4149771439647397
iteration: 170 loss: 2.03641128581574 grad: -0.39351724215399464
iteration: 180 loss: 1.9256447422335028 grad: -0.37415257029103827
iteration: 190 loss: 1.8262874996209824 grad: -0.3565941443727495
iteration: 200 loss: 1.7366670224767569 grad: -0.3406030417754978
iteration: 210 loss: 1.6554218574245334 grad: -0.3259801894506814
iteration: 220 loss: 1.5814326273977135 grad: -0.31255850473150204
iteration: 230 loss: 1.5137705929663008 grad: -0.30019679508334896
iteration: 240 loss: 1.451658780502178 grad: -0.2887749830671237
iteration: 250 loss: 1.3944422387141706 grad: -0.27819033787363046
iteration: 260 loss: 1.341565019044331 grad: -0.26835447771055043
iteration: 0 loss: 75.44934226041667 grad: 130.02713517737521
iteration: 10 loss: 21.06875413706466 grad: -0.7288309271991632
iteration: 20 loss: 13.299406480930077 grad: -1.7754872262798922
iteration: 30 loss: 9.696282561649571 grad: -1.5922248051546197
iteration: 40 loss: 7.613234056661853 grad: -1.3786037616782818
iteration: 50 loss: 6.2585336724874985 grad: -1.201075569620238
iteration: 60 loss: 5.308702601435718 grad: -1.058653721259974
iteration: 70 loss: 4.606737305042646 grad: -0.9438964144444058
iteration: 80 loss: 4.067301042258268 grad: -0.8502513698175739
iteration: 90 loss: 3.640080254275166 grad: -0.7727540326287347
iteration: 100 loss: 3.2935220461104935 grad: -0.7077505655344024
iteration: 110 loss: 3.0068530773914146 grad: -0.6525506704159356
iteration: 120 loss: 2.7658479539338923 grad: -0.6051535957878169
iteration: 130 loss: 2.560443052909415 grad: -0.5640515606570298
iteration: 140 loss: 2.383321398826328 grad: -0.5280922339285248
iteration: 150 loss: 2.229037335397689 grad: -0.49638272754147406
iteration: 160 loss: 2.0934553954533097 grad: -0.46822208477929367
iteration: 170 loss: 1.973379364849486 grad: -0.44305327231234337
iteration: 180 loss: 1.8663004354046857 grad: -0.42042860422507256
iteration: 190 loss: 1.7702221558313134 grad: -0.39998451158346027
iteration: 200 loss: 1.6835362099570623 grad: -0.38142289269116403
iteration: 210 loss: 1.604932617446221 grad: -0.3644971557473211
iteration: 220 loss: 1.5333337314727529 grad: -0.3490016496767575
iteration: 230 loss: 1.4678449947851235 grad: -0.33476357143143487
iteration: 240 loss: 1.407717696405019 grad: -0.3216367046761214
iteration: 250 loss: 1.3523204534901858 grad: -0.3094965279463746
iteration: 0 loss: 75.92323656806865 grad: 125.92196813452072
iteration: 10 loss: 21.640663343637243 grad: -0.7346658256563996
iteration: 20 loss: 13.614144781896336 grad: -1.676570542107648
iteration: 30 loss: 9.905639294914037 grad: -1.4964858674361334
iteration: 40 loss: 7.767825283289147 grad: -1.2944175575174468
iteration: 50 loss: 6.380091148078009 grad: -1.127827855062419
iteration: 60 loss: 5.408330541059306 grad: -0.9946251882364188
iteration: 70 loss: 4.690825402661672 grad: -0.8874742959834341
iteration: 80 loss: 4.139846649382062 grad: -0.8000962700984704
iteration: 90 loss: 3.703741842574139 grad: -0.7277897823648585
iteration: 100 loss: 3.3501520644113456 grad: -0.6671209625533567
iteration: 110 loss: 3.0577907382701763 grad: -0.6155743197443586
iteration: 120 loss: 2.812090794857051 grad: -0.5712852555928709
iteration: 130 loss: 2.6027527539620015 grad: -0.5328514351143846
iteration: 140 loss: 2.422291843719284 grad: -0.49920234873690417
iteration: 150 loss: 2.2651399452114758 grad: -0.4695089757362476
iteration: 160 loss: 2.127070233505327 grad: -0.44312060925058344
iteration: 170 loss: 2.004817030511607 grad: -0.41952010161423825
iteration: 180 loss: 1.8958178208608842 grad: -0.39829171387948215
iteration: 190 loss: 1.798034009851976 grad: -0.3790976951831335
iteration: 200 loss: 1.7098237728136652 grad: -0.3616609896711486
iteration: 210 loss: 1.6298501693795928 grad: -0.34575230299369053
iteration: 220 loss: 1.5570136284599854 grad: -0.3311803118272575
iteration: 230 loss: 1.4904015900731957 grad: -0.3177841683269238
iteration: 240 loss: 1.4292504294739956 grad: -0.30542770059528374
iteration: 250 loss: 1.37291630874039 grad: -0.2939948809118128
iteration: 260 loss: 1.3208526080320553 grad: -0.2833862518159755
iteration: 0 loss: 75.33899650993406 grad: 158.68056409891557
iteration: 10 loss: 21.48411951930443 grad: -1.7707525720058341
iteration: 20 loss: 13.548362734101374 grad: -1.9607586787259312
iteration: 30 loss: 9.859562067217253 grad: -1.6749004433122037
iteration: 40 loss: 7.727963299219606 grad: -1.429292091822969
iteration: 50 loss: 6.343258927543667 grad: -1.2374147834929508
iteration: 60 loss: 5.373705816865409 grad: -1.0869321634165006
iteration: 70 loss: 4.658141037953845 grad: -0.9669435706164231
iteration: 80 loss: 4.108955736456871 grad: -0.8695922186584779
iteration: 90 loss: 3.6745184232121657 grad: -0.7893214524174752
iteration: 100 loss: 3.3224733232201618 grad: -0.7221649626690866
iteration: 110 loss: 3.031537137276287 grad: -0.6652484735933382
iteration: 120 loss: 2.7871487123754624 grad: -0.6164543025854221
iteration: 130 loss: 2.5790163744501444 grad: -0.5741961861084313
iteration: 140 loss: 2.399664000920521 grad: -0.5372669360108012
iteration: 150 loss: 2.243532208786043 grad: -0.5047340935215824
iteration: 160 loss: 2.1064025461309526 grad: -0.4758674246891315
iteration: 170 loss: 1.9850171143785618 grad: -0.4500877749421055
iteration: 180 loss: 1.8768204906155006 grad: -0.4269304439928837
iteration: 190 loss: 1.7797804715964545 grad: -0.4060185741634925
iteration: 200 loss: 1.6922609522944185 grad: -0.3870435446734879
iteration: 210 loss: 1.6129300904346813 grad: -0.36975033760756365
iteration: 220 loss: 1.540692849995015 grad: -0.3539264803322548
iteration: 230 loss: 1.474640702660343 grad: -0.3393935942159607
iteration: 240 loss: 1.414013608863694 grad: -0.3260008660496204
iteration: 250 loss: 1.358170921759376 grad: -0.31361995429658107
iteration: 260 loss: 1.3065688656590944 grad: -0.30214097774153204
iteration: 0 loss: 77.49362515403604 grad: 119.44844584617618
iteration: 10 loss: 22.06089250697282 grad: -0.007841035625501508
iteration: 20 loss: 13.871771672245153 grad: -1.6853846130028385
iteration: 30 loss: 10.084284378531141 grad: -1.545739736888297
iteration: 40 loss: 7.901936087410935 grad: -1.3438474312335449
iteration: 50 loss: 6.486542814527473 grad: -1.1727109112258807
iteration: 60 loss: 5.496266461964462 grad: -1.0346902966546359
iteration: 70 loss: 4.765622441030535 grad: -0.9232474740451169
iteration: 80 loss: 4.204885278750928 grad: -0.8322045415666632
iteration: 90 loss: 3.761266566994643 grad: -0.7567996151116639
iteration: 100 loss: 3.401721262681349 grad: -0.6935079200985426
iteration: 110 loss: 3.1045270518536445 grad: -0.639728401591003
iteration: 120 loss: 2.8548281882123607 grad: -0.5935245982565721
iteration: 130 loss: 2.642126960684767 grad: -0.5534361692587749
iteration: 140 loss: 2.458798242720191 grad: -0.5183464085404016
iteration: 150 loss: 2.2991718864539674 grad: -0.48738958011798666
iteration: 160 loss: 2.1589451848437253 grad: -0.45988570120847233
iteration: 170 loss: 2.034794886759755 grad: -0.4352941427503724
iteration: 180 loss: 1.9241140220460693 grad: -0.41318019850185334
iteration: 190 loss: 1.824829133168309 grad: -0.39319068090313125
iteration: 200 loss: 1.7352706741974295 grad: -0.37503587516849596
iteration: 210 loss: 1.654079387473344 grad: -0.3584760285451108
iteration: 220 loss: 1.5801375337506132 grad: -0.3433111152651299
iteration: 230 loss: 1.5125176128016888 grad: -0.3293729965689185
iteration: 240 loss: 1.450443601043844 grad: -0.3165193525413775
iteration: 250 loss: 1.393261284541371 grad: -0.30462893935280616
iteration: 260 loss: 1.3404152936239535 grad: -0.2935978484517034
iteration: 0 loss: 73.52369460190043 grad: 147.38239728149765
iteration: 10 loss: 20.761437862660202 grad: -1.9061777605456527
iteration: 20 loss: 13.237025843409748 grad: -2.182626378649964
iteration: 30 loss: 9.677097178310987 grad: -1.8101183195335824
iteration: 40 loss: 7.602782214131366 grad: -1.5180690864844808
iteration: 50 loss: 6.24932098026168 grad: -1.3003526417375988
iteration: 60 loss: 5.299045139911414 grad: -1.1344291850506578
iteration: 70 loss: 4.59639581891712 grad: -1.0046140952169824
iteration: 80 loss: 4.056394705873035 grad: -0.9006438721905342
iteration: 90 loss: 3.6287900779701556 grad: -0.8156867769757825
iteration: 100 loss: 3.2820111478288463 grad: -0.7450685149347809
iteration: 110 loss: 2.9952503430637822 grad: -0.6855026614781394
iteration: 120 loss: 2.754249866688207 grad: -0.6346203722981534
iteration: 130 loss: 2.5489197690179606 grad: -0.590675728604791
iteration: 140 loss: 2.3719228661402254 grad: -0.5523561381358227
iteration: 150 loss: 2.2177983682705333 grad: -0.518657182336123
iteration: 160 loss: 2.0823996034615364 grad: -0.4887980274366459
iteration: 170 loss: 1.962522107081523 grad: -0.4621630008119435
iteration: 180 loss: 1.855651014791089 grad: -0.4382604345907857
iteration: 190 loss: 1.7597854418265177 grad: -0.41669314694992743
iteration: 200 loss: 1.6733138379041852 grad: -0.39713692021352476
iteration: 210 loss: 1.5949238763036275 grad: -0.37932457249406293
iteration: 220 loss: 1.5235362223009112 grad: -0.3630340062871256
iteration: 230 loss: 1.458255120291266 grad: -0.34807912745013425
iteration: 240 loss: 1.3983310255805257 grad: -0.334302864781879
iteration: 250 loss: 1.3431319935278803 grad: -0.32157174664535315
iteration: 0 loss: 77.29721045996929 grad: 116.39272961047635
iteration: 10 loss: 22.802179950727062 grad: 0.9446966131298709
iteration: 20 loss: 14.218837423148774 grad: -1.0947633721236025
iteration: 30 loss: 10.3021682612532 grad: -1.1964316948170253
iteration: 40 loss: 8.059406973427361 grad: -1.1046549264431769
iteration: 50 loss: 6.609756931365237 grad: -0.9935963929562437
iteration: 60 loss: 5.597556141420145 grad: -0.8927947559654974
iteration: 70 loss: 4.851693276030016 grad: -0.8064581011004108
iteration: 80 loss: 4.2797693444484715 grad: -0.7333949169088289
iteration: 90 loss: 3.827573948870436 grad: -0.6714474757639367
iteration: 100 loss: 3.4612385526226963 grad: -0.6185754181356418
iteration: 110 loss: 3.158531468550295 grad: -0.5730823917542323
iteration: 120 loss: 2.904264992946537 grad: -0.533613185763026
iteration: 130 loss: 2.6877157514697747 grad: -0.499097153984306
iteration: 140 loss: 2.5010997879578762 grad: -0.4686881940776606
iteration: 150 loss: 2.33863186847384 grad: -0.44171408718303856
iteration: 160 loss: 2.195923816057323 grad: -0.4176367440942833
iteration: 170 loss: 2.069587548501948 grad: -0.3960218389242588
iteration: 180 loss: 1.9569660101857485 grad: -0.3765158081974835
iteration: 190 loss: 1.8559464243952872 grad: -0.3588284263255506
iteration: 200 loss: 1.7648279382025398 grad: -0.3427195474654349
iteration: 210 loss: 1.682226051056866 grad: -0.32798895160460767
iteration: 220 loss: 1.6070024401038525 grad: -0.3144685092752189
iteration: 230 loss: 1.5382126500491557 grad: -0.30201608726075563
iteration: 240 loss: 1.4750665626807362 grad: -0.29051077036829004
iteration: 250 loss: 1.4168981493795316 grad: -0.27984908545677456
iteration: 260 loss: 1.3631420614000158 grad: -0.26994199463144886
iteration: 0 loss: 78.3623719922004 grad: 107.85142135853036
iteration: 10 loss: 22.536141555947946 grad: 0.7548854504486353
iteration: 20 loss: 14.035406124512251 grad: -1.3706150860935873
iteration: 30 loss: 10.17208420618063 grad: -1.346752329497575
iteration: 40 loss: 7.962466053827939 grad: -1.195660573454325
iteration: 50 loss: 6.534290883793884 grad: -1.0538445759197628
iteration: 60 loss: 5.536723120569894 grad: -0.9354623574136641
iteration: 70 loss: 4.80128706672167 grad: -0.8382769558582578
iteration: 80 loss: 4.237075949234333 grad: -0.7580931042294329
iteration: 90 loss: 3.790765256919134 grad: -0.6912311131420611
iteration: 100 loss: 3.4290369157611167 grad: -0.6348246986349464
iteration: 110 loss: 3.1300154747654316 grad: -0.5867022714742436
iteration: 120 loss: 2.8787520905787627 grad: -0.5452210224922429
iteration: 130 loss: 2.6646885502055655 grad: -0.509128523109067
iteration: 140 loss: 2.480158195068849 grad: -0.4774593742035147
iteration: 150 loss: 2.319461066258472 grad: -0.4494604779686764
iteration: 160 loss: 2.178272268813709 grad: -0.42453733754577033
iteration: 170 loss: 2.053251543216034 grad: -0.4022152999090219
iteration: 180 loss: 1.9417785632275064 grad: -0.3821113356190482
iteration: 190 loss: 1.8417691728650425 grad: -0.3639132699393558
iteration: 200 loss: 1.7515451137562443 grad: -0.34736432423780406
iteration: 210 loss: 1.669739933800553 grad: -0.3322514794875965
iteration: 220 loss: 1.5952298821259154 grad: -0.31839662052095086
iteration: 230 loss: 1.5270823838290695 grad: -0.3056497256846029
iteration: 240 loss: 1.4645170934672107 grad: -0.2938835773072169
iteration: 250 loss: 1.4068760876218882 grad: -0.2829896147739608
iteration: 260 loss: 1.3536007906885639 grad: -0.27287465465737676
iteration: 0 loss: 76.29298356360874 grad: 142.24468617918868
iteration: 10 loss: 22.292231916531797 grad: -0.44088260003505875
iteration: 20 loss: 13.9763928903409 grad: -1.4643496231839126
iteration: 30 loss: 10.148610643037479 grad: -1.3893203412642494
iteration: 40 loss: 7.947639185241811 grad: -1.2324909785597185
iteration: 50 loss: 6.521555975405715 grad: -1.0882359233717578
iteration: 60 loss: 5.524334922266007 grad: -0.9674098451485357
iteration: 70 loss: 4.7888265362522695 grad: -0.8677194921192268
iteration: 80 loss: 4.224507113344109 grad: -0.7851386516696871
iteration: 90 loss: 3.7781532614376787 grad: -0.7160809988172562
iteration: 100 loss: 3.416461058320196 grad: -0.6577080602086787
iteration: 110 loss: 3.1175440659893123 grad: -0.607842157386377
iteration: 120 loss: 2.8664374209569443 grad: -0.5648203788720059
iteration: 130 loss: 2.6525681103498755 grad: -0.5273661900417612
iteration: 140 loss: 2.46825744660344 grad: -0.49449057964649806
iteration: 150 loss: 2.307796213580869 grad: -0.4654190970723755
iteration: 160 loss: 2.166852591148985 grad: -0.43953864610228016
iteration: 170 loss: 2.0420812291522865 grad: -0.4163586681755925
iteration: 180 loss: 1.9308581094287547 grad: -0.3954826486870625
iteration: 190 loss: 1.8310964338269182 grad: -0.3765870163621664
iteration: 200 loss: 1.7411160831562815 grad: -0.3594053618931422
iteration: 210 loss: 1.6595493233715966 grad: -0.34371651338956244
iteration: 220 loss: 1.585271549203712 grad: -0.32933543405391114
iteration: 230 loss: 1.5173496462722473 grad: -0.31610620529051486
iteration: 240 loss: 1.4550029608467825 grad: -0.30389656610787896
iteration: 250 loss: 1.3975734301577012 grad: -0.2925936252841326
iteration: 260 loss: 1.34450246185336 grad: -0.2821004656510715
iteration: 0 loss: 72.85487934503972 grad: 108.60212300378166
iteration: 10 loss: 21.404810367725055 grad: 1.3138014275341088
iteration: 20 loss: 13.542024001796149 grad: -1.3147731999465804
iteration: 30 loss: 9.871568333694416 grad: -1.3522741528682893
iteration: 40 loss: 7.7461383311774625 grad: -1.2141547003343374
iteration: 50 loss: 6.3638491270624975 grad: -1.0757128954159754
iteration: 60 loss: 5.39508909371745 grad: -0.9577279604955479
iteration: 70 loss: 4.679512713213336 grad: -0.8598425158857307
iteration: 80 loss: 4.129903936200952 grad: -0.7785577373965564
iteration: 90 loss: 3.6948374629567464 grad: -0.710484011697777
iteration: 100 loss: 3.3420698081705185 grad: -0.6528803875810508
iteration: 110 loss: 3.0503798048044617 grad: -0.6036278424096592
iteration: 120 loss: 2.805240701213775 grad: -0.5611026167297537
iteration: 130 loss: 2.5963795260764857 grad: -0.5240557322402948
iteration: 140 loss: 2.4163298271634464 grad: -0.49151815900532014
iteration: 150 loss: 2.259536648363461 grad: -0.4627302264957464
iteration: 160 loss: 2.1217829198535583 grad: -0.43708994292165254
iteration: 170 loss: 1.9998103771344673 grad: -0.4141152054457032
iteration: 180 loss: 1.8910622572202143 grad: -0.3934160170623289
iteration: 190 loss: 1.7935045075263671 grad: -0.3746738870191332
iteration: 200 loss: 1.7054989438643546 grad: -0.35762640805906204
iteration: 210 loss: 1.6257115817188192 grad: -0.342055591872857
iteration: 220 loss: 1.5530452773802856 grad: -0.32777895752523367
iteration: 230 loss: 1.4865894844162375 grad: -0.31464265606290664
iteration: 240 loss: 1.4255822634410669 grad: -0.30251611598239325
iteration: 250 loss: 1.3693811986611102 grad: -0.29128783569100003
iteration: 260 loss: 1.3174408790965488 grad: -0.2808620491509939
iteration: 0 loss: 73.87160730613898 grad: 149.4126391685436
iteration: 10 loss: 21.427187652202374 grad: -1.3521241652525062
iteration: 20 loss: 13.530552087264512 grad: -1.8382033364524684
iteration: 30 loss: 9.851511947711721 grad: -1.5989133388195436
iteration: 40 loss: 7.723518315749391 grad: -1.3757913326221052
iteration: 50 loss: 6.340518424876749 grad: -1.1977521323472524
iteration: 60 loss: 5.371890010762979 grad: -1.0563589042419768
iteration: 70 loss: 4.656868535202772 grad: -0.9425807389150946
iteration: 80 loss: 4.108019134754531 grad: -0.8496359633211961
iteration: 90 loss: 3.6737971053139518 grad: -0.7726054006095426
iteration: 100 loss: 3.3218938164968677 grad: -0.7079078210961985
iteration: 110 loss: 3.031053108660878 grad: -0.6529082463902178
iteration: 120 loss: 2.786730132175101 grad: -0.605642743182141
iteration: 130 loss: 2.5786433395528134 grad: -0.5646271980072267
iteration: 140 loss: 2.3993230727699237 grad: -0.5287244464542037
iteration: 150 loss: 2.243214171881515 grad: -0.4970514045453484
iteration: 160 loss: 2.1061009987313146 grad: -0.46891353703627847
iteration: 170 loss: 1.9847275617501674 grad: -0.4437580779447067
iteration: 180 loss: 1.8765397497596412 grad: -0.421140218450627
iteration: 190 loss: 1.779506273464945 grad: -0.4006983562326533
iteration: 200 loss: 1.6919916717544188 grad: -0.3821357510901935
iteration: 210 loss: 1.612664560372154 grad: -0.36520676479154546
iteration: 220 loss: 1.5404302317336838 grad: -0.34970642110901434
iteration: 230 loss: 1.474380394620973 grad: -0.33546239898878183
iteration: 240 loss: 1.413755181540182 grad: -0.3223288290827391
iteration: 250 loss: 1.357914071016818 grad: -0.3101814413777498
iteration: 260 loss: 1.3063133789374577 grad: -0.2989137355022199
iteration: 0 loss: 73.3064391691967 grad: 99.73274913968856
iteration: 10 loss: 21.827931030885793 grad: 2.7569626535913874
iteration: 20 loss: 13.750511575165092 grad: -0.8766073509915532
iteration: 30 loss: 10.013052847108177 grad: -1.1767887509695902
iteration: 40 loss: 7.854001195547013 grad: -1.1262859063324724
iteration: 50 loss: 6.4511674223153515 grad: -1.0243136312536056
iteration: 60 loss: 5.46844596871448 grad: -0.9240590181157231
iteration: 70 loss: 4.742733427660204 grad: -0.8358551163715539
iteration: 80 loss: 4.185423977612242 grad: -0.7603597317974233
iteration: 90 loss: 3.7443082887160184 grad: -0.6960086492191815
iteration: 100 loss: 3.386664276605312 grad: -0.6409433630621152
iteration: 110 loss: 3.0909611573253475 grad: -0.5935064063311822
iteration: 120 loss: 2.842462718273344 grad: -0.5523318686761212
iteration: 130 loss: 2.6307491285060265 grad: -0.5163230741352515
iteration: 140 loss: 2.4482478991556658 grad: -0.48460533342851764
iteration: 150 loss: 2.289325601283343 grad: -0.4564798528391548
iteration: 160 loss: 2.1497058549043273 grad: -0.431385340233903
iteration: 170 loss: 2.0260847438516563 grad: -0.40886775675331544
iteration: 180 loss: 1.915869801325326 grad: -0.38855698862209676
iteration: 190 loss: 1.8169986546190557 grad: -0.370148973705775
iteration: 200 loss: 1.727810372367623 grad: -0.35339200008252913
iteration: 210 loss: 1.646952492372665 grad: -0.3380761592196063
iteration: 220 loss: 1.573312708250259 grad: -0.3240251786385491
iteration: 230 loss: 1.5059679161348536 grad: -0.31109005342603424
iteration: 240 loss: 1.4441456892553683 grad: -0.2991440441068427
iteration: 250 loss: 1.3871947858033722 grad: -0.2880787187115553
iteration: 260 loss: 1.3345623144551941 grad: -0.2778007982492342
iteration: 0 loss: 75.97550940586058 grad: 136.3197421258446
iteration: 10 loss: 22.292047030322053 grad: -0.2930497861258035
iteration: 20 loss: 13.913648173852339 grad: -1.4713977107376324
iteration: 30 loss: 10.083562223470175 grad: -1.4073652891909947
iteration: 40 loss: 7.888073555454887 grad: -1.2457777217862658
iteration: 50 loss: 6.468222223428628 grad: -1.0963484052296277
iteration: 60 loss: 5.476642117092069 grad: -0.9717155702913777
iteration: 70 loss: 4.745975334919463 grad: -0.8694185009464368
iteration: 80 loss: 4.1857580550765965 grad: -0.7850793298476626
iteration: 90 loss: 3.742881300282699 grad: -0.7148365853980964
iteration: 100 loss: 3.3841523618869114 grad: -0.6556639956755481
iteration: 110 loss: 3.0877789818552874 grad: -0.6052595895501099
iteration: 120 loss: 2.838872504262078 grad: -0.5618782612544728
iteration: 130 loss: 2.6269206625057366 grad: -0.5241885985947751
iteration: 140 loss: 2.4442930733369974 grad: -0.49116446563812144
iteration: 150 loss: 2.2853190580905323 grad: -0.4620059430671809
iteration: 160 loss: 2.145697726244841 grad: -0.4360822228157927
iteration: 170 loss: 2.022108729886358 grad: -0.41289027154935176
iteration: 180 loss: 1.9119484158770568 grad: -0.3920246866512635
iteration: 190 loss: 1.8131467010594364 grad: -0.37315549913715984
iteration: 200 loss: 1.7240372880122445 grad: -0.35601165328915574
iteration: 210 loss: 1.6432639541571503 grad: -0.3403685770727408
iteration: 220 loss: 1.569711747134575 grad: -0.32603873009564954
iteration: 230 loss: 1.5024556994020382 grad: -0.312864341534789
iteration: 240 loss: 1.4407220750088279 grad: -0.30071177564717916
iteration: 250 loss: 1.3838587191558376 grad: -0.28946711927516255
iteration: 260 loss: 1.3313121124243397 grad: -0.2790326958712609
iteration: 0 loss: 76.16138323122682 grad: 97.7723955236536
iteration: 10 loss: 22.58353960182378 grad: 2.65184339625566
iteration: 20 loss: 14.064727294696013 grad: -0.5666998404627053
iteration: 30 loss: 10.19526523062326 grad: -0.9396189960212853
iteration: 40 loss: 7.979952083555665 grad: -0.9501334252037916
iteration: 50 loss: 6.547571141499578 grad: -0.88867416969218
iteration: 60 loss: 5.547025865356483 grad: -0.8159704074388556
iteration: 70 loss: 4.809463474287399 grad: -0.7472671093410652
iteration: 80 loss: 4.2437008570579815 grad: -0.6860798934443968
iteration: 90 loss: 3.7962312417434574 grad: -0.6325526263239065
iteration: 100 loss: 3.43361785364393 grad: -0.5858894745596087
iteration: 110 loss: 3.133906908039907 grad: -0.5451195421071289
iteration: 120 loss: 2.882096774183455 grad: -0.5093356228384212
iteration: 130 loss: 2.6675928571975502 grad: -0.4777566876014816
iteration: 140 loss: 2.4827028698944726 grad: -0.44973097854702176
iteration: 150 loss: 2.3217084280880624 grad: -0.4247208169018729
iteration: 160 loss: 2.180271140317514 grad: -0.4022837539131783
iteration: 170 loss: 2.055040691082716 grad: -0.38205502532454555
iteration: 180 loss: 1.943389144731224 grad: -0.3637326834207878
iteration: 190 loss: 1.8432265055633417 grad: -0.34706548415879357
iteration: 200 loss: 1.7528699757853743 grad: -0.33184318750876446
iteration: 210 loss: 1.6709495331010658 grad: -0.3178888417007935
iteration: 220 loss: 1.596338588491521 grad: -0.30505265229308726
iteration: 230 loss: 1.5281022885637867 grad: -0.29320710089982505
iteration: 240 loss: 1.4654584420639531 grad: -0.282243044440312
iteration: 250 loss: 1.407747617741858 grad: -0.27206658336229594
iteration: 260 loss: 1.3544099987076068 grad: -0.2625965341904011
iteration: 0 loss: 77.11710693635563 grad: 132.37077878711625
iteration: 10 loss: 21.922434358880597 grad: -0.6902163977640475
iteration: 20 loss: 13.796070844391641 grad: -1.8617445647782804
iteration: 30 loss: 10.038402819446736 grad: -1.6289101214135695
iteration: 40 loss: 7.870731688624925 grad: -1.3886231995495515
iteration: 50 loss: 6.46341297913035 grad: -1.1989965332434152
iteration: 60 loss: 5.478020550988773 grad: -1.051217172226183
iteration: 70 loss: 4.750566006796072 grad: -0.9342257246830059
iteration: 80 loss: 4.19204403789205 grad: -0.839823014002893
iteration: 90 loss: 3.7500419419175572 grad: -0.7622733008939382
iteration: 100 loss: 3.3917246016271334 grad: -0.697550609205883
iteration: 110 loss: 3.0954940471337564 grad: -0.6427803610769722
iteration: 120 loss: 2.8465719404387695 grad: -0.5958689443393248
iteration: 130 loss: 2.6345108582455725 grad: -0.5552615906789041
iteration: 140 loss: 2.451719566275394 grad: -0.5197825997478146
iteration: 150 loss: 2.29255149932541 grad: -0.48852810342712477
iteration: 160 loss: 2.1527208229628574 grad: -0.46079272584709075
iteration: 170 loss: 2.028916656793089 grad: -0.4360184782551888
iteration: 180 loss: 1.9185412986989832 grad: -0.4137585074696594
iteration: 190 loss: 1.8195283689794286 grad: -0.39365094868068806
iteration: 200 loss: 1.730213819982096 grad: -0.3753997725773462
iteration: 210 loss: 1.649242731015792 grad: -0.3587605541289565
iteration: 220 loss: 1.57550083103826 grad: -0.3435297580915867
iteration: 230 loss: 1.5080634277977134 grad: -0.3295365734999305
iteration: 240 loss: 1.4461567967293212 grad: -0.3166366203928534
iteration: 250 loss: 1.3891286254122612 grad: -0.304707048761279
iteration: 260 loss: 1.33642513152749 grad: -0.2936426847139525
iteration: 0 loss: 76.2537079297987 grad: 109.73067533918183
iteration: 10 loss: 21.945816070971908 grad: 1.1422091023765286
iteration: 20 loss: 13.796405572323465 grad: -1.668195311413879
iteration: 30 loss: 10.036968240368857 grad: -1.599696711525544
iteration: 40 loss: 7.8691598971406 grad: -1.3905213276442616
iteration: 50 loss: 6.46197267772062 grad: -1.2084351007540457
iteration: 60 loss: 5.47675904434366 grad: -1.0623100884378942
iteration: 70 loss: 4.749478840349804 grad: -0.9452282419662237
iteration: 80 loss: 4.191112898475777 grad: -0.8502013856713185
iteration: 90 loss: 3.7492460049812837 grad: -0.7718969706098899
iteration: 100 loss: 3.3910443465071527 grad: -0.706430680078137
iteration: 110 loss: 3.0949123930928923 grad: -0.6509756571380214
iteration: 120 loss: 2.8460743697074906 grad: -0.6034503252535119
iteration: 130 loss: 2.6340851796540763 grad: -0.5622983330268907
iteration: 140 loss: 2.4513555818558745 grad: -0.5263375941542707
iteration: 150 loss: 2.2922406752254574 grad: -0.4946566480502371
iteration: 160 loss: 2.1524559992178016 grad: -0.4665427563025987
iteration: 170 loss: 2.0286918034020016 grad: -0.4414313156391371
iteration: 180 loss: 1.9183513151622884 grad: -0.4188697334321676
iteration: 190 loss: 1.8193689212164992 grad: -0.3984912470690012
iteration: 200 loss: 1.7300812081310306 grad: -0.37999567919151944
iteration: 210 loss: 1.6491337822696799 grad: -0.363135100804437
iteration: 220 loss: 1.5754128125847935 grad: -0.34770301596501973
iteration: 230 loss: 1.5079939758603276 grad: -0.33352610711896646
iteration: 240 loss: 1.4461038584931885 grad: -0.32045786585296915
iteration: 250 loss: 1.3890904112888414 grad: -0.3083736283476699
iteration: 260 loss: 1.336400075733929 grad: -0.29716666901140754
iteration: 0 loss: 78.1482259898581 grad: 125.15210481879642
iteration: 10 loss: 21.90354405338061 grad: -0.9957887272959475
iteration: 20 loss: 13.755949620661237 grad: -1.983288323168483
iteration: 30 loss: 9.996875636029348 grad: -1.6943791377083781
iteration: 40 loss: 7.831786313574454 grad: -1.4340434409022877
iteration: 50 loss: 6.427659277876211 grad: -1.2339546233387289
iteration: 60 loss: 5.4453296918993805 grad: -1.0793813869222653
iteration: 70 loss: 4.720631506466237 grad: -0.9574956333792655
iteration: 80 loss: 4.164537295508828 grad: -0.8593957665902462
iteration: 90 loss: 3.724660018655992 grad: -0.7789792234014306
iteration: 100 loss: 3.3682026135139824 grad: -0.711991777683199
iteration: 110 loss: 3.0736051602085683 grad: -0.6554043274078438
iteration: 120 loss: 2.826123315337637 grad: -0.6070144148181804
iteration: 130 loss: 2.6153386951779214 grad: -0.5651888413759187
iteration: 140 loss: 2.433684401672136 grad: -0.5286944386455399
iteration: 150 loss: 2.2755341147027055 grad: -0.49658461367362594
iteration: 160 loss: 2.136618818202516 grad: -0.4681217957726037
iteration: 170 loss: 2.0136416285534904 grad: -0.4427234253821278
iteration: 180 loss: 1.9040165332663823 grad: -0.4199236699219493
iteration: 190 loss: 1.805686958801578 grad: -0.3993458367514193
iteration: 200 loss: 1.7169971239347268 grad: -0.38068218786497654
iteration: 210 loss: 1.6365991117594687 grad: -0.36367895956782514
iteration: 220 loss: 1.5633846153749469 grad: -0.34812509818143617
iteration: 230 loss: 1.496434046700787 grad: -0.33384368652566065
iteration: 240 loss: 1.4349780705354698 grad: -0.32068534463090903
iteration: 250 loss: 1.3783681667391647 grad: -0.308523096836979
iteration: 260 loss: 1.3260538440660474 grad: -0.29724834060064875
iteration: 0 loss: 73.61782563296096 grad: 118.11318168088711
iteration: 10 loss: 21.610168455565475 grad: 1.620763791017912
iteration: 20 loss: 13.562986639133028 grad: -1.0496621049151025
iteration: 30 loss: 9.86083342831919 grad: -1.247815350841532
iteration: 40 loss: 7.729991141914112 grad: -1.174297204421335
iteration: 50 loss: 6.348167795492538 grad: -1.0637322335287678
iteration: 60 loss: 5.381159199141538 grad: -0.9588264309079517
iteration: 70 loss: 4.667436998000787 grad: -0.8674243176900014
iteration: 80 loss: 4.119485103954751 grad: -0.7894254836910105
iteration: 90 loss: 3.6858275952499753 grad: -0.7229841551085003
iteration: 100 loss: 3.33424038229346 grad: -0.6661159471667693
iteration: 110 loss: 3.0435374506207817 grad: -0.6170960740069033
iteration: 120 loss: 2.799226545704931 grad: -0.5745165767670911
iteration: 130 loss: 2.591064198546891 grad: -0.537251299857926
iteration: 140 loss: 2.411607992704086 grad: -0.5044032578740323
iteration: 150 loss: 2.25532220520964 grad: -0.4752559899709092
iteration: 160 loss: 2.118005080219146 grad: -0.4492338026461028
iteration: 170 loss: 1.996410561380344 grad: -0.42587070650535336
iteration: 180 loss: 1.8879916594365072 grad: -0.40478655138864106
iteration: 190 loss: 1.7907221917380411 grad: -0.38566877245606895
iteration: 200 loss: 1.7029703519849797 grad: -0.36825840475713584
iteration: 210 loss: 1.6234073589472577 grad: -0.3523393169611645
iteration: 220 loss: 1.5509403422278494 grad: -0.3377298705152072
iteration: 230 loss: 1.4846622877840177 grad: -0.32427641180294114
iteration: 240 loss: 1.423814194472429 grad: -0.3118481568388952
iteration: 250 loss: 1.3677561051924634 grad: -0.3003331406337433
iteration: 260 loss: 1.3159446781237008 grad: -0.28963498621837347
iteration: 0 loss: 75.5385709314079 grad: 94.42468442032494
iteration: 10 loss: 22.50477306088756 grad: 2.213124571789993
iteration: 20 loss: 14.006013830516064 grad: -0.7709073928647321
iteration: 30 loss: 10.141717441270078 grad: -1.0130982998697664
iteration: 40 loss: 7.932162288897809 grad: -0.9732752915194902
iteration: 50 loss: 6.505064289392811 grad: -0.890186380908318
iteration: 60 loss: 5.509037461204488 grad: -0.8073645444486053
iteration: 70 loss: 4.775270869315426 grad: -0.7337414312315063
iteration: 80 loss: 4.212696495654572 grad: -0.6701935172969797
iteration: 90 loss: 3.76792145961103 grad: -0.6156422398611878
iteration: 100 loss: 3.4076044249310473 grad: -0.56868037256079
iteration: 110 loss: 3.109867625416965 grad: -0.5280130925301542
iteration: 120 loss: 2.8597690773859044 grad: -0.49255394464827684
iteration: 130 loss: 2.646760938937653 grad: -0.46141944852567085
iteration: 140 loss: 2.4631877524006582 grad: -0.4338979372230272
iteration: 150 loss: 2.3033603673530623 grad: -0.40941621329119904
iteration: 160 loss: 2.162963797888055 grad: -0.3875106840844541
iteration: 170 loss: 2.0386666346248603 grad: -0.3678040995095717
iteration: 180 loss: 1.9278563471846362 grad: -0.3499873268320533
iteration: 190 loss: 1.8284555980475985 grad: -0.33380521850483214
iteration: 200 loss: 1.7387920732924365 grad: -0.3190456735078408
iteration: 210 loss: 1.6575044988808258 grad: -0.30553114887690447
iteration: 220 loss: 1.583473637225366 grad: -0.2931120406168434
iteration: 230 loss: 1.5157708533551242 grad: -0.2816614911157903
iteration: 240 loss: 1.453619248568099 grad: -0.2710712886971772
iteration: 250 loss: 1.3963639222251913 grad: -0.2612486075203258
iteration: 260 loss: 1.343448956833196 grad: -0.25211339793808724
iteration: 0 loss: 75.37218031879763 grad: 124.98287186045654
iteration: 10 loss: 22.099494434744944 grad: 0.9232357755674837
iteration: 20 loss: 13.815205355772854 grad: -1.2566794959028755
iteration: 30 loss: 10.034068930728898 grad: -1.3531080805528244
iteration: 40 loss: 7.861850610442781 grad: -1.2354116645725863
iteration: 50 loss: 6.454105605716863 grad: -1.1010739169078132
iteration: 60 loss: 5.469326377820463 grad: -0.9821249613129457
iteration: 70 loss: 4.742703973010314 grad: -0.8819479739560339
iteration: 80 loss: 4.184993660352905 grad: -0.798244339833283
iteration: 90 loss: 3.7437172626452426 grad: -0.7279796656235359
iteration: 100 loss: 3.3860298242918607 grad: -0.668486880598442
iteration: 110 loss: 3.090340958827214 grad: -0.6176310790754627
iteration: 120 loss: 2.8418843041165265 grad: -0.5737487308873492
iteration: 130 loss: 2.630224366760227 grad: -0.5355493654948769
iteration: 140 loss: 2.4477804294352112 grad: -0.502027446842859
iteration: 150 loss: 2.2889147725967716 grad: -0.47239307445937395
iteration: 160 loss: 2.1493488583715505 grad: -0.44601981524066947
iteration: 170 loss: 2.0257777823924035 grad: -0.42240596647836304
iteration: 180 loss: 1.9156087321987136 grad: -0.40114582351355377
iteration: 190 loss: 1.8167793378006123 grad: -0.3819082664692296
iteration: 200 loss: 1.7276288526778776 grad: -0.36442068101804204
iteration: 210 loss: 1.6468050871105582 grad: -0.34845677802877945
iteration: 220 loss: 1.5731960412859651 grad: -0.3338272811520323
iteration: 230 loss: 1.5058789221175588 grad: -0.32037274090481876
iteration: 240 loss: 1.444081601717616 grad: -0.30795793930356496
iteration: 250 loss: 1.3871531171921148 grad: -0.2964674948703986
iteration: 260 loss: 1.3345408327485198 grad: -0.28580238164872424
iteration: 0 loss: 73.5933877675314 grad: 129.57114675806105
iteration: 10 loss: 21.066022743292397 grad: -0.17243994042756797
iteration: 20 loss: 13.347455105387429 grad: -1.7101761336448946
iteration: 30 loss: 9.739731678323365 grad: -1.5887939622660978
iteration: 40 loss: 7.647993391108023 grad: -1.3839043530422972
iteration: 50 loss: 6.286306045640893 grad: -1.2066838315003516
iteration: 60 loss: 5.331291783168126 grad: -1.0632317598386447
iteration: 70 loss: 4.625475987351117 grad: -0.9474423389905244
iteration: 80 loss: 4.083126298279115 grad: -0.8529741821540843
iteration: 90 loss: 3.6536539620010444 grad: -0.7748495037141008
iteration: 100 loss: 3.305320270529798 grad: -0.7093690919702957
iteration: 110 loss: 3.017225532931744 grad: -0.6538031064182461
iteration: 120 loss: 2.775056893692884 grad: -0.6061215421414177
iteration: 130 loss: 2.568688932162662 grad: -0.5647957417303255
iteration: 140 loss: 2.3907601088909276 grad: -0.5286584806315304
iteration: 150 loss: 2.235792008636715 grad: -0.4968061242710414
iteration: 160 loss: 2.099624653735687 grad: -0.46852984609643755
iteration: 170 loss: 1.9790431930480488 grad: -0.44326676167862017
iteration: 180 loss: 1.871524438187862 grad: -0.42056477212930166
iteration: 190 loss: 1.775060693396509 grad: -0.40005693612654275
iteration: 200 loss: 1.6880347427670792 grad: -0.38144254385268206
iteration: 210 loss: 1.6091294837376815 grad: -0.36447296455262046
iteration: 220 loss: 1.5372615118552113 grad: -0.34894093749665667
iteration: 230 loss: 1.4715315720529196 grad: -0.3346723775669813
iteration: 240 loss: 1.4111870874753136 grad: -0.3215200389892284
iteration: 250 loss: 1.3555934689206879 grad: -0.30935856758095176
iteration: 260 loss: 1.3042118970699283 grad: -0.29808060162702105
iteration: 0 loss: 73.41533632217563 grad: 38.91818778667215
iteration: 10 loss: 41.40389799815537 grad: 5.601878539381452
iteration: 20 loss: 31.542874725217395 grad: -0.8941742147785877
iteration: 30 loss: 25.786714230840122 grad: -1.8120417123913686
iteration: 40 loss: 21.87739760311189 grad: -1.8531919106593588
iteration: 50 loss: 19.01666704305256 grad: -1.738816486911552
iteration: 60 loss: 16.82122278316768 grad: -1.6048256676107067
iteration: 70 loss: 15.078634316416894 grad: -1.4800119300589398
iteration: 80 loss: 13.659985460746777 grad: -1.369572004666929
iteration: 90 loss: 12.481850965834466 grad: -1.2729932910409176
iteration: 100 loss: 11.487569453548902 grad: -1.18849577321283
iteration: 110 loss: 10.637170021059774 grad: -1.1142217425752416
iteration: 120 loss: 9.901578710959154 grad: -1.0485395118203154
iteration: 130 loss: 9.259105543162997 grad: -0.990090163021484
iteration: 140 loss: 8.693220738568987 grad: -0.9377616559548794
iteration: 150 loss: 8.191095712568808 grad: -0.890647276578365
iteration: 160 loss: 7.742616363250446 grad: -0.8480058410005327
iteration: 170 loss: 7.339698168117841 grad: -0.8092281844356628
iteration: 180 loss: 6.975799903848796 grad: -0.7738102747208664
iteration: 190 loss: 6.645571476433898 grad: -0.7413320583784544
iteration: 200 loss: 6.344594367667758 grad: -0.7114409272561029
iteration: 210 loss: 6.069187333511061 grad: -0.6838387982233168
iteration: 220 loss: 5.816258902254834 grad: -0.6582719821709163
iteration: 230 loss: 5.583193980091437 grad: -0.6345231977544834
iteration: 240 loss: 5.367765675764076 grad: -0.6124052355977014
iteration: 250 loss: 5.168066018275278 grad: -0.5917558971355528
iteration: 260 loss: 4.982450998516635 grad: -0.5724339229821558
iteration: 270 loss: 4.809496590046282 grad: -0.5543156942028571
iteration: 280 loss: 4.6479632702192 grad: -0.5372925413001639
iteration: 290 loss: 4.496767183756261 grad: -0.5212685343034733
iteration: 300 loss: 4.354956541616006 grad: -0.506158656350338
iteration: 310 loss: 4.221692179111539 grad: -0.49188728502483303
iteration: 320 loss: 4.096231442978065 grad: -0.4783869223061306
iteration: 330 loss: 3.977914761371515 grad: -0.4655971266280924
iteration: 340 loss: 3.8661543902242723 grad: -0.453463610253002
iteration: 350 loss: 3.76042493582153 grad: -0.44193747265234906
iteration: 360 loss: 3.6602553353656564 grad: -0.4309745464063198
iteration: 370 loss: 3.5652220407969253 grad: -0.42053483668295266
iteration: 380 loss: 3.474943200735528 grad: -0.4105820389370257
iteration: 390 loss: 3.389073674395439 grad: -0.40108312230186416
iteration: 400 loss: 3.3073007421547285 grad: -0.39200796840317886
iteration: 410 loss: 3.229340402040386 grad: -0.3833290571307733
iteration: 420 loss: 3.1549341610141464 grad: -0.3750211923589026
iteration: 430 loss: 3.0838462457812637 grad: -0.3670612617840169
iteration: 440 loss: 3.0158611706274727 grad: -0.3594280260071585
iteration: 450 loss: 2.950781610196414 grad: -0.3521019327721227
iteration: 460 loss: 2.8884265336156534 grad: -0.34506495291446493
iteration: 470 loss: 2.828629563337803 grad: -0.3383004351078649
iteration: 480 loss: 2.771237527830387 grad: -0.33179297693483323
iteration: 490 loss: 2.7161091819567775 grad: -0.32552831017527073
iteration: 0 loss: 76.1116908127239 grad: 31.762932148167124
iteration: 10 loss: 44.74545402492556 grad: 6.64733966647337
iteration: 20 loss: 33.85083209199173 grad: 0.26252883290391377
iteration: 30 loss: 27.56808811839933 grad: -1.065974426039388
iteration: 40 loss: 23.324316976793895 grad: -1.3385840399614684
iteration: 50 loss: 20.22838457556211 grad: -1.3531449210793633
iteration: 60 loss: 17.858274071573863 grad: -1.2987906221669614
iteration: 70 loss: 15.98130731723679 grad: -1.2279086089330145
iteration: 80 loss: 14.456562925064913 grad: -1.1564733820587387
iteration: 90 loss: 13.192929267967616 grad: -1.089401996114506
iteration: 100 loss: 12.128564905404636 grad: -1.0279441087298862
iteration: 110 loss: 11.219877186223771 grad: -0.9720858635766185
iteration: 120 loss: 10.435187758979366 grad: -0.9213956417243989
iteration: 130 loss: 9.750890945016202 grad: -0.8753355735410421
iteration: 140 loss: 9.149020834482455 grad: -0.8333766580450076
iteration: 150 loss: 8.615654430460403 grad: -0.7950381839061957
iteration: 160 loss: 8.139831488995572 grad: -0.7598973492442864
iteration: 170 loss: 7.712804784596941 grad: -0.7275873765409648
iteration: 180 loss: 7.327507957411564 grad: -0.6977916372660042
iteration: 190 loss: 6.978170323689152 grad: -0.6702368869766587
iteration: 200 loss: 6.660033190779607 grad: -0.6446868403176882
iteration: 210 loss: 6.369137678319269 grad: -0.6209365083624082
iteration: 220 loss: 6.102163808684054 grad: -0.5988073768895824
iteration: 230 loss: 5.856306943325721 grad: -0.5781433667452455
iteration: 240 loss: 5.629181814266965 grad: -0.558807472089297
iteration: 250 loss: 5.418747211994287 grad: -0.5406789667610404
iteration: 260 loss: 5.2232463197004275 grad: -0.5236510790922002
iteration: 270 loss: 5.041159028130633 grad: -0.5076290503241566
iteration: 280 loss: 4.87116351608223 grad: -0.4925285066628694
iteration: 290 loss: 4.7121050631896075 grad: -0.4782740881631802
iteration: 300 loss: 4.562970556289408 grad: -0.4647982886224611
iteration: 310 loss: 4.422867513795278 grad: -0.4520404695790598
iteration: 320 loss: 4.2910067219101045 grad: -0.43994601864260935
iteration: 330 loss: 4.166687778365072 grad: -0.4284656280539196
iteration: 340 loss: 4.049286991975061 grad: -0.41755467387629924
iteration: 350 loss: 3.9382472027186437 grad: -0.4071726798043822
iteration: 360 loss: 3.8330691764994795 grad: -0.3972828524390379
iteration: 370 loss: 3.7333042980930755 grad: -0.38785167717355906
iteration: 380 loss: 3.638548339825692 grad: -0.37884856568824543
iteration: 390 loss: 3.548436126016151 grad: -0.37024554755170214
iteration: 400 loss: 3.4626369467676095 grad: -0.36201699965039646
iteration: 410 loss: 3.38085060138763 grad: -0.3541394081698268
iteration: 420 loss: 3.3028039730558123 grad: -0.34659115867509005
iteration: 430 loss: 3.2282480535166296 grad: -0.33935235052027213
iteration: 440 loss: 3.156955350443282 grad: -0.3324046323821188
iteration: 450 loss: 3.088717621379973 grad: -0.32573105618541787
iteration: 460 loss: 3.0233438873583096 grad: -0.3193159470826212
iteration: 470 loss: 2.9606586868158873 grad: -0.31314478748219887
iteration: 480 loss: 2.900500536647583 grad: -0.30720411340010234
iteration: 490 loss: 2.842720572335483 grad: -0.3014814216454774
iteration: 0 loss: 76.42937381390055 grad: 34.00257829046515
iteration: 10 loss: 45.5826295918197 grad: 6.635730217355299
iteration: 20 loss: 34.395902917275635 grad: -0.059911130459446016
iteration: 30 loss: 27.90753486364809 grad: -1.2740333166019664
iteration: 40 loss: 23.540501485877154 grad: -1.4835789862445017
iteration: 50 loss: 20.369051826205308 grad: -1.4691383993019376
iteration: 60 loss: 17.950839509144338 grad: -1.3988293480601586
iteration: 70 loss: 16.042170455587833 grad: -1.3169280856154368
iteration: 80 loss: 14.495952629006029 grad: -1.2366426871961373
iteration: 90 loss: 13.217458291300504 grad: -1.1619538603628392
iteration: 100 loss: 12.142636980643623 grad: -1.0937736225654437
iteration: 110 loss: 11.226497517522809 grad: -1.0319412837248017
iteration: 120 loss: 10.436449069933463 grad: -0.9759374774028149
iteration: 130 loss: 9.748275367527329 grad: -0.9251548770802194
iteration: 140 loss: 9.143593135189432 grad: -0.8790018368516513
iteration: 150 loss: 8.608188882998796 grad: -0.836939328833314
iteration: 160 loss: 8.130897000973288 grad: -0.7984905092860084
iteration: 170 loss: 7.70282308176938 grad: -0.763239191169435
iteration: 180 loss: 7.3167939960163 grad: -0.7308241896206908
iteration: 190 loss: 6.966960759679957 grad: -0.7009325495547731
iteration: 200 loss: 6.648506692079743 grad: -0.6732929224192271
iteration: 210 loss: 6.357429585668094 grad: -0.6476695779442692
iteration: 220 loss: 6.0903768302397445 grad: -0.6238571872900667
iteration: 230 loss: 5.844519032705111 grad: -0.6016763629147959
iteration: 240 loss: 5.617452026548066 grad: -0.5809698812864955
iteration: 250 loss: 5.4071200932842 grad: -0.5615994974792168
iteration: 260 loss: 5.2117552230790185 grad: -0.5434432621653782
iteration: 270 loss: 5.02982863653944 grad: -0.5263932603719598
iteration: 280 loss: 4.860011774517344 grad: -0.5103537023971032
iteration: 290 loss: 4.701144667463847 grad: -0.4952393081593218
iteration: 300 loss: 4.552210106474087 grad: -0.48097393603759986
iteration: 310 loss: 4.4123124123497375 grad: -0.4674894156781209
iteration: 320 loss: 4.280659876167936 grad: -0.4547245513092874
iteration: 330 loss: 4.156550152196567 grad: -0.4426242679582184
iteration: 340 loss: 4.039358040533568 grad: -0.43113887777231563
iteration: 350 loss: 3.9285252160784694 grad: -0.42022344758982627
iteration: 360 loss: 3.8235515519590577 grad: -0.4098372521271348
iteration: 370 loss: 3.7239877563711574 grad: -0.3999432997889748
iteration: 380 loss: 3.6294290969511334 grad: -0.3905079202700454
iteration: 390 loss: 3.5395100300956157 grad: -0.3815004048921101
iteration: 400 loss: 3.4538995868196336 grad: -0.37289269208225706
iteration: 410 loss: 3.372297393888485 grad: -0.3646590916043624
iteration: 420 loss: 3.2944302306554247 grad: -0.3567760421543248
iteration: 430 loss: 3.220049039459189 grad: -0.3492218977585652
iteration: 440 loss: 3.148926321497046 grad: -0.34197673910547144
iteration: 450 loss: 3.0808538615220558 grad: -0.3350222065157983
iteration: 460 loss: 3.01564073400608 grad: -0.328341351740762
iteration: 470 loss: 2.953111551051279 grad: -0.3219185061820185
iteration: 480 loss: 2.893104918588175 grad: -0.31573916346924563
iteration: 490 loss: 2.83547207259558 grad: -0.30978987461955815
iteration: 0 loss: 75.38702047793106 grad: 38.78807616378292
iteration: 10 loss: 43.89974282187182 grad: 5.319247721364325
iteration: 20 loss: 33.415353918919685 grad: -0.5234251832583826
iteration: 30 loss: 27.2646951820897 grad: -1.399065348409359
iteration: 40 loss: 23.08171827034322 grad: -1.5006843705804058
iteration: 50 loss: 20.020184718017017 grad: -1.4478376160193118
iteration: 60 loss: 17.672586004173333 grad: -1.3630835568842192
iteration: 70 loss: 15.812015345857885 grad: -1.2762809815765703
iteration: 80 loss: 14.300143656766254 grad: -1.1952837627077828
iteration: 90 loss: 13.04714052570057 grad: -1.1217975959446007
iteration: 100 loss: 11.991854297090475 grad: -1.0556708980717386
iteration: 110 loss: 11.091094348004782 grad: -0.9962158269089157
iteration: 120 loss: 10.313434563189235 grad: -0.9426450995267497
iteration: 130 loss: 9.63543853637293 grad: -0.8942164721365122
iteration: 140 loss: 9.039260905792514 grad: -0.8502741682114223
iteration: 150 loss: 8.511068934602463 grad: -0.8102533357532364
iteration: 160 loss: 8.039972742918446 grad: -0.7736719709468347
iteration: 170 loss: 7.617281699228318 grad: -0.7401195700985367
iteration: 180 loss: 7.235976047862798 grad: -0.7092459455565484
iteration: 190 loss: 6.890324164646741 grad: -0.6807513429020746
iteration: 200 loss: 6.575600529236619 grad: -0.6543780943461308
iteration: 210 loss: 6.287874720243218 grad: -0.6299037088746111
iteration: 220 loss: 6.023851370386307 grad: -0.6071351972114936
iteration: 230 loss: 5.780747261094058 grad: -0.5859044190045348
iteration: 240 loss: 5.556195868650362 grad: -0.566064261882367
iteration: 250 loss: 5.3481724630223235 grad: -0.5474854928408188
iteration: 260 loss: 5.154934775667036 grad: -0.5300541522960798
iteration: 270 loss: 4.974975588644528 grad: -0.5136693869292538
iteration: 280 loss: 4.806984542944974 grad: -0.49824163857754855
iteration: 290 loss: 4.649817142093195 grad: -0.4836911233133304
iteration: 300 loss: 4.502469419433717 grad: -0.4699465481875832
iteration: 310 loss: 4.364057098991949 grad: -0.45694402359826103
iteration: 320 loss: 4.233798348045019 grad: -0.4446261374839918
iteration: 330 loss: 4.110999420483082 grad: -0.43294116403449656
iteration: 340 loss: 3.9950426420270806 grad: -0.42184238474109476
iteration: 350 loss: 3.885376304214705 grad: -0.41128750368680916
iteration: 360 loss: 3.781506123163953 grad: -0.4012381422291374
iteration: 370 loss: 3.6829879881113565 grad: -0.3916594008388644
iteration: 380 loss: 3.5894217785291236 grad: -0.38251947796330465
iteration: 390 loss: 3.500446070895123 grad: -0.37378933748838744
iteration: 400 loss: 3.415733589569341 grad: -0.3654424177634694
iteration: 410 loss: 3.3349872827973077 grad: -0.35745437628965415
iteration: 420 loss: 3.257936926076995 grad: -0.34980286510686
iteration: 430 loss: 3.1843361721885644 grad: -0.3424673326861897
iteration: 440 loss: 3.113959980981194 grad: -0.3354288487734538
iteration: 450 loss: 3.046602373205174 grad: -0.3286699491616034
iteration: 460 loss: 2.982074461806943 grad: -0.3221744978140315
iteration: 470 loss: 2.9202027215987156 grad: -0.3159275641330045
iteration: 480 loss: 2.8608274643666762 grad: -0.3099153134805445
iteration: 490 loss: 2.8038014915744665 grad: -0.304124909323207
iteration: 0 loss: 76.80092546979368 grad: 29.900198738113957
iteration: 10 loss: 45.20371486442093 grad: 7.033741152817552
iteration: 20 loss: 34.04363619478301 grad: 0.7061638993044801
iteration: 30 loss: 27.636832565081736 grad: -0.8410820935899284
iteration: 40 loss: 23.327910457366276 grad: -1.2519191120154198
iteration: 50 loss: 20.195459960381786 grad: -1.3368176959819986
iteration: 60 loss: 17.804572725281954 grad: -1.3159405485410038
iteration: 70 loss: 15.916032616054652 grad: -1.2598766918506263
iteration: 80 loss: 14.385273193670132 grad: -1.1939331693948214
iteration: 90 loss: 13.119042312076937 grad: -1.1277328378025229
iteration: 100 loss: 12.054198966689372 grad: -1.064918916704079
iteration: 110 loss: 11.146342250509095 grad: -1.0067086341866527
iteration: 120 loss: 10.363281926790995 grad: -0.9533064183932612
iteration: 130 loss: 9.681083847788551 grad: -0.9044999725849191
iteration: 140 loss: 9.081569572023103 grad: -0.8599212315130389
iteration: 150 loss: 8.550678137035986 grad: -0.8191622196527235
iteration: 160 loss: 8.07735996170308 grad: -0.7818255997905681
iteration: 170 loss: 7.652810538168916 grad: -0.7475451177456934
iteration: 180 loss: 7.269927518706399 grad: -0.7159920782745491
iteration: 190 loss: 6.922918446782949 grad: -0.6868754511054072
iteration: 200 loss: 6.607012362945418 grad: -0.6599392442473668
iteration: 210 loss: 6.318244465175336 grad: -0.6349588904563368
iteration: 220 loss: 6.053293062337368 grad: -0.6117374713904091
iteration: 230 loss: 5.809354557764395 grad: -0.5901021498657639
iteration: 240 loss: 5.58404648957627 grad: -0.5699009565980774
iteration: 250 loss: 5.375331541288026 grad: -0.5509999689220957
iteration: 260 loss: 5.181457413719479 grad: -0.5332808682116111
iteration: 270 loss: 5.000908825546627 grad: -0.5166388412849036
iteration: 280 loss: 4.832368881932656 grad: -0.5009807842504138
iteration: 290 loss: 4.674687746520414 grad: -0.4862237674389611
iteration: 300 loss: 4.526857056408104 grad: -0.4722937233795129
iteration: 310 loss: 4.38798888945034 grad: -0.4591243242215513
iteration: 320 loss: 4.257298367182102 grad: -0.4466560196101458
iteration: 330 loss: 4.134089181643503 grad: -0.4348352103276245
iteration: 340 loss: 4.01774148920454 grad: -0.4236135368480116
iteration: 350 loss: 3.9077017324142287 grad: -0.41294726526406106
iteration: 360 loss: 3.803474041469475 grad: -0.4027967558641799
iteration: 370 loss: 3.704612936965532 grad: -0.39312600200800507
iteration: 380 loss: 3.6107171102200315 grad: -0.3839022289337129
iteration: 390 loss: 3.5214241003069136 grad: -0.3750955437850907
iteration: 400 loss: 3.4364057207788146 grad: -0.3666786295247725
iteration: 410 loss: 3.355364115944047 grad: -0.3586264765477591
iteration: 420 loss: 3.278028348043087 grad: -0.350916146765735
iteration: 430 loss: 3.2041514339285144 grad: -0.3435265657306127
iteration: 440 loss: 3.1335077637902278 grad: -0.33643833903259884
iteration: 450 loss: 3.0658908457792844 grad: -0.32963358976652546
iteration: 460 loss: 3.0011113296060845 grad: -0.32309581432884893
iteration: 470 loss: 2.9389952697437036 grad: -0.3168097542019698
iteration: 480 loss: 2.8793825950834533 grad: -0.31076128171495265
iteration: 490 loss: 2.8221257570215297 grad: -0.30493729805073455
iteration: 0 loss: 76.00381023673714 grad: 24.036208340998776
iteration: 10 loss: 43.59701796144152 grad: 7.1463690706065695
iteration: 20 loss: 32.81153042283775 grad: 1.2788110134048563
iteration: 30 loss: 26.71573293695294 grad: -0.46723262864555426
iteration: 40 loss: 22.623239332686765 grad: -1.0268806390020317
iteration: 50 loss: 19.641997839551696 grad: -1.195104985498849
iteration: 60 loss: 17.359521834666918 grad: -1.220651243476857
iteration: 70 loss: 15.550753586461889 grad: -1.191777325147535
iteration: 80 loss: 14.080061797018681 grad: -1.1427880083780348
iteration: 90 loss: 12.859984735454562 grad: -1.0878357696702334
iteration: 100 loss: 11.831242454313703 grad: -1.032915174155022
iteration: 110 loss: 10.952076066812838 grad: -0.9805168215109819
iteration: 120 loss: 10.19214478162121 grad: -0.9315662288387616
iteration: 130 loss: 9.528835251123816 grad: -0.8862778726948101
iteration: 140 loss: 8.944929209047007 grad: -0.8445474155106779
iteration: 150 loss: 8.427074926328611 grad: -0.806136918169331
iteration: 160 loss: 7.964754319215052 grad: -0.7707633470110692
iteration: 170 loss: 7.549566472228848 grad: -0.7381404635671351
iteration: 180 loss: 7.174719247778202 grad: -0.7079977635819318
iteration: 190 loss: 6.8346613172194255 grad: -0.6800880165474441
iteration: 200 loss: 6.524811119169541 grad: -0.6541891866127141
iteration: 210 loss: 6.241354075046399 grad: -0.6301036766134356
iteration: 220 loss: 5.981088737189979 grad: -0.6076564019096017
iteration: 230 loss: 5.7413085819882985 grad: -0.5866924602077004
iteration: 240 loss: 5.5197101463865526 grad: -0.567074776373498
iteration: 250 loss: 5.314320890150322 grad: -0.5486818980163859
iteration: 260 loss: 5.123442005989876 grad: -0.5314060117869001
iteration: 270 loss: 4.945602681331945 grad: -0.5151511965390214
iteration: 280 loss: 4.779523221746331 grad: -0.49983190356367446
iteration: 290 loss: 4.624085095577729 grad: -0.48537164291312884
iteration: 300 loss: 4.478306430745223 grad: -0.4717018512939707
iteration: 310 loss: 4.3413218407932535 grad: -0.4587609172504247
iteration: 320 loss: 4.212365714139733 grad: -0.44649334131980445
iteration: 330 loss: 4.090758292982917 grad: -0.43484901143140453
iteration: 340 loss: 3.975894013951223 grad: -0.4237825764914102
iteration: 350 loss: 3.86723169372102 grad: -0.4132529035929378
iteration: 360 loss: 3.7642862282725464 grad: -0.40322260650972513
iteration: 370 loss: 3.6666215407193787 grad: -0.39365763505070483
iteration: 380 loss: 3.5738445643448604 grad: -0.3845269164847088
iteration: 390 loss: 3.4856000881181486 grad: -0.3758020416197696
iteration: 400 loss: 3.4015663240871947 grad: -0.3674569892744246
iteration: 410 loss: 3.3214510816257063 grad: -0.35946788384273703
iteration: 420 loss: 3.2449884539570037 grad: -0.3518127814608169
iteration: 430 loss: 3.17193593882778 grad: -0.3444714809568681
iteration: 440 loss: 3.1020719285308007 grad: -0.3374253563315639
iteration: 450 loss: 3.03519351525909 grad: -0.3306572079895272
iteration: 460 loss: 2.9711145666243612 grad: -0.3241511303413969
iteration: 470 loss: 2.909664033395488 grad: -0.31789239373214484
iteration: 480 loss: 2.85068445748274 grad: -0.3118673389355137
iteration: 490 loss: 2.7940306531075874 grad: -0.3060632826953448
iteration: 0 loss: 77.16504121727165 grad: 30.152950313297122
iteration: 10 loss: 46.089475895141256 grad: 6.716767543495186
iteration: 20 loss: 34.73665141366077 grad: 0.595992718491529
iteration: 30 loss: 28.13560058922086 grad: -0.7901116852927329
iteration: 40 loss: 23.691964806243462 grad: -1.1374231867463882
iteration: 50 loss: 20.46974440513335 grad: -1.2082045125159482
iteration: 60 loss: 18.017877522014984 grad: -1.19314079795066
iteration: 70 loss: 16.086603352697136 grad: -1.1495145383869645
iteration: 80 loss: 14.524945076357819 grad: -1.0971510471599413
iteration: 90 loss: 13.235731113864672 grad: -1.0435985924173783
iteration: 100 loss: 12.153361531665206 grad: -0.9918567766534334
iteration: 110 loss: 11.231845640934951 grad: -0.9430778446442195
iteration: 120 loss: 10.437934834294042 grad: -0.897615775434788
iteration: 130 loss: 9.746971236537899 grad: -0.8554690013560575
iteration: 140 loss: 9.140269479164006 grad: -0.816479363513331
iteration: 150 loss: 8.603405832109296 grad: -0.780425532285888
iteration: 160 loss: 8.125066076677182 grad: -0.747067944627376
iteration: 170 loss: 7.69624928248252 grad: -0.7161704168172143
iteration: 180 loss: 7.309705002361997 grad: -0.6875101521033309
iteration: 190 loss: 6.959527501567788 grad: -0.660881853135082
iteration: 200 loss: 6.640858027815821 grad: -0.6360988230537696
iteration: 210 loss: 6.349662907154859 grad: -0.6129925523175739
iteration: 220 loss: 6.082565810290114 grad: -0.5914115830416848
iteration: 230 loss: 5.836719341955855 grad: -0.5712200732040986
iteration: 240 loss: 5.609705590945437 grad: -0.5522962849797395
iteration: 250 loss: 5.399458291245124 grad: -0.5345311134167201
iteration: 260 loss: 5.204201304651261 grad: -0.5178267121346801
iteration: 270 loss: 5.022399566504688 grad: -0.502095239876063
iteration: 280 loss: 4.852719645371756 grad: -0.48725773380569604
iteration: 290 loss: 4.693997788755826 grad: -0.4732431059443561
iteration: 300 loss: 4.545213848872539 grad: -0.4599872543954994
iteration: 310 loss: 4.405469864608511 grad: -0.44743227900486965
iteration: 320 loss: 4.27397235849102 grad: -0.4355257905837044
iteration: 330 loss: 4.150017618769528 grad: -0.4242203031334777
iteration: 340 loss: 4.032979396067813 grad: -0.4134726992302631
iteration: 350 loss: 3.9222985653225972 grad: -0.4032437596269568
iteration: 360 loss: 3.8174743967306726 grad: -0.3934977490795287
iteration: 370 loss: 3.718057151330868 grad: -0.3842020513264748
iteration: 380 loss: 3.6236417728294907 grad: -0.37532684701134017
iteration: 390 loss: 3.5338624911548155 grad: -0.36684482911989125
iteration: 400 loss: 3.4483881878618377 grad: -0.35873095120146503
iteration: 410 loss: 3.3669184009853037 grad: -0.350962204260148
iteration: 420 loss: 3.2891798688934117 grad: -0.3435174187411112
iteration: 430 loss: 3.214923530308087 grad: -0.336377088507637
iteration: 440 loss: 3.1439219118740933 grad: -0.32952321411258806
iteration: 450 loss: 3.075966846201895 grad: -0.32293916302165326
iteration: 460 loss: 3.0108674727022264 grad: -0.3166095447514882
iteration: 470 loss: 2.948448481220484 grad: -0.3105200991501556
iteration: 480 loss: 2.8885485648099594 grad: -0.3046575962756032
iteration: 490 loss: 2.831019053211259 grad: -0.29900974652523427
iteration: 0 loss: 78.04912297062587 grad: 21.719770173473187
iteration: 10 loss: 46.880397580877 grad: 7.332428502972722
iteration: 20 loss: 35.10908084051821 grad: 1.7799839618439295
iteration: 30 loss: 28.425843505128537 grad: -0.0192967757869419
iteration: 40 loss: 23.960741314957204 grad: -0.6615197787925434
iteration: 50 loss: 20.725430395303917 grad: -0.8988303544063698
iteration: 60 loss: 18.26057150914132 grad: -0.9772756362967738
iteration: 70 loss: 16.315879485179618 grad: -0.9884031864633873
iteration: 80 loss: 14.740932842453606 grad: -0.96991901001081
iteration: 90 loss: 13.439020930439277 grad: -0.9385787152355408
iteration: 100 loss: 12.344790625915142 grad: -0.9022370103300783
iteration: 110 loss: 11.412342335102473 grad: -0.8646959420161727
iteration: 120 loss: 10.608426965485272 grad: -0.8278105042992935
iteration: 130 loss: 9.908338476884781 grad: -0.7924618511150447
iteration: 140 loss: 9.2933219699921 grad: -0.7590304844750614
iteration: 150 loss: 8.748877545516859 grad: -0.7276359211629434
iteration: 160 loss: 8.263615943251663 grad: -0.6982620582994696
iteration: 170 loss: 7.828466089105363 grad: -0.6708243459869011
iteration: 180 loss: 7.436113840177148 grad: -0.6452063429019806
iteration: 190 loss: 7.080596573741207 grad: -0.6212797220661798
iteration: 200 loss: 6.757005211290644 grad: -0.5989151369235253
iteration: 210 loss: 6.46126178842641 grad: -0.5779879602618521
iteration: 220 loss: 6.189951092686235 grad: -0.558381120213804
iteration: 230 loss: 5.940191615184879 grad: -0.5399862903962693
iteration: 240 loss: 5.709535499730629 grad: -0.5227041555280371
iteration: 250 loss: 5.495890159841895 grad: -0.5064441709105034
iteration: 260 loss: 5.297456279973258 grad: -0.4911240597771074
iteration: 270 loss: 5.112678341284911 grad: -0.47666919067678987
iteration: 280 loss: 4.940204818035041 grad: -0.463011916911536
iteration: 290 loss: 4.778855910584753 grad: -0.45009092426125397
iteration: 300 loss: 4.627597202716846 grad: -0.43785061186973223
iteration: 310 loss: 4.485518013359028 grad: -0.4262405184469578
iteration: 320 loss: 4.351813496092249 grad: -0.415214798443559
iteration: 330 loss: 4.225769751736649 grad: -0.4047317485221765
iteration: 340 loss: 4.106751379315359 grad: -0.3947533822463134
iteration: 350 loss: 3.9941910125489812 grad: -0.38524504967077416
iteration: 360 loss: 3.887580482571097 grad: -0.3761750979826751
iteration: 370 loss: 3.7864633199287296 grad: -0.36751456922016135
iteration: 380 loss: 3.6904283652985543 grad: -0.3592369312096666
iteration: 390 loss: 3.5991043025831626 grad: -0.35131783810239353
iteration: 400 loss: 3.5121549629543964 grad: -0.3437349171899615
iteration: 410 loss: 3.4292752761438847 grad: -0.33646757899789814
iteration: 420 loss: 3.3501877674228293 grad: -0.32949684797048706
iteration: 430 loss: 3.274639516502785 grad: -0.3228052113585724
iteration: 440 loss: 3.2023995089583135 grad: -0.31637648419676323
iteration: 450 loss: 3.133256322405693 grad: -0.310195688505527
iteration: 460 loss: 3.0670160991926423 grad: -0.304248945076683
iteration: 470 loss: 3.003500765118712 grad: -0.2985233763988917
iteration: 480 loss: 2.9425464601023865 grad: -0.2930070194546594
iteration: 490 loss: 2.884002152005616 grad: -0.2876887472742517
iteration: 0 loss: 75.53393979802398 grad: 37.71383036311506
iteration: 10 loss: 45.0364736948351 grad: 5.144204235660234
iteration: 20 loss: 34.01018786082721 grad: -0.15551860769165787
iteration: 30 loss: 27.58688162714824 grad: -1.1166246926770216
iteration: 40 loss: 23.273767802749635 grad: -1.334323783919699
iteration: 50 loss: 20.144382189765565 grad: -1.3546853973740642
iteration: 60 loss: 17.758509363410624 grad: -1.3117183305591746
iteration: 70 loss: 15.874977833329034 grad: -1.24866331766058
iteration: 80 loss: 14.3486505993595 grad: -1.1814052811818652
iteration: 90 loss: 13.086186264637407 grad: -1.1160369074627554
iteration: 100 loss: 12.024504159947893 grad: -1.054793168432925
iteration: 110 loss: 11.119297924734632 grad: -0.9983109568094959
iteration: 120 loss: 10.33846849575366 grad: -0.9465579426571258
iteration: 130 loss: 9.658159371840012 grad: -0.8992336304695078
iteration: 140 loss: 9.06025533798255 grad: -0.8559471184354623
iteration: 150 loss: 8.530746094742467 grad: -0.8162959062786399
iteration: 160 loss: 8.058622506749295 grad: -0.7798994771550614
iteration: 170 loss: 7.635112417242578 grad: -0.746411892968007
iteration: 180 loss: 7.253139470087965 grad: -0.7155246944856452
iteration: 190 loss: 6.906932205334297 grad: -0.6869654776889285
iteration: 200 loss: 6.591736723124665 grad: -0.6604947243080237
iteration: 210 loss: 6.303602162112301 grad: -0.635902115349293
iteration: 220 loss: 6.039218286943444 grad: -0.6130028951752455
iteration: 230 loss: 5.795790965581463 grad: -0.5916345280329782
iteration: 240 loss: 5.57094559629075 grad: -0.5716537298217914
iteration: 250 loss: 5.362651422832104 grad: -0.5529338823270613
iteration: 260 loss: 5.169161647678059 grad: -0.535362803581405
iteration: 270 loss: 4.98896562480352 grad: -0.5188408353802023
iteration: 280 loss: 4.820750382263875 grad: -0.5032792065143589
iteration: 290 loss: 4.663369418088967 grad: -0.48859863248707835
iteration: 300 loss: 4.515817215462076 grad: -0.47472811656527414
iteration: 310 loss: 4.377208291429022 grad: -0.4616039215996154
iteration: 320 loss: 4.2467598662528045 grad: -0.449168686479233
iteration: 330 loss: 4.123777444681929 grad: -0.437370665092062
iteration: 340 loss: 4.007642754577608 grad: -0.42616306915254126
iteration: 350 loss: 3.897803605781395 grad: -0.4155034992389511
iteration: 360 loss: 3.7937653222837238 grad: -0.40535345089690816
iteration: 370 loss: 3.695083470544556 grad: -0.3956778847718053
iteration: 380 loss: 3.601357661176032 grad: -0.38644485149072677
iteration: 390 loss: 3.5122262438896787 grad: -0.377625163478886
iteration: 400 loss: 3.4273617492967787 grad: -0.3691921071154789
iteration: 410 loss: 3.3464669579136417 grad: -0.3611211896505847
iteration: 420 loss: 3.2692714981189552 grad: -0.35338991615332604
iteration: 430 loss: 3.1955288919990297 grad: -0.345977592470965
iteration: 440 loss: 3.1250139818823963 grad: -0.3388651507729986
iteration: 450 loss: 3.057520681641687 grad: -0.332034994753413
iteration: 460 loss: 2.9928600060184594 grad: -0.32547086198431385
iteration: 470 loss: 2.930858338749989 grad: -0.3191577012685733
iteration: 480 loss: 2.871355906471432 grad: -0.3130815631389055
iteration: 490 loss: 2.8142054304722395 grad: -0.30722950190495824
iteration: 0 loss: 76.74457863983883 grad: 29.584810358293517
iteration: 10 loss: 45.716081825465295 grad: 6.526670625611262
iteration: 20 loss: 34.48306681929821 grad: 0.5002313920329875
iteration: 30 loss: 27.98165742966419 grad: -0.8226569073456811
iteration: 40 loss: 23.60502362620373 grad: -1.1457815783962257
iteration: 50 loss: 20.425538270442882 grad: -1.2080000730893643
iteration: 60 loss: 18.000687851512485 grad: -1.1903115086280804
iteration: 70 loss: 16.086537884896334 grad: -1.1463683013495247
iteration: 80 loss: 14.53575622551679 grad: -1.0945850517667832
iteration: 90 loss: 13.253421662061472 grad: -1.041934177290727
iteration: 100 loss: 12.175337140775893 grad: -0.9911699174049866
iteration: 110 loss: 11.256401037290454 grad: -0.9433371757653151
iteration: 120 loss: 10.46393812427395 grad: -0.8987435338572052
iteration: 130 loss: 9.773666408869014 grad: -0.8573695163005651
iteration: 140 loss: 9.167150521354166 grad: -0.8190530673105072
iteration: 150 loss: 8.630135071968693 grad: -0.783576055950985
iteration: 160 loss: 8.151420360824824 grad: -0.7507056631213428
iteration: 170 loss: 7.722083993848773 grad: -0.7202140526816274
iteration: 180 loss: 7.334929699173451 grad: -0.6918872729754105
iteration: 190 loss: 6.984089241115436 grad: -0.6655287327489645
iteration: 200 loss: 6.66472984085257 grad: -0.6409599460463767
iteration: 210 loss: 6.372835766734872 grad: -0.6180199360348632
iteration: 220 loss: 6.105042997594617 grad: -0.5965640243637069
iteration: 230 loss: 5.858512472621235 grad: -0.576462386867146
iteration: 240 loss: 5.630831801900513 grad: -0.5575985728086598
iteration: 250 loss: 5.4199382449904245 grad: -0.5398680861865864
iteration: 260 loss: 5.224057773280782 grad: -0.5231770743354863
iteration: 270 loss: 5.041656429310231 grad: -0.5074411404179631
iteration: 280 loss: 4.871401182928573 grad: -0.49258428129618986
iteration: 290 loss: 4.712128190355625 grad: -0.4785379446459742
iteration: 300 loss: 4.562816873882887 grad: -0.465240195682007
iteration: 310 loss: 4.422568615048352 grad: -0.4526349826307456
iteration: 320 loss: 4.290589131953601 grad: -0.44067149006444817
iteration: 330 loss: 4.166173819279927 grad: -0.42930356980203077
iteration: 340 loss: 4.048695486558452 grad: -0.4184892399554851
iteration: 350 loss: 3.9375940497883346 grad: -0.4081902436689551
iteration: 360 loss: 3.8323678233381626 grad: -0.3983716600609009
iteration: 370 loss: 3.732566130081399 grad: -0.3890015607867606
iteration: 380 loss: 3.6377830030778155 grad: -0.380050706466747
iteration: 390 loss: 3.5476517955626563 grad: -0.37149227796321577
iteration: 400 loss: 3.4618405502785454 grad: -0.3633016381453583
iteration: 410 loss: 3.380048006438797 grad: -0.3554561203510652
iteration: 420 loss: 3.302000144387793 grad: -0.34793484025405674
iteration: 430 loss: 3.227447185492234 grad: -0.3407185282767338
iteration: 440 loss: 3.1561609789368545 grad: -0.3337893800635181
iteration: 450 loss: 3.0879327185479766 grad: -0.3271309228531083
iteration: 460 loss: 3.0225709421142297 grad: -0.32072789586774675
iteration: 470 loss: 2.9598997733310997 grad: -0.31456614307922187
iteration: 480 loss: 2.899757372792506 grad: -0.30863251692017735
iteration: 490 loss: 2.841994569645108 grad: -0.3029147916898772
iteration: 0 loss: 77.00791829978307 grad: 25.857831262442446
iteration: 10 loss: 45.27669203944328 grad: 7.779072558243946
iteration: 20 loss: 33.941615979142874 grad: 1.345344674320717
iteration: 30 loss: 27.51585789148884 grad: -0.5320138216282567
iteration: 40 loss: 23.21445060984671 grad: -1.1067151103688857
iteration: 50 loss: 20.093955112819913 grad: -1.2649861014612194
iteration: 60 loss: 17.71483350593212 grad: -1.277763392391214
iteration: 70 loss: 15.836730054833962 grad: -1.2380388793050783
iteration: 80 loss: 14.314894513190128 grad: -1.1805909041830158
iteration: 90 loss: 13.05618939058106 grad: -1.1191417138829576
iteration: 100 loss: 11.99767018365033 grad: -1.0591851652828663
iteration: 110 loss: 11.095136698582296 grad: -1.002814866618043
iteration: 120 loss: 10.316577278719416 grad: -0.9506711986577878
iteration: 130 loss: 9.638207827194016 grad: -0.9027696904411289
iteration: 140 loss: 9.041972600056893 grad: -0.8588673828685519
iteration: 150 loss: 8.513909658854345 grad: -0.8186285574585026
iteration: 160 loss: 8.043048898640585 grad: -0.7816998352807571
iteration: 170 loss: 7.620649528802894 grad: -0.7477431574429221
iteration: 180 loss: 7.2396603708447484 grad: -0.716448879563951
iteration: 190 loss: 6.894330204265481 grad: -0.687539483317054
iteration: 200 loss: 6.579921456455891 grad: -0.6607689754470168
iteration: 210 loss: 6.292496504572393 grad: -0.635920449947647
iteration: 220 loss: 6.028755914224141 grad: -0.6128030195473759
iteration: 230 loss: 5.785914427053037 grad: -0.5912486909721377
iteration: 240 loss: 5.561604785529901 grad: -0.5711094416087299
iteration: 250 loss: 5.35380235773201 grad: -0.5522545967908343
iteration: 260 loss: 5.1607654916495616 grad: -0.5345685293161377
iteration: 270 loss: 4.9809878962015235 grad: -0.517948666607135
iteration: 280 loss: 4.813160311352021 grad: -0.5023037758768167
iteration: 290 loss: 4.656139420245009 grad: -0.48755249320877125
iteration: 300 loss: 4.508922456481576 grad: -0.47362206310602073
iteration: 310 loss: 4.370626326274553 grad: -0.4604472579461458
iteration: 320 loss: 4.240470336754408 grad: -0.44796945044229497
iteration: 330 loss: 4.117761824882894 grad: -0.4361358159315103
iteration: 340 loss: 4.0018841348452785 grad: -0.4248986447643644
iteration: 350 loss: 3.892286508675914 grad: -0.4142147481264865
iteration: 360 loss: 3.7884755446177887 grad: -0.40404494325806817
iteration: 370 loss: 3.6900079471741845 grad: -0.3943536062747634
iteration: 380 loss: 3.596484346945834 grad: -0.38510828267528374
iteration: 390 loss: 3.507544010825107 grad: -0.3762793471956416
iteration: 400 loss: 3.4228602966794495 grad: -0.36783970598400934
iteration: 410 loss: 3.3421367332969734 grad: -0.3597645351656982
iteration: 420 loss: 3.2651036276896446 grad: -0.35203105078130364
iteration: 430 loss: 3.1915151189486592 grad: -0.34461830584366954
iteration: 440 loss: 3.121146611687767 grad: -0.3375070108968983
iteration: 450 loss: 3.053792533324213 grad: -0.33067937499477285
iteration: 460 loss: 2.989264368595245 grad: -0.324118964464365
iteration: 470 loss: 2.9273889322208477 grad: -0.31781057719800604
iteration: 480 loss: 2.8680068467703923 grad: -0.3117401305351559
iteration: 490 loss: 2.8109711979079126 grad: -0.3058945610649947
iteration: 0 loss: 75.27306338937386 grad: 37.1341130252397
iteration: 10 loss: 43.455647658101114 grad: 6.455386694029626
iteration: 20 loss: 32.84905797674653 grad: -0.515946893934522
iteration: 30 loss: 26.751522896970755 grad: -1.6545245792582821
iteration: 40 loss: 22.64223156832668 grad: -1.7770815928300232
iteration: 50 loss: 19.64914055048774 grad: -1.6961391706910511
iteration: 60 loss: 17.359710103250748 grad: -1.5779565982235424
iteration: 70 loss: 15.547163811831297 grad: -1.4616511292227317
iteration: 80 loss: 14.07459961241592 grad: -1.3562887313787968
iteration: 90 loss: 12.853764053262983 grad: -1.2629780726116149
iteration: 100 loss: 11.824900827019933 grad: -1.1806997178509486
iteration: 110 loss: 10.945966153551415 grad: -1.1079920520973299
iteration: 120 loss: 10.18644912139937 grad: -1.04344859306167
iteration: 130 loss: 9.523635352506828 grad: -0.9858474407188125
iteration: 140 loss: 8.940247433936417 grad: -0.9341641356907592
iteration: 150 loss: 8.422900007279623 grad: -0.8875499564155862
iteration: 160 loss: 7.96105691477822 grad: -0.8453031249097906
iteration: 170 loss: 7.546308596107048 grad: -0.8068417957055218
iteration: 180 loss: 7.171859902796975 grad: -0.7716811980519315
iteration: 190 loss: 6.832159769947959 grad: -0.7394150663534724
iteration: 200 loss: 6.522628709826387 grad: -0.7097008058898295
iteration: 210 loss: 6.2394551248842784 grad: -0.6822477099534281
iteration: 220 loss: 5.979440908379977 grad: -0.6568076011568273
iteration: 230 loss: 5.739882914313222 grad: -0.6331673758325758
iteration: 240 loss: 5.518480911870649 grad: -0.6111430367499673
iteration: 250 loss: 5.313265353534379 grad: -0.590574890433476
iteration: 260 loss: 5.122540144885616 grad: -0.571323658577492
iteration: 270 loss: 4.9448368981292825 grad: -0.5532673101832153
iteration: 280 loss: 4.778878065561168 grad: -0.5362984649948793
iteration: 290 loss: 4.623547003884224 grad: -0.5203222524171782
iteration: 300 loss: 4.477863495022575 grad: -0.5052545357508376
iteration: 310 loss: 4.340963597336171 grad: -0.49102043119492367
iteration: 320 loss: 4.21208295935536 grad: -0.47755306610228354
iteration: 330 loss: 4.0905429215177636 grad: -0.4647925325537239
iteration: 340 loss: 3.9757388775474283 grad: -0.4526850012764687
iteration: 350 loss: 3.867130478549701 grad: -0.44118196790281644
iteration: 360 loss: 3.764233348556569 grad: -0.4302396090179007
iteration: 370 loss: 3.666612046582941 grad: -0.41981822973576727
iteration: 380 loss: 3.5738740620232337 grad: -0.4098817879376989
iteration: 390 loss: 3.4856646708612145 grad: -0.4003974830079515
iteration: 400 loss: 3.401662512297166 grad: -0.3913353990632449
iteration: 410 loss: 3.321575770948915 grad: -0.3826681944106608
iteration: 420 loss: 3.24513887022526 grad: -0.3743708303739568
iteration: 430 loss: 3.1721095988995 grad: -0.3664203337701065
iteration: 440 loss: 3.102266606210502 grad: -0.35879558824991087
iteration: 450 loss: 3.035407211591064 grad: -0.35147715048082606
iteration: 460 loss: 2.9713454839649205 grad: -0.34444708777960964
iteration: 470 loss: 2.9099105527410427 grad: -0.33768883432294483
iteration: 480 loss: 2.850945118614494 grad: -0.3311870634964428
iteration: 490 loss: 2.794304137175831 grad: -0.32492757430269614
iteration: 0 loss: 77.02468037146778 grad: 25.22690167971314
iteration: 10 loss: 46.059642717099976 grad: 6.147277073067279
iteration: 20 loss: 34.72438289258085 grad: 0.9372985091849673
iteration: 30 loss: 28.150995534851788 grad: -0.38502299772826354
iteration: 40 loss: 23.72459428493647 grad: -0.7888530221024699
iteration: 50 loss: 20.510593717325367 grad: -0.9191861021535573
iteration: 60 loss: 18.061626347384347 grad: -0.9514366050857259
iteration: 70 loss: 16.130405895511043 grad: -0.9439269609282677
iteration: 80 loss: 14.56735980884481 grad: -0.9193328525485917
iteration: 90 loss: 13.27608279243728 grad: -0.8875384341699352
iteration: 100 loss: 12.19138871399427 grad: -0.8531986957894753
iteration: 110 loss: 11.267508053219588 grad: -0.8186066225650206
iteration: 120 loss: 10.471307381682601 grad: -0.7849064387546312
iteration: 130 loss: 9.778185186446452 grad: -0.7526534365620645
iteration: 140 loss: 9.16947972626386 grad: -0.7220902331785615
iteration: 150 loss: 8.630772822390822 grad: -0.6932902880930211
iteration: 160 loss: 8.150746078191021 grad: -0.6662354180492776
iteration: 170 loss: 7.720389442713398 grad: -0.6408588772136129
iteration: 180 loss: 7.332441149038707 grad: -0.6170697687665
iteration: 190 loss: 6.9809834986298975 grad: -0.5947670196024734
iteration: 200 loss: 6.661145989728497 grad: -0.5738473759892055
iteration: 210 loss: 6.368883861873738 grad: -0.5542099090217798
iteration: 220 loss: 6.100810571016113 grad: -0.5357584554918923
iteration: 230 loss: 5.854069449088249 grad: -0.5184028286035338
iteration: 240 loss: 5.62623424629361 grad: -0.502059295773231
iteration: 250 loss: 5.415231242849462 grad: -0.48665062408894144
iteration: 260 loss: 5.219277662143006 grad: -0.47210587702810847
iteration: 270 loss: 5.036832539581197 grad: -0.4583600752630649
iteration: 280 loss: 4.866557205177411 grad: -0.44535379093730076
iteration: 290 loss: 4.707283255893841 grad: -0.4330327178112021
iteration: 300 loss: 4.55798641371042 grad: -0.42134724276723484
iteration: 310 loss: 4.417765046316907 grad: -0.41025203351328415
iteration: 320 loss: 4.285822409351237 grad: -0.39970565060514496
iteration: 330 loss: 4.161451880002088 grad: -0.38967018768901257
iteration: 340 loss: 4.044024610965194 grad: -0.38011094124237244
iteration: 350 loss: 3.9329791549202624 grad: -0.3709961094961268
iteration: 360 loss: 3.8278127026877966 grad: -0.3622965192862628
iteration: 370 loss: 3.7280736501416785 grad: -0.3539853790732733
iteration: 380 loss: 3.6333552649890324 grad: -0.3460380561280484
iteration: 390 loss: 3.5432902684397463 grad: -0.3384318758134178
iteration: 400 loss: 3.457546181481782 grad: -0.33114594092520044
iteration: 410 loss: 3.3758213130029784 grad: -0.324160969152442
iteration: 420 loss: 3.2978412889932027 grad: -0.31745914684553667
iteration: 430 loss: 3.223356039719337 grad: -0.3110239974251505
iteration: 440 loss: 3.1521371760208736 grad: -0.30484026291285826
iteration: 450 loss: 3.083975697444781 grad: -0.29889379720920783
iteration: 460 loss: 3.0186799843523575 grad: -0.29317146988237763
iteration: 470 loss: 2.9560740338618583 grad: -0.2876610793585112
iteration: 480 loss: 2.895995905830577 grad: -0.28235127452219244
iteration: 490 loss: 2.8382963503234517 grad: -0.27723148384217744
iteration: 0 loss: 76.63437465792741 grad: 31.361863808273334
iteration: 10 loss: 44.506290732433264 grad: 7.22877629759582
iteration: 20 loss: 33.45403586119023 grad: 0.3051530258495885
iteration: 30 loss: 27.166482209929573 grad: -1.21395100546233
iteration: 40 loss: 22.944347624735943 grad: -1.5212281747246887
iteration: 50 loss: 19.87569260347341 grad: -1.5235593274564017
iteration: 60 loss: 17.533000391303815 grad: -1.4464326449160094
iteration: 70 loss: 15.681749497931461 grad: -1.3531310788134507
iteration: 80 loss: 14.180435743503825 grad: -1.2623907944332167
iteration: 90 loss: 12.93787114711933 grad: -1.1793921440238622
iteration: 100 loss: 11.89234494501627 grad: -1.1049600172019118
iteration: 110 loss: 11.000471235380163 grad: -1.03854088628407
iteration: 120 loss: 10.230795637576897 grad: -0.9792166503948112
iteration: 130 loss: 9.559927767668569 grad: -0.9260520168846073
iteration: 140 loss: 8.970097914383958 grad: -0.8782047015674247
iteration: 150 loss: 8.447557062187611 grad: -0.8349498868813383
iteration: 160 loss: 7.981496982105434 grad: -0.7956747331178479
iteration: 170 loss: 7.563302207425957 grad: -0.7598639196464188
iteration: 180 loss: 7.186020115440442 grad: -0.7270839992966336
iteration: 190 loss: 6.843978045104037 grad: -0.6969692363986626
iteration: 200 loss: 6.532501782992437 grad: -0.6692096206945723
iteration: 210 loss: 6.24770533360503 grad: -0.6435410153746421
iteration: 220 loss: 5.98633171379383 grad: -0.6197371602217896
iteration: 230 loss: 5.74563085519408 grad: -0.5976032074909141
iteration: 240 loss: 5.523264884971303 grad: -0.5769704949042725
iteration: 250 loss: 5.317233872104101 grad: -0.5576923069018542
iteration: 260 loss: 5.125817055531528 grad: -0.5396404226631955
iteration: 270 loss: 4.947525912994314 grad: -0.5227022908029795
iteration: 280 loss: 4.78106637749918 grad: -0.5067787045955192
iteration: 290 loss: 4.625308186964958 grad: -0.49178187860351086
iteration: 300 loss: 4.479259844467544 grad: -0.47763384876813153
iteration: 310 loss: 4.342048027079479 grad: -0.46426513450913315
iteration: 320 loss: 4.212900548487059 grad: -0.4516136141970562
iteration: 330 loss: 4.09113218050776 grad: -0.4396235753235078
iteration: 340 loss: 3.9761327896783523 grad: -0.42824490846360186
iteration: 350 loss: 3.8673573601161646 grad: -0.4174324202036249
iteration: 360 loss: 3.764317562255573 grad: -0.40714524498509574
iteration: 370 loss: 3.6665745954493434 grad: -0.3973463395901712
iteration: 380 loss: 3.5737330857339824 grad: -0.38800204698893637
iteration: 390 loss: 3.4854358619283876 grad: -0.37908171865916435
iteration: 400 loss: 3.4013594662472895 grad: -0.3705573864058324
iteration: 410 loss: 3.3212102819149796 grad: -0.36240347625254715
iteration: 420 loss: 3.2447211812223444 grad: -0.3545965582283471
iteration: 430 loss: 3.1716486143548703 grad: -0.3471151268918181
iteration: 440 loss: 3.1017700729449973 grad: -0.339939408267221
iteration: 450 loss: 3.034881873351961 grad: -0.3330511895513678
iteration: 460 loss: 2.9707972137079692 grad: -0.3264336685142548
iteration: 470 loss: 2.9093444661473935 grad: -0.3200713199839284
iteration: 480 loss: 2.8503656717285923 grad: -0.31394977719481976
iteration: 490 loss: 2.793715210575804 grad: -0.30805572610340803
iteration: 0 loss: 76.96870691640615 grad: 32.95069981781188
iteration: 10 loss: 46.04433310803402 grad: 6.067800171467065
iteration: 20 loss: 34.77324803334158 grad: 0.1861570695232243
iteration: 30 loss: 28.19825607061563 grad: -0.9587760671186503
iteration: 40 loss: 23.767332494706807 grad: -1.1997609454015232
iteration: 50 loss: 20.549780059179678 grad: -1.2229974321257022
iteration: 60 loss: 18.098047755646636 grad: -1.1851563677031987
iteration: 70 loss: 16.164602014094314 grad: -1.130096682661189
iteration: 80 loss: 14.599713614619635 grad: -1.0719721191529195
iteration: 90 loss: 13.30687275127682 grad: -1.0157010499923316
iteration: 100 loss: 12.2208200559923 grad: -0.9629544149890082
iteration: 110 loss: 11.29573413640591 grad: -0.914163704483757
iteration: 120 loss: 10.498445011262634 grad: -0.8692694422013254
iteration: 130 loss: 9.804325578835316 grad: -0.8280227809918901
iteration: 140 loss: 9.194696223974894 grad: -0.7901133286842277
iteration: 150 loss: 8.655126227525505 grad: -0.7552244065243849
iteration: 160 loss: 8.174288287777989 grad: -0.7230563723964145
iteration: 170 loss: 7.7431659026110955 grad: -0.6933354158003152
iteration: 180 loss: 7.35449250190835 grad: -0.6658156700246967
iteration: 190 loss: 7.002346690406119 grad: -0.640278311163362
iteration: 200 loss: 6.681855016405445 grad: -0.6165294002408196
iteration: 210 loss: 6.388970279411751 grad: -0.5943973144009549
iteration: 220 loss: 6.120303850173803 grad: -0.57373016803042
iteration: 230 loss: 5.872997229093678 grad: -0.5543934034492414
iteration: 240 loss: 5.6446225225765625 grad: -0.5362676204020158
iteration: 250 loss: 5.4331045116767775 grad: -0.5192466592201108
iteration: 260 loss: 5.236659036864197 grad: -0.5032359268435531
iteration: 270 loss: 5.05374384791474 grad: -0.48815094392573743
iteration: 280 loss: 4.883019073530952 grad: -0.47391608774850136
iteration: 290 loss: 4.7233151845206605 grad: -0.46046350585346396
iteration: 300 loss: 4.573606845099551 grad: -0.44773217715582747
iteration: 310 loss: 4.432991428330959 grad: -0.4356670998129153
iteration: 320 loss: 4.300671254059001 grad: -0.4242185877454926
iteration: 330 loss: 4.175938818820999 grad: -0.41334166020217744
iteration: 340 loss: 4.058164446509553 grad: -0.4029955110074784
iteration: 350 loss: 3.9467859098279985 grad: -0.39314304610909023
iteration: 360 loss: 3.841299665625105 grad: -0.38375047974462356
iteration: 370 loss: 3.7412534191421813 grad: -0.3747869810039134
iteration: 380 loss: 3.646239788258258 grad: -0.3662243637989413
iteration: 390 loss: 3.5558908827496536 grad: -0.35803681429870826
iteration: 400 loss: 3.469873648269434 grad: -0.3502006507688934
iteration: 410 loss: 3.387885852285928 grad: -0.34269411150069123
iteration: 420 loss: 3.309652611209789 grad: -0.3354971671415716
iteration: 430 loss: 3.234923375606782 grad: -0.32859135427138475
iteration: 440 loss: 3.163469304636072 grad: -0.3219596275160008
iteration: 450 loss: 3.0950809724300514 grad: -0.3155862278707465
iteration: 460 loss: 3.0295663585525703 grad: -0.30945656522833476
iteration: 470 loss: 2.9667490823899243 grad: -0.3035571133800819
iteration: 480 loss: 2.9064668476722875 grad: -0.29787531599260686
iteration: 490 loss: 2.84857006857911 grad: -0.29239950226142714
iteration: 0 loss: 74.11843590629638 grad: 43.73630544180705
iteration: 10 loss: 43.12518131593406 grad: 5.0454787202084965
iteration: 20 loss: 32.79269598822341 grad: -1.2843301378709355
iteration: 30 loss: 26.75166133047684 grad: -1.9274308283013697
iteration: 40 loss: 22.656977747717665 grad: -1.8731245244092358
iteration: 50 loss: 19.665276661245116 grad: -1.7271816044636799
iteration: 60 loss: 17.37252218735954 grad: -1.58354815785244
iteration: 70 loss: 15.555189245672699 grad: -1.4570926851448223
iteration: 80 loss: 14.07770896734062 grad: -1.3479629769975776
iteration: 90 loss: 12.85235772379106 grad: -1.253661576076111
iteration: 100 loss: 11.819555363869313 grad: -1.1716005883113594
iteration: 110 loss: 10.937278734404986 grad: -1.0996035895191745
iteration: 120 loss: 10.174972905276759 grad: -1.035931721039195
iteration: 130 loss: 9.509856946208878 grad: -0.9792100843492617
iteration: 140 loss: 8.92458351274728 grad: -0.928347616855773
iteration: 150 loss: 8.405702122245946 grad: -0.8824709246671565
iteration: 160 loss: 7.942619261448685 grad: -0.8408736168502361
iteration: 170 loss: 7.526876316301861 grad: -0.8029783761518782
iteration: 180 loss: 7.151636859666476 grad: -0.7683086445305402
iteration: 190 loss: 6.811315400903492 grad: -0.7364673688787726
iteration: 200 loss: 6.50130387569123 grad: -0.7071209004104397
iteration: 210 loss: 6.217767012923477 grad: -0.6799866696564312
iteration: 220 loss: 5.95748710025778 grad: -0.65482364965978
iteration: 230 loss: 5.717744742323413 grad: -0.6314248986292109
iteration: 240 loss: 5.496226220538409 grad: -0.6096116698843774
iteration: 250 loss: 5.2909507701723335 grad: -0.589228715593124
iteration: 260 loss: 5.100212947328725 grad: -0.5701405091623899
iteration: 270 loss: 4.922536553186257 grad: -0.5522281814944731
iteration: 280 loss: 4.756637498672924 grad: -0.5353870171183607
iteration: 290 loss: 4.601393649373097 grad: -0.5195243932545124
iteration: 300 loss: 4.455820167075424 grad: -0.5045580721648782
iteration: 310 loss: 4.319049214278906 grad: -0.4904147774451297
iteration: 320 loss: 4.190313147608859 grad: -0.4770290001661508
iteration: 330 loss: 4.068930520637465 grad: -0.4643419923297055
iteration: 340 loss: 3.9542943637269343 grad: -0.45230091393871796
iteration: 350 loss: 3.845862320726734 grad: -0.4408581067926576
iteration: 360 loss: 3.7431483086413873 grad: -0.4299704734083639
iteration: 370 loss: 3.6457154332323296 grad: -0.41959894360603356
iteration: 380 loss: 3.5531699456764727 grad: -0.40970801456188743
iteration: 390 loss: 3.4651560663792638 grad: -0.4002653527162942
iteration: 400 loss: 3.3813515344347915 grad: -0.3912414479913179
iteration: 410 loss: 3.3014637669770392 grad: -0.3826093124298131
iteration: 420 loss: 3.225226533284223 grad: -0.37434421670716844
iteration: 430 loss: 3.1523970650652946 grad: -0.36642345905377605
iteration: 440 loss: 3.0827535377473976 grad: -0.35882616201317896
iteration: 450 loss: 3.016092868478154 grad: -0.3515330931880474
iteration: 460 loss: 2.9522287854236064 grad: -0.34452650672506535
iteration: 470 loss: 2.890990130238482 grad: -0.3377900027853279
iteration: 480 loss: 2.832219361568398 grad: -0.3313084026584874
iteration: 490 loss: 2.7757712324064414 grad: -0.3250676375222662
iteration: 0 loss: 75.93005460777631 grad: 32.62653803840634
iteration: 10 loss: 43.96021774870791 grad: 6.6164990641165025
iteration: 20 loss: 33.30664236712805 grad: 0.2449724325983356
iteration: 30 loss: 27.154292041755255 grad: -1.1065287257996494
iteration: 40 loss: 22.98474267691863 grad: -1.396850689802728
iteration: 50 loss: 19.936671188822007 grad: -1.4170770024342012
iteration: 60 loss: 17.600866513765368 grad: -1.3615344015255455
iteration: 70 loss: 15.750364035057125 grad: -1.28649920615746
iteration: 80 loss: 14.247027901790283 grad: -1.2099578139134368
iteration: 90 loss: 13.001251406100913 grad: -1.1377517263791717
iteration: 100 loss: 11.95209065556258 grad: -1.071514359488873
iteration: 110 loss: 11.056535868571121 grad: -1.011361522116391
iteration: 120 loss: 10.283313869259617 grad: -0.956877110652161
iteration: 130 loss: 9.609115687156459 grad: -0.9074910673325015
iteration: 140 loss: 9.016201324772283 grad: -0.8626253384550082
iteration: 150 loss: 8.490824846979732 grad: -0.8217471484470094
iteration: 160 loss: 8.022167813289798 grad: -0.7843844866454729
iteration: 170 loss: 7.60159843305804 grad: -0.7501264003720434
iteration: 180 loss: 7.222145509468729 grad: -0.7186176330205811
iteration: 190 loss: 6.878117603064931 grad: -0.6895516493158911
iteration: 200 loss: 6.5648225590863065 grad: -0.6626637117068362
iteration: 210 loss: 6.2783577623451325 grad: -0.6377246305191626
iteration: 220 loss: 6.015451109789286 grad: -0.6145353582339469
iteration: 230 loss: 5.773338925912247 grad: -0.5929224097508132
iteration: 240 loss: 5.549671171381306 grad: -0.5727340203430342
iteration: 250 loss: 5.34243707734446 grad: -0.5538369350573304
iteration: 260 loss: 5.149906247044493 grad: -0.5361137272154715
iteration: 270 loss: 4.9705815973676515 grad: -0.519460555594986
iteration: 280 loss: 4.803161454427357 grad: -0.5037852836419745
iteration: 290 loss: 4.646508792121701 grad: -0.4890058971236294
iteration: 300 loss: 4.499626092297893 grad: -0.47504916803879765
iteration: 310 loss: 4.361634664560783 grad: -0.46184952218833386
iteration: 320 loss: 4.231757530321209 grad: -0.4493480756867917
iteration: 330 loss: 4.109305175343665 grad: -0.4374918121034454
iteration: 340 loss: 3.9936636259656306 grad: -0.42623287710040664
iteration: 350 loss: 3.884284419225506 grad: -0.41552797161516647
iteration: 360 loss: 3.7806761255650683 grad: -0.4053378280079657
iteration: 370 loss: 3.6823971512487166 grad: -0.39562675632104194
iteration: 380 loss: 3.589049601046926 grad: -0.386362250006421
iteration: 390 loss: 3.5002740236647942 grad: -0.3775146422751677
iteration: 400 loss: 3.4157448955367973 grad: -0.3690568056862228
iteration: 410 loss: 3.335166724933639 grad: -0.3609638887925519
iteration: 420 loss: 3.2582706794042595 grad: -0.35321308464811785
iteration: 430 loss: 3.1848116564920432 grad: -0.34578342679234525
iteration: 440 loss: 3.1145657313390984 grad: -0.33865560900199587
iteration: 450 loss: 3.0473279259177946 grad: -0.3318118256598396
iteration: 460 loss: 2.9829102536606857 grad: -0.3252356300560799
iteration: 470 loss: 2.921140000713504 grad: -0.3189118083289625
iteration: 480 loss: 2.8618582111278723 grad: -0.3128262670788559
iteration: 490 loss: 2.804918348365308 grad: -0.30696593296630686
iteration: 0 loss: 77.03988689539324 grad: 25.09345400605542
iteration: 10 loss: 45.19044064505824 grad: 6.933506638583279
iteration: 20 loss: 33.85654116735427 grad: 1.1649524081361622
iteration: 30 loss: 27.43788556272437 grad: -0.44968851256287534
iteration: 40 loss: 23.14903643720351 grad: -0.9556435506608594
iteration: 50 loss: 20.040318582284975 grad: -1.110672345727828
iteration: 60 loss: 17.67057081106317 grad: -1.1388988392294637
iteration: 70 loss: 15.799616056423536 grad: -1.1177685126637353
iteration: 80 loss: 14.283230723327796 grad: -1.077349042667219
iteration: 90 loss: 13.028745902740598 grad: -1.0302933890629742
iteration: 100 loss: 11.973562216537113 grad: -0.9821816140935269
iteration: 110 loss: 11.073719854068797 grad: -0.9354979503904484
iteration: 120 loss: 10.297372640595789 grad: -0.8912975632757427
iteration: 130 loss: 9.620851259568884 grad: -0.8499539237903936
iteration: 140 loss: 9.026180945346617 grad: -0.8115118146899287
iteration: 150 loss: 8.49945814207368 grad: -0.775860553239544
iteration: 160 loss: 8.029755835741401 grad: -0.7428211404492345
iteration: 170 loss: 7.608365789129121 grad: -0.7121905795468064
iteration: 180 loss: 7.228261957366819 grad: -0.6837642508906931
iteration: 190 loss: 6.88371290618666 grad: -0.6573468172065556
iteration: 200 loss: 6.5699969040463 grad: -0.6327570716502358
iteration: 210 loss: 6.283189192813428 grad: -0.6098295915297587
iteration: 220 loss: 6.020000911185145 grad: -0.5884147373749222
iteration: 230 loss: 5.777655577580348 grad: -0.5683778335461832
iteration: 240 loss: 5.55379328138005 grad: -0.5495979852894237
iteration: 250 loss: 5.346395584531944 grad: -0.531966777516585
iteration: 260 loss: 5.15372608891411 grad: -0.5153869842723184
iteration: 270 loss: 4.974282984009485 grad: -0.4997713530764594
iteration: 280 loss: 4.806760849199608 grad: -0.4850414924021855
iteration: 290 loss: 4.6500196719576214 grad: -0.47112687089767236
iteration: 300 loss: 4.5030595410681515 grad: -0.4579639265364873
iteration: 310 loss: 4.36499983900074 grad: -0.44549527868905403
iteration: 320 loss: 4.235062027996761 grad: -0.4336690338553326
iteration: 330 loss: 4.112555326818231 grad: -0.42243817517059706
iteration: 340 loss: 3.9968647279573486 grad: -0.4117600260333041
iteration: 350 loss: 3.887440921561682 grad: -0.4015957788711598
iteration: 360 loss: 3.783791781767542 grad: -0.39191008091135593
iteration: 370 loss: 3.6854751403333963 grad: -0.3826706697161373
iteration: 380 loss: 3.592092626446794 grad: -0.3738480521120127
iteration: 390 loss: 3.503284393872036 grad: -0.3654152209437506
iteration: 400 loss: 3.418724590083705 grad: -0.3573474048077977
iteration: 410 loss: 3.3381174485620333 grad: -0.3496218465607599
iteration: 420 loss: 3.261193906682489 grad: -0.3422176069602145
iteration: 430 loss: 3.1877086686760197 grad: -0.33511539028367815
iteration: 440 loss: 3.117437646919122 grad: -0.3282973891945268
iteration: 450 loss: 3.0501757259975917 grad: -0.32174714648874847
iteration: 460 loss: 2.985734803102332 grad: -0.3154494316710141
iteration: 470 loss: 2.923942065799896 grad: -0.30939013057939757
iteration: 480 loss: 2.864638474345983 grad: -0.30355614651121293
iteration: 490 loss: 2.807677420814757 grad: -0.29793531150318764
iteration: 0 loss: 75.32117763690381 grad: 41.56262175078377
iteration: 10 loss: 43.715617318530754 grad: 4.564339892113709
iteration: 20 loss: 33.1652624848569 grad: -0.8734852721827233
iteration: 30 loss: 27.03568536862971 grad: -1.5517178097542967
iteration: 40 loss: 22.892885934879814 grad: -1.5893071740235654
iteration: 50 loss: 19.868446136908915 grad: -1.5127747476344233
iteration: 60 loss: 17.550795474904074 grad: -1.417299063045283
iteration: 70 loss: 15.713509509646057 grad: -1.324418400739627
iteration: 80 loss: 14.21959813626003 grad: -1.2392027787435271
iteration: 90 loss: 12.98049464302833 grad: -1.1623549137433455
iteration: 100 loss: 11.93605141241617 grad: -1.093344218641369
iteration: 110 loss: 11.04383341674237 grad: -1.0313333888678495
iteration: 120 loss: 10.272973358348032 grad: -0.9754708246409596
iteration: 130 loss: 9.600447034694774 grad: -0.9249805245548511
iteration: 140 loss: 9.00871433813648 grad: -0.8791831463908738
iteration: 150 loss: 8.484170153597068 grad: -0.8374934165529578
iteration: 160 loss: 8.016095735126948 grad: -0.7994100728158011
iteration: 170 loss: 7.5959302423067045 grad: -0.7645042964712399
iteration: 180 loss: 7.21675322808708 grad: -0.7324087795207306
iteration: 190 loss: 6.872909721160837 grad: -0.7028081036898648
iteration: 200 loss: 6.5597338768102 grad: -0.675430538328869
iteration: 210 loss: 6.273342122735854 grad: -0.6500411529069203
iteration: 220 loss: 6.010476171532942 grad: -0.6264360740641947
iteration: 230 loss: 5.76838238460389 grad: -0.6044377101313845
iteration: 240 loss: 5.5447180155517595 grad: -0.5838907812121902
iteration: 250 loss: 5.337477587927672 grad: -0.5646590149185339
iteration: 260 loss: 5.1449345338490025 grad: -0.5466223902320708
iteration: 270 loss: 4.965594525401684 grad: -0.5296748322590481
iteration: 280 loss: 4.798157854571276 grad: -0.5137222780958737
iteration: 290 loss: 4.641488880096238 grad: -0.49868104861693585
iteration: 300 loss: 4.494591040817382 grad: -0.48447647301266394
iteration: 310 loss: 4.3565862885405044 grad: -0.47104172271208145
iteration: 320 loss: 4.2266980557751594 grad: -0.4583168192873035
iteration: 330 loss: 4.104237070394447 grad: -0.44624778738829257
iteration: 340 loss: 3.988589478039309 grad: -0.43478592897811635
iteration: 350 loss: 3.879206846641037 grad: -0.42388719936773084
iteration: 360 loss: 3.775597714738772 grad: -0.4135116689780268
iteration: 370 loss: 3.677320412978547 grad: -0.4036230575419456
iteration: 380 loss: 3.583976940982914 grad: -0.39418832972678886
iteration: 390 loss: 3.4952077232984182 grad: -0.3851773430073674
iteration: 400 loss: 3.410687100953179 grad: -0.3765625401352376
iteration: 410 loss: 3.3301194412590784 grad: -0.3683186797925329
iteration: 420 loss: 3.253235769388456 grad: -0.36042260004262805
iteration: 430 loss: 3.179790842051716 grad: -0.35285301003539404
iteration: 440 loss: 3.1095605971854225 grad: -0.345590306125448
iteration: 450 loss: 3.0423399246000455 grad: -0.3386164091441417
iteration: 460 loss: 2.97794071153856 grad: -0.3319146200516775
iteration: 470 loss: 2.916190124484977 grad: -0.32546949160193717
iteration: 480 loss: 2.856929094642551 grad: -0.3192667139934715
iteration: 490 loss: 2.800010979525227 grad: -0.313293012766961
iteration: 0 loss: 77.40132285198173 grad: 26.635037085372694
iteration: 10 loss: 45.88789014982629 grad: 6.745625007738571
iteration: 20 loss: 34.3395639561768 grad: 0.9159141169726728
iteration: 30 loss: 27.792912330192497 grad: -0.5817486572721963
iteration: 40 loss: 23.427840462455524 grad: -1.008802616961701
iteration: 50 loss: 20.270291531072726 grad: -1.1214330248577802
iteration: 60 loss: 17.867044899384858 grad: -1.128230562115681
iteration: 70 loss: 15.971790192876846 grad: -1.0967317143554043
iteration: 80 loss: 14.436979285696824 grad: -1.0516754318919839
iteration: 90 loss: 13.168022518448094 grad: -1.002942715316319
iteration: 100 loss: 12.10114588676321 grad: -0.95467202580273
iteration: 110 loss: 11.191635407082169 grad: -0.9085916512300538
iteration: 120 loss: 10.40714040134435 grad: -0.8653571324715519
iteration: 130 loss: 9.723642212655147 grad: -0.8251299553696976
iteration: 140 loss: 9.1229169905088 grad: -0.7878430372463098
iteration: 150 loss: 8.590879046085464 grad: -0.753327820980602
iteration: 160 loss: 8.116465217997115 grad: -0.7213767366411508
iteration: 170 loss: 7.690863481511459 grad: -0.6917741306480143
iteration: 180 loss: 7.306967265900849 grad: -0.6643113027387426
iteration: 190 loss: 6.958981654478356 grad: -0.6387933613816528
iteration: 200 loss: 6.642134139316568 grad: -0.615041830306681
iteration: 210 loss: 6.352458810739294 grad: -0.5928950646232611
iteration: 220 loss: 6.086633055189106 grad: -0.5722075734338863
iteration: 230 loss: 5.841852405605261 grad: -0.552848838828927
iteration: 240 loss: 5.615733517757772 grad: -0.5347019480002295
iteration: 250 loss: 5.406238155381785 grad: -0.5176622057515554
iteration: 260 loss: 5.21161305731539 grad: -0.5016358121467956
iteration: 270 loss: 5.030341943701015 grad: -0.48653864437787797
iteration: 280 loss: 4.861106894829638 grad: -0.4722951568807537
iteration: 290 loss: 4.702757034653917 grad: -0.4588374002928808
iteration: 300 loss: 4.554282956874514 grad: -0.44610415298556183
iteration: 310 loss: 4.414795702110206 grad: -0.43404015575318755
iteration: 320 loss: 4.283509369129123 grad: -0.4225954391553893
iteration: 330 loss: 4.159726648377603 grad: -0.4117247330405943
iteration: 340 loss: 4.042826721007962 grad: -0.40138694838516864
iteration: 350 loss: 3.9322550846051687 grad: -0.39154472244936267
iteration: 360 loss: 3.827514957389998 grad: -0.38216401920412546
iteration: 370 loss: 3.7281599827604377 grad: -0.3732137779258682
iteration: 380 loss: 3.6337880106284914 grad: -0.36466560374041324
iteration: 390 loss: 3.5440357748444016 grad: -0.356493494700466
iteration: 400 loss: 3.458574319822591 grad: -0.3486736006962029
iteration: 410 loss: 3.3771050563378098 grad: -0.3411840101273723
iteration: 420 loss: 3.2993563479366954 grad: -0.3340045608135069
iteration: 430 loss: 3.225080546639137 grad: -0.3271166720940135
iteration: 440 loss: 3.154051410532578 grad: -0.32050319548035333
iteration: 450 loss: 3.0860618471625787 grad: -0.31414828157618613
iteration: 460 loss: 3.0209219358346426 grad: -0.30803726128577885
iteration: 470 loss: 2.958457189487035 grad: -0.3021565395928226
iteration: 480 loss: 2.898507023006087 grad: -0.29649350041708755
iteration: 490 loss: 2.8409233999861745 grad: -0.29103642125021256
iteration: 0 loss: 76.3848708887732 grad: 35.120310273546096
iteration: 10 loss: 45.11283771601045 grad: 6.224080030008704
iteration: 20 loss: 34.0251896183275 grad: -0.2437431748129335
iteration: 30 loss: 27.597489072895065 grad: -1.349875599712326
iteration: 40 loss: 23.27078909688097 grad: -1.506263416499916
iteration: 50 loss: 20.12996579436235 grad: -1.4652164470447588
iteration: 60 loss: 17.73676067437532 grad: -1.3822568217974867
iteration: 70 loss: 15.849198087478788 grad: -1.2950027441336647
iteration: 80 loss: 14.321072590077598 grad: -1.2131105560613775
iteration: 90 loss: 13.058218896362591 grad: -1.1386804548511698
iteration: 100 loss: 11.996997530105462 grad: -1.0716346645244206
iteration: 110 loss: 11.092740862338236 grad: -1.0112957156101419
iteration: 120 loss: 10.313121609679188 grad: -0.9568787752547376
iteration: 130 loss: 9.634141357707016 grad: -0.9076446552529374
iteration: 140 loss: 9.037598094345144 grad: -0.8629407170040031
iteration: 150 loss: 8.509429664742253 grad: -0.8222042189768384
iteration: 160 loss: 8.038597126744373 grad: -0.7849538323059633
iteration: 170 loss: 7.616312358407723 grad: -0.7507784017422338
iteration: 180 loss: 7.235491658878138 grad: -0.7193260831720141
iteration: 190 loss: 6.89036152089842 grad: -0.6902948088749803
iteration: 200 loss: 6.576169179627331 grad: -0.6634242346664503
iteration: 210 loss: 6.288966746770587 grad: -0.6384890484580816
iteration: 220 loss: 6.025447947124576 grad: -0.6152934441052372
iteration: 230 loss: 5.782823061593965 grad: -0.5936665625344348
iteration: 240 loss: 5.55872202343868 grad: -0.5734587249244275
iteration: 250 loss: 5.351118533651763 grad: -0.5545383110192159
iteration: 260 loss: 5.158270058407115 grad: -0.5367891624135015
iteration: 270 loss: 4.978669959759785 grad: -0.5201084136645007
iteration: 280 loss: 4.811008990108638 grad: -0.5044046730519762
iteration: 290 loss: 4.654144081140244 grad: -0.4895964901233187
iteration: 300 loss: 4.507072864891432 grad: -0.4756110594046709
iteration: 310 loss: 4.368912735786497 grad: -0.46238311940865534
iteration: 320 loss: 4.238883537292085 grad: -0.4498540138270758
iteration: 330 loss: 4.116293162243644 grad: -0.4379708879780861
iteration: 340 loss: 4.000525510908863 grad: -0.4266859985147302
iteration: 350 loss: 3.891030368823315 grad: -0.41595611835867785
iteration: 360 loss: 3.78731485697819 grad: -0.4057420220060563
iteration: 370 loss: 3.688936176940677 grad: -0.3960080389230084
iteration: 380 loss: 3.595495428024027 grad: -0.38672166483336107
iteration: 390 loss: 3.5066323163757094 grad: -0.37785322239841385
iteration: 400 loss: 3.4220206096198043 grad: -0.36937556417693784
iteration: 410 loss: 3.3413642174668894 grad: -0.36126381189300505
iteration: 420 loss: 3.2643938001366966 grad: -0.35349512697848473
iteration: 430 loss: 3.1908638236093547 grad: -0.3460485081340806
iteration: 440 loss: 3.120549994616009 grad: -0.33890461229806745
iteration: 450 loss: 3.053247019525067 grad: -0.3320455959496612
iteration: 460 loss: 2.9887666404741307 grad: -0.325454974123692
iteration: 470 loss: 2.926935909606279 grad: -0.31911749489062174
iteration: 480 loss: 2.8675956684531396 grad: -0.3130190273736163
iteration: 490 loss: 2.810599204613771 grad: -0.3071464616426145
iteration: 0 loss: 74.80698717816256 grad: 39.55966702572563
iteration: 10 loss: 43.410678433566616 grad: 5.967811892894839
iteration: 20 loss: 32.81305596000363 grad: -0.9614413158639221
iteration: 30 loss: 26.715762858822142 grad: -1.8803103521660047
iteration: 40 loss: 22.60738749196112 grad: -1.886400877488837
iteration: 50 loss: 19.614747983722136 grad: -1.745936566947488
iteration: 60 loss: 17.325361865821833 grad: -1.5960279614178523
iteration: 70 loss: 15.512701695032176 grad: -1.4621434104616229
iteration: 80 loss: 14.040035563512488 grad: -1.3468010405766555
iteration: 90 loss: 12.819193889856706 grad: -1.2477916733608894
iteration: 100 loss: 11.79044951480583 grad: -1.162322139898223
iteration: 110 loss: 10.91175857254019 grad: -1.087936255995193
iteration: 120 loss: 10.152596407080678 grad: -1.0226464280244327
iteration: 130 loss: 9.490229837160086 grad: -0.9648792345543643
iteration: 140 loss: 8.907362026370906 grad: -0.9133914572248456
iteration: 150 loss: 8.390589678869198 grad: -0.8671951840129362
iteration: 160 loss: 7.929361032314049 grad: -0.8254986393070508
iteration: 170 loss: 7.515253446839159 grad: -0.7876612030999719
iteration: 180 loss: 7.141461091520713 grad: -0.7531595772963886
iteration: 190 loss: 6.8024243496641 grad: -0.7215623196659654
iteration: 200 loss: 6.4935570035959245 grad: -0.6925105696035391
iteration: 210 loss: 6.211042244961556 grad: -0.6657033485025377
iteration: 220 loss: 5.951678000945569 grad: -0.6408862537545731
iteration: 230 loss: 5.712758166259184 grad: -0.6178426869230769
iteration: 240 loss: 5.491980356899231 grad: -0.5963869888456914
iteration: 250 loss: 5.287373512115292 grad: -0.5763590212703986
iteration: 260 loss: 5.097240528455421 grad: -0.5576198547444433
iteration: 270 loss: 4.920112403412821 grad: -0.540048309378815
iteration: 280 loss: 4.754711280596149 grad: -0.5235381583883953
iteration: 290 loss: 4.5999204434824605 grad: -0.5079958507007876
iteration: 300 loss: 4.454759780103194 grad: -0.49333864318125087
iteration: 310 loss: 4.318365589780941 grad: -0.4794930584920525
iteration: 320 loss: 4.189973861741857 grad: -0.4663936036773883
iteration: 330 loss: 4.068906349218154 grad: -0.4539816989464761
iteration: 340 loss: 3.954558909154953 grad: -0.44220477704623007
iteration: 350 loss: 3.846391689390161 grad: -0.4310155219614145
iteration: 360 loss: 3.743920831042322 grad: -0.4203712221032678
iteration: 370 loss: 3.6467114204078412 grad: -0.4102332181233928
iteration: 380 loss: 3.554371476561135 grad: -0.40056642936940684
iteration: 390 loss: 3.466546801638494 grad: -0.3913389460432334
iteration: 400 loss: 3.3829165530169534 grad: -0.3825216765267217
iteration: 410 loss: 3.3031894222334897 grad: -0.3740880412487836
iteration: 420 loss: 3.227100325986736 grad: -0.366013705994034
iteration: 430 loss: 3.1544075310652757 grad: -0.35827634877903763
iteration: 440 loss: 3.0848901483528937 grad: -0.35085545541300645
iteration: 450 loss: 3.0183459419124783 grad: -0.3437321396645642
iteration: 460 loss: 2.95458940796334 grad: -0.3368889846132465
iteration: 470 loss: 2.8934500858239187 grad: -0.3303099023035041
iteration: 480 loss: 2.834771068856133 grad: -0.32398000926332465
iteration: 490 loss: 2.7784076883659012 grad: -0.31788551581755453
iteration: 0 loss: 76.5654843874588 grad: 24.477918910675154
iteration: 10 loss: 44.77153654714678 grad: 6.893968497402993
iteration: 20 loss: 33.73805204310704 grad: 1.324225028488497
iteration: 30 loss: 27.405178430699607 grad: -0.3401619431618613
iteration: 40 loss: 23.14218501596557 grad: -0.903642055503837
iteration: 50 loss: 20.040443465160788 grad: -1.0945973944950778
iteration: 60 loss: 17.67154588407557 grad: -1.1425368800614144
iteration: 70 loss: 15.799609532672362 grad: -1.1315937832566993
iteration: 80 loss: 14.281887055373648 grad: -1.0960834065958367
iteration: 90 loss: 13.026192131492115 grad: -1.0510581535899828
iteration: 100 loss: 11.970050454165941 grad: -1.0034199475189665
iteration: 110 loss: 11.0695022371477 grad: -0.9563820739852098
iteration: 120 loss: 10.29266307604404 grad: -0.9114047897639067
iteration: 130 loss: 9.615819131719098 grad: -0.86908787008003
iteration: 140 loss: 9.020955674767224 grad: -0.8296023461555723
iteration: 150 loss: 8.494136503108434 grad: -0.7929070622665715
iteration: 160 loss: 8.02440908297496 grad: -0.758860090479766
iteration: 170 loss: 7.603045679829847 grad: -0.7272768577946311
iteration: 180 loss: 7.223005538389068 grad: -0.6979604856391175
iteration: 190 loss: 6.878546204572723 grad: -0.6707173352596467
iteration: 200 loss: 6.564937727698427 grad: -0.6453645755706534
iteration: 210 loss: 6.278249238803213 grad: -0.6217334332981321
iteration: 220 loss: 6.015187347076624 grad: -0.5996701249314208
iteration: 230 loss: 5.772972226103362 grad: -0.5790355756552228
iteration: 240 loss: 5.549241508111498 grad: -0.5597045393093042
iteration: 250 loss: 5.341974963567778 grad: -0.5415644596640428
iteration: 260 loss: 5.149434902514134 grad: -0.524514259076394
iteration: 270 loss: 4.970118597868019 grad: -0.5084631531901425
iteration: 280 loss: 4.80271999433663 grad: -0.4933295407900717
iteration: 290 loss: 4.646098656312461 grad: -0.4790399900108725
iteration: 300 loss: 4.499254408036615 grad: -0.46552832665045063
iteration: 310 loss: 4.3613064858304 grad: -0.4527348220723575
iteration: 320 loss: 4.231476293747206 grad: -0.44060547405136496
iteration: 330 loss: 4.109073057205726 grad: -0.42909137214124166
iteration: 340 loss: 3.9934818226252404 grad: -0.41814813868473205
iteration: 350 loss: 3.884153367981815 grad: -0.4077354368346433
iteration: 360 loss: 3.780595678973937 grad: -0.3978165375582209
iteration: 370 loss: 3.6823667149396897 grad: -0.3883579383513283
iteration: 380 loss: 3.5890682428102165 grad: -0.37932902718005856
iteration: 390 loss: 3.5003405598485684 grad: -0.3707017859335209
iteration: 400 loss: 3.415857959469494 grad: -0.3624505283818298
iteration: 410 loss: 3.3353248210733 grad: -0.35455166827468165
iteration: 420 loss: 3.258472226123765 grad: -0.3469835137854618
iteration: 430 loss: 3.1850550197970153 grad: -0.33972608500605606
iteration: 440 loss: 3.114849251347633 grad: -0.33276095163372965
iteration: 450 loss: 3.047649937540138 grad: -0.32607108836995996
iteration: 460 loss: 2.9832691026421427 grad: -0.31964074587859503
iteration: 470 loss: 2.9215340559593312 grad: -0.3134553354335177
iteration: 480 loss: 2.8622858740467647 grad: -0.3075013256300056
iteration: 490 loss: 2.8053780598302596 grad: -0.30176614974443705
iteration: 0 loss: 75.96286061733899 grad: 31.22242606131404
iteration: 10 loss: 44.31197881163658 grad: 7.5701197781695715
iteration: 20 loss: 33.26772919928542 grad: 0.2531499039131273
iteration: 30 loss: 27.00543591431297 grad: -1.3063179017825368
iteration: 40 loss: 22.808493200564314 grad: -1.5935104691698125
iteration: 50 loss: 19.7613912872946 grad: -1.5779664364468062
iteration: 60 loss: 17.436236071984137 grad: -1.4899767531323025
iteration: 70 loss: 15.599039609292483 grad: -1.3903493984878765
iteration: 80 loss: 14.108979883528512 grad: -1.2957357188490781
iteration: 90 loss: 12.87547541232797 grad: -1.2101362572085548
iteration: 100 loss: 11.837302103732595 grad: -1.1337690119024693
iteration: 110 loss: 10.951450261111797 grad: -1.0657774834465148
iteration: 120 loss: 10.186752637431585 grad: -1.005093487347048
iteration: 130 loss: 9.520037651815839 grad: -0.9507071170126042
iteration: 140 loss: 8.933702977825657 grad: -0.9017383966274093
iteration: 150 loss: 8.414128500433476 grad: -0.8574427398478667
iteration: 160 loss: 7.950606038840533 grad: -0.8171961524089167
iteration: 170 loss: 7.534598426781982 grad: -0.7804762517887621
iteration: 180 loss: 7.159214830419015 grad: -0.74684445886523
iteration: 190 loss: 6.818831740369743 grad: -0.7159308281550748
iteration: 200 loss: 6.508814343101263 grad: -0.6874216115922984
iteration: 210 loss: 6.225308456826621 grad: -0.6610492188912392
iteration: 220 loss: 5.965082964616985 grad: -0.636584151123999
iteration: 230 loss: 5.7254089669707104 grad: -0.613828516034193
iteration: 240 loss: 5.503966023871354 grad: -0.5926107976127438
iteration: 250 loss: 5.298768645892406 grad: -0.572781617355981
iteration: 260 loss: 5.108108103573655 grad: -0.5542102805752017
iteration: 270 loss: 4.930505952920777 grad: -0.5367819463564838
iteration: 280 loss: 4.764676612947411 grad: -0.52039529528279
iteration: 290 loss: 4.6094970025759805 grad: -0.5049605965513968
iteration: 300 loss: 4.46398173072124 grad: -0.4903980973441982
iteration: 310 loss: 4.327262690047312 grad: -0.4766366736719523
iteration: 320 loss: 4.198572169145667 grad: -0.4636126945580782
iteration: 330 loss: 4.0772287956319815 grad: -0.45126906123501687
iteration: 340 loss: 3.962625772042925 grad: -0.43955439066792457
iteration: 350 loss: 3.854220980196508 grad: -0.42842231870473996
iteration: 360 loss: 3.7515286171163678 grad: -0.41783090286257135
iteration: 370 loss: 3.6541120932569027 grad: -0.4077421084885387
iteration: 380 loss: 3.5615779765212623 grad: -0.3981213649993069
iteration: 390 loss: 3.473570806966522 grad: -0.38893718127565347
iteration: 400 loss: 3.38976863977537 grad: -0.38016081119513
iteration: 410 loss: 3.309879200091845 grad: -0.3717659618261905
iteration: 420 loss: 3.233636554059103 grad: -0.36372853805739236
iteration: 430 loss: 3.1607982171298814 grad: -0.3560264184548898
iteration: 440 loss: 3.0911426341706583 grad: -0.34863925797664086
iteration: 450 loss: 3.024466976876118 grad: -0.34154831385893347
iteration: 460 loss: 2.9605852129043755 grad: -0.33473629155857126
iteration: 470 loss: 2.899326408486931 grad: -0.32818720810502117
iteration: 480 loss: 2.8405332322953325 grad: -0.32188627060902186
iteration: 490 loss: 2.784060633301372 grad: -0.3158197680019872
iteration: 0 loss: 76.11795081564247 grad: 32.534623871661125
iteration: 10 loss: 44.20148769387315 grad: 7.110111199370417
iteration: 20 loss: 33.404473831669726 grad: 0.28955249252846016
iteration: 30 loss: 27.199874586067814 grad: -1.1733960412529143
iteration: 40 loss: 23.015066289288473 grad: -1.4656243913102287
iteration: 50 loss: 19.964055763408602 grad: -1.467993085901902
iteration: 60 loss: 17.62870229138734 grad: -1.3956626964427006
iteration: 70 loss: 15.779086824143095 grad: -1.3080927677439353
iteration: 80 loss: 14.276231936261473 grad: -1.2227021839043481
iteration: 90 loss: 13.030400112278441 grad: -1.1443085415531007
iteration: 100 loss: 11.980730543158312 grad: -1.0737216244356909
iteration: 110 loss: 11.084339198049252 grad: -1.0104835346171244
iteration: 120 loss: 10.310066936280277 grad: -0.9537929062625456
iteration: 130 loss: 9.634694029773321 grad: -0.902821285291022
iteration: 140 loss: 9.040545616765918 grad: -0.8568151281095956
iteration: 150 loss: 8.513921260369216 grad: -0.8151203766797509
iteration: 160 loss: 8.044033166816895 grad: -0.7771796539804823
iteration: 170 loss: 7.622269392649245 grad: -0.7425208781578235
iteration: 180 loss: 7.241670925301853 grad: -0.7107443423707566
iteration: 190 loss: 6.896553170328968 grad: -0.6815107567677516
iteration: 200 loss: 6.582227158460103 grad: -0.654530972414678
iteration: 210 loss: 6.294790996428591 grad: -0.6295574311121604
iteration: 220 loss: 6.03097168220252 grad: -0.6063771520594966
iteration: 230 loss: 5.788003609244739 grad: -0.5848060099142194
iteration: 240 loss: 5.563534183386434 grad: -0.5646840692588773
iteration: 250 loss: 5.355549737700543 grad: -0.5458717726068354
iteration: 260 loss: 5.16231682477924 grad: -0.5282468147118375
iteration: 270 loss: 4.98233528587831 grad: -0.5117015683829935
iteration: 280 loss: 4.814300429998019 grad: -0.49614095431252403
iteration: 290 loss: 4.657072325230936 grad: -0.4814806695661593
iteration: 300 loss: 4.50965069045651 grad: -0.46764570700470914
iteration: 310 loss: 4.371154232062633 grad: -0.4545691117943045
iteration: 320 loss: 4.240803534980222 grad: -0.442190932070116
iteration: 330 loss: 4.117906815572786 grad: -0.43045732938001585
iteration: 340 loss: 4.001847993867821 grad: -0.4193198212659043
iteration: 350 loss: 3.8920766569713123 grad: -0.4087346336481376
iteration: 360 loss: 3.7880995734558383 grad: -0.3986621448794727
iteration: 370 loss: 3.6894734866348036 grad: -0.3890664066742211
iteration: 380 loss: 3.5957989677913247 grad: -0.3799147297847766
iteration: 390 loss: 3.5067151522033453 grad: -0.37117732443696805
iteration: 400 loss: 3.4218952137984657 grad: -0.3628269872602118
iteration: 410 loss: 3.3410424605431217 grad: -0.35483882784518683
iteration: 420 loss: 3.263886953661685 grad: -0.34719002919829167
iteration: 430 loss: 3.1901825706626865 grad: -0.3398596372911694
iteration: 440 loss: 3.1197044458129066 grad: -0.33282837566631546
iteration: 450 loss: 3.0522467327711884 grad: -0.32607848168866826
iteration: 460 loss: 2.9876206431561267 grad: -0.31959356155374996
iteration: 470 loss: 2.9256527222284023 grad: -0.3133584615956846
iteration: 480 loss: 2.866183328986236 grad: -0.30735915379951023
iteration: 490 loss: 2.809065293008726 grad: -0.30158263372457705
iteration: 0 loss: 75.4996334644275 grad: 29.340187954866867
iteration: 10 loss: 42.656179283620354 grad: 6.480643498554948
iteration: 20 loss: 32.29990150162283 grad: 0.6735388802696505
iteration: 30 loss: 26.382438393827588 grad: -0.7897317734416046
iteration: 40 loss: 22.38477182056214 grad: -1.2007575349718373
iteration: 50 loss: 19.4615491474392 grad: -1.2969985224508913
iteration: 60 loss: 17.217458776564772 grad: -1.2865228769834347
iteration: 70 loss: 15.435469457018247 grad: -1.2385424596059935
iteration: 80 loss: 13.984229825382272 grad: -1.1787365951585067
iteration: 90 loss: 12.778748355922557 grad: -1.1172367234351988
iteration: 100 loss: 11.76125305094948 grad: -1.0580783732486778
iteration: 110 loss: 10.890947528572145 grad: -1.0027529067585497
iteration: 120 loss: 10.138128837372484 grad: -0.9516538500380998
iteration: 130 loss: 9.480620204965094 grad: -0.9047014075836272
iteration: 140 loss: 8.901510702903414 grad: -0.8616228585546144
iteration: 150 loss: 8.387670337486373 grad: -0.8220808802435022
iteration: 160 loss: 7.928744475137012 grad: -0.7857319586798354
iteration: 170 loss: 7.516454976607168 grad: -0.752251818170556
iteration: 180 loss: 7.144103464622069 grad: -0.7213451189688178
iteration: 190 loss: 6.8062112595983475 grad: -0.6927477075056048
iteration: 200 loss: 6.4982538164103145 grad: -0.6662254685854334
iteration: 210 loss: 6.216461813795377 grad: -0.6415717709826944
iteration: 220 loss: 5.957670092328871 grad: -0.6186044773519572
iteration: 230 loss: 5.719201490373755 grad: -0.5971629771402815
iteration: 240 loss: 5.498776498950644 grad: -0.577105443362067
iteration: 250 loss: 5.294442267760091 grad: -0.5583063855057333
iteration: 260 loss: 5.104516287243314 grad: -0.5406545084488239
iteration: 270 loss: 4.927541322199431 grad: -0.5240508587959307
iteration: 280 loss: 4.7622490578671455 grad: -0.508407228821228
iteration: 290 loss: 4.6075305546676395 grad: -0.49364478550232993
iteration: 300 loss: 4.462412069349946 grad: -0.47969289345452776
iteration: 310 loss: 4.32603513940661 grad: -0.4664881035853294
iteration: 320 loss: 4.197640079505246 grad: -0.45397328281477617
iteration: 330 loss: 4.076552227566977 grad: -0.44209686368130197
iteration: 340 loss: 3.962170421096289 grad: -0.4308121958268315
iteration: 350 loss: 3.8539572935172313 grad: -0.4200769841414218
iteration: 360 loss: 3.751431064259164 grad: -0.40985280073969954
iteration: 370 loss: 3.654158561461662 grad: -0.40010465996759736
iteration: 380 loss: 3.5617492670346773 grad: -0.3908006473413305
iteration: 390 loss: 3.4738502137736096 grad: -0.3819115947466628
iteration: 400 loss: 3.3901415958816457 grad: -0.3734107954181695
iteration: 410 loss: 3.3103329794034835 grad: -0.36527375321358974
iteration: 420 loss: 3.2341600192328106 grad: -0.3574779615302822
iteration: 430 loss: 3.1613816055799115 grad: -0.35000270790702226
iteration: 440 loss: 3.0917773758775513 grad: -0.34282890093805907
iteration: 450 loss: 3.0251455387874557 grad: -0.33593891661666775
iteration: 460 loss: 2.9613009656616835 grad: -0.3293164616382868
iteration: 470 loss: 2.9000735119572956 grad: -0.32294645154175017
iteration: 480 loss: 2.8413065369836246 grad: -0.3168149018619115
iteration: 490 loss: 2.7848555952358023 grad: -0.310908830716971
iteration: 0 loss: 74.81956471894647 grad: 41.30340746356789
iteration: 10 loss: 43.27566477012206 grad: 4.429311525228398
iteration: 20 loss: 32.81143125484353 grad: -0.7773572083572298
iteration: 30 loss: 26.727378780677554 grad: -1.445356752068135
iteration: 40 loss: 22.619342724799253 grad: -1.4932261203705175
iteration: 50 loss: 19.624371699308934 grad: -1.4273927759011915
iteration: 60 loss: 17.33221406528444 grad: -1.3415009290446935
iteration: 70 loss: 15.516993605135275 grad: -1.2571296544973474
iteration: 80 loss: 14.042145812375276 grad: -1.1794389261412963
iteration: 90 loss: 12.819503832602384 grad: -1.109209648603633
iteration: 100 loss: 11.789291960865388 grad: -1.0459939835047674
iteration: 110 loss: 10.909408434869936 grad: -0.989042187699766
iteration: 120 loss: 10.149275847465436 grad: -0.9375898426688409
iteration: 130 loss: 9.486117227462005 grad: -0.8909433478076443
iteration: 140 loss: 8.902600779041181 grad: -0.8484984914729423
iteration: 150 loss: 8.385295719563125 grad: -0.8097368862101046
iteration: 160 loss: 7.923628740186979 grad: -0.7742158839623068
iteration: 170 loss: 7.509160248805198 grad: -0.7415574970139951
iteration: 180 loss: 7.135070980055182 grad: -0.7114381955333542
iteration: 190 loss: 6.795790575672485 grad: -0.6835800969765201
iteration: 200 loss: 6.486724148940704 grad: -0.6577435653576964
iteration: 210 loss: 6.204047831232773 grad: -0.6337210707086164
iteration: 220 loss: 5.944553750862145 grad: -0.6113321186936654
iteration: 230 loss: 5.705531003122773 grad: -0.5904190675762702
iteration: 240 loss: 5.484673204636692 grad: -0.5708436726176058
iteration: 250 loss: 5.280005941848561 grad: -0.5524842238391438
iteration: 260 loss: 5.089829285620164 grad: -0.5352331671123262
iteration: 270 loss: 4.912671840863285 grad: -0.5189951192395306
iteration: 280 loss: 4.74725371694653 grad: -0.5036852048779273
iteration: 290 loss: 4.592456461422774 grad: -0.4892276571525468
iteration: 300 loss: 4.447298476100886 grad: -0.4755546350789122
iteration: 310 loss: 4.310914784111571 grad: -0.46260521994686243
iteration: 320 loss: 4.182540275921507 grad: -0.4503245600332818
iteration: 330 loss: 4.0614957564833025 grad: -0.43866313877441176
iteration: 340 loss: 3.9471762625463187 grad: -0.42757614613494016
iteration: 350 loss: 3.8390412311232467 grad: -0.4170229366003775
iteration: 360 loss: 3.7366061861849134 grad: -0.4069665601817522
iteration: 370 loss: 3.6394356773373393 grad: -0.39737335520820705
iteration: 380 loss: 3.5471372562470522 grad: -0.38821259361248545
iteration: 390 loss: 3.4593563174468973 grad: -0.37945617098009093
iteration: 400 loss: 3.3757716624545537 grad: -0.37107833490871406
iteration: 410 loss: 3.296091671806401 grad: -0.36305544626824504
iteration: 420 loss: 3.2200509901756713 grad: -0.35536576880908666
iteration: 430 loss: 3.147407646248749 grad: -0.34798928327365025
iteration: 440 loss: 3.077940542389652 grad: -0.3409075227515497
iteration: 450 loss: 3.0114472599784676 grad: -0.33410342650586006
iteration: 460 loss: 2.9477421351557496 grad: -0.327561209904133
iteration: 470 loss: 2.8866545669682804 grad: -0.3212662484281781
iteration: 480 loss: 2.8280275258807706 grad: -0.31520497402272385
iteration: 490 loss: 2.7717162355706932 grad: -0.30936478228439096
iteration: 0 loss: 72.70075181044832 grad: 41.16656869413077
iteration: 10 loss: 41.926817565624994 grad: 4.180442549821083
iteration: 20 loss: 32.04228696936311 grad: -0.9671253972592322
iteration: 30 loss: 26.2540473621561 grad: -1.5773120221469312
iteration: 40 loss: 22.313988176128614 grad: -1.5961421153563708
iteration: 50 loss: 19.421602027682308 grad: -1.5110753350213493
iteration: 60 loss: 17.195271269397153 grad: -1.4110357745915798
iteration: 70 loss: 15.42380826784812 grad: -1.3159666705717061
iteration: 80 loss: 13.978825940175925 grad: -1.2301136850758803
iteration: 90 loss: 12.776987524920886 grad: -1.1536111177081305
iteration: 100 loss: 11.761499084448257 grad: -1.0855268621024772
iteration: 110 loss: 10.892164112549091 grad: -1.0247485028934087
iteration: 120 loss: 10.139657183582916 grad: -0.9702429190120934
iteration: 130 loss: 9.482043626443017 grad: -0.9211192723816268
iteration: 140 loss: 8.902569938715404 grad: -0.8766294009173847
iteration: 150 loss: 8.388209512362334 grad: -0.8361501083135402
iteration: 160 loss: 7.928676035163004 grad: -0.7991623958833183
iteration: 170 loss: 7.515736624747288 grad: -0.7652324773361869
iteration: 180 loss: 7.14272281501272 grad: -0.7339958061362968
iteration: 190 loss: 6.804175529264409 grad: -0.7051440955763415
iteration: 200 loss: 6.495582846500053 grad: -0.6784149280268335
iteration: 210 loss: 6.2131833168654005 grad: -0.6535834821237547
iteration: 220 loss: 5.953816403746879 grad: -0.6304559476038811
iteration: 230 loss: 5.714807346031318 grad: -0.6088642682364689
iteration: 240 loss: 5.4938775195605976 grad: -0.5886619245603562
iteration: 250 loss: 5.289073933456951 grad: -0.5697205301417974
iteration: 260 loss: 5.098713254641098 grad: -0.5519270657034501
iteration: 270 loss: 4.9213369817862676 grad: -0.5351816154893827
iteration: 280 loss: 4.755675260504879 grad: -0.5193955012768795
iteration: 290 loss: 4.600617457072283 grad: -0.5044897332828142
iteration: 300 loss: 4.455188062984565 grad: -0.4903937154126451
iteration: 310 loss: 4.3185268373946535 grad: -0.47704415616501444
iteration: 320 loss: 4.18987234334488 grad: -0.4643841470694629
iteration: 330 loss: 4.068548220526116 grad: -0.45236237860187345
iteration: 340 loss: 3.9539516788431683 grad: -0.44093246970601735
iteration: 350 loss: 3.8455438051839583 grad: -0.4300523918143853
iteration: 360 loss: 3.7428413590719174 grad: -0.41968397195635787
iteration: 370 loss: 3.645409797475705 grad: -0.4097924624276539
iteration: 380 loss: 3.5528573195450175 grad: -0.40034616676673085
iteration: 390 loss: 3.4648297617314214 grad: -0.39131611358651974
iteration: 400 loss: 3.3810062051970817 grad: -0.38267577125206065
iteration: 410 loss: 3.3010951824218697 grad: -0.3744007975574457
iteration: 420 loss: 3.224831389973298 grad: -0.3664688195001111
iteration: 430 loss: 3.1519728305236394 grad: -0.358859239023228
iteration: 440 loss: 3.082298320261006 grad: -0.3515530612330541
iteration: 450 loss: 3.015605308451334 grad: -0.34453274212476803
iteration: 460 loss: 2.9517079645828 grad: -0.33778205328865485
iteration: 470 loss: 2.8904354956421643 grad: -0.33128596143500333
iteration: 480 loss: 2.8316306619283917 grad: -0.32503052088382456
iteration: 490 loss: 2.7751484646761044 grad: -0.31900277742488536
iteration: 0 loss: 76.0318344891532 grad: 34.446589631849974
iteration: 10 loss: 44.409493415812726 grad: 7.36926271083519
iteration: 20 loss: 33.38343555702331 grad: -0.06331182981184066
iteration: 30 loss: 27.096313574761574 grad: -1.4878986982619926
iteration: 40 loss: 22.879158574078225 grad: -1.7118787064628294
iteration: 50 loss: 19.817513016281488 grad: -1.6672674078706382
iteration: 60 loss: 17.481850679828252 grad: -1.5627690801263237
iteration: 70 loss: 15.636960468672958 grad: -1.4517816385517903
iteration: 80 loss: 14.141189473984634 grad: -1.3484651826024199
iteration: 90 loss: 12.903388197766018 grad: -1.2558708072839169
iteration: 100 loss: 11.861939632456307 grad: -1.1737620969462195
iteration: 110 loss: 10.97355917205451 grad: -1.101006180518378
iteration: 120 loss: 10.206884981910372 grad: -1.0363380311948767
iteration: 130 loss: 9.538606103537631 grad: -0.9785969573250399
iteration: 140 loss: 8.951019194250607 grad: -0.926784378749087
iteration: 150 loss: 8.430430091102505 grad: -0.880061642620197
iteration: 160 loss: 7.966075754426418 grad: -0.8377299322130161
iteration: 170 loss: 7.5493779759763955 grad: -0.7992072075352136
iteration: 180 loss: 7.173414941612753 grad: -0.7640071628276919
iteration: 190 loss: 6.832539565981485 grad: -0.7317214952900536
iteration: 200 loss: 6.522098960973918 grad: -0.7020054567465599
iteration: 210 loss: 6.238224993620613 grad: -0.6745662453823055
iteration: 220 loss: 5.97767571097959 grad: -0.6491537222045007
iteration: 230 loss: 5.737713748064669 grad: -0.6255529835648838
iteration: 240 loss: 5.516012015192204 grad: -0.6035783997247341
iteration: 250 loss: 5.310579772634975 grad: -0.5830688075584936
iteration: 260 loss: 5.119704125119452 grad: -0.5638836125990401
iteration: 270 loss: 4.941903307684118 grad: -0.5458996098539227
iteration: 280 loss: 4.775889079748677 grad: -0.5290083753700792
iteration: 290 loss: 4.620536220757706 grad: -0.5131141134369043
iteration: 300 loss: 4.474857610948799 grad: -0.4981318696212908
iteration: 310 loss: 4.337983740105301 grad: -0.4839860392557094
iteration: 320 loss: 4.209145753321454 grad: -0.47060911593849997
iteration: 330 loss: 4.087661342005007 grad: -0.4579406361293578
iteration: 340 loss: 3.9729229387288054 grad: -0.44592628485178876
iteration: 350 loss: 3.8643877891406855 grad: -0.4345171344672849
iteration: 360 loss: 3.7615695621280296 grad: -0.4236689939296149
iteration: 370 loss: 3.664031227528953 grad: -0.41334185021338476
iteration: 380 loss: 3.5713789837544607 grad: -0.40349938700412413
iteration: 390 loss: 3.483257059352871 grad: -0.39410856843853603
iteration: 400 loss: 3.3993432454168544 grad: -0.3851392778459394
iteration: 410 loss: 3.3193450418916655 grad: -0.3765640031821325
iteration: 420 loss: 3.242996321724728 grad: -0.36835756225449856
iteration: 430 loss: 3.170054433584193 grad: -0.36049686198143727
iteration: 440 loss: 3.100297677424144 grad: -0.35296068686381604
iteration: 450 loss: 3.0335230981902233 grad: -0.34572951261299295
iteration: 460 loss: 2.9695445519312837 grad: -0.33878534151199347
iteration: 470 loss: 2.9081910059319607 grad: -0.33211155660945413
iteration: 480 loss: 2.849305040541739 grad: -0.32569279228060066
iteration: 490 loss: 2.792741525364239 grad: -0.319514819052065
iteration: 0 loss: 76.6821801817465 grad: 26.272183989010998
iteration: 10 loss: 44.59053674997081 grad: 6.997509232364973
iteration: 20 loss: 33.532273228439976 grad: 0.9727058795813918
iteration: 30 loss: 27.24014726941085 grad: -0.6537397573290986
iteration: 40 loss: 23.010623884786295 grad: -1.1253961027662154
iteration: 50 loss: 19.9333033579975 grad: -1.2440054786855943
iteration: 60 loss: 17.582348152733235 grad: -1.2425509864405322
iteration: 70 loss: 15.723874625645005 grad: -1.1988635919666026
iteration: 80 loss: 14.216473354611377 grad: -1.1416134016045416
iteration: 90 loss: 12.96884965743185 grad: -1.0819872093207379
iteration: 100 loss: 11.919133948043013 grad: -1.024398989982833
iteration: 110 loss: 11.023785274835808 grad: -0.9704783594524724
iteration: 120 loss: 10.251215348487015 grad: -0.9206713123729082
iteration: 130 loss: 9.577923707755211 grad: -0.8749199676646007
iteration: 140 loss: 8.986052061778269 grad: -0.832962749617418
iteration: 150 loss: 8.461780706387572 grad: -0.7944697104557588
iteration: 160 loss: 7.994245099545093 grad: -0.7591033320544975
iteration: 170 loss: 7.574784833109954 grad: -0.7265447124572331
iteration: 180 loss: 7.196411284027942 grad: -0.696503466838932
iteration: 190 loss: 6.853422814284799 grad: -0.6687200138397668
iteration: 200 loss: 6.5411217610560675 grad: -0.6429644284932146
iteration: 210 loss: 6.255603045355201 grad: -0.6190338926266247
iteration: 220 loss: 5.993594064634428 grad: -0.5967497210156513
iteration: 230 loss: 5.752331893711181 grad: -0.5759544195721658
iteration: 240 loss: 5.529468017962534 grad: -0.5565089721915384
iteration: 250 loss: 5.322993650379431 grad: -0.5382904249843308
iteration: 260 loss: 5.131180621607985 grad: -0.5211897754284391
iteration: 270 loss: 4.952534181086073 grad: -0.5051101465664569
iteration: 280 loss: 4.78575500042168 grad: -0.4899652158845871
iteration: 290 loss: 4.629708352524176 grad: -0.47567786627745345
iteration: 300 loss: 4.48339893467842 grad: -0.46217902808048117
iteration: 310 loss: 4.345950166460542 grad: -0.44940668428022656
iteration: 320 loss: 4.21658706220077 grad: -0.43730501457409643
iteration: 330 loss: 4.094621978874454 grad: -0.42582365741574296
iteration: 340 loss: 3.979442692260446 grad: -0.41491707232560393
iteration: 350 loss: 3.8705023699892327 grad: -0.40454398749443343
iteration: 360 loss: 3.76731109903151 grad: -0.3946669200613008
iteration: 370 loss: 3.6694286939905396 grad: -0.3852517584389863
iteration: 380 loss: 3.576458566230072 grad: -0.37626739773314155
iteration: 390 loss: 3.4880424759536997 grad: -0.36768542070262056
iteration: 400 loss: 3.403856022618253 grad: -0.35947981787952243
iteration: 410 loss: 3.323604755476357 grad: -0.35162674144618655
iteration: 420 loss: 3.24702080717076 grad: -0.34410428828484774
iteration: 430 loss: 3.1738599702631207 grad: -0.33689230830100503
iteration: 440 loss: 3.1038991502895112 grad: -0.3299722346964171
iteration: 450 loss: 3.036934140059698 grad: -0.3233269333508003
iteration: 460 loss: 2.9727776689911396 grad: -0.3169405688782505
iteration: 470 loss: 2.91125768870002 grad: -0.31079848526799936
iteration: 480 loss: 2.8522158621912324 grad: -0.30488709930982494
iteration: 490 loss: 2.7955062290376933 grad: -0.2991938052510729
iteration: 0 loss: 72.84083524121456 grad: 46.97354622084204
iteration: 10 loss: 42.842497131755934 grad: 3.819711060420882
iteration: 20 loss: 32.89667033313183 grad: -1.2016751243859283
iteration: 30 loss: 26.94114107135922 grad: -1.7100193617878081
iteration: 40 loss: 22.86252176781576 grad: -1.6968264500140577
iteration: 50 loss: 19.86575491549198 grad: -1.5972668977906164
iteration: 60 loss: 17.561245977284237 grad: -1.4873872655972726
iteration: 70 loss: 15.730448021772004 grad: -1.3842862105428202
iteration: 80 loss: 14.239638818581936 grad: -1.2915279034595226
iteration: 90 loss: 13.00178364165725 grad: -1.2090175318315697
iteration: 100 loss: 11.957527871148427 grad: -1.135688472739122
iteration: 110 loss: 11.064879245753817 grad: -1.0703191367217053
iteration: 120 loss: 10.293231470297894 grad: -1.011781757034628
iteration: 130 loss: 9.61971986769043 grad: -0.9591041497578023
iteration: 140 loss: 9.02690394938579 grad: -0.911469246026807
iteration: 150 loss: 8.501241377390091 grad: -0.8681959330935676
iteration: 160 loss: 8.032052862388438 grad: -0.8287166169393168
iteration: 170 loss: 7.610801970485557 grad: -0.7925566659041389
iteration: 180 loss: 7.23058283256391 grad: -0.7593170936388349
iteration: 190 loss: 6.885748600804701 grad: -0.7286604745530094
iteration: 200 loss: 6.571637316612189 grad: -0.7002996556766425
iteration: 210 loss: 6.284366530305595 grad: -0.6739887526389607
iteration: 220 loss: 6.020677300938752 grad: -0.64951596396262
iteration: 230 loss: 5.7778142244826745 grad: -0.6266978170005799
iteration: 240 loss: 5.553432124639957 grad: -0.6053745372454717
iteration: 250 loss: 5.345522731267672 grad: -0.5854062998378003
iteration: 260 loss: 5.152356519787935 grad: -0.5666701761210882
iteration: 270 loss: 4.972436175107512 grad: -0.5490576303504756
iteration: 280 loss: 4.8044590572973735 grad: -0.5324724542645081
iteration: 290 loss: 4.6472867021576265 grad: -0.5168290522346892
iteration: 300 loss: 4.499919866373561 grad: -0.5020510088550789
iteration: 310 loss: 4.361477977290923 grad: -0.4880698855126061
iteration: 320 loss: 4.231182107532649 grad: -0.4748242037616244
iteration: 330 loss: 4.108340789875084 grad: -0.46225858203205933
iteration: 340 loss: 3.992338135557723 grad: -0.4503229989485579
iteration: 350 loss: 3.8826238320108875 grad: -0.43897216179518583
iteration: 360 loss: 3.77870468281927 grad: -0.4281649627782066
iteration: 370 loss: 3.6801374200563672 grad: -0.4178640089835869
iteration: 380 loss: 3.586522571713472 grad: -0.4080352144964471
iteration: 390 loss: 3.4974992082710825 grad: -0.3986474451986594
iteration: 400 loss: 3.4127404251661244 grad: -0.3896722084035272
iteration: 410 loss: 3.3319494439349664 grad: -0.3810833808112637
iteration: 420 loss: 3.254856235631244 grad: -0.37285696934333296
iteration: 430 loss: 3.1812145868936814 grad: -0.3649709002897785
iteration: 440 loss: 3.110799542576677 grad: -0.3574048329218573
iteration: 450 loss: 3.0434051698852356 grad: -0.35013999431400233
iteration: 460 loss: 2.978842597949841 grad: -0.3431590326090342
iteration: 470 loss: 2.916938294151061 grad: -0.33644588636800565
iteration: 480 loss: 2.8575325445794575 grad: -0.3299856679864595
iteration: 490 loss: 2.8004781110461905 grad: -0.3237645594443969
iteration: 0 loss: 77.07983003520002 grad: 31.541890479016715
iteration: 10 loss: 45.84201346302046 grad: 6.911714683006025
iteration: 20 loss: 34.523052775092374 grad: 0.32100828059800585
iteration: 30 loss: 28.006151767825827 grad: -1.1005323746330762
iteration: 40 loss: 23.62386963181908 grad: -1.3942861491399106
iteration: 50 loss: 20.441614809601912 grad: -1.4071136684539507
iteration: 60 loss: 18.015320589288606 grad: -1.3454949328557693
iteration: 70 loss: 16.10041050913337 grad: -1.266957781722583
iteration: 80 loss: 14.549226926213494 grad: -1.1887923874748048
iteration: 90 loss: 13.266675162581082 grad: -1.1161049581278117
iteration: 100 loss: 12.188464370986015 grad: -1.050031599189904
iteration: 110 loss: 11.269442153824622 grad: -0.9903884195619306
iteration: 120 loss: 10.476906120303966 grad: -0.9365862561273496
iteration: 130 loss: 9.786560190384535 grad: -0.8879569829315593
iteration: 140 loss: 9.179962168972203 grad: -0.8438676791306817
iteration: 150 loss: 8.642853856361109 grad: -0.8037554117107948
iteration: 160 loss: 8.164034932242554 grad: -0.7671321611463666
iteration: 170 loss: 7.734583539600015 grad: -0.7335790885578892
iteration: 180 loss: 7.347304516030652 grad: -0.7027377108031336
iteration: 190 loss: 6.996330976265406 grad: -0.674300915746385
iteration: 200 loss: 6.676831546641634 grad: -0.6480048615204532
iteration: 210 loss: 6.384791853302431 grad: -0.6236220336235931
iteration: 220 loss: 6.116849132540872 grad: -0.6009554312877909
iteration: 230 loss: 5.870165456579178 grad: -0.5798337495969252
iteration: 240 loss: 5.642329436897274 grad: -0.5601074010498859
iteration: 250 loss: 5.4312792052808785 grad: -0.5416452296883103
iteration: 260 loss: 5.235241483958403 grad: -0.5243317909304883
iteration: 270 loss: 5.052682955331792 grad: -0.5080650918081715
iteration: 280 loss: 4.882271129547669 grad: -0.49275470590532217
iteration: 290 loss: 4.722842614996849 grad: -0.4783201939038794
iteration: 300 loss: 4.573377208939167 grad: -0.464689774227067
iteration: 310 loss: 4.432976600768763 grad: -0.4517991991883749
iteration: 320 loss: 4.300846758472786 grad: -0.43959080075225004
iteration: 330 loss: 4.176283276803216 grad: -0.42801267691915795
iteration: 340 loss: 4.058659122740623 grad: -0.4170179952315487
iteration: 350 loss: 3.9474143334163356 grad: -0.4065643942600056
iteration: 360 loss: 3.8420473134899775 grad: -0.39661346741070524
iteration: 370 loss: 3.7421074500304545 grad: -0.3871303161850946
iteration: 380 loss: 3.647188818293836 grad: -0.3780831622676295
iteration: 390 loss: 3.556924795227174 grad: -0.3694430096321259
iteration: 400 loss: 3.470983431821217 grad: -0.36118334933095975
iteration: 410 loss: 3.3890634626605705 grad: -0.35327990083349436
iteration: 420 loss: 3.3108908527900565 grad: -0.3457103847651728
iteration: 430 loss: 3.236215799493245 grad: -0.3384543227093271
iteration: 440 loss: 3.164810120692643 grad: -0.331492860403636
iteration: 450 loss: 3.096464973141048 grad: -0.3248086112189271
iteration: 460 loss: 3.0309888529023654 grad: -0.3183855172709221
iteration: 470 loss: 2.96820583828255 grad: -0.3122087259024478
iteration: 480 loss: 2.9079540416560534 grad: -0.30626447959825814
iteration: 490 loss: 2.8500842418225147 grad: -0.3005400176678257
iteration: 0 loss: 76.26560330010656 grad: 32.230516644175495
iteration: 10 loss: 44.08447654447488 grad: 6.734980657698688
iteration: 20 loss: 33.28455565569952 grad: 0.2531015206851054
iteration: 30 loss: 27.095958943942005 grad: -1.1093773114160737
iteration: 40 loss: 22.92392232028847 grad: -1.3929027053433904
iteration: 50 loss: 19.883506225814948 grad: -1.4063242388323873
iteration: 60 loss: 17.557500156312244 grad: -1.3465398882250508
iteration: 70 loss: 15.716284622623313 grad: -1.2694335364882485
iteration: 80 loss: 14.220968790882104 grad: -1.1922626862698835
iteration: 90 loss: 12.981863076676643 grad: -1.1202987538714175
iteration: 100 loss: 11.938160981057907 grad: -1.054795455101704
iteration: 110 loss: 11.047043059300698 grad: -0.9956299647395505
iteration: 120 loss: 10.277419794330028 grad: -0.9422405946866842
iteration: 130 loss: 9.606140872613794 grad: -0.8939705955382258
iteration: 140 loss: 9.015597177947113 grad: -0.8501918749575262
iteration: 150 loss: 8.492148510965349 grad: -0.8103442443034358
iteration: 160 loss: 8.025061083239239 grad: -0.7739421542497706
iteration: 170 loss: 7.605770680292897 grad: -0.7405695391312783
iteration: 180 loss: 7.227360075675733 grad: -0.709871056408612
iteration: 190 loss: 6.884181036522725 grad: -0.681542986957975
iteration: 200 loss: 6.571576117255063 grad: -0.6553249822232272
iteration: 210 loss: 6.285670698444929 grad: -0.6309929861615187
iteration: 220 loss: 6.023215355661705 grad: -0.6083533192712011
iteration: 230 loss: 5.7814648660365 grad: -0.5872377915911015
iteration: 240 loss: 5.558084270401733 grad: -0.5674996833111167
iteration: 250 loss: 5.351075176882494 grad: -0.5490104397878076
iteration: 260 loss: 5.158717389136035 grad: -0.53165694817076
iteration: 270 loss: 4.9795222639380965 grad: -0.5153392853581108
iteration: 280 loss: 4.812195136863548 grad: -0.49996884766120997
iteration: 290 loss: 4.655604823931878 grad: -0.48546679013557803
iteration: 300 loss: 4.508758692419303 grad: -0.47176271793714175
iteration: 310 loss: 4.3707821500769235 grad: -0.45879358363106626
iteration: 320 loss: 4.240901666048847 grad: -0.4465027535773808
iteration: 330 loss: 4.118430634467793 grad: -0.43483921379445933
iteration: 340 loss: 4.002757541161207 grad: -0.4237568914519261
iteration: 350 loss: 3.8933360078105 grad: -0.4132140726933611
iteration: 360 loss: 3.789676375458302 grad: -0.40317290109644316
iteration: 370 loss: 3.691338557060838 grad: -0.39359894394876666
iteration: 380 loss: 3.597925941651738 grad: -0.38446081581140096
iteration: 390 loss: 3.5090801742189797 grad: -0.375729850683563
iteration: 400 loss: 3.424476668188659 grad: -0.36737981556686616
iteration: 410 loss: 3.3438207335212113 grad: -0.3593866594312285
iteration: 420 loss: 3.2668442242534184 grad: -0.35172829256478444
iteration: 430 loss: 3.1933026261193103 grad: -0.34438439209230065
iteration: 440 loss: 3.1229725184063515 grad: -0.33733623010613467
iteration: 450 loss: 3.0556493552310466 grad: -0.330566521398467
iteration: 460 loss: 2.991145520382774 grad: -0.324059288235398
iteration: 470 loss: 2.929288617247039 grad: -0.31779973998990074
iteration: 480 loss: 2.8699199613863793 grad: -0.3117741657654457
iteration: 490 loss: 2.8128932483478324 grad: -0.305969838406435
iteration: 0 loss: 75.28809487816771 grad: 36.76917953607043
iteration: 10 loss: 44.797467874223045 grad: 5.463418259477981
iteration: 20 loss: 33.98920969607105 grad: -0.39697163443638717
iteration: 30 loss: 27.63877196674953 grad: -1.3551452054304454
iteration: 40 loss: 23.339043548258587 grad: -1.4978810879907374
iteration: 50 loss: 20.20600739743767 grad: -1.4614987825617285
iteration: 60 loss: 17.812603507658277 grad: -1.3821925608264598
iteration: 70 loss: 15.92149349344777 grad: -1.296251184600276
iteration: 80 loss: 14.388521260241594 grad: -1.2143077418449597
iteration: 90 loss: 13.12046414537595 grad: -1.1392531181869368
iteration: 100 loss: 12.054122009803036 grad: -1.0714378872691004
iteration: 110 loss: 11.145026293983062 grad: -1.0103799296555418
iteration: 120 loss: 10.360930111554621 grad: -0.9553669362207617
iteration: 130 loss: 9.677855887514541 grad: -0.905673743250771
iteration: 140 loss: 9.07759281083802 grad: -0.8606367098423476
iteration: 150 loss: 8.546055873190694 grad: -0.819673406708064
iteration: 160 loss: 8.072177469225277 grad: -0.7822816257844805
iteration: 170 loss: 7.6471393653248265 grad: -0.7480312520139626
iteration: 180 loss: 7.263828556928894 grad: -0.7165544221113427
iteration: 190 loss: 6.916444140432222 grad: -0.6875360678501463
iteration: 200 loss: 6.6002083231834066 grad: -0.6607055573456045
iteration: 210 loss: 6.311150670311153 grad: -0.6358295782692467
iteration: 220 loss: 6.045944769358564 grad: -0.6127061852813956
iteration: 230 loss: 5.801783009900109 grad: -0.5911598611555553
iteration: 240 loss: 5.576279477619499 grad: -0.5710374303909815
iteration: 250 loss: 5.367393858118868 grad: -0.5522046774846314
iteration: 260 loss: 5.173371229189053 grad: -0.5345435428772984
iteration: 270 loss: 4.9926940006418015 grad: -0.5179497909390443
iteration: 280 loss: 4.824043235602982 grad: -0.5023310635616574
iteration: 290 loss: 4.666267284797579 grad: -0.4876052492223254
iteration: 300 loss: 4.518356170892854 grad: -0.47369911080941485
iteration: 310 loss: 4.379420530483582 grad: -0.4605471263904346
iteration: 320 loss: 4.2486741957780145 grad: -0.4480905058610829
iteration: 330 loss: 4.1254197033874584 grad: -0.4362763534327152
iteration: 340 loss: 4.009036172682252 grad: -0.4250569515334374
iteration: 350 loss: 3.8989691142653347 grad: -0.4143891461978394
iteration: 360 loss: 3.794721819801535 grad: -0.4042338176312489
iteration: 370 loss: 3.695848054585936 grad: -0.3945554225394142
iteration: 380 loss: 3.6019458289266697 grad: -0.38532159715896774
iteration: 390 loss: 3.5126520672967505 grad: -0.3765028118225042
iteration: 400 loss: 3.4276380280954815 grad: -0.36807206943512294
iteration: 410 loss: 3.3466053537578606 grad: -0.3600046414980557
iteration: 420 loss: 3.269282652460623 grad: -0.35227783634586957
iteration: 430 loss: 3.1954225299377867 grad: -0.34487079511115043
iteration: 440 loss: 3.124799003880774 grad: -0.33776431162988807
iteration: 450 loss: 3.05720524470218 grad: -0.3309406730799768
iteration: 460 loss: 2.9924515956933893 grad: -0.3243835186268158
iteration: 470 loss: 2.930363833152569 grad: -0.3180777137517117
iteration: 480 loss: 2.870781633292154 grad: -0.3120092382751978
iteration: 490 loss: 2.8135572178600174 grad: -0.3061650863699512
iteration: 0 loss: 75.57774634821088 grad: 37.0600293546232
iteration: 10 loss: 45.35188440352479 grad: 6.553153009214936
iteration: 20 loss: 34.40455825457027 grad: 0.19420658555686743
iteration: 30 loss: 27.974622970969595 grad: -1.1205010943274436
iteration: 40 loss: 23.625780497848133 grad: -1.4233870550529755
iteration: 50 loss: 20.45813891569992 grad: -1.45802583968281
iteration: 60 loss: 18.03799824276879 grad: -1.4102207071972184
iteration: 70 loss: 16.125072511862196 grad: -1.337658499699022
iteration: 80 loss: 14.573735898763413 grad: -1.2605649840501474
iteration: 90 loss: 13.289932908190188 grad: -1.1863147230989828
iteration: 100 loss: 12.209932151782782 grad: -1.1174199866262398
iteration: 110 loss: 11.288892262144042 grad: -1.0544599948699624
iteration: 120 loss: 10.49428621163679 grad: -0.9972457816225838
iteration: 130 loss: 9.801916207252816 grad: -0.9453069689928598
iteration: 140 loss: 9.193393337155014 grad: -0.8980992114961647
iteration: 150 loss: 8.65448597830881 grad: -0.8550910060190442
iteration: 160 loss: 8.174004455151465 grad: -0.8157969896172094
iteration: 170 loss: 7.743028118065405 grad: -0.779787407171626
iteration: 180 loss: 7.354357552162955 grad: -0.7466872662115023
iteration: 190 loss: 7.002118617581056 grad: -0.7161714179963938
iteration: 200 loss: 6.681471205346084 grad: -0.6879584264501626
iteration: 210 loss: 6.388391661237744 grad: -0.6618044907131684
iteration: 220 loss: 6.119507961717153 grad: -0.6374979300111693
iteration: 230 loss: 5.871973270239778 grad: -0.6148543854285595
iteration: 240 loss: 5.643367821833222 grad: -0.5937127344801445
iteration: 250 loss: 5.4316219911182255 grad: -0.573931650125563
iteration: 260 loss: 5.234955390674011 grad: -0.5553867163296282
iteration: 270 loss: 5.051828233236517 grad: -0.5379680128879387
iteration: 280 loss: 4.880902170867144 grad: -0.5215780910058972
iteration: 290 loss: 4.721008525751503 grad: -0.5061302723235002
iteration: 300 loss: 4.571122335923477 grad: -0.4915472151396416
iteration: 310 loss: 4.430341012259222 grad: -0.47775970147413394
iteration: 320 loss: 4.297866679604464 grad: -0.46470560701561514
iteration: 330 loss: 4.1729914819154805 grad: -0.45232902297213295
iteration: 340 loss: 4.0550852876974455 grad: -0.4405795045322224
iteration: 350 loss: 3.943585351215006 grad: -0.42941142525682274
iteration: 360 loss: 3.837987576536679 grad: -0.4187834204485203
iteration: 370 loss: 3.737839102354056 grad: -0.40865790555304954
iteration: 380 loss: 3.6427319807875063 grad: -0.39900065807995555
iteration: 390 loss: 3.552297766780033 grad: -0.38978045350000246
iteration: 400 loss: 3.4662028689486264 grad: -0.38096874717880236
iteration: 410 loss: 3.3841445399915426 grad: -0.3725393957129265
iteration: 420 loss: 3.3058474065350807 grad: -0.3644684121049151
iteration: 430 loss: 3.231060455785692 grad: -0.35673375009342484
iteration: 440 loss: 3.159554410491141 grad: -0.3493151136810152
iteration: 450 loss: 3.0911194351890363 grad: -0.3421937885039077
iteration: 460 loss: 3.025563126075998 grad: -0.33535249218872987
iteration: 470 loss: 2.9627087445034537 grad: -0.3287752412593272
iteration: 480 loss: 2.9023936604028013 grad: -0.3224472325071076
iteration: 490 loss: 2.8444679771709658 grad: -0.3163547370330435
iteration: 0 loss: 75.03703698567824 grad: 36.0992335738984
iteration: 10 loss: 42.5976825739823 grad: 5.848581518724581
iteration: 20 loss: 32.43240326479872 grad: -0.5744605197746153
iteration: 30 loss: 26.538692290979313 grad: -1.6154665814088833
iteration: 40 loss: 22.531007008288476 grad: -1.722962015350229
iteration: 50 loss: 19.592000806292017 grad: -1.6430962549176797
iteration: 60 loss: 17.332411284409048 grad: -1.529572966834674
iteration: 70 loss: 15.536582558074459 grad: -1.4184078630311703
iteration: 80 loss: 14.073356442758062 grad: -1.3177937060070148
iteration: 90 loss: 12.857603706093187 grad: -1.228626408504995
iteration: 100 loss: 11.831330549947863 grad: -1.1498891486833953
iteration: 110 loss: 10.953514982445457 grad: -1.0801895019675127
iteration: 120 loss: 10.194250969960525 grad: -1.018203472146206
iteration: 130 loss: 9.531187909849367 grad: -0.9627861898329342
iteration: 140 loss: 8.947269587856164 grad: -0.9129792016259666
iteration: 150 loss: 8.429245947153317 grad: -0.8679882349962833
iteration: 160 loss: 7.966663579200196 grad: -0.8271556489794664
iteration: 170 loss: 7.551163123755983 grad: -0.7899351257933857
iteration: 180 loss: 7.175979289637863 grad: -0.7558704601342916
iteration: 190 loss: 6.835578085989834 grad: -0.7245784084341726
iteration: 200 loss: 6.52538905670107 grad: -0.6957350135721144
iteration: 210 loss: 6.241604597054985 grad: -0.6690647435927153
iteration: 220 loss: 5.981027472104333 grad: -0.6443318558534921
iteration: 230 loss: 5.740953517178034 grad: -0.6213335049114688
iteration: 240 loss: 5.519080383422864 grad: -0.5998942138420186
iteration: 250 loss: 5.3134358135099236 grad: -0.5798614134604896
iteration: 260 loss: 5.1223207349353705 grad: -0.5611018211885392
iteration: 270 loss: 4.944263716949908 grad: -0.5434984834105148
iteration: 280 loss: 4.777984228917169 grad: -0.5269483450819299
iteration: 290 loss: 4.622362778225207 grad: -0.5113602408133329
iteration: 300 loss: 4.4764164713120325 grad: -0.4966532249006179
iteration: 310 loss: 4.339278883548311 grad: -0.48275517555913927
iteration: 320 loss: 4.210183377943725 grad: -0.4696016222802328
iteration: 330 loss: 4.0884492033495174 grad: -0.4571347557718611
iteration: 340 loss: 3.9734698472433263 grad: -0.4453025881249195
iteration: 350 loss: 3.864703228430244 grad: -0.43405823722825776
iteration: 360 loss: 3.7616633998541973 grad: -0.42335931446227765
iteration: 370 loss: 3.6639134975440273 grad: -0.41316739865136376
iteration: 380 loss: 3.571059723104389 grad: -0.40344758238973855
iteration: 390 loss: 3.482746187581969 grad: -0.39416807935571263
iteration: 400 loss: 3.3986504765112344 grad: -0.3852998832345396
iteration: 410 loss: 3.318479821389453 grad: -0.376816470486502
iteration: 420 loss: 3.2419677832166682 grad: -0.3686935405063108
iteration: 430 loss: 3.1688713701115705 grad: -0.36090878778572366
iteration: 440 loss: 3.098968524285779 grad: -0.35344170156285115
iteration: 450 loss: 3.0320559244469223 grad: -0.34627338915744654
iteration: 460 loss: 2.967947058484242 grad: -0.3393864197819134
iteration: 470 loss: 2.9064705285283954 grad: -0.3327646861067777
iteration: 480 loss: 2.847468556420071 grad: -0.32639328126595657
iteration: 490 loss: 2.7907956625373034 grad: -0.32025838932651607
iteration: 0 loss: 75.45038196898622 grad: 33.44771760787302
iteration: 10 loss: 43.375315607423 grad: 6.166293768761369
iteration: 20 loss: 32.90355598330754 grad: -0.03530832436524817
iteration: 30 loss: 26.82611813082266 grad: -1.203779776476325
iteration: 40 loss: 22.711106787548406 grad: -1.420170530828159
iteration: 50 loss: 19.707069983027456 grad: -1.416259966334944
iteration: 60 loss: 17.406699635388268 grad: -1.3551346124485661
iteration: 70 loss: 15.584546315913919 grad: -1.2807815153641107
iteration: 80 loss: 14.103907255913452 grad: -1.2066486660094675
iteration: 90 loss: 12.876394692492898 grad: -1.1369865527696814
iteration: 100 loss: 11.842038761344321 grad: -1.0729398471324858
iteration: 110 loss: 10.958586446528587 grad: -1.014536046999405
iteration: 120 loss: 10.195346335923018 grad: -0.9614033405033902
iteration: 130 loss: 9.529455776119242 grad: -0.9130479316593755
iteration: 140 loss: 8.943517913875274 grad: -0.8689646477438
iteration: 150 loss: 8.424051760278045 grad: -0.8286801904166914
iteration: 160 loss: 7.9604445398087424 grad: -0.7917679562842053
iteration: 170 loss: 7.544225063535743 grad: -0.7578506307739659
iteration: 180 loss: 7.16854834394367 grad: -0.7265976248182733
iteration: 190 loss: 6.82782277887732 grad: -0.6977205430636557
iteration: 200 loss: 6.517435724446965 grad: -0.6709681449739695
iteration: 210 loss: 6.233548323316262 grad: -0.646121460237162
iteration: 220 loss: 5.9729399511226635 grad: -0.6229893375113263
iteration: 230 loss: 5.7328887823298995 grad: -0.6014045194098854
iteration: 240 loss: 5.5110790307639945 grad: -0.5812202465328421
iteration: 250 loss: 5.305528149581691 grad: -0.5623073518764881
iteration: 260 loss: 5.114529145751889 grad: -0.5445517905673996
iteration: 270 loss: 4.9366044663918895 grad: -0.5278525463684981
iteration: 280 loss: 4.770468834510004 grad: -0.5121198592619236
iteration: 290 loss: 4.614999070780496 grad: -0.4972737240769612
iteration: 300 loss: 4.469209415932255 grad: -0.48324261669160273
iteration: 310 loss: 4.332231219038146 grad: -0.46996241080516027
iteration: 320 loss: 4.203296116996336 grad: -0.4573754541885206
iteration: 330 loss: 4.081722025297123 grad: -0.4454297784886023
iteration: 340 loss: 3.9669014074023092 grad: -0.43407842106920946
iteration: 350 loss: 3.8582914023452095 grad: -0.4232788410627633
iteration: 360 loss: 3.7554054764942437 grad: -0.4129924148696698
iteration: 370 loss: 3.6578063322895518 grad: -0.4031839988659478
iteration: 380 loss: 3.565099858944615 grad: -0.39382154915286627
iteration: 390 loss: 3.4769299510974387 grad: -0.3848757898830881
iteration: 400 loss: 3.3929740537897404 grad: -0.3763199230934009
iteration: 410 loss: 3.3129393179458684 grad: -0.3681293741208295
iteration: 420 loss: 3.23655927111775 grad: -0.3602815676231403
iteration: 430 loss: 3.163590924856759 grad: -0.35275573000425686
iteration: 440 loss: 3.093812253484471 grad: -0.34553271469061375
iteration: 450 loss: 3.02701998989931 grad: -0.3385948472407095
iteration: 460 loss: 2.963027692975588 grad: -0.33192578771708714
iteration: 470 loss: 2.90166404837985 grad: -0.3255104081237875
iteration: 480 loss: 2.842771370630048 grad: -0.3193346830260345
iteration: 490 loss: 2.786204279198955 grad: -0.3133855917330368
iteration: 0 loss: 74.14262305236716 grad: 44.26025174801957
iteration: 10 loss: 43.470687542271634 grad: 5.226169437694876
iteration: 20 loss: 33.12215221009394 grad: -1.2621090485672104
iteration: 30 loss: 27.008423728387367 grad: -1.912030320834887
iteration: 40 loss: 22.854895314681904 grad: -1.8562321097649903
iteration: 50 loss: 19.821698555911635 grad: -1.7111537641425083
iteration: 60 loss: 17.499978646517608 grad: -1.5692666802771704
iteration: 70 loss: 15.662022557601631 grad: -1.4444642840036042
iteration: 80 loss: 14.169445961140706 grad: -1.3366323124345425
iteration: 90 loss: 12.932720767636965 grad: -1.243290890286348
iteration: 100 loss: 11.891110774230935 grad: -1.1619385423769422
iteration: 110 loss: 11.001841333954479 grad: -1.0904811034786772
iteration: 120 loss: 10.233859207000911 grad: -1.0272403108594719
iteration: 130 loss: 9.564045750826194 grad: -0.9708811367338377
iteration: 140 loss: 8.974819555459714 grad: -0.9203370208203562
iteration: 150 loss: 8.452564233844628 grad: -0.8747488821772477
iteration: 160 loss: 7.986566072389513 grad: -0.8334183407562625
iteration: 170 loss: 7.568277512686564 grad: -0.7957724068787431
iteration: 180 loss: 7.190794985531821 grad: -0.7613368202950161
iteration: 190 loss: 6.848481367739706 grad: -0.7297157880635448
iteration: 200 loss: 6.536688209710312 grad: -0.7005764491476796
iteration: 210 loss: 6.251548164140001 grad: -0.6736368508932564
iteration: 220 loss: 5.989817690000936 grad: -0.6486565575119341
iteration: 230 loss: 5.748756338622777 grad: -0.6254292500133469
iteration: 240 loss: 5.526033044051804 grad: -0.6037768471971267
iteration: 250 loss: 5.319652610166734 grad: -0.5835447988510206
iteration: 260 loss: 5.1278974848964936 grad: -0.5645982898613733
iteration: 270 loss: 4.949281233027705 grad: -0.5468191576843519
iteration: 280 loss: 4.782511052348375 grad: -0.5301033725259576
iteration: 290 loss: 4.626457346075023 grad: -0.5143589644369606
iteration: 300 loss: 4.480128848937098 grad: -0.49950430767696535
iteration: 310 loss: 4.342652159545733 grad: -0.4854666924788643
iteration: 320 loss: 4.213254794995464 grad: -0.4721811294141255
iteration: 330 loss: 4.091251080811534 grad: -0.45958934311819627
iteration: 340 loss: 3.976030338307365 grad: -0.44763892105620995
iteration: 350 loss: 3.8670469449902654 grad: -0.436282589933972
iteration: 360 loss: 3.7638119308944624 grad: -0.4254775977643602
iteration: 370 loss: 3.6658858413130804 grad: -0.41518518384101444
iteration: 380 loss: 3.5728726490882434 grad: -0.40537012221758284
iteration: 390 loss: 3.4844145410089284 grad: -0.3960003269446766
iteration: 400 loss: 3.4001874355798547 grad: -0.387046509432269
iteration: 410 loss: 3.3198971154175854 grad: -0.37848188000044597
iteration: 420 loss: 3.243275878336755 grad: -0.37028188704661347
iteration: 430 loss: 3.1700796279097134 grad: -0.3624239883622012
iteration: 440 loss: 3.100085337784797 grad: -0.3548874500306265
iteration: 450 loss: 3.033088835046659 grad: -0.3476531690727377
iteration: 460 loss: 2.968902856839081 grad: -0.3407035166090063
iteration: 470 loss: 2.9073553418279516 grad: -0.3340221988051345
iteration: 480 loss: 2.8482879241182673 grad: -0.3275941332797719
iteration: 490 loss: 2.7915546022410043 grad: -0.3214053389958394
iteration: 0 loss: 76.00812718843248 grad: 28.665605800327004
iteration: 10 loss: 43.58328180822063 grad: 7.060309618811237
iteration: 20 loss: 32.899872624256155 grad: 0.5888166441589897
iteration: 30 loss: 26.809987685771684 grad: -1.0011303257621615
iteration: 40 loss: 22.7051826508698 grad: -1.3895310933996337
iteration: 50 loss: 19.70970818630419 grad: -1.442386912770314
iteration: 60 loss: 17.414387040268636 grad: -1.3949707552541408
iteration: 70 loss: 15.594784731789291 grad: -1.319540330858588
iteration: 80 loss: 14.115208014469628 grad: -1.2399874999254192
iteration: 90 loss: 12.887940531413092 grad: -1.1642839076260763
iteration: 100 loss: 11.853412516184335 grad: -1.0947861497312457
iteration: 110 loss: 10.96959671851011 grad: -1.0318098874418338
iteration: 120 loss: 10.205923614704984 grad: -0.9749497173352035
iteration: 130 loss: 9.539592919892335 grad: -0.923584689864607
iteration: 140 loss: 8.953237064626116 grad: -0.8770724418472313
iteration: 150 loss: 8.433386560835205 grad: -0.8348196690111305
iteration: 160 loss: 7.969431015777755 grad: -0.7963027213426145
iteration: 170 loss: 7.552897300019182 grad: -0.7610684235636818
iteration: 180 loss: 7.176936736887001 grad: -0.7287276668827063
iteration: 190 loss: 6.835953608509499 grad: -0.6989470027112286
iteration: 200 loss: 6.525331358812227 grad: -0.6714403468809877
iteration: 210 loss: 6.2412276805243065 grad: -0.6459615581445841
iteration: 220 loss: 5.980419029526646 grad: -0.6222980824493469
iteration: 230 loss: 5.740181167475591 grad: -0.6002656225813778
iteration: 240 loss: 5.5181963413567905 grad: -0.5797037107919587
iteration: 250 loss: 5.312480412148116 grad: -0.5604720445577274
iteration: 260 loss: 5.121325100772951 grad: -0.5424474541966074
iteration: 270 loss: 4.943251814150202 grad: -0.5255213884667449
iteration: 280 loss: 4.776974430419324 grad: -0.5095978230839566
iteration: 290 loss: 4.621369079615471 grad: -0.4945915143454481
iteration: 300 loss: 4.4754494332605645 grad: -0.48042653479961417
iteration: 310 loss: 4.338346366781604 grad: -0.46703504007821145
iteration: 320 loss: 4.209291118764626 grad: -0.4543562258710391
iteration: 330 loss: 4.087601265976487 grad: -0.442335441936014
iteration: 340 loss: 3.972668980514465 grad: -0.43092343635660146
iteration: 350 loss: 3.8639511479264 grad: -0.42007570829883883
iteration: 360 loss: 3.760961011612587 grad: -0.40975195154385036
iteration: 370 loss: 3.6632610758406785 grad: -0.3999155742910524
iteration: 380 loss: 3.5704570519856134 grad: -0.39053328331089476
iteration: 390 loss: 3.4821926736909847 grad: -0.3815747226071099
iteration: 400 loss: 3.3981452391068903 grad: -0.37301215843144386
iteration: 410 loss: 3.3180217641978413 grad: -0.36482020386057407
iteration: 420 loss: 3.241555651768067 grad: -0.35697557725954693
iteration: 430 loss: 3.1685037974545014 grad: -0.34945688986891843
iteration: 440 loss: 3.098644067381594 grad: -0.34224445850358165
iteration: 450 loss: 3.031773093067386 grad: -0.3353201399712663
iteration: 460 loss: 2.96770433807966 grad: -0.32866718433278674
iteration: 470 loss: 2.90626639823532 grad: -0.32227010455400484
iteration: 480 loss: 2.847301503148339 grad: -0.3161145604569927
iteration: 490 loss: 2.7906641918960884 grad: -0.310187255177644
iteration: 0 loss: 76.37560501807229 grad: 31.16628955719145
iteration: 10 loss: 44.402108854410926 grad: 6.537489714532567
iteration: 20 loss: 33.41612667022614 grad: 0.3128367194605041
iteration: 30 loss: 27.12639847649075 grad: -1.0168359519863908
iteration: 40 loss: 22.90320196036855 grad: -1.3133081894639345
iteration: 50 loss: 19.836366227715356 grad: -1.3463294239374672
iteration: 60 loss: 17.496653081351123 grad: -1.303687395080217
iteration: 70 loss: 15.648631151623352 grad: -1.2397418875737762
iteration: 80 loss: 14.150425636551786 grad: -1.1721863692579784
iteration: 90 loss: 12.910718851285186 grad: -1.1070935044338797
iteration: 100 loss: 11.867769858896674 grad: -1.0464461591347816
iteration: 110 loss: 10.978201632785323 grad: -0.9906897458173769
iteration: 120 loss: 10.210582461034143 grad: -0.9396805295394588
iteration: 130 loss: 9.541548065474899 grad: -0.8930617347393832
iteration: 140 loss: 8.953354186860203 grad: -0.8504183785779774
iteration: 150 loss: 8.432275136995342 grad: -0.8113411485836107
iteration: 160 loss: 7.967523498489424 grad: -0.7754512929440684
iteration: 170 loss: 7.550502139996339 grad: -0.7424084435176568
iteration: 180 loss: 7.174274496001875 grad: -0.7119110137264686
iteration: 190 loss: 6.833181928312983 grad: -0.6836935324105782
iteration: 200 loss: 6.52256245602997 grad: -0.6575229094556092
iteration: 210 loss: 6.238540752600164 grad: -0.6331945373503788
iteration: 220 loss: 5.97786914405366 grad: -0.610528619227138
iteration: 230 loss: 5.737805691508873 grad: -0.5893668707732467
iteration: 240 loss: 5.516019629466634 grad: -0.5695696296288315
iteration: 250 loss: 5.310517249090356 grad: -0.5510133548677146
iteration: 260 loss: 5.119583244993257 grad: -0.5335884786694551
iteration: 270 loss: 4.941733886546279 grad: -0.517197566468856
iteration: 280 loss: 4.77567932267117 grad: -0.5017537428333823
iteration: 290 loss: 4.620293007559832 grad: -0.4871793441478933
iteration: 300 loss: 4.4745867263908705 grad: -0.4734047639536092
iteration: 310 loss: 4.337690060528695 grad: -0.46036746157939523
iteration: 320 loss: 4.208833398665195 grad: -0.4480111091157845
iteration: 330 loss: 4.087333800153332 grad: -0.4362848556699467
iteration: 340 loss: 3.972583167655516 grad: -0.42514269118194725
iteration: 350 loss: 3.8640383011499413 grad: -0.4145428949148069
iteration: 360 loss: 3.7612124935924593 grad: -0.40444755611087496
iteration: 370 loss: 3.6636683968358326 grad: -0.3948221562977102
iteration: 380 loss: 3.571011939621175 grad: -0.3856352043878174
iteration: 390 loss: 3.4828871212453536 grad: -0.3768579171017058
iteration: 400 loss: 3.3989715374688205 grad: -0.3684639383990137
iteration: 410 loss: 3.318972521456172 grad: -0.3604290925669653
iteration: 420 loss: 3.24262380347843 grad: -0.35273116642191193
iteration: 430 loss: 3.1696826099276803 grad: -0.3453497167552614
iteration: 440 loss: 3.099927135797522 grad: -0.33826589972215504
iteration: 450 loss: 3.0331543358042836 grad: -0.3314623193482778
iteration: 460 loss: 2.9691779883277873 grad: -0.32492289273239283
iteration: 470 loss: 2.9078269937219208 grad: -0.31863272986212743
iteration: 480 loss: 2.8489438745985933 grad: -0.31257802624851594
iteration: 490 loss: 2.792383450714624 grad: -0.30674596682934774
iteration: 0 loss: 75.23471332292598 grad: 32.671507492558376
iteration: 10 loss: 42.83850849505702 grad: 5.664884687774945
iteration: 20 loss: 32.47417563884374 grad: 0.2578760793712106
iteration: 30 loss: 26.482955563026177 grad: -0.9370710184944344
iteration: 40 loss: 22.429377365127223 grad: -1.2477116905418268
iteration: 50 loss: 19.468422216056545 grad: -1.307617360212377
iteration: 60 loss: 17.199510999850997 grad: -1.284102719279524
iteration: 70 loss: 15.401357046223232 grad: -1.2318761468824722
iteration: 80 loss: 13.939703615776663 grad: -1.1712450202507094
iteration: 90 loss: 12.727646459871226 grad: -1.110237979255767
iteration: 100 loss: 11.706158242207618 grad: -1.052031235417692
iteration: 110 loss: 10.833611925910729 grad: -0.9977580878136869
iteration: 120 loss: 10.079748345947378 grad: -0.9476654573385518
iteration: 130 loss: 9.422013585795247 grad: -0.9016187687474928
iteration: 140 loss: 8.843237934204994 grad: -0.8593307451961745
iteration: 150 loss: 8.330111927373453 grad: -0.82046677750315
iteration: 160 loss: 7.872155324326079 grad: -0.7846929466338123
iteration: 170 loss: 7.461001423128561 grad: -0.7516967418957523
iteration: 180 loss: 7.089889062115287 grad: -0.7211946602389927
iteration: 190 loss: 6.753294908279465 grad: -0.6929335717722871
iteration: 200 loss: 6.446662637752649 grad: -0.6666892507853289
iteration: 210 loss: 6.16620036934728 grad: -0.6422637587557847
iteration: 220 loss: 5.908727031546608 grad: -0.6194825065282845
iteration: 230 loss: 5.671554371898806 grad: -0.5981913864926482
iteration: 240 loss: 5.452395302150845 grad: -0.578254143607732
iteration: 250 loss: 5.249291957300391 grad: -0.5595500425464214
iteration: 260 loss: 5.060558687963707 grad: -0.5419718340000859
iteration: 270 loss: 4.884736488495616 grad: -0.5254239986873355
iteration: 280 loss: 4.720556270604489 grad: -0.5098212383505754
iteration: 290 loss: 4.566909042458602 grad: -0.49508718132556606
iteration: 300 loss: 4.422821525174682 grad: -0.48115327209496633
iteration: 310 loss: 4.287436084942204 grad: -0.4679578174888628
iteration: 320 loss: 4.159994116001102 grad: -0.455445165815547
iteration: 330 loss: 4.039822202214592 grad: -0.44356499869012367
iteration: 340 loss: 3.926320530561415 grad: -0.4322717184633964
iteration: 350 loss: 3.81895314090215 grad: -0.4215239168736293
iteration: 360 loss: 3.717239681748569 grad: -0.4112839128558205
iteration: 370 loss: 3.620748407894966 grad: -0.4015173493850388
iteration: 380 loss: 3.529090207382896 grad: -0.39219284085068706
iteration: 390 loss: 3.4419134858022433 grad: -0.3832816638062843
iteration: 400 loss: 3.358899767976089 grad: -0.374757485059242
iteration: 410 loss: 3.2797599025621325 grad: -0.3665961219961734
iteration: 420 loss: 3.204230775481392 grad: -0.3587753308145062
iteration: 430 loss: 3.132072454491787 grad: -0.3512746189780136
iteration: 440 loss: 3.0630657004506414 grad: -0.3440750787549809
iteration: 450 loss: 2.9970097915969003 grad: -0.33715923915160745
iteration: 460 loss: 2.9337206159511466 grad: -0.33051093393509035
iteration: 470 loss: 2.8730289941413916 grad: -0.32411518376299797
iteration: 480 loss: 2.8147792008867327 grad: -0.3179580907081719
iteration: 490 loss: 2.7588276582737308 grad: -0.3120267436997643
iteration: 0 loss: 77.3208969382198 grad: 24.28109445645444
iteration: 10 loss: 45.60179865084028 grad: 7.738572172828855
iteration: 20 loss: 34.10000204566161 grad: 1.6751700886067487
iteration: 30 loss: 27.62917539575152 grad: -0.21159551806502364
iteration: 40 loss: 23.315017088594605 grad: -0.8585221797429385
iteration: 50 loss: 20.189716916409782 grad: -1.082003190715918
iteration: 60 loss: 17.807559198441513 grad: -1.1431842084675061
iteration: 70 loss: 15.926664749612923 grad: -1.1377032385008667
iteration: 80 loss: 14.401972152623557 grad: -1.104344757512579
iteration: 90 loss: 13.1403390841579 grad: -1.0600116386764717
iteration: 100 loss: 12.078877305828284 grad: -1.0124112807803236
iteration: 110 loss: 11.17344020795624 grad: -0.965120277515121
iteration: 120 loss: 10.392054313617894 grad: -0.919766697184759
iteration: 130 loss: 9.710959243992214 grad: -0.8770289738666553
iteration: 140 loss: 9.112112211732644 grad: -0.837115253692072
iteration: 150 loss: 8.581556926590027 grad: -0.8000018196536041
iteration: 160 loss: 8.108324211188823 grad: -0.7655547256991708
iteration: 170 loss: 7.6836712630949515 grad: -0.7335927619048341
iteration: 180 loss: 7.300543129118648 grad: -0.7039201139747329
iteration: 190 loss: 6.953183811749775 grad: -0.6763430402090971
iteration: 200 loss: 6.636850445239071 grad: -0.6506780052858971
iteration: 210 loss: 6.347599906335993 grad: -0.6267552236936116
iteration: 220 loss: 6.082127248133516 grad: -0.6044197506903023
iteration: 230 loss: 5.8376418098105205 grad: -0.5835312917495485
iteration: 240 loss: 5.611771116433167 grad: -0.5639633762908669
iteration: 250 loss: 5.4024855479815 grad: -0.5456022517466521
iteration: 260 loss: 5.208038717635935 grad: -0.5283456922298789
iteration: 270 loss: 5.026919863317372 grad: -0.5121018250177467
iteration: 280 loss: 4.857815519364356 grad: -0.4967880266853538
iteration: 290 loss: 4.699578424283903 grad: -0.4823299118431832
iteration: 300 loss: 4.551202119768153 grad: -0.468660421421438
iteration: 310 loss: 4.411800062133972 grad: -0.45571900882826133
iteration: 320 loss: 4.2805883384698555 grad: -0.44345091794096647
iteration: 330 loss: 4.156871282642524 grad: -0.43180654494570414
iteration: 340 loss: 4.040029439531116 grad: -0.42074087545828115
iteration: 350 loss: 3.929509442589816 grad: -0.41021298851602833
iteration: 360 loss: 3.8248154594729056 grad: -0.40018561956570164
iteration: 370 loss: 3.725501929830898 grad: -0.3906247752763466
iteration: 380 loss: 3.6311673734784233 grad: -0.38149939376024133
iteration: 390 loss: 3.541449089563974 grad: -0.3727810445244582
iteration: 400 loss: 3.456018600882213 grad: -0.3644436631668414
iteration: 410 loss: 3.3745777241307553 grad: -0.35646331645829976
iteration: 420 loss: 3.2968551681778164 grad: -0.3488179940138183
iteration: 430 loss: 3.2226035795231174 grad: -0.34148742324891396
iteration: 440 loss: 3.1515969679600278 grad: -0.3344529047507494
iteration: 450 loss: 3.08362845664876 grad: -0.32769716556956047
iteration: 460 loss: 3.018508309979924 grad: -0.3212042282625652
iteration: 470 loss: 2.9560622000831245 grad: -0.31495929380513626
iteration: 480 loss: 2.896129679026093 grad: -0.30894863672834133
iteration: 490 loss: 2.8385628288240032 grad: -0.3031595110530544
iteration: 0 loss: 72.79603161265372 grad: 51.554723064488826
iteration: 10 loss: 42.84845843950161 grad: 3.0422912028016422
iteration: 20 loss: 32.72176012187865 grad: -1.8595017002227452
iteration: 30 loss: 26.735580094204828 grad: -2.0771176247353877
iteration: 40 loss: 22.66499898674376 grad: -1.9093112933471927
iteration: 50 loss: 19.684140455585915 grad: -1.7266335895326512
iteration: 60 loss: 17.39565910901737 grad: -1.5695547190796708
iteration: 70 loss: 15.579253371400311 grad: -1.4381267457238507
iteration: 80 loss: 14.101013060479069 grad: -1.3275110100412324
iteration: 90 loss: 12.874095561516386 grad: -1.2332686816462133
iteration: 100 loss: 11.83939621081413 grad: -1.1519697500883515
iteration: 110 loss: 10.95514848729586 grad: -1.0810410214097166
iteration: 120 loss: 10.190932912669744 grad: -1.0185450767265882
iteration: 130 loss: 9.524035799216295 grad: -0.9630042046654681
iteration: 140 loss: 8.937138448795498 grad: -0.9132741445289527
iteration: 150 loss: 8.41679738344202 grad: -0.8684554375614448
iteration: 160 loss: 7.952414316304964 grad: -0.8278309621843375
iteration: 170 loss: 7.535519858790105 grad: -0.7908214116725487
iteration: 180 loss: 7.159264179728573 grad: -0.7569531247877009
iteration: 190 loss: 6.818047689696083 grad: -0.7258345185673629
iteration: 200 loss: 6.5072486022284215 grad: -0.697138589998073
iteration: 210 loss: 6.223018854325016 grad: -0.6705897552544298
iteration: 220 loss: 5.9621291183932295 grad: -0.6459538280418804
iteration: 230 loss: 5.7218496285788865 grad: -0.6230302971692905
iteration: 240 loss: 5.499857509244314 grad: -0.6016463079130632
iteration: 250 loss: 5.29416396902314 grad: -0.5816519203385417
iteration: 260 loss: 5.103056561685579 grad: -0.5629163353043303
iteration: 270 loss: 4.925052997773698 grad: -0.5453248616780508
iteration: 280 loss: 4.758863899486476 grad: -0.5287764571877747
iteration: 290 loss: 4.603362543444043 grad: -0.5131817176322118
iteration: 300 loss: 4.457560109886757 grad: -0.4984612198468454
iteration: 310 loss: 4.320585305231858 grad: -0.4845441462833019
iteration: 320 loss: 4.191667483659537 grad: -0.4713671356634601
iteration: 330 loss: 4.070122587507368 grad: -0.45887331656169295
iteration: 340 loss: 3.9553413731635128 grad: -0.44701149010298596
iteration: 350 loss: 3.846779501313573 grad: -0.4357354350595262
iteration: 360 loss: 3.743949156713246 grad: -0.4250033140683013
iteration: 370 loss: 3.64641192956924 grad: -0.41477716389837327
iteration: 380 loss: 3.5537727428604327 grad: -0.40502245597492414
iteration: 390 loss: 3.4656746510067107 grad: -0.3957077159422456
iteration: 400 loss: 3.381794367757514 grad: -0.3868041930852739
iteration: 410 loss: 3.301838407039048 grad: -0.37828557205261026
iteration: 420 loss: 3.22553974115346 grad: -0.3701277206259413
iteration: 430 loss: 3.1526548973898487 grad: -0.3623084683315789
iteration: 440 loss: 3.0829614275317825 grad: -0.35480741154292816
iteration: 450 loss: 3.0162556956982125 grad: -0.34760574141925843
iteration: 460 loss: 2.9523509388641287 grad: -0.3406860915978458
iteration: 470 loss: 2.8910755617368356 grad: -0.3340324030282008
iteration: 480 loss: 2.832271633674488 grad: -0.32762980372804174
iteration: 490 loss: 2.7757935603299715 grad: -0.32146450156614204
iteration: 0 loss: 75.59403635297797 grad: 28.2726997342955
iteration: 10 loss: 43.25430825958149 grad: 6.093837211436666
iteration: 20 loss: 32.6242244051865 grad: 0.7855278816071503
iteration: 30 loss: 26.535925169834275 grad: -0.5666718971510414
iteration: 40 loss: 22.440963359040282 grad: -0.9863342838339135
iteration: 50 loss: 19.46162575631175 grad: -1.1172936677428604
iteration: 60 loss: 17.18481230916849 grad: -1.1412845438699546
iteration: 70 loss: 15.38379357924497 grad: -1.1216831441711108
iteration: 80 loss: 13.921756321759279 grad: -1.0840037145200025
iteration: 90 loss: 12.710528755214703 grad: -1.0395049981853077
iteration: 100 loss: 11.690431661011727 grad: -0.9934202865603188
iteration: 110 loss: 10.819495714696545 grad: -0.9482225206034329
iteration: 120 loss: 10.067281350802949 grad: -0.9050491904715723
iteration: 130 loss: 9.411141343691746 grad: -0.864369336815285
iteration: 140 loss: 8.833860406232464 grad: -0.8263122816415516
iteration: 150 loss: 8.322109881042747 grad: -0.7908357880849259
iteration: 160 loss: 7.865404856181406 grad: -0.7578142888279987
iteration: 170 loss: 7.455381778593661 grad: -0.7270858361286292
iteration: 180 loss: 7.085286626455234 grad: -0.6984771292530574
iteration: 190 loss: 6.749605004159506 grad: -0.6718166588965179
iteration: 200 loss: 6.443790075683203 grad: -0.6469413252764047
iteration: 210 loss: 6.16405930502027 grad: -0.623699452837826
iteration: 220 loss: 5.907240456417185 grad: -0.6019518242175063
iteration: 230 loss: 5.670653429116597 grad: -0.5815716453343578
iteration: 240 loss: 5.452018539818172 grad: -0.5624439576075444
iteration: 250 loss: 5.249384582866903 grad: -0.5444647895016743
iteration: 260 loss: 5.061071858497888 grad: -0.5275402116269186
iteration: 270 loss: 4.885626654139659 grad: -0.5115853858829941
iteration: 280 loss: 4.721784578139457 grad: -0.4965236564989205
iteration: 290 loss: 4.568440799891547 grad: -0.4822857062107579
iteration: 300 loss: 4.424625724909716 grad: -0.4688087867424291
iteration: 310 loss: 4.289484981355682 grad: -0.4560360248623773
iteration: 320 loss: 4.162262852503276 grad: -0.44391580103363254
iteration: 330 loss: 4.042288482701141 grad: -0.4324011955519145
iteration: 340 loss: 3.928964330310299 grad: -0.4214494961815206
iteration: 350 loss: 3.8217564523108027 grad: -0.4110217611186847
iteration: 360 loss: 3.7201862906962644 grad: -0.4010824313188447
iteration: 370 loss: 3.62382369696752 grad: -0.391598986628834
iteration: 380 loss: 3.532280982606856 grad: -0.3825416406562185
iteration: 390 loss: 3.4452078239388686 grad: -0.37388306982165004
iteration: 400 loss: 3.3622868817907063 grad: -0.36559817254035776
iteration: 410 loss: 3.2832300218070403 grad: -0.3576638549470246
iteration: 420 loss: 3.20777504162933 grad: -0.35005884000568804
iteration: 430 loss: 3.1356828274980133 grad: -0.3427634972303406
iteration: 440 loss: 3.0667348760619215 grad: -0.3357596905833158
iteration: 450 loss: 3.000731127904477 grad: -0.3290306424198479
iteration: 460 loss: 2.937488068064795 grad: -0.32256081161183325
iteration: 470 loss: 2.876837056004206 grad: -0.31633578421541925
iteration: 480 loss: 2.8186228533705533 grad: -0.31034217524935614
iteration: 490 loss: 2.7627023228190577 grad: -0.304567540327496
iteration: 0 loss: 75.5700495703402 grad: 39.893686830529035
iteration: 10 loss: 43.874089609153025 grad: 6.386032765631425
iteration: 20 loss: 33.33325896758813 grad: -0.8747429788275766
iteration: 30 loss: 27.19913703885769 grad: -1.886975049559136
iteration: 40 loss: 23.03116986725455 grad: -1.9095074076534273
iteration: 50 loss: 19.98155634788081 grad: -1.7708757421459185
iteration: 60 loss: 17.643217102536564 grad: -1.6206162641977775
iteration: 70 loss: 15.78975381514126 grad: -1.486203478845862
iteration: 80 loss: 14.283322664546445 grad: -1.370320724326974
iteration: 90 loss: 13.034498862207267 grad: -1.2706959911731015
iteration: 100 loss: 11.982445052098145 grad: -1.184501226101886
iteration: 110 loss: 11.084205385906733 grad: -1.1092851188802846
iteration: 120 loss: 10.308526701053758 grad: -1.0430853112960068
iteration: 130 loss: 9.632099838566795 grad: -0.9843592458020368
iteration: 140 loss: 9.037173562350869 grad: -0.9318925330949575
iteration: 150 loss: 8.509985363282402 grad: -0.8847209824197425
iteration: 160 loss: 8.039698281220415 grad: -0.8420707854853553
iteration: 170 loss: 7.617661970906531 grad: -0.8033140035080055
iteration: 180 loss: 7.236887652587277 grad: -0.7679356132666271
iteration: 190 loss: 6.891667746602004 grad: -0.7355089875323202
iteration: 200 loss: 6.577295564319973 grad: -0.7056774812115396
iteration: 210 loss: 6.289855557180669 grad: -0.6781404516588811
iteration: 220 loss: 6.026064195937207 grad: -0.6526425254677534
iteration: 230 loss: 5.783147752963817 grad: -0.628965266512499
iteration: 240 loss: 5.5587473650421915 grad: -0.6069206395281791
iteration: 250 loss: 5.3508445238208795 grad: -0.5863458310554146
iteration: 260 loss: 5.157702043052103 grad: -0.5670991074692313
iteration: 270 loss: 4.977816878519082 grad: -0.5490564734845029
iteration: 280 loss: 4.809882115672033 grad: -0.5321089545032289
iteration: 290 loss: 4.652756113559587 grad: -0.5161603695949466
iteration: 300 loss: 4.505437282701265 grad: -0.5011254936685118
iteration: 310 loss: 4.367043333682804 grad: -0.48692853086450716
iteration: 320 loss: 4.236794099759043 grad: -0.4735018387072383
iteration: 330 loss: 4.113997236461686 grad: -0.46078485573947764
iteration: 340 loss: 3.9980362522245825 grad: -0.4487231953789173
iteration: 350 loss: 3.888360439241048 grad: -0.4372678764088921
iteration: 360 loss: 3.7844763623016577 grad: -0.42637466644281696
iteration: 370 loss: 3.685940631983158 grad: -0.4160035193148973
iteration: 380 loss: 3.592353742061195 grad: -0.40611809096660645
iteration: 390 loss: 3.503354793053859 grad: -0.3966853212540974
iteration: 400 loss: 3.418616957025196 grad: -0.3876750713712707
iteration: 410 loss: 3.33784356518597 grad: -0.37905980839830045
iteration: 420 loss: 3.2607647209559385 grad: -0.3708143299456833
iteration: 430 loss: 3.1871343581246947 grad: -0.36291552304533586
iteration: 440 loss: 3.1167276774806605 grad: -0.35534215240125877
iteration: 450 loss: 3.049338906415621 grad: -0.34807467389796864
iteration: 460 loss: 2.984779335104825 grad: -0.3410950699103231
iteration: 470 loss: 2.922875590320638 grad: -0.33438670349104194
iteration: 480 loss: 2.8634681140667286 grad: -0.3279341889537327
iteration: 490 loss: 2.8064098192844953 grad: -0.3217232767367354
iteration: 0 loss: 75.65096472588469 grad: 33.12543732295249
iteration: 10 loss: 44.072161420582376 grad: 5.684374558836469
iteration: 20 loss: 33.40001089983077 grad: 0.20397999043795506
iteration: 30 loss: 27.19180166974415 grad: -0.9242163670049461
iteration: 40 loss: 22.997258611383646 grad: -1.1939072454426745
iteration: 50 loss: 19.939582197331365 grad: -1.2386105168922037
iteration: 60 loss: 17.60044898584779 grad: -1.2128850218888614
iteration: 70 loss: 15.749048479921095 grad: -1.1639512236888307
iteration: 80 loss: 14.245694859622146 grad: -1.108561114743638
iteration: 90 loss: 13.000166786385618 grad: -1.0531739054033538
iteration: 100 loss: 11.951285768366894 grad: -1.0003184639798584
iteration: 110 loss: 11.05596191750816 grad: -0.950896572097659
iteration: 120 loss: 10.282903567467077 grad: -0.9051062127344486
iteration: 130 loss: 9.608806258123671 grad: -0.8628397810656523
iteration: 140 loss: 9.015941554096736 grad: -0.8238644329120028
iteration: 150 loss: 8.490575795159327 grad: -0.7879059087178941
iteration: 160 loss: 8.02190127145617 grad: -0.7546874873996678
iteration: 170 loss: 7.601294819612576 grad: -0.7239473972816672
iteration: 180 loss: 7.221791869602797 grad: -0.6954457108810757
iteration: 190 loss: 6.877705927085324 grad: -0.6689661152911341
iteration: 200 loss: 6.564348450713293 grad: -0.6443152550204294
iteration: 210 loss: 6.277819419009657 grad: -0.6213210112174058
iteration: 220 loss: 6.014848557806958 grad: -0.5998304052549568
iteration: 230 loss: 5.772673454598465 grad: -0.5797074661918707
iteration: 240 loss: 5.548944918409358 grad: -0.5608312206772555
iteration: 250 loss: 5.341652727274653 grad: -0.5430938702143278
iteration: 260 loss: 5.149066813697903 grad: -0.526399173091025
iteration: 270 loss: 4.969690267938528 grad: -0.5106610250668235
iteration: 280 loss: 4.80222147887575 grad: -0.49580222255725087
iteration: 290 loss: 4.64552340568794 grad: -0.4817533883983468
iteration: 300 loss: 4.498598462194753 grad: -0.4684520399780464
iteration: 310 loss: 4.360567854271474 grad: -0.45584178080969906
iteration: 320 loss: 4.230654476684859 grad: -0.4438715985538453
iteration: 330 loss: 4.108168674886986 grad: -0.43249525458384414
iteration: 340 loss: 3.9924963278840964 grad: -0.4216707522022396
iteration: 350 loss: 3.883088823107052 grad: -0.4113598724436565
iteration: 360 loss: 3.779454582448786 grad: -0.4015277680115982
iteration: 370 loss: 3.6811518669882886 grad: -0.39214260729078876
iteration: 380 loss: 3.5877826412265263 grad: -0.38317526156981707
iteration: 390 loss: 3.498987319508987 grad: -0.37459902962333735
iteration: 400 loss: 3.4144402504118094 grad: -0.3663893946626312
iteration: 410 loss: 3.333845821149308 grad: -0.3585238093904566
iteration: 420 loss: 3.2569350851056864 grad: -0.3509815055109457
iteration: 430 loss: 3.1834628324899397 grad: -0.34374332456549744
iteration: 440 loss: 3.1132050377787395 grad: -0.33679156740613836
iteration: 450 loss: 3.0459566287129083 grad: -0.3301098599914921
iteration: 460 loss: 2.9815295306534195 grad: -0.3236830335079569
iteration: 470 loss: 2.919750947525327 grad: -0.31749701708892075
iteration: 480 loss: 2.8604618466946374 grad: -0.31153874163534623
iteration: 490 loss: 2.803515620150227 grad: -0.30579605343805805
iteration: 0 loss: 76.05158293594944 grad: 26.23237942575638
iteration: 10 loss: 44.64213525719569 grad: 6.213654966400616
iteration: 20 loss: 33.73251331444171 grad: 1.0745720496946376
iteration: 30 loss: 27.444794258570628 grad: -0.3647774700063813
iteration: 40 loss: 23.21026220513576 grad: -0.8561412721093212
iteration: 50 loss: 20.126608020979862 grad: -1.0298940289214098
iteration: 60 loss: 17.768292797448588 grad: -1.078412884547037
iteration: 70 loss: 15.90173740552175 grad: -1.0731407566128246
iteration: 80 loss: 14.385924766802873 grad: -1.0441722412817935
iteration: 90 loss: 13.12988722402127 grad: -1.0052392312627052
iteration: 100 loss: 12.071980919590509 grad: -0.9629138762857288
iteration: 110 loss: 11.168802647702437 grad: -0.9204073260376247
iteration: 120 loss: 10.38883749852107 grad: -0.8792733634612733
iteration: 130 loss: 9.70861508598155 grad: -0.840221176239798
iteration: 140 loss: 9.110279675716377 grad: -0.8035216682869308
iteration: 150 loss: 8.579997557282116 grad: -0.7692178421472443
iteration: 160 loss: 8.106880897673095 grad: -0.7372365341861105
iteration: 170 loss: 7.682241400085249 grad: -0.7074487323330005
iteration: 180 loss: 7.299060890171652 grad: -0.6797023517545925
iteration: 190 loss: 6.951608295879354 grad: -0.6538399460214813
iteration: 200 loss: 6.635157669251757 grad: -0.62970805571508
iteration: 210 loss: 6.345777349493718 grad: -0.607161875610814
iteration: 220 loss: 6.08017011066518 grad: -0.5860672987793223
iteration: 230 loss: 5.83555043394476 grad: -0.566301504262226
iteration: 240 loss: 5.609549202911736 grad: -0.547752755642263
iteration: 250 loss: 5.400138921001268 grad: -0.5303197937851722
iteration: 260 loss: 5.205574470302881 grad: -0.5139110434178649
iteration: 270 loss: 5.0243457684979065 grad: -0.4984437580804665
iteration: 280 loss: 4.855139626395323 grad: -0.48384317236011015
iteration: 290 loss: 4.696808786213131 grad: -0.47004169773111026
iteration: 300 loss: 4.548346612402094 grad: -0.4569781793158121
iteration: 310 loss: 4.408866267655454 grad: -0.4445972199172875
iteration: 320 loss: 4.277583474367888 grad: -0.4328485715006498
iteration: 330 loss: 4.153802162305758 grad: -0.4216865909749193
iteration: 340 loss: 4.036902454796333 grad: -0.4110697554898671
iteration: 350 loss: 3.9263305613318895 grad: -0.4009602318120322
iteration: 360 loss: 3.821590233307978 grad: -0.39132349425854723
iteration: 370 loss: 3.722235508430789 grad: -0.3821279858916705
iteration: 380 loss: 3.6278645230046838 grad: -0.37334481805819036
iteration: 390 loss: 3.5381142134588117 grad: -0.36494750380559265
iteration: 400 loss: 3.4526557617937215 grad: -0.3569117211685161
iteration: 410 loss: 3.3711906661104547 grad: -0.34921510276532813
iteration: 420 loss: 3.2934473385723986 grad: -0.34183704856033437
iteration: 430 loss: 3.219178150177041 grad: -0.33475855902547746
iteration: 440 loss: 3.148156855481912 grad: -0.3279620862744559
iteration: 450 loss: 3.0801763415992025 grad: -0.32143140104305856
iteration: 460 loss: 3.015046654904009 grad: -0.3151514736545362
iteration: 470 loss: 2.9525932663691905 grad: -0.30910836734122965
iteration: 480 loss: 2.8926555425938947 grad: -0.3032891424967784
iteration: 490 loss: 2.8350853946832433 grad: -0.29768177061038903
iteration: 0 loss: 76.66174052375408 grad: 29.40990566357705
iteration: 10 loss: 44.64411017927682 grad: 7.349027492293126
iteration: 20 loss: 33.49381255353788 grad: 0.8711084923970875
iteration: 30 loss: 27.172874457410817 grad: -0.7699462022546927
iteration: 40 loss: 22.944047188974682 grad: -1.2178039828997123
iteration: 50 loss: 19.875963493818414 grad: -1.3181312856435428
iteration: 60 loss: 17.535304755860952 grad: -1.3048764543804219
iteration: 70 loss: 15.685935080077094 grad: -1.253377919535283
iteration: 80 loss: 14.18599266178049 grad: -1.1906000267415902
iteration: 90 loss: 12.944302582087754 grad: -1.1267859646717935
iteration: 100 loss: 11.899258691089555 grad: -1.0658420107646838
iteration: 110 loss: 11.00758387005817 grad: -1.0091183552014216
iteration: 120 loss: 10.237912895976892 grad: -0.9569004695221994
iteration: 130 loss: 9.566922112484338 grad: -0.909033280994639
iteration: 140 loss: 8.976889501725593 grad: -0.8651929095455296
iteration: 150 loss: 8.454099195905057 grad: -0.8250070914130718
iteration: 160 loss: 7.987765536975902 grad: -0.7881079502846915
iteration: 170 loss: 7.569288143367223 grad: -0.7541537215318586
iteration: 180 loss: 7.1917242643418255 grad: -0.7228360932702337
iteration: 190 loss: 6.849407519221669 grad: -0.6938809723942367
iteration: 200 loss: 6.537667521382446 grad: -0.6670463991055191
iteration: 210 loss: 6.2526204397956295 grad: -0.6421193929517872
iteration: 220 loss: 5.991010340596983 grad: -0.6189125719613097
iteration: 230 loss: 5.750087466559845 grad: -0.5972609243601769
iteration: 240 loss: 5.527513776953666 grad: -0.5770188854293496
iteration: 250 loss: 5.321288871349804 grad: -0.5580577622734835
iteration: 260 loss: 5.129691338874429 grad: -0.5402634982738638
iteration: 270 loss: 4.951231909141535 grad: -0.5235347474353012
iteration: 280 loss: 4.7846157238142375 grad: -0.5077812217923755
iteration: 290 loss: 4.628711722684371 grad: -0.4929222748981919
iteration: 300 loss: 4.482527627461505 grad: -0.47888568730661346
iteration: 310 loss: 4.345189365268528 grad: -0.46560662393766195
iteration: 320 loss: 4.215924039827934 grad: -0.4530267373464489
iteration: 330 loss: 4.094045757414775 grad: -0.4410933947743134
iteration: 340 loss: 3.978943765082063 grad: -0.4297590102834255
iteration: 350 loss: 3.8700724733339484 grad: -0.41898046622780427
iteration: 360 loss: 3.7669430234888996 grad: -0.4087186108186929
iteration: 370 loss: 3.6691161281844207 grad: -0.39893782064863975
iteration: 380 loss: 3.576195966640419 grad: -0.38960561880102385
iteration: 390 loss: 3.487824958055791 grad: -0.38069234064281343
iteration: 400 loss: 3.4036792694737414 grad: -0.37217084062540107
iteration: 410 loss: 3.323464940684599 grad: -0.3640162344423613
iteration: 420 loss: 3.246914529680825 grad: -0.35620567174856593
iteration: 430 loss: 3.1737841990182214 grad: -0.34871813536115803
iteration: 440 loss: 3.1038511770606916 grad: -0.3415342634634202
iteration: 450 loss: 3.036911539122023 grad: -0.3346361918373053
iteration: 460 loss: 2.972778262536684 grad: -0.3280074135756218
iteration: 470 loss: 2.9112795170801955 grad: -0.32163265408396224
iteration: 480 loss: 2.852257158236832 grad: -0.31549775948646386
iteration: 490 loss: 2.795565395831545 grad: -0.30958959680745135
iteration: 0 loss: 74.56800053104645 grad: 38.29624647159577
iteration: 10 loss: 43.208986083841815 grad: 5.371460500663416
iteration: 20 loss: 32.83386823473384 grad: -0.6109964175195759
iteration: 30 loss: 26.7798174596706 grad: -1.4967790194151154
iteration: 40 loss: 22.67190096865538 grad: -1.588947846657995
iteration: 50 loss: 19.668854619055743 grad: -1.5252058674454418
iteration: 60 loss: 17.367484131231034 grad: -1.431149670418097
iteration: 70 loss: 15.543988235365862 grad: -1.336846611177879
iteration: 80 loss: 14.062262005524376 grad: -1.2498296313256092
iteration: 90 loss: 12.834086494068266 grad: -1.1714579531429283
iteration: 100 loss: 11.799492605742795 grad: -1.1013042872121028
iteration: 110 loss: 10.916168574402098 grad: -1.038476973943435
iteration: 120 loss: 10.1533495560855 grad: -0.9820402148289106
iteration: 130 loss: 9.488102676293094 grad: -0.9311428579151483
iteration: 140 loss: 8.902970407407482 grad: -0.8850482243197322
iteration: 150 loss: 8.384422020816952 grad: -0.8431305377313918
iteration: 160 loss: 7.921805285235388 grad: -0.8048613375937899
iteration: 170 loss: 7.506618452734156 grad: -0.7697942925800724
iteration: 180 loss: 7.131993332220852 grad: -0.737551287714576
iteration: 190 loss: 6.792321028278076 grad: -0.7078105668807761
iteration: 200 loss: 6.482976259703388 grad: -0.6802969391018576
iteration: 210 loss: 6.200111145299781 grad: -0.6547737995474225
iteration: 220 loss: 5.940498807568681 grad: -0.6310366603948946
iteration: 230 loss: 5.701413270195644 grad: -0.6089079057295316
iteration: 240 loss: 5.480536175793945 grad: -0.5882325269783959
iteration: 250 loss: 5.275883581192954 grad: -0.5688746403730792
iteration: 260 loss: 5.085747961208199 grad: -0.5507146281324613
iteration: 270 loss: 4.908651858000709 grad: -0.5336467784350312
iteration: 280 loss: 4.743310537110119 grad: -0.517577325980128
iteration: 290 loss: 4.588601673665959 grad: -0.5024228159436664
iteration: 300 loss: 4.4435405730588915 grad: -0.48810873048983167
iteration: 310 loss: 4.307259783336948 grad: -0.4745683296907981
iteration: 320 loss: 4.178992218446225 grad: -0.4617416685547563
iteration: 330 loss: 4.058057107665842 grad: -0.4495747595243633
iteration: 340 loss: 3.9438482349234865 grad: -0.43801885579278
iteration: 350 loss: 3.8358240448524983 grad: -0.4270298354832536
iteration: 360 loss: 3.733499279410229 grad: -0.41656767044769616
iteration: 370 loss: 3.636437876277424 grad: -0.40659596638539564
iteration: 380 loss: 3.5442469128147835 grad: -0.3970815633359842
iteration: 390 loss: 3.456571420628403 grad: -0.3879941874913584
iteration: 400 loss: 3.3730899284407148 grad: -0.3793061467989727
iteration: 410 loss: 3.293510616892242 grad: -0.3709920640702117
iteration: 420 loss: 3.2175679896503095 grad: -0.3630286423214153
iteration: 430 loss: 3.1450199818826317 grad: -0.35539445790741064
iteration: 440 loss: 3.0756454406324667 grad: -0.3480697776938309
iteration: 450 loss: 3.0092419225724454 grad: -0.3410363970832115
iteration: 460 loss: 2.9456237635583666 grad: -0.33427749618303115
iteration: 470 loss: 2.884620381713624 grad: -0.3277775117991363
iteration: 480 loss: 2.826074781806669 grad: -0.3215220232694601
iteration: 490 loss: 2.769842233663274 grad: -0.3154976504319019
iteration: 0 loss: 75.3673566450414 grad: 38.510552994752985
iteration: 10 loss: 44.01756941183456 grad: 5.411367974512737
iteration: 20 loss: 33.364816296716896 grad: -0.5929977978557335
iteration: 30 loss: 27.17765701955467 grad: -1.4961718072221881
iteration: 40 loss: 22.99833822638028 grad: -1.5969842166195807
iteration: 50 loss: 19.95047558586049 grad: -1.5342135279584905
iteration: 60 loss: 17.617401426265733 grad: -1.4379772451525152
iteration: 70 loss: 15.769568752437552 grad: -1.340788248810934
iteration: 80 loss: 14.268132766050837 grad: -1.2511455626385346
iteration: 90 loss: 13.023428710284758 grad: -1.1706953592586533
iteration: 100 loss: 11.974651136715941 grad: -1.0990267409548167
iteration: 110 loss: 11.078962719693829 grad: -1.0351712109673326
iteration: 120 loss: 10.30524365897519 grad: -0.9780950824760611
iteration: 130 loss: 9.63030656967327 grad: -0.9268554498425458
iteration: 140 loss: 9.036502589586503 grad: -0.880638453531509
iteration: 150 loss: 8.510151842540752 grad: -0.8387566733205319
iteration: 160 loss: 8.040482617116679 grad: -0.8006338301230007
iteration: 170 loss: 7.618895460626845 grad: -0.7657871995593922
iteration: 180 loss: 7.238441018202304 grad: -0.7338113771744919
iteration: 190 loss: 6.8934421411205555 grad: -0.7043644260696809
iteration: 200 loss: 6.579215589745507 grad: -0.6771564560997252
iteration: 210 loss: 6.291863874399579 grad: -0.651940351359562
iteration: 220 loss: 6.028117374667173 grad: -0.6285042867625653
iteration: 230 loss: 5.785213079382381 grad: -0.6066656942467081
iteration: 240 loss: 5.560800385610946 grad: -0.5862663887007262
iteration: 250 loss: 5.352867153842495 grad: -0.567168617005405
iteration: 260 loss: 5.159681108215409 grad: -0.5492518412829959
iteration: 270 loss: 4.979742988587077 grad: -0.5324101070856204
iteration: 280 loss: 4.811748793272769 grad: -0.5165498790541136
iteration: 290 loss: 4.654559119225351 grad: -0.5015882516503001
iteration: 300 loss: 4.507174091177479 grad: -0.48745146217020474
iteration: 310 loss: 4.368712727083221 grad: -0.47407364852798006
iteration: 320 loss: 4.238395851173085 grad: -0.46139580621176646
iteration: 330 loss: 4.115531863735049 grad: -0.44936490811163077
iteration: 340 loss: 3.999504826306665 grad: -0.4379331581982018
iteration: 350 loss: 3.889764435036506 grad: -0.42705735574606807
iteration: 360 loss: 3.7858175427086693 grad: -0.41669835130032135
iteration: 370 loss: 3.687220957878377 grad: -0.40682057914813874
iteration: 380 loss: 3.5935753026045942 grad: -0.39739165388786246
iteration: 390 loss: 3.5045197519303986 grad: -0.388382020945731
iteration: 400 loss: 3.419727511192129 grad: -0.37976465269879645
iteration: 410 loss: 3.338901913442979 grad: -0.37151478331730886
iteration: 420 loss: 3.261773040228408 grad: -0.3636096766151812
iteration: 430 loss: 3.1880947858085165 grad: -0.3560284221509067
iteration: 440 loss: 3.117642298538902 grad: -0.34875175559863586
iteration: 450 loss: 3.050209744197673 grad: -0.3417619000454405
iteration: 460 loss: 2.985608345077956 grad: -0.3350424253939249
iteration: 470 loss: 2.923664656067393 grad: -0.3285781234812596
iteration: 480 loss: 2.864219045040086 grad: -0.32235489688381924
iteration: 490 loss: 2.8071243499184675 grad: -0.3163596596747191
iteration: 0 loss: 73.63289319272388 grad: 43.28228467884622
iteration: 10 loss: 42.438237583520326 grad: 4.219019682105349
iteration: 20 loss: 32.18692604527776 grad: -1.179264376648426
iteration: 30 loss: 26.253644647108207 grad: -1.754361695511418
iteration: 40 loss: 22.25067302241101 grad: -1.7350898854583523
iteration: 50 loss: 19.328821609934494 grad: -1.620919007563457
iteration: 60 loss: 17.088500649901604 grad: -1.4987202041835896
iteration: 70 loss: 15.310968598604155 grad: -1.3865717487785525
iteration: 80 loss: 13.864257818107427 grad: -1.2874060776011655
iteration: 90 loss: 12.663164606014147 grad: -1.2004148999588276
iteration: 100 loss: 11.649850329236353 grad: -1.123979727907681
iteration: 110 loss: 10.783503824488559 grad: -1.0564849784276054
iteration: 120 loss: 10.034425538126099 grad: -0.9965264173054003
iteration: 130 loss: 9.380448264788129 grad: -0.9429372625846018
iteration: 140 loss: 8.80467256215909 grad: -0.8947620479404871
iteration: 150 loss: 8.293980328613356 grad: -0.8512200016264858
iteration: 160 loss: 7.838028662701328 grad: -0.8116708860061024
iteration: 170 loss: 7.428550758167106 grad: -0.7755865384354735
iteration: 180 loss: 7.058859052233195 grad: -0.7425281635138495
iteration: 190 loss: 6.723485095376046 grad: -0.7121285158991224
iteration: 200 loss: 6.417913953798151 grad: -0.6840779864909672
iteration: 210 loss: 6.1383852872969245 grad: -0.658113722738872
iteration: 220 loss: 5.881742292938849 grad: -0.6340110835412479
iteration: 230 loss: 5.645315559371614 grad: -0.6115768868255453
iteration: 240 loss: 5.426832748805612 grad: -0.5906440368528978
iteration: 250 loss: 5.224347635627385 grad: -0.5710672183931725
iteration: 260 loss: 5.036183823980826 grad: -0.5527194208004553
iteration: 270 loss: 4.8608897179327535 grad: -0.5354891119643727
iteration: 280 loss: 4.697202203774065 grad: -0.5192779247186418
iteration: 290 loss: 4.544017139752958 grad: -0.5039987501985208
iteration: 300 loss: 4.400365210416484 grad: -0.4895741566192172
iteration: 310 loss: 4.265392042105635 grad: -0.4759350700549296
iteration: 320 loss: 4.13834172820874 grad: -0.4630196675485416
iteration: 330 loss: 4.018543101781275 grad: -0.4507724433878888
iteration: 340 loss: 3.9053982361932125 grad: -0.4391434174630884
iteration: 350 loss: 3.798372763661064 grad: -0.42808746087212524
iteration: 360 loss: 3.696987685545206 grad: -0.4175637188130128
iteration: 370 loss: 3.600812413425088 grad: -0.4075351146200772
iteration: 380 loss: 3.5094588308382466 grad: -0.3979679218153366
iteration: 390 loss: 3.4225762055363536 grad: -0.3888313934380326
iteration: 400 loss: 3.339846813737991 grad: -0.38009743982550215
iteration: 410 loss: 3.260982163031382 grad: -0.37174034755249064
iteration: 420 loss: 3.185719720699994 grad: -0.3637365334745465
iteration: 430 loss: 3.1138200704689223 grad: -0.35606432882637506
iteration: 440 loss: 3.045064433759721 grad: -0.3487037891460655
iteration: 450 loss: 2.9792525022033427 grad: -0.34163652646817244
iteration: 460 loss: 2.9162005368444954 grad: -0.3348455607820834
iteration: 470 loss: 2.855739696620607 grad: -0.3283151882097664
iteration: 480 loss: 2.7977145645521557 grad: -0.3220308637371385
iteration: 490 loss: 2.741981844961174 grad: -0.3159790966502457
iteration: 0 loss: 75.19957111173595 grad: 33.082986437779105
iteration: 10 loss: 42.644238704344275 grad: 6.385836439793767
iteration: 20 loss: 32.40347132148485 grad: -0.2575023303548943
iteration: 30 loss: 26.4928873448009 grad: -1.477081431806242
iteration: 40 loss: 22.47214064944532 grad: -1.6365294485075026
iteration: 50 loss: 19.52427746051529 grad: -1.570303497042191
iteration: 60 loss: 17.259976348628182 grad: -1.4613861181766132
iteration: 70 loss: 15.462534139263198 grad: -1.3533879660312018
iteration: 80 loss: 13.999740102394092 grad: -1.2560770997898658
iteration: 90 loss: 12.78567319693339 grad: -1.1704508534384237
iteration: 100 loss: 11.761802650782204 grad: -1.0953238532665688
iteration: 110 loss: 10.886759454523702 grad: -1.0291484621932756
iteration: 120 loss: 10.130419453321668 grad: -0.9705030596876627
iteration: 130 loss: 9.470297905448454 grad: -0.9181931145871611
iteration: 140 loss: 8.889258167480172 grad: -0.8712423025260666
iteration: 150 loss: 8.37400381346916 grad: -0.8288576050157563
iteration: 160 loss: 7.914056571497192 grad: -0.7903933251462207
iteration: 170 loss: 7.501045739103699 grad: -0.7553204903114275
iteration: 180 loss: 7.128203117578075 grad: -0.723202425105673
iteration: 190 loss: 6.78999698827705 grad: -0.6936756974518987
iteration: 200 loss: 6.481862253831051 grad: -0.6664353631670423
iteration: 210 loss: 6.19999840433812 grad: -0.6412235377594635
iteration: 220 loss: 5.941216164823428 grad: -0.6178205105808521
iteration: 230 loss: 5.702819637448184 grad: -0.5960377942182715
iteration: 240 loss: 5.482514694209633 grad: -0.5757126484042319
iteration: 250 loss: 5.278337035586281 grad: -0.5567037316380055
iteration: 260 loss: 5.088595156680722 grad: -0.5388876200748635
iteration: 270 loss: 4.9118247363967384 grad: -0.5221559978797282
iteration: 280 loss: 4.7467518669899285 grad: -0.5064133713325957
iteration: 290 loss: 4.5922631883403975 grad: -0.49157519468600064
iteration: 300 loss: 4.447381461196735 grad: -0.47756632232245716
iteration: 310 loss: 4.311245458857477 grad: -0.4643197215557244
iteration: 320 loss: 4.1830933130204775 grad: -0.4517753952500565
iteration: 330 loss: 4.06224864168342 grad: -0.4398794746066252
iteration: 340 loss: 3.9481089323196765 grad: -0.4285834509429106
iteration: 350 loss: 3.84013576448371 grad: -0.4178435217666999
iteration: 360 loss: 3.7378465413187936 grad: -0.4076200314305741
iteration: 370 loss: 3.640807465554231 grad: -0.39787699051800496
iteration: 380 loss: 3.5486275471965367 grad: -0.3885816611321939
iteration: 390 loss: 3.460953470657387 grad: -0.3797041976358319
iteration: 400 loss: 3.3774651811406162 grad: -0.3712173342742883
iteration: 410 loss: 3.297872075602258 grad: -0.36309611261876107
iteration: 420 loss: 3.2219097040137554 grad: -0.3553176429742767
iteration: 430 loss: 3.1493369030595204 grad: -0.3478608948742931
iteration: 440 loss: 3.0799332976888434 grad: -0.34070651257805185
iteration: 450 loss: 3.0134971166935016 grad: -0.33383665213643354
iteration: 460 loss: 2.949843277313076 grad: -0.32723483712610785
iteration: 470 loss: 2.888801701064908 grad: -0.32088583059293857
iteration: 480 loss: 2.8302158289376083 grad: -0.3147755211118251
iteration: 490 loss: 2.773941309013067 grad: -0.30889082117545086
iteration: 0 loss: 74.94291152387095 grad: 43.01538301356093
iteration: 10 loss: 43.4027321134043 grad: 5.343013163175121
iteration: 20 loss: 32.83467427497727 grad: -1.0557614215453208
iteration: 30 loss: 26.71447600771082 grad: -1.8289920509831128
iteration: 40 loss: 22.588736404729243 grad: -1.824143572809976
iteration: 50 loss: 19.58494682869429 grad: -1.6975389149933202
iteration: 60 loss: 17.288666139923468 grad: -1.5625966328514167
iteration: 70 loss: 15.471921751299265 grad: -1.4406524844032842
iteration: 80 loss: 13.997034793779408 grad: -1.3341725157177007
iteration: 90 loss: 12.775214445876289 grad: -1.2415886065739168
iteration: 100 loss: 11.746319548701525 grad: -1.1607285715107867
iteration: 110 loss: 10.868026519875126 grad: -1.0896205725012824
iteration: 120 loss: 10.109619880417188 grad: -1.0266349379449018
iteration: 130 loss: 9.448235481804742 grad: -0.9704593743898977
iteration: 140 loss: 8.866486331996818 grad: -0.9200412079262642
iteration: 150 loss: 8.350907066537543 grad: -0.8745321364467633
iteration: 160 loss: 7.890903322724956 grad: -0.833243155751873
iteration: 170 loss: 7.478023429824176 grad: -0.7956096067481402
iteration: 180 loss: 7.105442024029716 grad: -0.7611645053897711
iteration: 190 loss: 6.767586622901655 grad: -0.7295182363393055
iteration: 200 loss: 6.459862829106731 grad: -0.7003430280193961
iteration: 210 loss: 6.178448946337374 grad: -0.6733609993384984
iteration: 220 loss: 5.920140319137855 grad: -0.6483348790634659
iteration: 230 loss: 5.6822298641785975 grad: -0.6250607359335951
iteration: 240 loss: 5.462415324639386 grad: -0.6033622325018
iteration: 250 loss: 5.258726515555406 grad: -0.5830860430486897
iteration: 260 loss: 5.069467703005682 grad: -0.56409816839512
iteration: 270 loss: 4.893171565799135 grad: -0.5462809477268487
iteration: 280 loss: 4.728562111113702 grad: -0.5295306167289704
iteration: 290 loss: 4.5745245765478595 grad: -0.5137552974948998
iteration: 300 loss: 4.430080830472186 grad: -0.4988733324511519
iteration: 310 loss: 4.294369134245202 grad: -0.4848118945074754
iteration: 320 loss: 4.166627390660076 grad: -0.4715058206453737
iteration: 330 loss: 4.046179198263582 grad: -0.45889662751760896
iteration: 340 loss: 3.9324221787753064 grad: -0.44693167629824354
iteration: 350 loss: 3.824818157345362 grad: -0.4355634606860531
iteration: 360 loss: 3.722884861853221 grad: -0.42474899712570424
iteration: 370 loss: 3.626188874409515 grad: -0.41444930033895105
iteration: 380 loss: 3.534339620418326 grad: -0.40462893042393
iteration: 390 loss: 3.446984221584857 grad: -0.3952556002861579
iteration: 400 loss: 3.3638030716292633 grad: -0.3862998341611002
iteration: 410 loss: 3.2845060192327185 grad: -0.37773466958883417
iteration: 420 loss: 3.2088290633295484 grad: -0.36953539649263145
iteration: 430 loss: 3.1365314824186252 grad: -0.3616793280610717
iteration: 440 loss: 3.067393332937007 grad: -0.35414559898817677
iteration: 450 loss: 3.0012132626042414 grad: -0.3469149873271846
iteration: 460 loss: 2.9378065935163082 grad: -0.33996975679147523
iteration: 470 loss: 2.877003637011512 grad: -0.3332935168146723
iteration: 480 loss: 2.8186482083314415 grad: -0.32687109807997095
iteration: 490 loss: 2.762596314022713 grad: -0.3206884415611633
iteration: 0 loss: 76.35869280887584 grad: 32.692227020718285
iteration: 10 loss: 44.19153160875876 grad: 6.370628110550477
iteration: 20 loss: 33.27986687169121 grad: 0.17575361318831306
iteration: 30 loss: 27.050266638389406 grad: -1.1232351603742003
iteration: 40 loss: 22.866272554110495 grad: -1.4117585700505773
iteration: 50 loss: 19.822640572583374 grad: -1.438469358790992
iteration: 60 loss: 17.49608404843951 grad: -1.3879534158151845
iteration: 70 loss: 15.655233134697482 grad: -1.315454062553708
iteration: 80 loss: 14.160677629013518 grad: -1.2396113345654294
iteration: 90 loss: 12.9225581894352 grad: -1.1669834047565586
iteration: 100 loss: 11.879997615182985 grad: -1.0997101610199844
iteration: 110 loss: 10.99013422858033 grad: -1.03822386990154
iteration: 120 loss: 10.221845796826052 grad: -0.9822952242271838
iteration: 130 loss: 9.551954306508378 grad: -0.9314599753922439
iteration: 140 loss: 8.962827125309643 grad: -0.8851971591330744
iteration: 150 loss: 8.440804557672084 grad: -0.8430023820040343
iteration: 160 loss: 7.9751371201803485 grad: -0.804415389080193
iteration: 170 loss: 7.5572483403861535 grad: -0.7690275629002548
iteration: 180 loss: 7.180211716531506 grad: -0.7364808312867845
iteration: 190 loss: 6.838372244549729 grad: -0.706463207056539
iteration: 200 loss: 6.527067755111395 grad: -0.6787033314747413
iteration: 210 loss: 6.242420543079209 grad: -0.6529650664027934
iteration: 220 loss: 5.9811793844100976 grad: -0.6290425575853356
iteration: 230 loss: 5.740598249052834 grad: -0.6067559018586668
iteration: 240 loss: 5.5183421233743335 grad: -0.5859474207064893
iteration: 250 loss: 5.312413121352702 grad: -0.566478488644834
iteration: 260 loss: 5.121091960452346 grad: -0.5482268471789489
iteration: 270 loss: 4.94289119990303 grad: -0.5310843338942696
iteration: 280 loss: 4.77651757388503 grad: -0.5149549620401968
iteration: 290 loss: 4.6208414221447045 grad: -0.4997532941364827
iteration: 300 loss: 4.474871706780835 grad: -0.48540306154581936
iteration: 310 loss: 4.337735460814904 grad: -0.4718359897016085
iteration: 320 loss: 4.208660778901916 grad: -0.4589907954383975
iteration: 330 loss: 4.086962658832032 grad: -0.44681232860304965
iteration: 340 loss: 3.9720311524060063 grad: -0.43525083490608274
iteration: 350 loss: 3.86332139857917 grad: -0.42426132092638547
iteration: 360 loss: 3.76034519963423 grad: -0.4138030054365799
iteration: 370 loss: 3.66266386918435 grad: -0.4038388438892233
iteration: 380 loss: 3.5698821339013773 grad: -0.39433511509901376
iteration: 390 loss: 3.481642912517931 grad: -0.3852610609603138
iteration: 400 loss: 3.3976228285990313 grad: -0.3765885715246923
iteration: 410 loss: 3.31752833976109 grad: -0.36829190898875264
iteration: 420 loss: 3.241092386935138 grad: -0.36034746515626814
iteration: 430 loss: 3.168071484110318 grad: -0.35273354777942506
iteration: 440 loss: 3.098243182584817 grad: -0.3454301918832906
iteration: 450 loss: 3.0314038547872273 grad: -0.3384189927609319
iteration: 460 loss: 2.967366751745348 grad: -0.3316829578145831
iteration: 470 loss: 2.9059602956527586 grad: -0.3252063748276647
iteration: 480 loss: 2.8470265750601027 grad: -0.3189746945969152
iteration: 490 loss: 2.790420015239678 grad: -0.3129744261445479
iteration: 0 loss: 78.14933628759096 grad: 27.402598030036636
iteration: 10 loss: 47.62153335663343 grad: 7.32818929782931
iteration: 20 loss: 35.613427228442134 grad: 1.0085685946011407
iteration: 30 loss: 28.721481236356308 grad: -0.6138860483965414
iteration: 40 loss: 24.12478892078682 grad: -1.067472768805474
iteration: 50 loss: 20.809474437352677 grad: -1.1818576958235965
iteration: 60 loss: 18.295409265692527 grad: -1.1839295815127107
iteration: 70 loss: 16.319923575597468 grad: -1.1463370031871938
iteration: 80 loss: 14.725441229549089 grad: -1.095362257353238
iteration: 90 loss: 13.411048367379903 grad: -1.0413298337074595
iteration: 100 loss: 12.30885814238071 grad: -0.9884539815474684
iteration: 110 loss: 11.371403718016733 grad: -0.9384156821019949
iteration: 120 loss: 10.564443330523318 grad: -0.8917881380969112
iteration: 130 loss: 9.862628679088981 grad: -0.8486477783223482
iteration: 140 loss: 9.246781376702096 grad: -0.8088508542385855
iteration: 150 loss: 8.7021175670351 grad: -0.7721633774571897
iteration: 160 loss: 8.217054764508351 grad: -0.7383232847697666
iteration: 170 loss: 7.782388717798519 grad: -0.7070700165791766
iteration: 180 loss: 7.390712489862918 grad: -0.6781579912954283
iteration: 190 loss: 7.035998219991308 grad: -0.651362006670599
iteration: 200 loss: 6.713290644276268 grad: -0.626478601468672
iteration: 210 loss: 6.4184789428653115 grad: -0.6033254469582541
iteration: 220 loss: 6.148124472880309 grad: -0.5817398427203598
iteration: 230 loss: 5.899329021206953 grad: -0.5615768748751331
iteration: 240 loss: 5.669632866253654 grad: -0.5427075221327063
iteration: 250 loss: 5.45693506079839 grad: -0.5250168495829454
iteration: 260 loss: 5.259430480784596 grad: -0.5084023523773715
iteration: 270 loss: 5.075559665096245 grad: -0.4927724702862968
iteration: 280 loss: 4.903968513885337 grad: -0.4780452728930352
iteration: 290 loss: 4.743475657386088 grad: -0.4641473048711785
iteration: 300 loss: 4.5930458452598115 grad: -0.4510125764033567
iteration: 310 loss: 4.4517681000914475 grad: -0.4385816825790953
iteration: 320 loss: 4.318837669607464 grad: -0.4268010360181374
iteration: 330 loss: 4.193541029434073 grad: -0.4156221981506834
iteration: 340 loss: 4.075243351962027 grad: -0.40500129607938784
iteration: 350 loss: 3.963377981378926 grad: -0.3948985134991645
iteration: 360 loss: 3.8574375503642178 grad: -0.38527764563223377
iteration: 370 loss: 3.756966447667847 grad: -0.37610570948851585
iteration: 380 loss: 3.661554403156887 grad: -0.36735260196441466
iteration: 390 loss: 3.570831001857476 grad: -0.35899079934539113
iteration: 400 loss: 3.4844609739687837 grad: -0.3509950926888046
iteration: 410 loss: 3.402140135933223 grad: -0.34334235434698734
iteration: 420 loss: 3.3235918800977 grad: -0.33601133156150614
iteration: 430 loss: 3.2485641285000963 grad: -0.3289824636327769
iteration: 440 loss: 3.1768266808458065 grad: -0.32223771965828735
iteration: 450 loss: 3.108168898516107 grad: -0.31576045424980415
iteration: 460 loss: 3.0423976760386555 grad: -0.3095352789957808
iteration: 470 loss: 2.979335659306054 grad: -0.30354794773894367
iteration: 480 loss: 2.9188196762794507 grad: -0.29778525399858763
iteration: 490 loss: 2.860699351236359 grad: -0.2922349390891795
iteration: 0 loss: 75.78047910012448 grad: 33.31063240698883
iteration: 10 loss: 44.29892388658639 grad: 6.556362268901554
iteration: 20 loss: 33.57654573613986 grad: -0.03101753010664661
iteration: 30 loss: 27.37964892339966 grad: -1.2967974067637913
iteration: 40 loss: 23.178451256547802 grad: -1.508890751802621
iteration: 50 loss: 20.105504291340182 grad: -1.480643665349955
iteration: 60 loss: 17.74909614810531 grad: -1.396703846038649
iteration: 70 loss: 15.881152890521868 grad: -1.305033602342371
iteration: 80 loss: 14.362921860213213 grad: -1.2186438225948804
iteration: 90 loss: 13.104367907076142 grad: -1.1405646469979762
iteration: 100 loss: 12.044209486935932 grad: -1.0708367773178424
iteration: 110 loss: 11.1391577164465 grad: -1.0086550616867187
iteration: 120 loss: 10.357705950286366 grad: -0.9530567173398394
iteration: 130 loss: 9.67634987961959 grad: -0.903138032269984
iteration: 140 loss: 9.077186404900988 grad: -0.8581116238404637
iteration: 150 loss: 8.546333572383302 grad: -0.8173099224307792
iteration: 160 loss: 8.072859478697596 grad: -0.7801720920288241
iteration: 170 loss: 7.648037531132352 grad: -0.7462275615419276
iteration: 180 loss: 7.26481711441759 grad: -0.715080587780889
iteration: 190 loss: 6.917440042831004 grad: -0.6863970555458657
iteration: 200 loss: 6.601157868145864 grad: -0.6598935808229462
iteration: 210 loss: 6.312020327566643 grad: -0.6353286304018197
iteration: 220 loss: 6.046714846816775 grad: -0.6124953081750852
iteration: 230 loss: 5.802443258374421 grad: -0.5912154910363439
iteration: 240 loss: 5.576826031278663 grad: -0.571335052054075
iteration: 250 loss: 5.367827101686709 grad: -0.5527199602910167
iteration: 260 loss: 5.1736943115515714 grad: -0.5352530889847127
iteration: 270 loss: 4.992911801109089 grad: -0.5188315970898861
iteration: 280 loss: 4.824161648022213 grad: -0.5033647753207908
iteration: 290 loss: 4.66629272525283 grad: -0.4887722686403664
iteration: 300 loss: 4.51829524280017 grad: -0.47498260394629643
iteration: 310 loss: 4.379279800450149 grad: -0.46193196538743286
iteration: 320 loss: 4.248460047295381 grad: -0.44956317090756814
iteration: 330 loss: 4.125138245050452 grad: -0.43782481269518214
iteration: 340 loss: 4.008693184415609 grad: -0.4266705315668017
iteration: 350 loss: 3.8985700198414137 grad: -0.4160584012237157
iteration: 360 loss: 3.794271677331339 grad: -0.4059504030529111
iteration: 370 loss: 3.6953515590988157 grad: -0.3963119759139377
iteration: 380 loss: 3.6014073228617978 grad: -0.38711162835146395
iteration: 390 loss: 3.5120755559722636 grad: -0.3783206030547285
iteration: 400 loss: 3.42702719809102 grad: -0.369912585277841
iteration: 410 loss: 3.3459635927796296 grad: -0.3618634484420511
iteration: 420 loss: 3.268613069700595 grad: -0.35415103134481796
iteration: 430 loss: 3.1947279762670076 grad: -0.3467549423655854
iteration: 440 loss: 3.1240820914355356 grad: -0.3396563868354434
iteration: 450 loss: 3.056468365588981 grad: -0.332838014367002
iteration: 460 loss: 2.991696939654172 grad: -0.326283783452771
iteration: 470 loss: 2.9295934041040876 grad: -0.31997884105923324
iteration: 480 loss: 2.8699972647174365 grad: -0.31390941528842
iteration: 490 loss: 2.8127605870616375 grad: -0.30806271946385844
iteration: 0 loss: 77.70070270516412 grad: 29.75738909574308
iteration: 10 loss: 46.24965427897057 grad: 7.81322198770769
iteration: 20 loss: 34.51008992680025 grad: 0.7354389342597356
iteration: 30 loss: 27.88341966899698 grad: -0.9795242822074477
iteration: 40 loss: 23.47672639813713 grad: -1.387336094753661
iteration: 50 loss: 20.29620534170962 grad: -1.4414972304935567
iteration: 60 loss: 17.87990144216302 grad: -1.3927293001332313
iteration: 70 loss: 15.977064151821134 grad: -1.3158092382427573
iteration: 80 loss: 14.437780478073302 grad: -1.2349558632293858
iteration: 90 loss: 13.166161827705475 grad: -1.1581444107519636
iteration: 100 loss: 12.097701419735834 grad: -1.0877090478268503
iteration: 110 loss: 11.187261629232683 grad: -1.0239493135106053
iteration: 120 loss: 10.402241242082875 grad: -0.9664468458416948
iteration: 130 loss: 9.718470298913395 grad: -0.9145676410544282
iteration: 140 loss: 9.117631795277413 grad: -0.8676554392091748
iteration: 150 loss: 8.585581672160474 grad: -0.825102644638577
iteration: 160 loss: 8.111219613459793 grad: -0.7863716194995084
iteration: 170 loss: 7.685709618438502 grad: -0.7509960704098085
iteration: 180 loss: 7.301929468187237 grad: -0.7185749870146401
iteration: 190 loss: 6.954073944554404 grad: -0.6887643896136851
iteration: 200 loss: 6.637363721099105 grad: -0.6612690496714597
iteration: 210 loss: 6.347828368208576 grad: -0.6358349987883396
iteration: 220 loss: 6.082142286674463 grad: -0.6122430569243948
iteration: 230 loss: 5.83749905774415 grad: -0.5903033662132015
iteration: 240 loss: 5.611514087578929 grad: -0.5698508247648801
iteration: 250 loss: 5.402148369589626 grad: -0.5507412903191966
iteration: 260 loss: 5.207648200388865 grad: -0.5328484274735663
iteration: 270 loss: 5.026497082442967 grad: -0.5160610866481611
iteration: 280 loss: 4.857377031414134 grad: -0.500281119984928
iteration: 290 loss: 4.699137209987922 grad: -0.4854215556403751
iteration: 300 loss: 4.550768319257341 grad: -0.471405066191393
iteration: 310 loss: 4.41138155158317 grad: -0.45816267885998346
iteration: 320 loss: 4.2801911847733125 grad: -0.44563268511312837
iteration: 330 loss: 4.15650010366405 grad: -0.4337597151893496
iteration: 340 loss: 4.039687690816911 grad: -0.4224939495516161
iteration: 350 loss: 3.9291996464746095 grad: -0.411790444452198
iteration: 360 loss: 3.8245393888245913 grad: -0.4016085529640989
iteration: 370 loss: 3.7252607559114703 grad: -0.3919114261888637
iteration: 380 loss: 3.630961785268689 grad: -0.38266558205638257
iteration: 390 loss: 3.5412793903123285 grad: -0.37384053132046663
iteration: 400 loss: 3.4558847864018802 grad: -0.3654084521291462
iteration: 410 loss: 3.3744795464099298 grad: -0.35734390599370447
iteration: 420 loss: 3.296792187136651 grad: -0.34962358916108105
iteration: 430 loss: 3.2225752051740115 grad: -0.3422261143623625
iteration: 440 loss: 3.1516024947722228 grad: -0.3351318187068353
iteration: 450 loss: 3.0836670915674733 grad: -0.32832259414922693
iteration: 460 loss: 3.0185791952632246 grad: -0.3217817375034001
iteration: 470 loss: 2.9561644319059353 grad: -0.31549381742971044
iteration: 480 loss: 2.8962623226050876 grad: -0.3094445562022415
iteration: 490 loss: 2.8387249306992173 grad: -0.3036207243796034
iteration: 0 loss: 76.41728353177312 grad: 33.63481593885765
iteration: 10 loss: 45.10922850892294 grad: 6.1415643129395505
iteration: 20 loss: 34.12083129468775 grad: -0.1526305915134305
iteration: 30 loss: 27.727870659472604 grad: -1.2805211523600164
iteration: 40 loss: 23.40900529839839 grad: -1.4549924571838062
iteration: 50 loss: 20.264768892441182 grad: -1.4233991181055088
iteration: 60 loss: 17.863505754703365 grad: -1.3457134542730471
iteration: 70 loss: 15.966248056564423 grad: -1.2618781863351645
iteration: 80 loss: 14.428206991585181 grad: -1.1825545528971928
iteration: 90 loss: 13.155874707676771 grad: -1.110270434756885
iteration: 100 loss: 12.085892159862293 grad: -1.0451297593827884
iteration: 110 loss: 11.173682326751221 grad: -0.986531498277104
iteration: 120 loss: 10.386914114311532 grad: -0.9337229295649996
iteration: 130 loss: 9.701540639316507 grad: -0.8859792364986918
iteration: 140 loss: 9.09929292092114 grad: -0.8426557715134438
iteration: 150 loss: 8.566036963500808 grad: -0.803195976272219
iteration: 160 loss: 8.090663983610222 grad: -0.7671245170329684
iteration: 170 loss: 7.664321227439256 grad: -0.7340364510286879
iteration: 180 loss: 7.279866810564396 grad: -0.703586323908405
iteration: 190 loss: 6.931475691826315 grad: -0.6754784726052688
iteration: 200 loss: 6.6143499034006625 grad: -0.6494588113223443
iteration: 210 loss: 6.324502131376366 grad: -0.6253080182132129
iteration: 220 loss: 6.05859181881533 grad: -0.6028359308148306
iteration: 230 loss: 5.813799476673384 grad: -0.5818769450106569
iteration: 240 loss: 5.58772918934954 grad: -0.5622862328180624
iteration: 250 loss: 5.378332197820865 grad: -0.5439366236217397
iteration: 260 loss: 5.183846427910297 grad: -0.5267160221113505
iteration: 270 loss: 5.002748213003873 grad: -0.5105252610020526
iteration: 280 loss: 4.833713436789822 grad: -0.4952763070477877
iteration: 290 loss: 4.6755860205843085 grad: -0.48089075526627545
iteration: 300 loss: 4.5273521865473905 grad: -0.46729855932124725
iteration: 310 loss: 4.388119299646612 grad: -0.45443695630084796
iteration: 320 loss: 4.257098366567789 grad: -0.44224955225926965
iteration: 330 loss: 4.133589475832278 grad: -0.4306855413174179
iteration: 340 loss: 4.016969619032931 grad: -0.41969903621832916
iteration: 350 loss: 3.9066824516687784 grad: -0.4092484922941806
iteration: 360 loss: 3.802229643119269 grad: -0.39929621004934357
iteration: 370 loss: 3.70316353579024 grad: -0.38980790417259115
iteration: 380 loss: 3.6090808883745096 grad: -0.3807523288965009
iteration: 390 loss: 3.5196175212784677 grad: -0.37210095132824283
iteration: 400 loss: 3.4344437163085786 grad: -0.36382766576504255
iteration: 410 loss: 3.353260249747357 grad: -0.3559085431434893
iteration: 420 loss: 3.275794959563055 grad: -0.34832161070462564
iteration: 430 loss: 3.2017997648588543 grad: -0.34104665772584725
iteration: 440 loss: 3.131048069687205 grad: -0.3340650638073008
iteration: 450 loss: 3.063332494735484 grad: -0.3273596467295137
iteration: 460 loss: 2.99846288967282 grad: -0.32091452734020615
iteration: 470 loss: 2.9362645865438193 grad: -0.3147150092975285
iteration: 480 loss: 2.876576860848383 grad: -0.30874747180710654
iteration: 490 loss: 2.819251572122539 grad: -0.30299927375158153
iteration: 0 loss: 75.66292519670282 grad: 41.52473984855335
iteration: 10 loss: 44.57034434384995 grad: 4.991054242198432
iteration: 20 loss: 33.86391006935974 grad: -0.8650393689086024
iteration: 30 loss: 27.574740491164924 grad: -1.6019529481539492
iteration: 40 loss: 23.3113321074213 grad: -1.6293845871803236
iteration: 50 loss: 20.200180122047083 grad: -1.5366485410668473
iteration: 60 loss: 17.819952391153638 grad: -1.4282940025192266
iteration: 70 loss: 15.936706360968588 grad: -1.3263659212871017
iteration: 80 loss: 14.408332585901773 grad: -1.235104182353997
iteration: 90 loss: 13.142850738251953 grad: -1.1543651839496174
iteration: 100 loss: 12.077821749801636 grad: -1.0829611309310567
iteration: 110 loss: 11.16924857026844 grad: -1.0195795466321655
iteration: 120 loss: 10.385179929782948 grad: -0.9630345546919763
iteration: 130 loss: 9.70182571867657 grad: -0.9123181642270337
iteration: 140 loss: 9.101095200140167 grad: -0.8665914038318155
iteration: 150 loss: 8.568980877931216 grad: -0.8251599169980264
iteration: 160 loss: 8.094465429662932 grad: -0.7874484163635175
iteration: 170 loss: 7.668763278891344 grad: -0.752978260436677
iteration: 180 loss: 7.2847825511473525 grad: -0.7213489655530463
iteration: 190 loss: 6.936735870135842 grad: -0.692223347799913
iteration: 200 loss: 6.619853923685654 grad: -0.6653157001481196
iteration: 210 loss: 6.330171393586955 grad: -0.6403824065849858
iteration: 220 loss: 6.0643647390957085 grad: -0.6172144784194016
iteration: 230 loss: 5.819627726492275 grad: -0.5956315960194574
iteration: 240 loss: 5.593574829182961 grad: -0.5754773279032803
iteration: 250 loss: 5.384165474682091 grad: -0.5566152721972111
iteration: 260 loss: 5.189644070371452 grad: -0.5389259233060544
iteration: 270 loss: 5.008492102400396 grad: -0.5223041114885716
iteration: 280 loss: 4.839389565195936 grad: -0.506656897479435
iteration: 290 loss: 4.681183669060167 grad: -0.49190183065473136
iteration: 300 loss: 4.532863273745988 grad: -0.47796549939176713
iteration: 310 loss: 4.393537863035404 grad: -0.46478231770916556
iteration: 320 loss: 4.262420147489055 grad: -0.45229350413010283
iteration: 330 loss: 4.138811586334296 grad: -0.4404462178555532
iteration: 340 loss: 4.02209027341649 grad: -0.42919282441886436
iteration: 350 loss: 3.9117007495005964 grad: -0.41849026850958043
iteration: 360 loss: 3.807145393362837 grad: -0.40829953597326174
iteration: 370 loss: 3.707977113916968 grad: -0.39858519039389023
iteration: 380 loss: 3.6137931200332374 grad: -0.3893149723569944
iteration: 390 loss: 3.5242295874299585 grad: -0.3804594516343971
iteration: 400 loss: 3.438957075763483 grad: -0.3719917242469355
iteration: 410 loss: 3.357676575859097 grad: -0.3638871477423997
iteration: 420 loss: 3.280116088465299 grad: -0.35612310914343237
iteration: 430 loss: 3.2060276531434613 grad: -0.3486788209292574
iteration: 440 loss: 3.1351847598194498 grad: -0.34153514115831973
iteration: 450 loss: 3.0673800868244556 grad: -0.33467441444954954
iteration: 460 loss: 3.0024235184689796 grad: -0.3280803310438918
iteration: 470 loss: 2.9401404027455347 grad: -0.32173780158554727
iteration: 480 loss: 2.880370015967277 grad: -0.31563284561018146
iteration: 490 loss: 2.8229642062804654 grad: -0.3097524920181117
iteration: 0 loss: 77.67931877598366 grad: 30.04274430900427
iteration: 10 loss: 46.18145927109527 grad: 7.130791302147381
iteration: 20 loss: 34.67025993260316 grad: 0.6925644660558727
iteration: 30 loss: 28.07673905812389 grad: -0.8283201845957573
iteration: 40 loss: 23.658896060637765 grad: -1.2141627196195026
iteration: 50 loss: 20.458118530524477 grad: -1.2889181383336774
iteration: 60 loss: 18.021531568639624 grad: -1.2667141219232905
iteration: 70 loss: 16.10069459738022 grad: -1.2132457415852376
iteration: 80 loss: 14.546035144235805 grad: -1.151271131110557
iteration: 90 loss: 13.261434177344034 grad: -1.0892546923778
iteration: 100 loss: 12.182025030618764 grad: -1.030365506729194
iteration: 110 loss: 11.262320453895779 grad: -0.9756600435736729
iteration: 120 loss: 10.469418518390945 grad: -0.9253180295119847
iteration: 130 loss: 9.778902056447475 grad: -0.8791570788064677
iteration: 140 loss: 9.172254367785028 grad: -0.836857290199867
iteration: 150 loss: 8.635170919600093 grad: -0.7980614702505775
iteration: 160 loss: 8.156422343318457 grad: -0.7624194303762934
iteration: 170 loss: 7.727068461746884 grad: -0.7296063525694092
iteration: 180 loss: 7.33990251432715 grad: -0.6993289836405101
iteration: 190 loss: 6.989050263229274 grad: -0.6713261826134084
iteration: 200 loss: 6.669675681901646 grad: -0.6453669784071487
iteration: 210 loss: 6.3777614699341605 grad: -0.6212476746436786
iteration: 220 loss: 6.109943045872197 grad: -0.5987887404159808
iteration: 230 loss: 5.863381378466618 grad: -0.577831827605638
iteration: 240 loss: 5.63566443634727 grad: -0.5582370562039
iteration: 250 loss: 5.424730005075369 grad: -0.5398806107628435
iteration: 260 loss: 5.228804650753194 grad: -0.5226526442422972
iteration: 270 loss: 5.046355020298176 grad: -0.506455464918292
iteration: 280 loss: 4.876048663633852 grad: -0.491201974655875
iteration: 290 loss: 4.7167222745184025 grad: -0.4768143259589033
iteration: 300 loss: 4.567355761807076 grad: -0.4632227672857937
iteration: 310 loss: 4.427050940143048 grad: -0.45036464936669457
iteration: 320 loss: 4.29501390832109 grad: -0.43818356877747056
iteration: 330 loss: 4.170540392333516 grad: -0.42662862839495735
iteration: 340 loss: 4.053003487663194 grad: -0.4156537973970295
iteration: 350 loss: 3.941843355327778 grad: -0.4052173561256494
iteration: 360 loss: 3.836558518230504 grad: -0.3952814134070832
iteration: 370 loss: 3.7366984755594785 grad: -0.3858114858536492
iteration: 380 loss: 3.641857408440046 grad: -0.37677613029784573
iteration: 390 loss: 3.551668793534976 grad: -0.368146621875601
iteration: 400 loss: 3.465800775620292 grad: -0.3598966714206434
iteration: 410 loss: 3.38395217744003 grad: -0.35200217679201673
iteration: 420 loss: 3.3058490469046857 grad: -0.3444410035619703
iteration: 430 loss: 3.23124165922317 grad: -0.3371927911675982
iteration: 440 loss: 3.159901905646083 grad: -0.3302387811982637
iteration: 450 loss: 3.091621011987875 grad: -0.32356166496998867
iteration: 460 loss: 3.026207539425245 grad: -0.3171454479424999
iteration: 470 loss: 2.963485627718249 grad: -0.3109753288768887
iteration: 480 loss: 2.903293447308401 grad: -0.30503759192203506
iteration: 490 loss: 2.8454818319274553 grad: -0.2993195100645716
iteration: 0 loss: 73.11214032446809 grad: 40.234048227292035
iteration: 10 loss: 42.28668531461374 grad: 5.08989657666667
iteration: 20 loss: 32.40704312955899 grad: -1.1883441707086215
iteration: 30 loss: 26.526291361491488 grad: -1.8472104896472048
iteration: 40 loss: 22.4995421210746 grad: -1.7884111100964404
iteration: 50 loss: 19.542405536855128 grad: -1.642389937698892
iteration: 60 loss: 17.270015210316263 grad: -1.5031163156521345
iteration: 70 loss: 15.466139854826535 grad: -1.3829418980062116
iteration: 80 loss: 13.998361075108939 grad: -1.2804891656947701
iteration: 90 loss: 12.780463419618462 grad: -1.1925692569606199
iteration: 100 loss: 11.753655157728065 grad: -1.1163373376859442
iteration: 110 loss: 10.876358847366935 grad: -1.0495571602219642
iteration: 120 loss: 10.118289890809262 grad: -0.9905119131689677
iteration: 130 loss: 9.45684189564401 grad: -0.9378786784374684
iteration: 140 loss: 8.874786486394669 grad: -0.8906259266202472
iteration: 150 loss: 8.358758072395279 grad: -0.8479381896677585
iteration: 160 loss: 7.898226049695495 grad: -0.8091620086625575
iteration: 170 loss: 7.484779878862685 grad: -0.7737670825440548
iteration: 180 loss: 7.111620796645597 grad: -0.7413180444437801
iteration: 190 loss: 6.7731934228330495 grad: -0.7114536806816356
iteration: 200 loss: 6.464914167982815 grad: -0.683871416294932
iteration: 210 loss: 6.1829679322495394 grad: -0.6583155771354401
iteration: 220 loss: 5.924153823584137 grad: -0.6345683977212805
iteration: 230 loss: 5.685766614703467 grad: -0.6124430521174704
iteration: 240 loss: 5.465504626539976 grad: -0.5917781940447313
iteration: 250 loss: 5.26139740492247 grad: -0.5724336360262186
iteration: 260 loss: 5.071748397330214 grad: -0.5542868974986673
iteration: 270 loss: 4.895089120523732 grad: -0.5372304225665886
iteration: 280 loss: 4.730142218727409 grad: -0.521169318715642
iteration: 290 loss: 4.575791464009399 grad: -0.5060195044664154
iteration: 300 loss: 4.431057223912376 grad: -0.49170618078236883
iteration: 310 loss: 4.295076269089598 grad: -0.4781625608763497
iteration: 320 loss: 4.167085051725983 grad: -0.46532880785075503
iteration: 330 loss: 4.046405778929181 grad: -0.45315114073338403
iteration: 340 loss: 3.9324347515453155 grad: -0.4415810779106091
iteration: 350 loss: 3.8246325504368213 grad: -0.4305747934111551
iteration: 360 loss: 3.722515738078734 grad: -0.4200925664631332
iteration: 370 loss: 3.62564980979629 grad: -0.4100983086027226
iteration: 380 loss: 3.533643180873035 grad: -0.40055915562579225
iteration: 390 loss: 3.4461420364955715 grad: -0.39144511404430704
iteration: 400 loss: 3.3628259037392043 grad: -0.3827287535869394
iteration: 410 loss: 3.283403830420205 grad: -0.3743849387800218
iteration: 420 loss: 3.207611076142753 grad: -0.36639059384553396
iteration: 430 loss: 3.1352062373626226 grad: -0.35872449612157215
iteration: 440 loss: 3.0659687416050474 grad: -0.3513670939969832
iteration: 450 loss: 2.9996966568198995 grad: -0.3443003459934769
iteration: 460 loss: 2.9362047706799785 grad: -0.3375075781549103
iteration: 470 loss: 2.875322901877925 grad: -0.330973357337461
iteration: 480 loss: 2.8168944114516887 grad: -0.32468337835401717
iteration: 490 loss: 2.760774887083409 grad: -0.3186243632254057
iteration: 0 loss: 76.04294052565439 grad: 41.441433418202635
iteration: 10 loss: 45.33725905318533 grad: 5.8765625387279385
iteration: 20 loss: 34.413737652126315 grad: -0.9174278867241403
iteration: 30 loss: 28.00467560109936 grad: -1.7515426310470419
iteration: 40 loss: 23.659613739816905 grad: -1.7476702233465282
iteration: 50 loss: 20.490410775598278 grad: -1.6227589162618705
iteration: 60 loss: 18.06749283806477 grad: -1.4931923329514398
iteration: 70 loss: 16.15188116369181 grad: -1.3778903226064858
iteration: 80 loss: 14.598301083251686 grad: -1.2778660190553126
iteration: 90 loss: 13.312725760130991 grad: -1.1910035673095634
iteration: 100 loss: 12.231354105220651 grad: -1.1150213237125466
iteration: 110 loss: 11.309256828293718 grad: -1.048010071788004
iteration: 120 loss: 10.513827667135315 grad: -0.9884547558980907
iteration: 130 loss: 9.820805206782593 grad: -0.935161145182638
iteration: 140 loss: 9.211752226068215 grad: -0.8871809319197139
iteration: 150 loss: 8.672401572811458 grad: -0.8437523433778886
iteration: 160 loss: 8.191537968838889 grad: -0.8042559295586155
iteration: 170 loss: 7.760222577426785 grad: -0.7681820934869685
iteration: 180 loss: 7.3712432075128 grad: -0.7351071506786295
iteration: 190 loss: 7.018716831355215 grad: -0.7046754863509024
iteration: 200 loss: 6.697797216584149 grad: -0.6765860807936626
iteration: 210 loss: 6.404456535205224 grad: -0.6505821937318531
iteration: 220 loss: 6.135319955396136 grad: -0.626443362157606
iteration: 230 loss: 5.887538782279122 grad: -0.6039791156151886
iteration: 240 loss: 5.658692048475876 grad: -0.5830239838904913
iteration: 250 loss: 5.446709374962004 grad: -0.5634334899896675
iteration: 260 loss: 5.2498099240019656 grad: -0.5450809034998897
iteration: 270 loss: 5.066453659669605 grad: -0.5278545874406009
iteration: 280 loss: 4.895302116258625 grad: -0.5116558131910185
iteration: 290 loss: 4.735186580156386 grad: -0.4963969481218066
iteration: 300 loss: 4.5850821020194 grad: -0.4819999425959512
iteration: 310 loss: 4.444086131037486 grad: -0.46839505936609677
iteration: 320 loss: 4.311400840923894 grad: -0.45551980068433784
iteration: 330 loss: 4.186318425214929 grad: -0.44331799776962677
iteration: 340 loss: 4.068208796556106 grad: -0.4317390344319103
iteration: 350 loss: 3.95650924430261 grad: -0.42073718218954925
iteration: 360 loss: 3.850715696684512 grad: -0.4102710285403366
iteration: 370 loss: 3.750375304910821 grad: -0.40030298344985427
iteration: 380 loss: 3.655080122025538 grad: -0.39079885182021723
iteration: 390 loss: 3.5644616928326616 grad: -0.3817274618580625
iteration: 400 loss: 3.4781864055657934 grad: -0.37306034099370844
iteration: 410 loss: 3.395951483273827 grad: -0.3647714324050568
iteration: 420 loss: 3.3174815147038896 grad: -0.3568368463399333
iteration: 430 loss: 3.242525441999946 grad: -0.3492346413628445
iteration: 440 loss: 3.1708539366722603 grad: -0.3419446314182828
iteration: 450 loss: 3.1022571067962663 grad: -0.33494821523531565
iteration: 460 loss: 3.036542487762414 grad: -0.32822822512279537
iteration: 470 loss: 2.9735332765718256 grad: -0.32176879264141434
iteration: 480 loss: 2.9130667759842583 grad: -0.31555522900408006
iteration: 490 loss: 2.854993020051717 grad: -0.30957391836259207
iteration: 0 loss: 77.80201517134168 grad: 24.305376886212173
iteration: 10 loss: 47.02425679628884 grad: 6.659786616101694
iteration: 20 loss: 35.203236502630574 grad: 1.107867115868749
iteration: 30 loss: 28.434824707260592 grad: -0.4227049292631522
iteration: 40 loss: 23.918226986324893 grad: -0.8865609104034204
iteration: 50 loss: 20.655910707844587 grad: -1.0202225996988492
iteration: 60 loss: 18.178159564280183 grad: -1.0384702961307446
iteration: 70 loss: 16.22838161463177 grad: -1.0137398008139535
iteration: 80 loss: 14.652604052836729 grad: -0.9732848580490523
iteration: 90 loss: 13.35213489859482 grad: -0.928152759931935
iteration: 100 loss: 12.260512177952664 grad: -0.8829995454043047
iteration: 110 loss: 11.331214032740284 grad: -0.8397752593994605
iteration: 120 loss: 10.530643086519541 grad: -0.7992263528431511
iteration: 130 loss: 9.833900760301235 grad: -0.7615492896808732
iteration: 140 loss: 9.222128796588116 grad: -0.7266894025821491
iteration: 150 loss: 8.680775189421825 grad: -0.6944824772548278
iteration: 160 loss: 8.19842797693863 grad: -0.6647232193265111
iteration: 170 loss: 7.766010092361884 grad: -0.6371984391346825
iteration: 180 loss: 7.376210682698622 grad: -0.6117027413135119
iteration: 190 loss: 7.023075339644782 grad: -0.5880453931827293
iteration: 200 loss: 6.701705569408618 grad: -0.5660527274842555
iteration: 210 loss: 6.408034877369573 grad: -0.5455683151184163
iteration: 220 loss: 6.13865955784174 grad: -0.5264520718266472
iteration: 230 loss: 5.890709178529588 grad: -0.5085789078712508
iteration: 240 loss: 5.661746289884294 grad: -0.4918372371252793
iteration: 250 loss: 5.449687937256178 grad: -0.4761275057167294
iteration: 260 loss: 5.252743636058188 grad: -0.4613608164515083
iteration: 270 loss: 5.069365916199331 grad: -0.44745768031635424
iteration: 280 loss: 4.898210561173983 grad: -0.4343469026964585
iteration: 290 loss: 4.738104395325949 grad: -0.4219645999325457
iteration: 300 loss: 4.588018999537987 grad: -0.4102533362201232
iteration: 310 loss: 4.447049121132667 grad: -0.3991613686983952
iteration: 320 loss: 4.314394828939881 grad: -0.3886419882583491
iteration: 330 loss: 4.189346677603887 grad: -0.37865294420869633
iteration: 340 loss: 4.071273305897979 grad: -0.36915594196933615
iteration: 350 loss: 3.959611016098444 grad: -0.3601162041384637
iteration: 360 loss: 3.853854975253729 grad: -0.35150208645493897
iteration: 370 loss: 3.753551751674298 grad: -0.34328474127914244
iteration: 380 loss: 3.6582929564109774 grad: -0.3354378222113983
iteration: 390 loss: 3.5677098037361 grad: -0.32793722434793515
iteration: 400 loss: 3.481468439533025 grad: -0.32076085544284394
iteration: 410 loss: 3.3992659142334314 grad: -0.3138884339090699
iteration: 420 loss: 3.320826699035272 grad: -0.3073013101628954
iteration: 430 loss: 3.2458996619181395 grad: -0.3009823083060187
iteration: 440 loss: 3.174255434294537 grad: -0.2949155855580324
iteration: 450 loss: 3.105684110757383 grad: -0.28908650720979734
iteration: 460 loss: 3.039993233872627 grad: -0.2834815351737421
iteration: 470 loss: 2.9770060237024567 grad: -0.278088128468154
iteration: 480 loss: 2.916559818136689 grad: -0.27289465419575953
iteration: 490 loss: 2.8585046953651063 grad: -0.2678903077679564
iteration: 0 loss: 77.02938503223207 grad: 29.742144706610333
iteration: 10 loss: 45.713588092637316 grad: 7.406055763609542
iteration: 20 loss: 34.41582186319106 grad: 0.4112914306919023
iteration: 30 loss: 27.95492337157664 grad: -1.1341764593463972
iteration: 40 loss: 23.606747148923866 grad: -1.4452069314435645
iteration: 50 loss: 20.443443774977162 grad: -1.4512975552263994
iteration: 60 loss: 18.027638899771713 grad: -1.3797872062244523
iteration: 70 loss: 16.11852484229588 grad: -1.2928194066251169
iteration: 80 loss: 14.5704699126084 grad: -1.2082711419269963
iteration: 90 loss: 13.289489456553982 grad: -1.130930991888067
iteration: 100 loss: 12.21192047715656 grad: -1.0615064511835564
iteration: 110 loss: 11.292977393490652 grad: -0.9994547540017672
iteration: 120 loss: 10.500178504715139 grad: -0.943918707512502
iteration: 130 loss: 9.809363867224398 grad: -0.8940374170743325
iteration: 140 loss: 9.202176966646242 grad: -0.8490408803654919
iteration: 150 loss: 8.664414325744866 grad: -0.8082687557317113
iteration: 160 loss: 8.184910880826857 grad: -0.7711635736883939
iteration: 170 loss: 7.7547675346253815 grad: -0.7372567583213976
iteration: 180 loss: 7.366803725358171 grad: -0.7061540597091008
iteration: 190 loss: 7.015161772995601 grad: -0.6775225630095717
iteration: 200 loss: 6.695015910533254 grad: -0.6510797731514935
iteration: 210 loss: 6.402354960102499 grad: -0.6265846773708804
iteration: 220 loss: 6.133817739242246 grad: -0.6038305065555897
iteration: 230 loss: 5.886566825414733 grad: -0.582638892417592
iteration: 240 loss: 5.658190626625996 grad: -0.5628551481979285
iteration: 250 loss: 5.446626614001409 grad: -0.5443444454678914
iteration: 260 loss: 5.250100564489333 grad: -0.5269887034274615
iteration: 270 loss: 5.0670780488666916 grad: -0.5106840449592172
iteration: 280 loss: 4.8962253800438535 grad: -0.49533870466187335
iteration: 290 loss: 4.736377938222673 grad: -0.48087129873790885
iteration: 300 loss: 4.586514298027189 grad: -0.46720938595627226
iteration: 310 loss: 4.445734955623183 grad: -0.45428826398745425
iteration: 320 loss: 4.3132447302203385 grad: -0.4420499571229421
iteration: 330 loss: 4.188338121177656 grad: -0.4304423604921458
iteration: 340 loss: 4.070387058174922 grad: -0.4194185129722055
iteration: 350 loss: 3.9588306009566376 grad: -0.40893597651371294
iteration: 360 loss: 3.8531662365889217 grad: -0.3989563039362969
iteration: 370 loss: 3.7529424929313624 grad: -0.38944458065664855
iteration: 380 loss: 3.657752642192794 grad: -0.38036902850758536
iteration: 390 loss: 3.5672293117235396 grad: -0.3717006619503103
iteration: 400 loss: 3.481039853389575 grad: -0.363412988695503
iteration: 410 loss: 3.3988823500413177 grad: -0.3554817481259906
iteration: 420 loss: 3.320482159298618 grad: -0.34788468202650363
iteration: 430 loss: 3.245588912317916 grad: -0.3406013330299385
iteration: 440 loss: 3.1739738992965942 grad: -0.33361286692761494
iteration: 450 loss: 3.1054277849144785 grad: -0.32690191559666226
iteration: 460 loss: 3.039758606222098 grad: -0.3204524377970793
iteration: 470 loss: 2.9767900131431486 grad: -0.3142495955046781
iteration: 480 loss: 2.9163597180352308 grad: -0.30827964379031686
iteration: 490 loss: 2.8583181259509316 grad: -0.30252983254339294
iteration: 0 loss: 77.01118109018316 grad: 31.164678715805962
iteration: 10 loss: 44.94015583435651 grad: 6.462159706750878
iteration: 20 loss: 33.80460311595322 grad: 0.1995053998437363
iteration: 30 loss: 27.447655308881437 grad: -1.087724454193226
iteration: 40 loss: 23.176822028964 grad: -1.3357735741836068
iteration: 50 loss: 20.072463719062853 grad: -1.3351647884593447
iteration: 60 loss: 17.702470044797067 grad: -1.2723075831089272
iteration: 70 loss: 15.829736728199247 grad: -1.1971857798787773
iteration: 80 loss: 14.311172274715256 grad: -1.1240381875861414
iteration: 90 loss: 13.054534431660592 grad: -1.056685209938364
iteration: 100 loss: 11.997374429095668 grad: -0.9957248406062726
iteration: 110 loss: 11.095771424875403 grad: -0.9407672817151331
iteration: 120 loss: 10.31787380719158 grad: -0.8911679915613329
iteration: 130 loss: 9.639990916841043 grad: -0.8462705837968931
iteration: 140 loss: 9.04412285069388 grad: -0.8054815437695533
iteration: 150 loss: 8.516342040870665 grad: -0.7682861953931355
iteration: 160 loss: 8.04570031766188 grad: -0.7342448396083295
iteration: 170 loss: 7.623471488952253 grad: -0.7029832018998221
iteration: 180 loss: 7.242614538521758 grad: -0.674182247845805
iteration: 190 loss: 6.897385639244641 grad: -0.6475690257350739
iteration: 200 loss: 6.583052815361746 grad: -0.6229089277107558
iteration: 210 loss: 6.295682823029025 grad: -0.5999993119745837
iteration: 220 loss: 6.031979742940456 grad: -0.5786642944196277
iteration: 230 loss: 5.789161192117387 grad: -0.5587505011529447
iteration: 240 loss: 5.564862296771852 grad: -0.5401235959230942
iteration: 250 loss: 5.357060419414128 grad: -0.5226654284308817
iteration: 260 loss: 5.164015587135881 grad: -0.5062716800097042
iteration: 270 loss: 4.984222928331994 grad: -0.49084990896460823
iteration: 280 loss: 4.816374386220559 grad: -0.4763179185966869
iteration: 290 loss: 4.659327665693127 grad: -0.4626023872251812
iteration: 300 loss: 4.512080868915208 grad: -0.4496377121871361
iteration: 310 loss: 4.373751640898098 grad: -0.43736502962445495
iteration: 320 loss: 4.243559917361588 grad: -0.4257313795094018
iteration: 330 loss: 4.120813570075742 grad: -0.41468899132334625
iteration: 340 loss: 4.004896398123657 grad: -0.40419467048282776
iteration: 350 loss: 3.8952580302716755 grad: -0.39420926930030714
iteration: 360 loss: 3.791405393289675 grad: -0.3846972292001454
iteration: 370 loss: 3.692895470472939 grad: -0.37562618325427366
iteration: 380 loss: 3.599329128685771 grad: -0.3669666099864184
iteration: 390 loss: 3.5103458346981244 grad: -0.3586915309175511
iteration: 400 loss: 3.4256191151029354 grad: -0.350776245564285
iteration: 410 loss: 3.3448526407362973 grad: -0.34319809861474104
iteration: 420 loss: 3.2677768377974314 grad: -0.3359362748384182
iteration: 430 loss: 3.194145944973694 grad: -0.3289716179732526
iteration: 440 loss: 3.123735449679936 grad: -0.32228647040228153
iteration: 450 loss: 3.056339847733687 grad: -0.3158645309061622
iteration: 460 loss: 2.9917706799307613 grad: -0.30969072817377097
iteration: 470 loss: 2.929854806464267 grad: -0.30375110808526606
iteration: 480 loss: 2.8704328863132105 grad: -0.2980327330615858
iteration: 490 loss: 2.813358033783557 grad: -0.2925235920105063
iteration: 0 loss: 76.5738290118793 grad: 30.890249012867102
iteration: 10 loss: 45.48365782446294 grad: 6.329025437080896
iteration: 20 loss: 34.3621399551881 grad: 0.3304861617479738
iteration: 30 loss: 27.905500562374062 grad: -0.9241318192066992
iteration: 40 loss: 23.54709423668919 grad: -1.2083783173199527
iteration: 50 loss: 20.375764045574837 grad: -1.2487336597799081
iteration: 60 loss: 17.955193366238827 grad: -1.217665502405936
iteration: 70 loss: 16.043785056187787 grad: -1.1647968172552177
iteration: 80 loss: 14.495101543115796 grad: -1.106642381586542
iteration: 90 loss: 13.21456721824765 grad: -1.0492197509893453
iteration: 100 loss: 12.138118570751802 grad: -0.9947641047800933
iteration: 110 loss: 11.220705435966135 grad: -0.9440150850751032
iteration: 120 loss: 10.429672831647002 grad: -0.8970870122961401
iteration: 130 loss: 9.740747680332106 grad: -0.8538297868974364
iteration: 140 loss: 9.13549979187455 grad: -0.8139881010395452
iteration: 150 loss: 8.59967795461214 grad: -0.7772743169827938
iteration: 160 loss: 8.122086506500098 grad: -0.7434021036440501
iteration: 170 loss: 7.693807132927801 grad: -0.712101451166815
iteration: 180 loss: 7.307647650247046 grad: -0.6831246147245164
iteration: 190 loss: 6.957743842328207 grad: -0.6562476080714352
iteration: 200 loss: 6.639266813811446 grad: -0.6312695535628647
iteration: 210 loss: 6.348204534823543 grad: -0.6080110632936657
iteration: 220 loss: 6.081196478486825 grad: -0.5863122537669484
iteration: 230 loss: 5.835406859608611 grad: -0.566030700137617
iteration: 240 loss: 5.608426343853063 grad: -0.5470394800677781
iteration: 250 loss: 5.398195031165125 grad: -0.5292253744923009
iteration: 260 loss: 5.202941526723159 grad: -0.5124872488727152
iteration: 270 loss: 5.02113431096144 grad: -0.4967346158767446
iteration: 280 loss: 4.851442607496458 grad: -0.4818863692595017
iteration: 290 loss: 4.692704654333738 grad: -0.4678696738137815
iteration: 300 loss: 4.543901795675703 grad: -0.454618994716732
iteration: 310 loss: 4.404137186876307 grad: -0.4420752498233569
iteration: 320 loss: 4.272618183048383 grad: -0.4301850695481977
iteration: 330 loss: 4.148641689782752 grad: -0.41890015043805595
iteration: 340 loss: 4.031581911467459 grad: -0.40817669009951196
iteration: 350 loss: 3.920880052290944 grad: -0.39797489266255115
iteration: 360 loss: 3.8160356168568796 grad: -0.3882585353660729
iteration: 370 loss: 3.716599028373118 grad: -0.3789945881138261
iteration: 380 loss: 3.622165337764355 grad: -0.3701528789642442
iteration: 390 loss: 3.532368840484444 grad: -0.36170579949063375
iteration: 400 loss: 3.446878452112411 grad: -0.3536280447907212
iteration: 410 loss: 3.3653937210520284 grad: -0.34589638365052056
iteration: 420 loss: 3.2876413784383245 grad: -0.3384894549909649
iteration: 430 loss: 3.2133723428232357 grad: -0.3313875872602058
iteration: 440 loss: 3.1423591113486755 grad: -0.3245726378921703
iteration: 450 loss: 3.0743934805680264 grad: -0.31802785034379044
iteration: 460 loss: 3.009284549411818 grad: -0.31173772655884424
iteration: 470 loss: 2.9468569644653724 grad: -0.3056879129937955
iteration: 480 loss: 2.8869493739911047 grad: -0.29986509858755367
iteration: 490 loss: 2.8294130623586913 grad: -0.2942569232686715
iteration: 0 loss: 77.67629471884875 grad: 23.88847629450308
iteration: 10 loss: 46.764507718232906 grad: 7.227356645268953
iteration: 20 loss: 35.11865359692722 grad: 1.7008132402086937
iteration: 30 loss: 28.407391766823192 grad: -0.018698712429628075
iteration: 40 loss: 23.912729648088565 grad: -0.6419501803848897
iteration: 50 loss: 20.660013788732012 grad: -0.8853386222031177
iteration: 60 loss: 18.186679835898985 grad: -0.9756663263892287
iteration: 70 loss: 16.238881208891744 grad: -0.9977209070872387
iteration: 80 loss: 14.663863056049173 grad: -0.9875223139907375
iteration: 90 loss: 13.363524131933383 grad: -0.9618686243746011
iteration: 100 loss: 12.27170559311346 grad: -0.929121491298444
iteration: 110 loss: 11.342047127220605 grad: -0.8936226316679744
iteration: 120 loss: 10.541039651083008 grad: -0.8576757541042577
iteration: 130 loss: 9.843833503559237 grad: -0.822502766518519
iteration: 140 loss: 9.231597518205406 grad: -0.7887322106919297
iteration: 150 loss: 8.689794384976597 grad: -0.7566601979004017
iteration: 160 loss: 8.207019679216955 grad: -0.7263947644818146
iteration: 170 loss: 7.774199703495738 grad: -0.6979378922089887
iteration: 180 loss: 7.384024555006425 grad: -0.671233025401409
iteration: 190 loss: 7.030539396901862 grad: -0.6461929471084116
iteration: 200 loss: 6.708844553729457 grad: -0.6227162203154075
iteration: 210 loss: 6.414871973357094 grad: -0.6006968532770456
iteration: 220 loss: 6.145216242865486 grad: -0.580029896845117
iteration: 230 loss: 5.897005206170723 grad: -0.5606145785619446
iteration: 240 loss: 5.667799749110103 grad: -0.5423559399377864
iteration: 250 loss: 5.455515351871327 grad: -0.5251655664368979
iteration: 260 loss: 5.2583600827429935 grad: -0.5089617733072841
iteration: 270 loss: 5.0747851480759065 grad: -0.4936694723900473
iteration: 280 loss: 4.9034451292827725 grad: -0.47921985987875787
iteration: 290 loss: 4.743165763784956 grad: -0.4655500119064948
iteration: 300 loss: 4.592917652274991 grad: -0.4526024414811334
iteration: 310 loss: 4.451794659360241 grad: -0.440324649220473
iteration: 320 loss: 4.318996059323333 grad: -0.42866868699828287
iteration: 330 loss: 4.193811691488421 grad: -0.41759074517319744
iteration: 340 loss: 4.075609550217781 grad: -0.40705076876212576
iteration: 350 loss: 3.9638253566665544 grad: -0.397012104616057
iteration: 360 loss: 3.8579537531772985 grad: -0.387441179642087
iteration: 370 loss: 3.7575408336104954 grad: -0.37830720893014824
iteration: 380 loss: 3.662177779338642 grad: -0.36958193198808553
iteration: 390 loss: 3.57149541485755 grad: -0.3612393749707041
iteration: 400 loss: 3.485159531874656 grad: -0.3532556366811602
iteration: 410 loss: 3.4028668584348285 grad: -0.3456086961444943
iteration: 420 loss: 3.3243415717772056 grad: -0.33827823964991305
iteration: 430 loss: 3.2493322713801884 grad: -0.3312455052962554
iteration: 440 loss: 3.1776093429858436 grad: -0.3244931432319065
iteration: 450 loss: 3.1089626560290693 grad: -0.31800508994252946
iteration: 460 loss: 3.0431995463717696 grad: -0.31176645509892587
iteration: 470 loss: 2.98014304400654 grad: -0.30576341962824083
iteration: 480 loss: 2.919630311771884 grad: -0.2999831438120024
iteration: 490 loss: 2.8615112663871347 grad: -0.2944136843429838
iteration: 0 loss: 75.20921834643381 grad: 38.27456479832095
iteration: 10 loss: 43.85254487893208 grad: 5.442980268323024
iteration: 20 loss: 33.237739712954905 grad: -0.7212080790213632
iteration: 30 loss: 27.066866828856828 grad: -1.5656077424093486
iteration: 40 loss: 22.89620639072566 grad: -1.61557929031995
iteration: 50 loss: 19.855281602774085 grad: -1.5270489133217011
iteration: 60 loss: 17.528496394510615 grad: -1.4195776539307554
iteration: 70 loss: 15.686484767863083 grad: -1.3181791341029088
iteration: 80 loss: 14.190436939796406 grad: -1.2275504533682722
iteration: 90 loss: 12.950704767838674 grad: -1.1475293015491685
iteration: 100 loss: 11.90650123592315 grad: -1.0768545045138413
iteration: 110 loss: 11.015011258680405 grad: -1.0141615776525859
iteration: 120 loss: 10.24514009108874 grad: -0.9582382100725351
iteration: 130 loss: 9.57372686443798 grad: -0.9080676556236233
iteration: 140 loss: 8.983149494431183 grad: -0.8628124020822332
iteration: 150 loss: 8.459754226172395 grad: -0.8217846435346852
iteration: 160 loss: 7.99279418627473 grad: -0.7844176221582277
iteration: 170 loss: 7.573693102017304 grad: -0.7502414711070629
iteration: 180 loss: 7.195522959190757 grad: -0.71886385381382
iteration: 190 loss: 6.852626065496803 grad: -0.6899547388437623
iteration: 200 loss: 6.540336797400869 grad: -0.6632344892908704
iteration: 210 loss: 6.254773538233539 grad: -0.638464531504004
iteration: 220 loss: 5.992680923201946 grad: -0.6154400088710694
iteration: 230 loss: 5.751308717120165 grad: -0.5939839600699555
iteration: 240 loss: 5.528317752729766 grad: -0.5739426711177646
iteration: 250 loss: 5.3217061205645715 grad: -0.5551819359080541
iteration: 260 loss: 5.1297506956362176 grad: -0.5375840245342254
iteration: 270 loss: 4.950960406019434 grad: -0.5210452070685166
iteration: 280 loss: 4.7840385815210205 grad: -0.5054737165641864
iteration: 290 loss: 4.627852389265048 grad: -0.4907880620180554
iteration: 300 loss: 4.48140784819229 grad: -0.4769156222553654
iteration: 310 loss: 4.343829270480538 grad: -0.4637914669465279
iteration: 320 loss: 4.214342242005594 grad: -0.45135736253292924
iteration: 330 loss: 4.0922594517724065 grad: -0.4395609296748606
iteration: 340 loss: 3.9769688297947554 grad: -0.42835492563208577
iteration: 350 loss: 3.86792356696279 grad: -0.4176966302546568
iteration: 360 loss: 3.764633678082811 grad: -0.40754731837213287
iteration: 370 loss: 3.6666588371874904 grad: -0.3978718046000268
iteration: 380 loss: 3.5736022671773586 grad: -0.3886380491386768
iteration: 390 loss: 3.4851055074622392 grad: -0.3798168151760104
iteration: 400 loss: 3.400843916135521 grad: -0.37138137013782446
iteration: 410 loss: 3.320522789385326 grad: -0.3633072243453566
iteration: 420 loss: 3.2438740017125216 grad: -0.35557190170735986
iteration: 430 loss: 3.1706530873819827 grad: -0.348154737944282
iteration: 440 loss: 3.1006366970796493 grad: -0.34103670255558155
iteration: 450 loss: 3.0336203748039554 grad: -0.33420024132879483
iteration: 460 loss: 2.9694166090188117 grad: -0.32762913667520904
iteration: 470 loss: 2.9078531194733674 grad: -0.32130838348106106
iteration: 480 loss: 2.8487713471635785 grad: -0.3152240785004364
iteration: 490 loss: 2.7920251199476214 grad: -0.3093633215985717
iteration: 0 loss: 74.85074364097342 grad: 31.85712042905612
iteration: 10 loss: 42.23278803506375 grad: 5.222691432958417
iteration: 20 loss: 31.979160543962973 grad: 0.15496707008377447
iteration: 30 loss: 26.09248932418727 grad: -0.9402828044880385
iteration: 40 loss: 22.11891998157154 grad: -1.223383345980206
iteration: 50 loss: 19.216877634559236 grad: -1.2764909292128488
iteration: 60 loss: 16.99157451973532 grad: -1.2523589533334534
iteration: 70 loss: 15.226279169163861 grad: -1.201335394514345
iteration: 80 loss: 13.789898424723127 grad: -1.1423775944255676
iteration: 90 loss: 12.597671054756264 grad: -1.083095575051355
iteration: 100 loss: 11.592028643035105 grad: -1.0265419748191296
iteration: 110 loss: 10.732352868633143 grad: -0.973819671760011
iteration: 120 loss: 9.989095368437699 grad: -0.9251733099044032
iteration: 130 loss: 9.340212987505248 grad: -0.8804735212691505
iteration: 140 loss: 8.768910026855709 grad: -0.839439533207113
iteration: 150 loss: 8.26215578853457 grad: -0.8017429600318117
iteration: 160 loss: 7.809681230921714 grad: -0.7670556000832618
iteration: 170 loss: 7.403282075655335 grad: -0.7350702434438197
iteration: 180 loss: 7.036323790439176 grad: -0.7055083794035761
iteration: 190 loss: 6.7033829986241305 grad: -0.6781216278811086
iteration: 200 loss: 6.399983172600101 grad: -0.6526902982855084
iteration: 210 loss: 6.122396789364467 grad: -0.6290207778094655
iteration: 220 loss: 5.8674951706345215 grad: -0.6069425888828275
iteration: 230 loss: 5.632633081339009 grad: -0.5863055142500329
iteration: 240 loss: 5.4155590291228775 grad: -0.5669769624162583
iteration: 250 loss: 5.214344815840749 grad: -0.5488396323357847
iteration: 260 loss: 5.0273296818611675 grad: -0.5317894807223292
iteration: 270 loss: 4.853075632122537 grad: -0.5157339702499022
iteration: 280 loss: 4.690331416069677 grad: -0.5005905673915051
iteration: 290 loss: 4.538003267048018 grad: -0.48628545687378577
iteration: 300 loss: 4.395130966688405 grad: -0.47275244157492846
iteration: 310 loss: 4.2608681376759705 grad: -0.4599320000116085
iteration: 320 loss: 4.134465919061106 grad: -0.4477704772634375
iteration: 330 loss: 4.01525936628712 grad: -0.43621938875145416
iteration: 340 loss: 3.9026560603228972 grad: -0.4252348194999913
iteration: 350 loss: 3.7961265188348743 grad: -0.4147769043001014
iteration: 360 loss: 3.695196085803714 grad: -0.4048093765629732
iteration: 370 loss: 3.599438040701402 grad: -0.39529917564157735
iteration: 380 loss: 3.5084677188448783 grad: -0.3862161040574129
iteration: 390 loss: 3.421937474231899 grad: -0.37753252744684146
iteration: 400 loss: 3.33953234754261 grad: -0.3692231111840649
iteration: 410 loss: 3.260966326970447 grad: -0.36126458858560306
iteration: 420 loss: 3.1859791095065373 grad: -0.3536355563882412
iteration: 430 loss: 3.1143332863898237 grad: -0.34631629384732593
iteration: 440 loss: 3.0458118894103294 grad: -0.3392886023484541
iteration: 450 loss: 2.9802162453271936 grad: -0.3325356628822941
iteration: 460 loss: 2.9173640942706034 grad: -0.3260419091152156
iteration: 470 loss: 2.857087935072245 grad: -0.31979291411045624
iteration: 480 loss: 2.7992335662841636 grad: -0.31377528902617
iteration: 490 loss: 2.743658796461287 grad: -0.30797659234650593
iteration: 0 loss: 75.88682355832006 grad: 36.07827713316739
iteration: 10 loss: 43.52155960543187 grad: 6.776763586832671
iteration: 20 loss: 32.89676098111213 grad: -0.3256690842075803
iteration: 30 loss: 26.80923621605077 grad: -1.6078862384822337
iteration: 40 loss: 22.697934049314632 grad: -1.771858634542355
iteration: 50 loss: 19.696574004195057 grad: -1.6976225253363042
iteration: 60 loss: 17.396983676861034 grad: -1.5775803365140162
iteration: 70 loss: 15.574422543073792 grad: -1.4582899264998452
iteration: 80 loss: 14.092814471679752 grad: -1.350542297275972
iteration: 90 loss: 12.864161659902349 grad: -1.2556214127547232
iteration: 100 loss: 11.82870607274733 grad: -1.1723377696423252
iteration: 110 loss: 10.944295348747945 grad: -1.099036242952117
iteration: 120 loss: 10.180274797152855 grad: -1.0341627144519254
iteration: 130 loss: 9.513784131014125 grad: -0.9763955642432399
iteration: 140 loss: 8.927412026035116 grad: -0.9246465853809815
iteration: 150 loss: 8.407656130859984 grad: -0.8780275333763932
iteration: 160 loss: 7.943880798239454 grad: -0.8358126182544456
iteration: 170 loss: 7.527593168324322 grad: -0.7974056343158281
iteration: 180 loss: 7.15192897590222 grad: -0.7623133397108434
iteration: 190 loss: 6.811280098277124 grad: -0.7301245318101548
iteration: 200 loss: 6.5010200664911615 grad: -0.7004937637165136
iteration: 210 loss: 6.2172986357262 grad: -0.6731286818970609
iteration: 220 loss: 5.956885903692679 grad: -0.6477801411382857
iteration: 230 loss: 5.717052543961348 grad: -0.6242344391434012
iteration: 240 loss: 5.495476740687206 grad: -0.6023071708947552
iteration: 250 loss: 5.290171121690822 grad: -0.5818383265014998
iteration: 260 loss: 5.0994248471949755 grad: -0.5626883497919652
iteration: 270 loss: 4.921757308947773 grad: -0.5447349446540112
iteration: 280 loss: 4.75588081266952 grad: -0.5278704678623591
iteration: 290 loss: 4.600670275311421 grad: -0.5119997855183558
iteration: 300 loss: 4.4551384468317 grad: -0.49703849880481143
iteration: 310 loss: 4.318415517406513 grad: -0.4829114661439642
iteration: 320 loss: 4.1897322316924805 grad: -0.46955156495133954
iteration: 330 loss: 4.068405827140685 grad: -0.4568986483905548
iteration: 340 loss: 3.9538282611825624 grad: -0.4448986618621761
iteration: 350 loss: 3.8454563048678874 grad: -0.43350289113742124
iteration: 360 loss: 3.74280316725832 grad: -0.4226673196111437
iteration: 370 loss: 3.6454313820823203 grad: -0.41235207649229766
iteration: 380 loss: 3.552946740600667 grad: -0.4025209611652121
iteration: 390 loss: 3.4649930958310673 grad: -0.3931410316582514
iteration: 400 loss: 3.3812478958499756 grad: -0.38418224731010375
iteration: 410 loss: 3.3014183298052804 grad: -0.37561715745040397
iteration: 420 loss: 3.2252379909854922 grad: -0.36742062930360386
iteration: 430 loss: 3.1524639779704557 grad: -0.35956960945395533
iteration: 440 loss: 3.0828743683453634 grad: -0.35204291412988803
iteration: 450 loss: 3.0162660104169845 grad: -0.34482104432018473
iteration: 460 loss: 2.952452587292514 grad: -0.33788602235527876
iteration: 470 loss: 2.891262915010223 grad: -0.3312212471005141
iteration: 480 loss: 2.832539442432996 grad: -0.32481136533476673
iteration: 490 loss: 2.7761369256052357 grad: -0.31864215724358613
iteration: 0 loss: 75.54259752095415 grad: 38.70381305653365
iteration: 10 loss: 44.523988436099565 grad: 5.900241072280295
iteration: 20 loss: 33.803132769920644 grad: -0.6806408396155919
iteration: 30 loss: 27.540230549459363 grad: -1.5910244025654672
iteration: 40 loss: 23.292287276158422 grad: -1.6437940054956754
iteration: 50 loss: 20.18918792514821 grad: -1.5499724697340218
iteration: 60 loss: 17.81297629041038 grad: -1.4377163745480386
iteration: 70 loss: 15.931597316841218 grad: -1.3329170161955144
iteration: 80 loss: 14.403952815395769 grad: -1.2399678161978527
iteration: 90 loss: 13.13860799473731 grad: -1.1583466123270774
iteration: 100 loss: 12.073421346284682 grad: -1.086534002169161
iteration: 110 loss: 11.16455881855555 grad: -1.0229982145512118
iteration: 120 loss: 10.380157744197732 grad: -0.9664215256181721
iteration: 130 loss: 9.696474934927954 grad: -0.9157211708883923
iteration: 140 loss: 9.095443089954973 grad: -0.8700182882335743
iteration: 150 loss: 8.563064963060077 grad: -0.8285991328682674
iteration: 160 loss: 8.088326193557537 grad: -0.790880642027106
iteration: 170 loss: 7.662440273481232 grad: -0.7563826665276662
iteration: 180 loss: 7.278312455111642 grad: -0.724706347096286
iteration: 190 loss: 6.9301516580296925 grad: -0.6955174699448804
iteration: 200 loss: 6.613184654129906 grad: -0.668533666871568
iteration: 210 loss: 6.323442328908848 grad: -0.6435145281501504
iteration: 220 loss: 6.05759762924295 grad: -0.6202539102426765
iteration: 230 loss: 5.812841161366412 grad: -0.5985738997431103
iteration: 240 loss: 5.586784605802635 grad: -0.5783200336935028
iteration: 250 loss: 5.377384950520578 grad: -0.5593574799674237
iteration: 260 loss: 5.182884488824708 grad: -0.5415679575130907
iteration: 270 loss: 5.001762884721591 grad: -0.524847231869253
iteration: 280 loss: 4.832698567947773 grad: -0.5091030620356707
iteration: 290 loss: 4.674537408623068 grad: -0.49425350462978734
iteration: 300 loss: 4.526267120638956 grad: -0.4802255033001546
iteration: 310 loss: 4.386996209262096 grad: -0.46695370775672074
iteration: 320 loss: 4.255936550187099 grad: -0.4543794790636238
iteration: 330 loss: 4.132388890815822 grad: -0.44245004712624836
iteration: 340 loss: 4.015730718433546 grad: -0.43111779338291867
iteration: 350 loss: 3.905406057241883 grad: -0.4203396371529646
iteration: 360 loss: 3.800916846388108 grad: -0.4100765083103418
iteration: 370 loss: 3.7018156209268125 grad: -0.4002928922469482
iteration: 380 loss: 3.60769927211992 grad: -0.390956435683906
iteration: 390 loss: 3.518203706205978 grad: -0.382037603946404
iteration: 400 loss: 3.432999254566849 grad: -0.373509381960762
iteration: 410 loss: 3.351786715051441 grad: -0.3653470125532614
iteration: 420 loss: 3.274293925688529 grad: -0.35752776669882036
iteration: 430 loss: 3.200272789271656 grad: -0.350030741237094
iteration: 440 loss: 3.1294966812263074 grad: -0.3428366802850692
iteration: 450 loss: 3.061758184501865 grad: -0.33592781716044284
iteration: 460 loss: 2.9968671044489383 grad: -0.32928773411386614
iteration: 470 loss: 2.934648724203767 grad: -0.32290123756986033
iteration: 480 loss: 2.874942267341339 grad: -0.3167542469113622
iteration: 490 loss: 2.817599539683711 grad: -0.31083369512351544
iteration: 0 loss: 72.92700169600927 grad: 47.01907658849356
iteration: 10 loss: 41.59948317690271 grad: 4.0539642973933825
iteration: 20 loss: 31.70751190778858 grad: -1.36605103717574
iteration: 30 loss: 25.94752852061573 grad: -1.9016921089370555
iteration: 40 loss: 22.039085372196414 grad: -1.8564399146058257
iteration: 50 loss: 19.175121476821086 grad: -1.7247673273978
iteration: 60 loss: 16.973166200535616 grad: -1.5898923793225876
iteration: 70 loss: 15.222436703353548 grad: -1.467969706056142
iteration: 80 loss: 13.795147249004087 grad: -1.3609323531272906
iteration: 90 loss: 12.608508686277156 grad: -1.267422404658922
iteration: 100 loss: 11.606176885293445 grad: -1.1854904665984913
iteration: 110 loss: 10.74831450779365 grad: -1.1133018117741047
iteration: 120 loss: 10.00588163589036 grad: -1.0492953717071418
iteration: 130 loss: 9.357171494583678 grad: -0.9921860942500401
iteration: 140 loss: 8.785613792031302 grad: -0.9409264802086337
iteration: 150 loss: 8.278330365874064 grad: -0.8946628314781631
iteration: 160 loss: 7.825156109356158 grad: -0.8526964804833799
iteration: 170 loss: 7.417957828317909 grad: -0.8144520810207165
iteration: 180 loss: 7.050149616002975 grad: -0.7794524830899362
iteration: 190 loss: 6.71634122277142 grad: -0.7472990665864955
iteration: 200 loss: 6.412078473974899 grad: -0.7176563944315617
iteration: 210 loss: 6.133648670573373 grad: -0.6902402155192338
iteration: 220 loss: 5.87793268102777 grad: -0.6648080453597469
iteration: 230 loss: 5.642291115862256 grad: -0.6411517275415846
iteration: 240 loss: 5.424475738114283 grad: -0.6190915207234936
iteration: 250 loss: 5.2225598022402915 grad: -0.5984713655911318
iteration: 260 loss: 5.034882758754355 grad: -0.5791550695636225
iteration: 270 loss: 4.860005979991249 grad: -0.5610232097775032
iteration: 280 loss: 4.696677025394448 grad: -0.5439706019595203
iteration: 290 loss: 4.543800584393673 grad: -0.5279042181512986
iteration: 300 loss: 4.400414685442642 grad: -0.5127414628646685
iteration: 310 loss: 4.265671090992452 grad: -0.4984087373689988
iteration: 320 loss: 4.138819044357206 grad: -0.4848402371028855
iteration: 330 loss: 4.019191719103343 grad: -0.4719769388878826
iteration: 340 loss: 3.906194861519742 grad: -0.4597657436054105
iteration: 350 loss: 3.79929722354605 grad: -0.44815874694685454
iteration: 360 loss: 3.698022465828382 grad: -0.43711261625529313
iteration: 370 loss: 3.6019422743806486 grad: -0.4265880557128491
iteration: 380 loss: 3.510670484211708 grad: -0.41654934546484945
iteration: 390 loss: 3.4238580424777285 grad: -0.4069639429170455
iteration: 400 loss: 3.3411886747832216 grad: -0.3978021365507981
iteration: 410 loss: 3.262375142953591 grad: -0.38903674429162904
iteration: 420 loss: 3.1871560024035883 grad: -0.38064284982917096
iteration: 430 loss: 3.1152927831560855 grad: -0.3725975713906767
iteration: 440 loss: 3.0465675314659495 grad: -0.36487985836944653
iteration: 450 loss: 2.980780659468978 grad: -0.357470311945377
iteration: 460 loss: 2.917749058873886 grad: -0.35035102643985305
iteration: 470 loss: 2.857304441701837 grad: -0.34350544864684324
iteration: 480 loss: 2.7992918769113286 grad: -0.3369182527964677
iteration: 490 loss: 2.743568496498607 grad: -0.33057522915247095
iteration: 0 loss: 76.9722219499812 grad: 27.286710502127413
iteration: 10 loss: 45.46078386040419 grad: 6.856503579510397
iteration: 20 loss: 34.266451569937175 grad: 1.0176700178412894
iteration: 30 loss: 27.830812322593786 grad: -0.5196262074278393
iteration: 40 loss: 23.505617498826766 grad: -0.9869201894189599
iteration: 50 loss: 20.36269788946329 grad: -1.1268047791237248
iteration: 60 loss: 17.963594622949234 grad: -1.149414217705288
iteration: 70 loss: 16.067728062908206 grad: -1.1261565082239224
iteration: 80 loss: 14.530077446284498 grad: -1.0848108090496633
iteration: 90 loss: 13.257266668170773 grad: -1.037210405576916
iteration: 100 loss: 12.186137760008789 grad: -0.9886601559356977
iteration: 110 loss: 11.272299510574964 grad: -0.941563266353567
iteration: 120 loss: 10.483571600541278 grad: -0.8969588721741342
iteration: 130 loss: 9.79602308862193 grad: -0.8552231751179102
iteration: 140 loss: 9.191471697155107 grad: -0.8164068716375096
iteration: 150 loss: 8.655847127199381 grad: -0.780403848876708
iteration: 160 loss: 8.178086731069305 grad: -0.7470375703692653
iteration: 170 loss: 7.74937070003588 grad: -0.7161057448184855
iteration: 180 loss: 7.36258027899645 grad: -0.6874032515507972
iteration: 190 loss: 7.0119062961862255 grad: -0.6607335198424854
iteration: 200 loss: 6.6925613085690285 grad: -0.6359137204908065
iteration: 210 loss: 6.400564609541437 grad: -0.6127766474013563
iteration: 220 loss: 6.1325793921226 grad: -0.5911708585313885
iteration: 230 loss: 5.885787846172537 grad: -0.5709599394523499
iteration: 240 loss: 5.657794246703403 grad: -0.5520213649460434
iteration: 250 loss: 5.446548968727794 grad: -0.5342452182113201
iteration: 260 loss: 5.250288335150344 grad: -0.5175329061526825
iteration: 270 loss: 5.067486575885554 grad: -0.501795941074507
iteration: 280 loss: 4.896817145105036 grad: -0.48695482090187403
iteration: 290 loss: 4.737121336998164 grad: -0.4729380189121012
iteration: 300 loss: 4.587382643060044 grad: -0.45968108258816215
iteration: 310 loss: 4.4467056624770605 grad: -0.44712583541388345
iteration: 320 loss: 4.31429865028717 grad: -0.43521967280734664
iteration: 330 loss: 4.1894589924263945 grad: -0.42391494253390044
iteration: 340 loss: 4.071561051190698 grad: -0.4131684000414764
iteration: 350 loss: 3.960045942310775 grad: -0.40294072974886586
iteration: 360 loss: 3.854412895236857 grad: -0.39319612412325666
iteration: 370 loss: 3.754211918193753 grad: -0.3839019132556162
iteration: 380 loss: 3.6590375441100647 grad: -0.3750282384995248
iteration: 390 loss: 3.5685234763619023 grad: -0.36654776453925764
iteration: 400 loss: 3.4823379870765 grad: -0.3584354249780344
iteration: 410 loss: 3.400179947619687 grad: -0.3506681971821951
iteration: 420 loss: 3.321775392405264 grad: -0.3432249026836386
iteration: 430 loss: 3.246874534395498 grad: -0.3360860299366675
iteration: 440 loss: 3.17524916463923 grad: -0.3292335766535809
iteration: 450 loss: 3.106690379512987 grad: -0.32265090931338053
iteration: 460 loss: 3.0410065885613964 grad: -0.31632263775711733
iteration: 470 loss: 2.978021763415944 grad: -0.3102345030583825
iteration: 480 loss: 2.9175738944883793 grad: -0.304373277094294
iteration: 490 loss: 2.8595136272936297 grad: -0.298726672446356
iteration: 0 loss: 75.14527478416308 grad: 38.99354912957658
iteration: 10 loss: 43.84416812645646 grad: 6.034772720117942
iteration: 20 loss: 33.26477772789928 grad: -0.7230552622167535
iteration: 30 loss: 27.118908943746874 grad: -1.7165819930653003
iteration: 40 loss: 22.9574113611907 grad: -1.7854055130530264
iteration: 50 loss: 19.917918319967818 grad: -1.6856207639799774
iteration: 60 loss: 17.58927818906684 grad: -1.5616244949265214
iteration: 70 loss: 15.744156905990518 grad: -1.4444587321232323
iteration: 80 loss: 14.244647665763999 grad: -1.3400605217021129
iteration: 90 loss: 13.001494924963481 grad: -1.2482835015347442
iteration: 100 loss: 11.954072549775033 grad: -1.1676038061135792
iteration: 110 loss: 11.059620614739783 grad: -1.096367572109299
iteration: 120 loss: 10.287051718845015 grad: -1.0331080671537936
iteration: 130 loss: 9.613191569074429 grad: -0.9765983141574567
iteration: 140 loss: 9.020396645323093 grad: -0.9258304323265724
iteration: 150 loss: 8.49498973909098 grad: -0.8799792022050743
iteration: 160 loss: 8.02620123961614 grad: -0.8383668989854186
iteration: 170 loss: 7.605433998563058 grad: -0.8004336925585295
iteration: 180 loss: 7.225741360501491 grad: -0.76571392257411
iteration: 190 loss: 6.8814492314419855 grad: -0.7338174340685084
iteration: 200 loss: 6.5678776669866155 grad: -0.7044149761338984
iteration: 210 loss: 6.28113259010756 grad: -0.6772267699232429
iteration: 220 loss: 6.01794780452019 grad: -0.6520135206271941
iteration: 230 loss: 5.775563652979847 grad: -0.6285693085046294
iteration: 240 loss: 5.55163275810012 grad: -0.6067159270374689
iteration: 250 loss: 5.344146039375243 grad: -0.5862983403971542
iteration: 260 loss: 5.151374091027967 grad: -0.5671810117747533
iteration: 270 loss: 4.971820323615578 grad: -0.5492449138686215
iteration: 280 loss: 4.804183204898933 grad: -0.5323850775732677
iteration: 290 loss: 4.647325604134204 grad: -0.5165085684196391
iteration: 300 loss: 4.500249729277377 grad: -0.5015328054674353
iteration: 310 loss: 4.362076502924372 grad: -0.4873841563105188
iteration: 320 loss: 4.232028487203562 grad: -0.47399675623044646
iteration: 330 loss: 4.109415665946926 grad: -0.4613115104993267
iteration: 340 loss: 3.9936235422826694 grad: -0.44927524725664825
iteration: 350 loss: 3.8841031240499846 grad: -0.4378399949003565
iteration: 360 loss: 3.780362457298493 grad: -0.4269623630079806
iteration: 370 loss: 3.681959436193169 grad: -0.41660300978480863
iteration: 380 loss: 3.5884956707349045 grad: -0.40672618218014533
iteration: 390 loss: 3.499611235433018 grad: -0.3972993173118221
iteration: 400 loss: 3.4149801550239234 grad: -0.3882926958380951
iteration: 410 loss: 3.3343065095440663 grad: -0.37967913952460464
iteration: 420 loss: 3.257321062047512 grad: -0.3714337465557117
iteration: 430 loss: 3.183778329102758 grad: -0.36353365919859687
iteration: 440 loss: 3.113454027837427 grad: -0.3559578592945807
iteration: 450 loss: 3.046142844373745 grad: -0.34868698776381957
iteration: 460 loss: 2.981656477515695 grad: -0.3417031848970668
iteration: 470 loss: 2.91982191896688 grad: -0.33498994869532633
iteration: 480 loss: 2.8604799374422574 grad: -0.32853200892380774
iteration: 490 loss: 2.8034837390797622 grad: -0.3223152148855928
iteration: 0 loss: 76.27251783778334 grad: 30.067588324428883
iteration: 10 loss: 43.62247420925336 grad: 6.862492164614798
iteration: 20 loss: 32.72248165055431 grad: 0.5604377674752452
iteration: 30 loss: 26.593151076629386 grad: -0.9687219739836871
iteration: 40 loss: 22.499156302974413 grad: -1.359680737192979
iteration: 50 loss: 19.526227783383504 grad: -1.4258481137387304
iteration: 60 loss: 17.254179806096257 grad: -1.3892906220954473
iteration: 70 loss: 15.455499587876064 grad: -1.3207638284488237
iteration: 80 loss: 13.993863837840081 grad: -1.2450936993706168
iteration: 90 loss: 12.781705298614414 grad: -1.1713194470980168
iteration: 100 loss: 11.759825763723773 grad: -1.1025760727411353
iteration: 110 loss: 10.886597630327431 grad: -1.0396816906209536
iteration: 120 loss: 10.131813596889948 grad: -0.9825392524303607
iteration: 130 loss: 9.47298079460341 grad: -0.9307117591039917
iteration: 140 loss: 8.89298479713625 grad: -0.883663991079174
iteration: 150 loss: 8.3785615676078 grad: -0.8408632150857416
iteration: 160 loss: 7.919266387471742 grad: -0.8018184494334648
iteration: 170 loss: 7.506759476035106 grad: -0.7660926798413689
iteration: 180 loss: 7.1342995835385326 grad: -0.7333033663632431
iteration: 190 loss: 6.79637776569292 grad: -0.7031182124318398
iteration: 200 loss: 6.488447822684066 grad: -0.6752493718644897
iteration: 210 loss: 6.206724749859652 grad: -0.649447509990599
iteration: 220 loss: 5.948031905974528 grad: -0.6254963095952197
iteration: 230 loss: 5.709683644134202 grad: -0.6032076260281664
iteration: 240 loss: 5.489394134266811 grad: -0.5824173192862563
iteration: 250 loss: 5.285205786188313 grad: -0.5629817152982779
iteration: 260 loss: 5.095432518043666 grad: -0.5447746213901339
iteration: 270 loss: 4.918614392820845 grad: -0.5276848162064145
iteration: 280 loss: 4.753481048542736 grad: -0.5116139395411132
iteration: 290 loss: 4.598921994444812 grad: -0.4964747164001254
iteration: 300 loss: 4.453962314484753 grad: -0.482189459242224
iteration: 310 loss: 4.317742663689552 grad: -0.4686888014095074
iteration: 320 loss: 4.189502698099329 grad: -0.4559106227556009
iteration: 330 loss: 4.068567270259378 grad: -0.4437991352925577
iteration: 340 loss: 3.95433486680126 grad: -0.4323041023651509
iteration: 350 loss: 3.8462678749096995 grad: -0.4213801695541719
iteration: 360 loss: 3.7438843492520224 grad: -0.41098628935662285
iteration: 370 loss: 3.6467510166476957 grad: -0.40108522483218684
iteration: 380 loss: 3.5544773070140434 grad: -0.39164311996806345
iteration: 390 loss: 3.4667102394019325 grad: -0.38262912660523596
iteration: 400 loss: 3.383130023784733 grad: -0.37401507947750745
iteration: 410 loss: 3.303446264594723 grad: -0.3657752123130075
iteration: 420 loss: 3.227394672275626 grad: -0.35788590909536966
iteration: 430 loss: 3.1547342054220944 grad: -0.3503254855260942
iteration: 440 loss: 3.0852445792569267 grad: -0.3430739965090222
iteration: 450 loss: 3.0187240869165426 grad: -0.33611306612324343
iteration: 460 loss: 2.954987688756236 grad: -0.3294257370868524
iteration: 470 loss: 2.8938653320610683 grad: -0.3229963371607605
iteration: 480 loss: 2.835200469448203 grad: -0.3168103603153233
iteration: 490 loss: 2.778848749141203 grad: -0.31085436079586254
iteration: 0 loss: 76.64221747726525 grad: 25.9124135022977
iteration: 10 loss: 45.05931451888002 grad: 6.725750083007582
iteration: 20 loss: 33.929226851311576 grad: 0.9542755274278056
iteration: 30 loss: 27.534026692205007 grad: -0.5864024045114132
iteration: 40 loss: 23.236627879910657 grad: -1.0332312807267336
iteration: 50 loss: 20.115902781252814 grad: -1.148653479364653
iteration: 60 loss: 17.73579402873148 grad: -1.1517103444294658
iteration: 70 loss: 15.856560675673853 grad: -1.1149150841405568
iteration: 80 loss: 14.3335903605459 grad: -1.064867041885083
iteration: 90 loss: 13.073791403367212 grad: -1.0119783216390448
iteration: 100 loss: 12.014239490169281 grad: -0.960419783487729
iteration: 110 loss: 11.110742883990373 grad: -0.91179503006825
iteration: 120 loss: 10.331291543668634 grad: -0.866607590508562
iteration: 130 loss: 9.652099010877835 grad: -0.8248831985444386
iteration: 140 loss: 9.055105117397488 grad: -0.786445661850945
iteration: 150 loss: 8.526342662910379 grad: -0.7510426562194692
iteration: 160 loss: 8.054836163782118 grad: -0.7184036143994948
iteration: 170 loss: 7.631839714342112 grad: -0.6882658755941836
iteration: 180 loss: 7.250297454148825 grad: -0.660385686149987
iteration: 190 loss: 6.904453963029565 grad: -0.6345419188699921
iteration: 200 loss: 6.589567937857075 grad: -0.6105363367024286
iteration: 210 loss: 6.301698453436116 grad: -0.5881922919962854
iteration: 220 loss: 6.037543151267403 grad: -0.5673528017478854
iteration: 230 loss: 5.794314177759713 grad: -0.5478784624452543
iteration: 240 loss: 5.569641964346664 grad: -0.5296454257501166
iteration: 250 loss: 5.361499813572251 grad: -0.5125435324077097
iteration: 260 loss: 5.1681442206037325 grad: -0.4964746388511438
iteration: 270 loss: 4.988067226701952 grad: -0.4813511397230587
iteration: 280 loss: 4.819958066230811 grad: -0.4670946748472626
iteration: 290 loss: 4.662672059332511 grad: -0.453635003111873
iteration: 300 loss: 4.51520520272313 grad: -0.4409090240686717
iteration: 310 loss: 4.376673277778991 grad: -0.4288599285726192
iteration: 320 loss: 4.24629456674124 grad: -0.41743646129133477
iteration: 330 loss: 4.123375471141654 grad: -0.40659227978242013
iteration: 340 loss: 4.007298480050436 grad: -0.3962853967476627
iteration: 350 loss: 3.8975120526714075 grad: -0.38647769387358144
iteration: 360 loss: 3.793522069621221 grad: -0.377134497292007
iteration: 370 loss: 3.6948845767060035 grad: -0.3682242061235499
iteration: 380 loss: 3.6011995991801253 grad: -0.3597179668054167
iteration: 390 loss: 3.512105846967064 grad: -0.3515893869685035
iteration: 400 loss: 3.427276164889794 grad: -0.3438142835366043
iteration: 410 loss: 3.3464136086317096 grad: -0.3363704604930318
iteration: 420 loss: 3.26924804845827 grad: -0.3292375124160518
iteration: 430 loss: 3.1955332198505277 grad: -0.3223966504414101
iteration: 440 loss: 3.1250441540510523 grad: -0.3158305477829364
iteration: 450 loss: 3.057574932723624 grad: -0.3095232023437381
iteration: 460 loss: 2.992936720109934 grad: -0.30345981429191476
iteration: 470 loss: 2.930956033553643 grad: -0.2976266767654034
iteration: 480 loss: 2.8714732194329042 grad: -0.29201107811843885
iteration: 490 loss: 2.8143411066526327 grad: -0.2866012143338183
iteration: 0 loss: 76.29334316851772 grad: 28.080719234757325
iteration: 10 loss: 44.86120084246082 grad: 6.195523579766208
iteration: 20 loss: 33.86133335681837 grad: 0.914560646911384
iteration: 30 loss: 27.50502316044607 grad: -0.48501065273862115
iteration: 40 loss: 23.228703103824934 grad: -0.9240355079593087
iteration: 50 loss: 20.119794735508776 grad: -1.0603547663718813
iteration: 60 loss: 17.746159820079978 grad: -1.0856702940282477
iteration: 70 loss: 15.870340325772965 grad: -1.066927211849249
iteration: 80 loss: 14.349028803956488 grad: -1.030550838577897
iteration: 90 loss: 13.089875362802848 grad: -0.987850853065788
iteration: 100 loss: 12.030381341527201 grad: -0.9439019041514515
iteration: 110 loss: 11.126603037603115 grad: -0.901011263058958
iteration: 120 loss: 10.346677391619984 grad: -0.8601921758626347
iteration: 130 loss: 9.66690643171296 grad: -0.8218320449949515
iteration: 140 loss: 9.069283958328656 grad: -0.78601111108651
iteration: 150 loss: 8.539875963164853 grad: -0.7526603421841949
iteration: 160 loss: 8.067727402008627 grad: -0.7216417779132955
iteration: 170 loss: 7.64410483547958 grad: -0.692790024331883
iteration: 180 loss: 7.261959802118037 grad: -0.6659336912602419
iteration: 190 loss: 6.91554101722729 grad: -0.6409062396064036
iteration: 200 loss: 6.60010919263087 grad: -0.6175511567268193
iteration: 210 loss: 6.31172403616012 grad: -0.5957240789543601
iteration: 220 loss: 6.047082926281788 grad: -0.5752932833522862
iteration: 230 loss: 5.803397174772396 grad: -0.5561393313416889
iteration: 240 loss: 5.578296026006848 grad: -0.5381542979959713
iteration: 250 loss: 5.369751392129522 grad: -0.5212408271956376
iteration: 260 loss: 5.176018276161275 grad: -0.5053111440153942
iteration: 270 loss: 4.995587194399129 grad: -0.49028609405248924
iteration: 280 loss: 4.827145869627367 grad: -0.4760942443781551
iteration: 290 loss: 4.669548154071725 grad: -0.4626710609937401
iteration: 300 loss: 4.521788639272762 grad: -0.44995816665152644
iteration: 310 loss: 4.382981775381268 grad: -0.4379026769617518
iteration: 320 loss: 4.252344593077561 grad: -0.42645660970986704
iteration: 330 loss: 4.129182323903065 grad: -0.4155763609991862
iteration: 340 loss: 4.012876367832012 grad: -0.4052222414777854
iteration: 350 loss: 3.902874173481078 grad: -0.3953580660715076
iteration: 360 loss: 3.7986806859122555 grad: -0.38595079107310265
iteration: 370 loss: 3.6998510862983327 grad: -0.37697019298312684
iteration: 380 loss: 3.6059846017397335 grad: -0.3683885840771911
iteration: 390 loss: 3.5167192059373686 grad: -0.3601805602401006
iteration: 400 loss: 3.4317270649131717 grad: -0.3523227771365851
iteration: 410 loss: 3.350710608604128 grad: -0.3447937512699396
iteration: 420 loss: 3.2733991304115246 grad: -0.33757368291086803
iteration: 430 loss: 3.1995458339078637 grad: -0.33064429826017294
iteration: 440 loss: 3.128925259706317 grad: -0.32398870854389483
iteration: 450 loss: 3.0613310367071294 grad: -0.31759128403226844
iteration: 460 loss: 2.996573911103207 grad: -0.31143754122892353
iteration: 470 loss: 2.9344800139994978 grad: -0.3055140416985568
iteration: 480 loss: 2.8748893346818973 grad: -0.29980830119393287
iteration: 490 loss: 2.8176543716710345 grad: -0.2943087079103184
iteration: 0 loss: 77.22813623244888 grad: 25.868269931456027
iteration: 10 loss: 45.5160058428361 grad: 7.061056407531309
iteration: 20 loss: 33.98775445482208 grad: 1.123927427806262
iteration: 30 loss: 27.49511132804291 grad: -0.47969000735918554
iteration: 40 loss: 23.17471386001212 grad: -0.95947316720421
iteration: 50 loss: 20.051746789653386 grad: -1.0973095679919411
iteration: 60 loss: 17.675598145064647 grad: -1.116540085402306
iteration: 70 loss: 15.802007909925743 grad: -1.0914418699015107
iteration: 80 loss: 14.284832157300611 grad: -1.0498111846026226
iteration: 90 loss: 13.030449467843606 grad: -1.0029665000116283
iteration: 100 loss: 11.97577291848616 grad: -0.9557546187998607
iteration: 110 loss: 11.076590086633525 grad: -0.9102698035045054
iteration: 120 loss: 10.300925178075783 grad: -0.867364942882709
iteration: 130 loss: 9.625045105020614 grad: -0.8273117689952688
iteration: 140 loss: 9.030946903800213 grad: -0.7901064400687295
iteration: 150 loss: 8.504717907080414 grad: -0.7556173134469373
iteration: 160 loss: 8.035431937434272 grad: -0.7236585676806973
iteration: 170 loss: 7.61438644117034 grad: -0.6940274045923869
iteration: 180 loss: 7.2345630721520555 grad: -0.6665227289088331
iteration: 190 loss: 6.8902385721605075 grad: -0.6409541657203197
iteration: 200 loss: 6.576699066410459 grad: -0.6171459623717894
iteration: 210 loss: 6.290026959917296 grad: -0.5949381778593315
iteration: 220 loss: 6.026939722781125 grad: -0.5741864594293301
iteration: 230 loss: 5.784666362302977 grad: -0.5547611202021248
iteration: 240 loss: 5.560851667612207 grad: -0.536545912057902
iteration: 250 loss: 5.353481192743237 grad: -0.5194367098191882
iteration: 260 loss: 5.160821913565294 grad: -0.5033402218415468
iteration: 270 loss: 4.981374862600618 grad: -0.4881727845589068
iteration: 280 loss: 4.81383701102401 grad: -0.47385926581338067
iteration: 290 loss: 4.657070357174399 grad: -0.4603320835344194
iteration: 300 loss: 4.510076680456305 grad: -0.44753033652708624
iteration: 310 loss: 4.371976785338327 grad: -0.4353990392629709
iteration: 320 loss: 4.241993330961288 grad: -0.4238884505446938
iteration: 330 loss: 4.119436544366925 grad: -0.41295348548437105
iteration: 340 loss: 4.003692268183758 grad: -0.40255320065280537
iteration: 350 loss: 3.8942119099672876 grad: -0.39265034308165137
iteration: 360 loss: 3.790503949708471 grad: -0.38321095478379186
iteration: 370 loss: 3.692126731129726 grad: -0.3742040254583917
iteration: 380 loss: 3.59868231621621 grad: -0.3656011869942408
iteration: 390 loss: 3.509811224675063 grad: -0.3573764442448195
iteration: 400 loss: 3.4251879133593404 grad: -0.3495059373098998
iteration: 410 loss: 3.3445168771950606 grad: -0.3419677312223871
iteration: 420 loss: 3.2675292743154554 grad: -0.3347416295123351
iteration: 430 loss: 3.19397999512053 grad: -0.32780900861214823
iteration: 440 loss: 3.1236451087048325 grad: -0.32115267048791935
iteration: 450 loss: 3.056319631264242 grad: -0.3147567112413512
iteration: 460 loss: 2.991815570168745 grad: -0.30860640373361414
iteration: 470 loss: 2.929960204848571 grad: -0.3026880925445823
iteration: 480 loss: 2.870594571758953 grad: -0.29698909980493315
iteration: 490 loss: 2.813572125762882 grad: -0.29149764063039896
iteration: 0 loss: 76.32112545378692 grad: 35.16013851532988
iteration: 10 loss: 44.15490854281158 grad: 6.575864919778281
iteration: 20 loss: 33.30585505802669 grad: -0.2866263472198607
iteration: 30 loss: 27.082596711395272 grad: -1.498997577999433
iteration: 40 loss: 22.89099397051103 grad: -1.654641157594662
iteration: 50 loss: 19.840284844800717 grad: -1.5905781048315624
iteration: 60 loss: 17.509078867068805 grad: -1.484419808219659
iteration: 70 loss: 15.665494937622869 grad: -1.3780975480672522
iteration: 80 loss: 14.169437797002233 grad: -1.2812784872205953
iteration: 90 loss: 12.930551769049378 grad: -1.1952571036066217
iteration: 100 loss: 11.88764459270878 grad: -1.1191619738430538
iteration: 110 loss: 10.997663922691283 grad: -1.0516876924414316
iteration: 120 loss: 10.229374887317734 grad: -0.9915808759883098
iteration: 130 loss: 9.559534262168343 grad: -0.9377579133312774
iteration: 140 loss: 8.970472661817336 grad: -0.8893127981862862
iteration: 150 loss: 8.448510573993728 grad: -0.8454950696140013
iteration: 160 loss: 7.982888678548596 grad: -0.8056829454936665
iteration: 170 loss: 7.565026453243428 grad: -0.7693590366739225
iteration: 180 loss: 7.187996611414912 grad: -0.7360902552439015
iteration: 190 loss: 6.846145119638219 grad: -0.7055117260298118
iteration: 200 loss: 6.534811640500183 grad: -0.677314045475556
iteration: 210 loss: 6.250120641417983 grad: -0.6512332054962402
iteration: 220 loss: 5.988823117292626 grad: -0.6270425989275193
iteration: 230 loss: 5.748175145458803 grad: -0.6045466415464111
iteration: 240 loss: 5.525843631273305 grad: -0.5835756504877783
iteration: 250 loss: 5.319832389828662 grad: -0.563981703159171
iteration: 260 loss: 5.128423619132305 grad: -0.5456352658148472
iteration: 270 loss: 4.950131150036793 grad: -0.5284224303091452
iteration: 280 loss: 4.783662798001925 grad: -0.5122426347703208
iteration: 290 loss: 4.627889814868592 grad: -0.4970067719996521
iteration: 300 loss: 4.481821926927089 grad: -0.4826356106277132
iteration: 310 loss: 4.344586803589581 grad: -0.4690584701975524
iteration: 320 loss: 4.215413066382015 grad: -0.4562121036886496
iteration: 330 loss: 4.093616146690279 grad: -0.4440397505008259
iteration: 340 loss: 3.9785864508386855 grad: -0.4324903302868717
iteration: 350 loss: 3.869779405514443 grad: -0.4215177537756147
iteration: 360 loss: 3.7667070444732746 grad: -0.41108033124827925
iteration: 370 loss: 3.6689308655373596 grad: -0.4011402629071479
iteration: 380 loss: 3.5760557399565354 grad: -0.39166319822187284
iteration: 390 loss: 3.487724697882806 grad: -0.3826178536177277
iteration: 400 loss: 3.4036144466148555 grad: -0.3739756797050892
iteration: 410 loss: 3.323431504430573 grad: -0.3657105707349042
iteration: 420 loss: 3.2469088537478785 grad: -0.35779861017355424
iteration: 430 loss: 3.1738030341455836 grad: -0.3502178472787414
iteration: 440 loss: 3.103891609376743 grad: -0.34294810036967066
iteration: 450 loss: 3.0369709535139835 grad: -0.3359707831543894
iteration: 460 loss: 2.9728543103733087 grad: -0.32926875103181535
iteration: 470 loss: 2.9113700877226787 grad: -0.3228261647472641
iteration: 480 loss: 2.8523603538537414 grad: -0.3166283691653936
iteration: 490 loss: 2.795679509106776 grad: -0.3106617852471018
iteration: 0 loss: 75.43078859813464 grad: 37.59218475799965
iteration: 10 loss: 44.152788829815194 grad: 5.5552653763409126
iteration: 20 loss: 33.43990218131034 grad: -0.263227927823673
iteration: 30 loss: 27.22804584131886 grad: -1.275361698388497
iteration: 40 loss: 23.04058019257284 grad: -1.4617595876659057
iteration: 50 loss: 19.990469188262193 grad: -1.4546667135380669
iteration: 60 loss: 17.6570678836291 grad: -1.3941293536588764
iteration: 70 loss: 15.809407587821914 grad: -1.3198864167734061
iteration: 80 loss: 14.308157693283254 grad: -1.2450006841827974
iteration: 90 loss: 13.063506657150397 grad: -1.173984802467254
iteration: 100 loss: 12.01462289647853 grad: -1.1082538464307894
iteration: 110 loss: 11.118683465848603 grad: -1.0480210416631284
iteration: 120 loss: 10.344594060973225 grad: -0.9930282769573102
iteration: 130 loss: 9.66919333721326 grad: -0.9428476614199008
iteration: 140 loss: 9.074855511110771 grad: -0.8970106101036659
iteration: 150 loss: 8.547919979863185 grad: -0.8550629362104005
iteration: 160 loss: 8.077630637265697 grad: -0.8165867905938509
iteration: 170 loss: 7.655400431140069 grad: -0.7812074362458575
iteration: 180 loss: 7.274289733717689 grad: -0.7485931421459496
iteration: 190 loss: 6.928628949155609 grad: -0.7184521251179957
iteration: 200 loss: 6.613740649263533 grad: -0.6905284254545536
iteration: 210 loss: 6.325731774323355 grad: -0.6645976124271811
iteration: 220 loss: 6.061336044599267 grad: -0.6404627264729738
iteration: 230 loss: 5.817792934061427 grad: -0.617950621212905
iteration: 240 loss: 5.592753654653858 grad: -0.596908748436408
iteration: 250 loss: 5.384207357611269 grad: -0.5772023722766008
iteration: 260 loss: 5.190422648617393 grad: -0.5587121742281289
iteration: 270 loss: 5.009900830217455 grad: -0.541332202479209
iteration: 280 loss: 4.841338215573284 grad: -0.5249681188912078
iteration: 290 loss: 4.683595524534625 grad: -0.5095357004852732
iteration: 300 loss: 4.535672856817035 grad: -0.49495955721579893
iteration: 310 loss: 4.396689092183393 grad: -0.48117203298063815
iteration: 320 loss: 4.265864830937392 grad: -0.46811226169513276
iteration: 330 loss: 4.142508185392175 grad: -0.45572535461610847
iteration: 340 loss: 4.026002882199339 grad: -0.4439616988813219
iteration: 350 loss: 3.9157982492599994 grad: -0.43277635044788565
iteration: 360 loss: 3.811400748439527 grad: -0.4221285073235437
iteration: 370 loss: 3.7123667831221856 grad: -0.41198105125274487
iteration: 380 loss: 3.6182965625601935 grad: -0.40230014791075086
iteration: 390 loss: 3.52882884651527 grad: -0.3930548972336153
iteration: 400 loss: 3.4436364265856025 grad: -0.384217026822503
iteration: 410 loss: 3.3624222267181225 grad: -0.3757606224525014
iteration: 420 loss: 3.284915926337445 grad: -0.36766189062659493
iteration: 430 loss: 3.2108710263294764 grad: -0.35989894887625773
iteration: 440 loss: 3.1400622917128866 grad: -0.3524516401471127
iteration: 450 loss: 3.072283515880868 grad: -0.3453013681426751
iteration: 460 loss: 3.007345560306724 grad: -0.33843095094888687
iteration: 470 loss: 2.9450746310020888 grad: -0.3318244906414002
iteration: 480 loss: 2.8853107590935823 grad: -0.3254672568981799
iteration: 490 loss: 2.8279064579326754 grad: -0.31934558291174653
iteration: 0 loss: 74.32396177917035 grad: 43.896276295384155
iteration: 10 loss: 44.02213234169203 grad: 5.187628766319943
iteration: 20 loss: 33.769043787602044 grad: -1.1878883740717876
iteration: 30 loss: 27.614202163352832 grad: -1.8246938204660126
iteration: 40 loss: 23.391883475372094 grad: -1.7776611848959214
iteration: 50 loss: 20.2912200224113 grad: -1.6460248745347668
iteration: 60 loss: 17.910285073855427 grad: -1.5166981202026801
iteration: 70 loss: 16.022114332712178 grad: -1.4022149903916035
iteration: 80 loss: 14.48738505831412 grad: -1.3024759196898947
iteration: 90 loss: 13.21529771662848 grad: -1.2153773448960683
iteration: 100 loss: 12.143918973722458 grad: -1.138824820950703
iteration: 110 loss: 11.229455474471562 grad: -1.071069384003031
iteration: 120 loss: 10.44001969715685 grad: -1.0107024393196862
iteration: 130 loss: 9.751820074155914 grad: -0.9565927018939671
iteration: 140 loss: 9.146735112326867 grad: -0.9078255750797479
iteration: 150 loss: 8.610714930582265 grad: -0.8636549967761205
iteration: 160 loss: 8.132696931778954 grad: -0.8234668303529977
iteration: 170 loss: 7.703851605185884 grad: -0.786751177167277
iteration: 180 loss: 7.317046394286039 grad: -0.7530812957600781
iteration: 190 loss: 6.966457205782345 grad: -0.7220973923593296
iteration: 200 loss: 6.647282075992238 grad: -0.6934940310308895
iteration: 210 loss: 6.35552690094871 grad: -0.6670102625138763
iteration: 220 loss: 6.0878428871164525 grad: -0.6424218169139961
iteration: 230 loss: 5.841401703927576 grad: -0.6195348784642634
iteration: 240 loss: 5.613798508637584 grad: -0.598181083739217
iteration: 250 loss: 5.402975842300665 grad: -0.5782134736644697
iteration: 260 loss: 5.207163338309494 grad: -0.5595031947462328
iteration: 270 loss: 5.0248295404532906 grad: -0.5419367930836916
iteration: 280 loss: 4.854643086957615 grad: -0.52541398065722
iteration: 290 loss: 4.695441205231625 grad: -0.5098457804137844
iteration: 300 loss: 4.546203961798338 grad: -0.49515297714640305
iteration: 310 loss: 4.406033078856622 grad: -0.4812648167787112
iteration: 320 loss: 4.27413440128 grad: -0.46811790864891123
iteration: 330 loss: 4.149803301910817 grad: -0.45565529464196164
iteration: 340 loss: 4.0324124673501585 grad: -0.4438256562105801
iteration: 350 loss: 3.9214016241372436 grad: -0.4325826359487568
iteration: 360 loss: 3.816268855699863 grad: -0.42188425480483316
iteration: 370 loss: 3.716563230557478 grad: -0.41169240952154484
iteration: 380 loss: 3.621878516936621 grad: -0.4019724376764817
iteration: 390 loss: 3.531847801894649 grad: -0.3926927399264969
iteration: 400 loss: 3.446138867004017 grad: -0.3838244508544491
iteration: 410 loss: 3.364450199616799 grad: -0.37534115126884293
iteration: 420 loss: 3.2865075403145223 grad: -0.3672186159878461
iteration: 430 loss: 3.2120608844939356 grad: -0.35943459210417206
iteration: 440 loss: 3.1408818700582 grad: -0.3519686035195595
iteration: 450 loss: 3.072761494564392 grad: -0.34480177819092506
iteration: 460 loss: 3.0075081144596383 grad: -0.33791669507134137
iteration: 470 loss: 2.944945686665588 grad: -0.33129724817893486
iteration: 480 loss: 2.884912219012883 grad: -0.32492852560242214
iteration: 490 loss: 2.827258401220578 grad: -0.31879670156673984
iteration: 0 loss: 74.8537361201674 grad: 40.112475007497835
iteration: 10 loss: 44.20513122040366 grad: 4.849036671562954
iteration: 20 loss: 33.73905591369202 grad: -0.7682760341734842
iteration: 30 loss: 27.54471417903958 grad: -1.515975518688741
iteration: 40 loss: 23.32369146524167 grad: -1.5658798479739815
iteration: 50 loss: 20.232977298502977 grad: -1.487943241075609
iteration: 60 loss: 17.862802532165688 grad: -1.3891821893717613
iteration: 70 loss: 15.984204867273569 grad: -1.293941297271654
iteration: 80 loss: 14.457486517789572 grad: -1.2076644381689485
iteration: 90 loss: 13.191948675231123 grad: -1.1308264480345378
iteration: 100 loss: 12.125880237004491 grad: -1.0625792788882893
iteration: 110 loss: 11.215716332642016 grad: -1.0018125138077416
iteration: 120 loss: 10.429769089054492 grad: -0.9474692439418908
iteration: 130 loss: 9.744410858052477 grad: -0.8986287630508739
iteration: 140 loss: 9.141651047494303 grad: -0.8545141228389617
iteration: 150 loss: 8.607543034582394 grad: -0.8144768935523847
iteration: 160 loss: 8.131105326916895 grad: -0.7779770078346269
iteration: 170 loss: 7.70357212998463 grad: -0.7445636383494849
iteration: 180 loss: 7.317861083818488 grad: -0.7138587998531104
iteration: 190 loss: 6.968187813904933 grad: -0.6855438550806063
iteration: 200 loss: 6.649781951429966 grad: -0.6593486092229973
iteration: 210 loss: 6.358674671500177 grad: -0.6350425610352181
iteration: 220 loss: 6.091537529480237 grad: -0.6124278966629615
iteration: 230 loss: 5.845558676074358 grad: -0.5913338715606433
iteration: 240 loss: 5.61834669907115 grad: -0.5716122910494472
iteration: 250 loss: 5.407855149728593 grad: -0.5531338587958341
iteration: 260 loss: 5.2123227400501575 grad: -0.5357852114929683
iteration: 270 loss: 5.0302255416792665 grad: -0.5194664974368559
iteration: 280 loss: 4.8602384683163935 grad: -0.5040893877798747
iteration: 290 loss: 4.701204005544366 grad: -0.4895754335196305
iteration: 300 loss: 4.552106647026407 grad: -0.4758547001332459
iteration: 310 loss: 4.412051859502174 grad: -0.4628646263740096
iteration: 320 loss: 4.280248668736187 grad: -0.4505490650618455
iteration: 330 loss: 4.155995160671368 grad: -0.43885747247377416
iteration: 340 loss: 4.038666344883453 grad: -0.42774421976109855
iteration: 350 loss: 3.927703944018752 grad: -0.41716800513820174
iteration: 360 loss: 3.8226077625348065 grad: -0.4070913497502544
iteration: 370 loss: 3.7229283575104337 grad: -0.3974801633989621
iteration: 380 loss: 3.628260788483782 grad: -0.3883033688893779
iteration: 390 loss: 3.53823926582316 grad: -0.3795325758115338
iteration: 400 loss: 3.4525325508094613 grad: -0.3711417962066852
iteration: 410 loss: 3.3708399873306587 grad: -0.36310719587999785
iteration: 420 loss: 3.2928880665146028 grad: -0.3554068761793715
iteration: 430 loss: 3.218427442822262 grad: -0.3480206819174769
iteration: 440 loss: 3.1472303340254837 grad: -0.3409300318126429
iteration: 450 loss: 3.079088248803518 grad: -0.33411776839630214
iteration: 460 loss: 3.0138099948957957 grad: -0.32756802480549296
iteration: 470 loss: 2.9512199283102416 grad: -0.3212661062682433
iteration: 480 loss: 2.891156410309453 grad: -0.3151983844130916
iteration: 490 loss: 2.83347044402622 grad: -0.30935220280390696
iteration: 0 loss: 76.12723698144154 grad: 36.60605302710681
iteration: 10 loss: 44.917635177035294 grad: 6.003097748066585
iteration: 20 loss: 33.916337352431725 grad: -0.27976704667226787
iteration: 30 loss: 27.550354850762275 grad: -1.3956571122856682
iteration: 40 loss: 23.26689628017607 grad: -1.5703366559367484
iteration: 50 loss: 20.15284288242217 grad: -1.531469546646113
iteration: 60 loss: 17.7752016377077 grad: -1.441963102667871
iteration: 70 loss: 15.896106784101532 grad: -1.3454315963598373
iteration: 80 loss: 14.372048923373107 grad: -1.2543264979086919
iteration: 90 loss: 13.110560370549075 grad: -1.17177184010601
iteration: 100 loss: 12.049063207608263 grad: -1.0979220223514716
iteration: 110 loss: 11.14355529712074 grad: -1.0320204389724936
iteration: 120 loss: 10.36212815990538 grad: -0.9731041764195083
iteration: 130 loss: 9.681047427422216 grad: -0.9202441622018513
iteration: 140 loss: 9.082277198313125 grad: -0.8726178181152926
iteration: 150 loss: 8.551858952462867 grad: -0.8295207579415708
iteration: 160 loss: 8.078817273108804 grad: -0.7903574750125185
iteration: 170 loss: 7.6544017105991315 grad: -0.7546262314773411
iteration: 180 loss: 7.271549556129892 grad: -0.7219038001541431
iteration: 190 loss: 6.924497564047504 grad: -0.6918319451484976
iteration: 200 loss: 6.6084963847678075 grad: -0.6641060589290142
iteration: 210 loss: 6.319597246065107 grad: -0.6384658327804119
iteration: 220 loss: 6.05449036397464 grad: -0.6146876713612215
iteration: 230 loss: 5.810380986134243 grad: -0.5925785423643035
iteration: 240 loss: 5.584893208505078 grad: -0.5719709845106986
iteration: 250 loss: 5.375994558707837 grad: -0.5527190428257454
iteration: 260 loss: 5.181936293028602 grad: -0.534694944555998
iteration: 270 loss: 5.001205714263506 grad: -0.5177863673160891
iteration: 280 loss: 4.83248777835698 grad: -0.5018941822538401
iteration: 290 loss: 4.67463394572178 grad: -0.48693057982387017
iteration: 300 loss: 4.526636731858658 grad: -0.47281750522450333
iteration: 310 loss: 4.387608777608586 grad: -0.4594853457441905
iteration: 320 loss: 4.256765530431866 grad: -0.4468718241051854
iteration: 330 loss: 4.133410831011009 grad: -0.43492106113491136
iteration: 340 loss: 4.016924852752089 grad: -0.42358277833104224
iteration: 350 loss: 3.9067539585819926 grad: -0.41281161657291315
iteration: 360 loss: 3.802402129155243 grad: -0.40256655172138356
iteration: 370 loss: 3.703423686061838 grad: -0.3928103914109003
iteration: 380 loss: 3.6094170877771914 grad: -0.3835093401771838
iteration: 390 loss: 3.520019618601468 grad: -0.37463262233947126
iteration: 400 loss: 3.434902824418879 grad: -0.36615215388880373
iteration: 410 loss: 3.353768575794923 grad: -0.3580422561168857
iteration: 420 loss: 3.276345660259928 grad: -0.3502794049260257
iteration: 430 loss: 3.2023868227712144 grad: -0.3428420107458212
iteration: 440 loss: 3.1316661871969287 grad: -0.3357102247906777
iteration: 450 loss: 3.0639770029064164 grad: -0.3288657680584469
iteration: 460 loss: 2.999129669727691 grad: -0.32229178002178793
iteration: 470 loss: 2.9369500020401658 grad: -0.31597268442186877
iteration: 480 loss: 2.8772776989638014 grad: -0.3098940699559995
iteration: 490 loss: 2.8199649927058257 grad: -0.30404258397053546
iteration: 0 loss: 75.45567742828129 grad: 39.42634802044378
iteration: 10 loss: 44.53235348823586 grad: 5.053611292211268
iteration: 20 loss: 33.860203767980984 grad: -0.5297615772487553
iteration: 30 loss: 27.57645142998608 grad: -1.3497156905869356
iteration: 40 loss: 23.31383421747573 grad: -1.458140736584204
iteration: 50 loss: 20.20257202249811 grad: -1.4195294343842313
iteration: 60 loss: 17.821993392292992 grad: -1.3460906412626006
iteration: 70 loss: 15.938270419034428 grad: -1.2669498744281438
iteration: 80 loss: 14.409334231399642 grad: -1.190862970337121
iteration: 90 loss: 13.143242033950711 grad: -1.1204430993877152
iteration: 100 loss: 12.077589967645384 grad: -1.0561971114772393
iteration: 110 loss: 11.168409053933395 grad: -0.9978735937343303
iteration: 120 loss: 10.383767460721732 grad: -0.9449648883105467
iteration: 130 loss: 9.699886767672814 grad: -0.8969053803435202
iteration: 140 loss: 9.098681927595367 grad: -0.8531501699873952
iteration: 150 loss: 8.566146904149532 grad: -0.8132038179021353
iteration: 160 loss: 8.091263070910786 grad: -0.7766277299393749
iteration: 170 loss: 7.665241881480969 grad: -0.7430385462730824
iteration: 180 loss: 7.2809875977123655 grad: -0.7121030586482813
iteration: 190 loss: 6.932708603358316 grad: -0.6835321408309022
iteration: 200 loss: 6.615631301382192 grad: -0.6570747820527156
iteration: 210 loss: 6.3257862450191995 grad: -0.6325126617966944
iteration: 220 loss: 6.05984603446144 grad: -0.6096554008909384
iteration: 230 loss: 5.8150009017170055 grad: -0.5883364868114748
iteration: 240 loss: 5.588862129120488 grad: -0.5684098141226852
iteration: 250 loss: 5.37938629209012 grad: -0.5497467622778154
iteration: 260 loss: 5.18481526752769 grad: -0.5322337319934038
iteration: 270 loss: 5.003628308423975 grad: -0.5157700679608281
iteration: 280 loss: 4.834503446107605 grad: -0.5002663048228941
iteration: 290 loss: 4.6762861700654375 grad: -0.4856426827932662
iteration: 300 loss: 4.527963834690374 grad: -0.47182788801329456
iteration: 310 loss: 4.388644608782422 grad: -0.45875798035657633
iteration: 320 loss: 4.257540055372291 grad: -0.44637547784994397
iteration: 330 loss: 4.133950632957087 grad: -0.4346285722629534
iteration: 340 loss: 4.017253563045994 grad: -0.4234704548607105
iteration: 350 loss: 3.906892626187197 grad: -0.41285873496129855
iteration: 360 loss: 3.8023695387457583 grad: -0.40275493692407444
iteration: 370 loss: 3.7032366324917594 grad: -0.3931240636370435
iteration: 380 loss: 3.6090906134680734 grad: -0.38393421657050075
iteration: 390 loss: 3.519567219338734 grad: -0.3751562641030174
iteration: 400 loss: 3.43433662816518 grad: -0.36676355117243775
iteration: 410 loss: 3.353099498413981 grad: -0.35873164441384503
iteration: 420 loss: 3.2755835414241945 grad: -0.3510381078628497
iteration: 430 loss: 3.2015405448349736 grad: -0.3436623050619258
iteration: 440 loss: 3.130743779386359 grad: -0.33658522403869484
iteration: 450 loss: 3.0629857328299295 grad: -0.3297893221513704
iteration: 460 loss: 2.998076123905569 grad: -0.3232583882368046
iteration: 470 loss: 2.9358401569153845 grad: -0.3169774198659865
iteration: 480 loss: 2.8761169836331892 grad: -0.3109325138227229
iteration: 490 loss: 2.8187583444413247 grad: -0.30511076818372335
iteration: 0 loss: 75.56075591449309 grad: 38.83260023008291
iteration: 10 loss: 45.050977429318806 grad: 5.089583835724737
iteration: 20 loss: 34.172168187661605 grad: -0.7718300472062334
iteration: 30 loss: 27.788659655490992 grad: -1.5310486114023842
iteration: 40 loss: 23.471077598534023 grad: -1.5735415307061815
iteration: 50 loss: 20.326087855558214 grad: -1.4917027388526813
iteration: 60 loss: 17.92326344783793 grad: -1.391586472133385
iteration: 70 loss: 16.024102475938413 grad: -1.2960011836569088
iteration: 80 loss: 14.484016599416009 grad: -1.209657343485849
iteration: 90 loss: 13.209592820165405 grad: -1.1327486365871735
iteration: 100 loss: 12.137523331944749 grad: -1.0643433376268403
iteration: 110 loss: 11.223262447714784 grad: -1.003324982760706
iteration: 120 loss: 10.434499618258355 grad: -0.9486569373295828
iteration: 130 loss: 9.747201529947295 grad: -0.899444257951968
iteration: 140 loss: 9.14310773832825 grad: -0.8549334582132173
iteration: 150 loss: 8.60808900501018 grad: -0.8144949171651068
iteration: 160 loss: 8.131038471000217 grad: -0.7776025396500069
iteration: 170 loss: 7.7031032280176115 grad: -0.7438152309617976
iteration: 180 loss: 7.31713975842612 grad: -0.7127613005598858
iteration: 190 loss: 6.967320377695821 grad: -0.6841257465813448
iteration: 200 loss: 6.648843821784587 grad: -0.6576400259892425
iteration: 210 loss: 6.357719093992273 grad: -0.6330738598322752
iteration: 220 loss: 6.090601762158175 grad: -0.6102286656540763
iteration: 230 loss: 5.844668408032579 grad: -0.588932277209991
iteration: 240 loss: 5.617519229174212 grad: -0.5690346789788227
iteration: 250 loss: 5.407101687128075 grad: -0.5504045410052798
iteration: 260 loss: 5.211650077768519 grad: -0.5329263867803921
iteration: 270 loss: 5.029637279468468 grad: -0.5164982640769273
iteration: 280 loss: 4.859735909396932 grad: -0.5010298175541406
iteration: 290 loss: 4.700786816017626 grad: -0.48644068421509973
iteration: 300 loss: 4.551773341677248 grad: -0.47265914991096325
iteration: 310 loss: 4.411800160013792 grad: -0.45962101823793033
iteration: 320 loss: 4.280075767751772 grad: -0.4472686533048712
iteration: 330 loss: 4.155897916124766 grad: -0.43555016568614735
iteration: 340 loss: 4.0386414225401746 grad: -0.42441871696469485
iteration: 350 loss: 3.927747921452878 grad: -0.413831923030133
iteration: 360 loss: 3.822717204342718 grad: -0.40375134003837787
iteration: 370 loss: 3.72309986905632 grad: -0.3941420198985613
iteration: 380 loss: 3.6284910536063846 grad: -0.3849721245080037
iteration: 390 loss: 3.5385250725889406 grad: -0.3762125898413502
iteration: 400 loss: 3.4528708083555766 grad: -0.36783683251829624
iteration: 410 loss: 3.3712277361052903 grad: -0.35982049270409233
iteration: 420 loss: 3.293322483642359 grad: -0.35214120819848277
iteration: 430 loss: 3.2189058439000537 grad: -0.3447784153885869
iteration: 440 loss: 3.147750172336683 grad: -0.3377131734156987
iteration: 450 loss: 3.0796471126897234 grad: -0.3309280084635091
iteration: 460 loss: 3.0144056038488585 grad: -0.324406775538126
iteration: 470 loss: 2.951850128205727 grad: -0.31813453549622717
iteration: 480 loss: 2.8918191680922973 grad: -0.3120974454007317
iteration: 490 loss: 2.8341638420873676 grad: -0.3062826605548699
iteration: 0 loss: 76.86116293293959 grad: 28.58258828928486
iteration: 10 loss: 46.00641005326054 grad: 5.565628263803373
iteration: 20 loss: 34.57721923573749 grad: 0.9353924395155774
iteration: 30 loss: 27.978136782900048 grad: -0.25727133718316014
iteration: 40 loss: 23.567938832455617 grad: -0.6839727636219364
iteration: 50 loss: 20.378004735009302 grad: -0.8567603839414478
iteration: 60 loss: 17.95118227264284 grad: -0.9230650050424742
iteration: 70 loss: 16.038305533404976 grad: -0.9388922799397786
iteration: 80 loss: 14.49002422972657 grad: -0.9293847332402385
iteration: 90 loss: 13.210581558863767 grad: -0.9070156213997583
iteration: 100 loss: 12.135418573714809 grad: -0.8783629700220853
iteration: 110 loss: 11.219276216582578 grad: -0.8470252956134922
iteration: 120 loss: 10.429415337564334 grad: -0.8150073133402191
iteration: 130 loss: 9.74153438968857 grad: -0.7834263897856273
iteration: 140 loss: 9.137199706861463 grad: -0.7528918123968691
iteration: 150 loss: 8.602167484552927 grad: -0.7237165505304775
iteration: 160 loss: 8.125253722310246 grad: -0.6960391172727773
iteration: 170 loss: 7.697552884403339 grad: -0.669895344383964
iteration: 180 loss: 7.3118852226036575 grad: -0.6452613952883217
iteration: 190 loss: 6.962397974382384 grad: -0.6220798500791516
iteration: 200 loss: 6.644272485821933 grad: -0.6002756261289006
iteration: 210 loss: 6.353505727387446 grad: -0.5797656970415774
iteration: 220 loss: 6.086744998476082 grad: -0.5604649808131359
iteration: 230 loss: 5.841161274557965 grad: -0.5422898408978905
iteration: 240 loss: 5.614351037858217 grad: -0.5251600923221518
iteration: 250 loss: 5.404259380887527 grad: -0.5090000708252405
iteration: 260 loss: 5.209119189102901 grad: -0.4937391173429746
iteration: 270 loss: 5.027402611310001 grad: -0.47931170184768424
iteration: 280 loss: 4.857782015888006 grad: -0.4656573295964487
iteration: 290 loss: 4.699098338602611 grad: -0.4527203212570081
iteration: 300 loss: 4.550335240276788 grad: -0.4404495252606875
iteration: 310 loss: 4.4105978680253095 grad: -0.42879799933925233
iteration: 320 loss: 4.279095291766722 grad: -0.417722684330224
iteration: 330 loss: 4.155125895615336 grad: -0.40718408432358444
iteration: 340 loss: 4.038065160691779 grad: -0.39714596137424607
iteration: 350 loss: 3.927355395359717 grad: -0.3875750492252573
iteration: 360 loss: 3.8224970606161564 grad: -0.37844078806608106
iteration: 370 loss: 3.7230414093028643 grad: -0.36971508082380355
iteration: 380 loss: 3.6285842130585233 grad: -0.3613720705470958
iteration: 390 loss: 3.5387603943032775 grad: -0.35338793789104067
iteration: 400 loss: 3.4532394147412644 grad: -0.3457407174114422
iteration: 410 loss: 3.371721299075737 grad: -0.33841013124068053
iteration: 420 loss: 3.2939331943131367 grad: -0.33137743868418923
iteration: 430 loss: 3.219626382495376 grad: -0.3246253003076885
iteration: 440 loss: 3.1485736787714234 grad: -0.3181376551542333
iteration: 450 loss: 3.0805671581353056 grad: -0.3118996098196867
iteration: 460 loss: 3.0154161634916155 grad: -0.3058973382142098
iteration: 470 loss: 2.9529455553187374 grad: -0.30011799093857416
iteration: 480 loss: 2.892994169487379 grad: -0.29454961330311
iteration: 490 loss: 2.835413454968437 grad: -0.2891810711112394
iteration: 0 loss: 74.5971019542395 grad: 42.0550018839703
iteration: 10 loss: 43.38375503658137 grad: 3.55345128885247
iteration: 20 loss: 33.09476129169278 grad: -0.7200767037390454
iteration: 30 loss: 27.028268350156374 grad: -1.2109352413096923
iteration: 40 loss: 22.9040151196256 grad: -1.2682240957414384
iteration: 50 loss: 19.885341412658672 grad: -1.2430560824289774
iteration: 60 loss: 17.56934344286213 grad: -1.1963012917277611
iteration: 70 loss: 15.732265901971584 grad: -1.1438253283378073
iteration: 80 loss: 14.238012738270692 grad: -1.091054730588942
iteration: 90 loss: 12.998342006012349 grad: -1.0401247961170559
iteration: 100 loss: 11.953236236326946 grad: -0.9918899199080611
iteration: 110 loss: 11.060319328108852 grad: -0.9466371723737563
iteration: 120 loss: 10.288754326864343 grad: -0.9043851000875573
iteration: 130 loss: 9.615534331788053 grad: -0.865023421180603
iteration: 140 loss: 9.023129623924682 grad: -0.828382967551808
iteration: 150 loss: 8.49794129422135 grad: -0.7942720316607734
iteration: 160 loss: 8.029254047195908 grad: -0.7624954677147175
iteration: 170 loss: 7.608508607832061 grad: -0.7328645648414849
iteration: 180 loss: 7.228784800110423 grad: -0.7052018588301634
iteration: 190 loss: 6.884427051865672 grad: -0.6793431395382101
iteration: 200 loss: 6.57076835249274 grad: -0.6551379132690744
iteration: 210 loss: 6.283923620498792 grad: -0.632449036379928
iteration: 220 loss: 6.0206328736606505 grad: -0.6111519311005538
iteration: 230 loss: 5.778140702581368 grad: -0.5911336186807458
iteration: 240 loss: 5.554102588192688 grad: -0.5722917021171643
iteration: 250 loss: 5.346511327779023 grad: -0.5545333702410585
iteration: 260 loss: 5.1536387034486575 grad: -0.5377744595922151
iteration: 270 loss: 4.973988830501557 grad: -0.5219385901515428
iteration: 280 loss: 4.806260545499216 grad: -0.5069563795980087
iteration: 290 loss: 4.649316855348636 grad: -0.4927647346284838
iteration: 300 loss: 4.502159949058626 grad: -0.4793062148148966
iteration: 310 loss: 4.363910626616664 grad: -0.46652846314489016
iteration: 320 loss: 4.233791261361603 grad: -0.4543836970176953
iteration: 330 loss: 4.111111608561384 grad: -0.4428282535933295
iteration: 340 loss: 3.9952569214738105 grad: -0.4318221837640821
iteration: 350 loss: 3.8856779495493936 grad: -0.42132888949074176
iteration: 360 loss: 3.781882480646635 grad: -0.41131479974628365
iteration: 370 loss: 3.6834281567413942 grad: -0.4017490807977614
iteration: 380 loss: 3.5899163453846086 grad: -0.3926033770145066
iteration: 390 loss: 3.5009868906433272 grad: -0.3838515788098062
iteration: 400 loss: 3.416313600048852 grad: -0.37546961470229623
iteration: 410 loss: 3.3356003501920153 grad: -0.3674352648234359
iteration: 420 loss: 3.258577714456888 grad: -0.3597279935010983
iteration: 430 loss: 3.185000033212377 grad: -0.35232879881953694
iteration: 440 loss: 3.114642860336227 grad: -0.34522007729590365
iteration: 450 loss: 3.04730073099563 grad: -0.33838550202610984
iteration: 460 loss: 2.9827852046140504 grad: -0.3318099128410948
iteration: 470 loss: 2.9209231443286297 grad: -0.3254792171810207
iteration: 480 loss: 2.861555200347787 grad: -0.3193803005420995
iteration: 490 loss: 2.804534469618788 grad: -0.31350094548072605
iteration: 0 loss: 75.59466299992151 grad: 33.00659794528613
iteration: 10 loss: 43.724095786517246 grad: 5.614085118550186
iteration: 20 loss: 33.125046496146744 grad: 0.3738640885248423
iteration: 30 loss: 26.9942767130534 grad: -0.778153568320056
iteration: 40 loss: 22.85088365731876 grad: -1.1132598279574633
iteration: 50 loss: 19.82520158109628 grad: -1.2070738155673846
iteration: 60 loss: 17.506436797094445 grad: -1.211667203041198
iteration: 70 loss: 15.668463635859036 grad: -1.1800825922339884
iteration: 80 loss: 14.174317610201156 grad: -1.1340457355034668
iteration: 90 loss: 12.935365350836719 grad: -1.0832539526356337
iteration: 100 loss: 11.89137464962077 grad: -1.0322012276113035
iteration: 110 loss: 10.999833013379595 grad: -0.9829603513408792
iteration: 120 loss: 10.229809179247138 grad: -0.936426702018567
iteration: 130 loss: 9.558228430309782 grad: -0.892909967379651
iteration: 140 loss: 8.967511664658993 grad: -0.8524280534173562
iteration: 150 loss: 8.444024084141677 grad: -0.8148574313312063
iteration: 160 loss: 7.97702459648506 grad: -0.7800114077282163
iteration: 170 loss: 7.5579357283337565 grad: -0.7476810623330583
iteration: 180 loss: 7.179824822522278 grad: -0.717656398452049
iteration: 190 loss: 6.837028107774999 grad: -0.6897368495475917
iteration: 200 loss: 6.52487355828126 grad: -0.6637360293211552
iteration: 210 loss: 6.239473421959329 grad: -0.6394833876780389
iteration: 220 loss: 5.977566751391185 grad: -0.616824240146612
iteration: 230 loss: 5.736398392962201 grad: -0.5956189831225509
iteration: 240 loss: 5.5136249394611445 grad: -0.575741942749289
iteration: 250 loss: 5.307240883090096 grad: -0.5570801007597757
iteration: 260 loss: 5.11552008121895 grad: -0.5395318256726405
iteration: 270 loss: 4.936968955508374 grad: -0.5230056733778967
iteration: 280 loss: 4.770288771190156 grad: -0.5074192854691837
iteration: 290 loss: 4.614345007812765 grad: -0.4926983942664633
iteration: 300 loss: 4.468142315443319 grad: -0.47877593325360773
iteration: 310 loss: 4.330803904967607 grad: -0.46559124659680745
iteration: 320 loss: 4.20155448445497 grad: -0.4530893892145069
iteration: 330 loss: 4.079706050979604 grad: -0.4412205082317942
iteration: 340 loss: 3.964645996691822 grad: -0.4299392968326852
iteration: 350 loss: 3.855827101927554 grad: -0.41920451210689097
iteration: 360 loss: 3.75275907583525 grad: -0.4089785492423861
iteration: 370 loss: 3.6550013729753035 grad: -0.3992270652153849
iteration: 380 loss: 3.562157067365371 grad: -0.38991864590884595
iteration: 390 loss: 3.4738676071551295 grad: -0.3810245113167665
iteration: 400 loss: 3.389808306026184 grad: -0.37251825415053497
iteration: 410 loss: 3.309684453660628 grad: -0.3643756077521101
iteration: 420 loss: 3.233227948547086 grad: -0.3565742397388853
iteration: 430 loss: 3.16019437329206 grad: -0.3490935682615924
iteration: 440 loss: 3.0903604461919216 grad: -0.3419145981554845
iteration: 450 loss: 3.023521793926234 grad: -0.33501977461252663
iteration: 460 loss: 2.9594909992389065 grad: -0.3283928523045006
iteration: 470 loss: 2.8980958848942464 grad: -0.32201877814937596
iteration: 480 loss: 2.839178001289011 grad: -0.31588358614116063
iteration: 490 loss: 2.7825912901287833 grad: -0.30997430286120153
iteration: 0 loss: 77.02863513133202 grad: 24.77454479399386
iteration: 10 loss: 46.18239113182844 grad: 7.11183929005547
iteration: 20 loss: 34.64895299305525 grad: 1.2585146423773654
iteration: 30 loss: 28.068730428385038 grad: -0.4087851480033981
iteration: 40 loss: 23.66797216316013 grad: -0.9400470810404071
iteration: 50 loss: 20.478938938401203 grad: -1.1081507903534453
iteration: 60 loss: 18.049082676180163 grad: -1.143262090703809
iteration: 70 loss: 16.13156489653687 grad: -1.1255865625938828
iteration: 80 loss: 14.578110997986704 grad: -1.0866567075064064
iteration: 90 loss: 13.293465426976725 grad: -1.0399642789725334
iteration: 100 loss: 12.21330216550105 grad: -0.9915841568829389
iteration: 110 loss: 11.292464314300126 grad: -0.9442951293074584
iteration: 120 loss: 10.498249013301182 grad: -0.899327540282094
iteration: 130 loss: 9.806357421402398 grad: -0.8571581319856988
iteration: 140 loss: 9.198341758868166 grad: -0.8178914071978919
iteration: 150 loss: 8.659936250640818 grad: -0.7814499596670843
iteration: 160 loss: 8.179931926978519 grad: -0.747672026125785
iteration: 170 loss: 7.7493979066923036 grad: -0.7163621759408607
iteration: 180 loss: 7.361130095531652 grad: -0.6873176113285642
iteration: 190 loss: 7.0092530552812855 grad: -0.6603414998413004
iteration: 200 loss: 6.688927398752846 grad: -0.6352493234848073
iteration: 210 loss: 6.3961313547934795 grad: -0.6118714561196408
iteration: 220 loss: 6.127495401690836 grad: -0.5900537240685444
iteration: 230 loss: 5.88017548263534 grad: -0.5696569203780872
iteration: 240 loss: 5.651754678979449 grad: -0.5505558121730015
iteration: 250 loss: 5.440166150446864 grad: -0.5326379399927873
iteration: 260 loss: 5.243632159523424 grad: -0.5158023722311229
iteration: 270 loss: 5.060615394254166 grad: -0.49995850067813763
iteration: 280 loss: 4.889779790011537 grad: -0.4850249193345755
iteration: 290 loss: 4.72995875666931 grad: -0.47092840393312996
iteration: 300 loss: 4.580129229087768 grad: -0.4576029959009749
iteration: 310 loss: 4.439390333743301 grad: -0.44498918718749003
iteration: 320 loss: 4.306945742086265 grad: -0.4330331987641694
iteration: 330 loss: 4.1820889890343755 grad: -0.42168634409974265
iteration: 340 loss: 4.064191191971656 grad: -0.4109044685935571
iteration: 350 loss: 3.952690725156737 grad: -0.40064745626785947
iteration: 360 loss: 3.8470844962772306 grad: -0.3908787956549483
iteration: 370 loss: 3.746920542909059 grad: -0.3815651975832195
iteration: 380 loss: 3.6517917220270317 grad: -0.3726762583622903
iteration: 390 loss: 3.5613303091519644 grad: -0.3641841626348922
iteration: 400 loss: 3.4752033580239186 grad: -0.35606342087363635
iteration: 410 loss: 3.3931086989675183 grad: -0.3482906371421163
iteration: 420 loss: 3.3147714758750806 grad: -0.34084430330961624
iteration: 430 loss: 3.239941139252799 grad: -0.3337046164095669
iteration: 440 loss: 3.168388826896262 grad: -0.3268533162690034
iteration: 450 loss: 3.099905075243193 grad: -0.32027354091596777
iteration: 460 loss: 3.0342978137950096 grad: -0.31394969760056124
iteration: 470 loss: 2.9713906026675185 grad: -0.30786734754948075
iteration: 480 loss: 2.911021079636984 grad: -0.3020131028191715
iteration: 490 loss: 2.8530395882519106 grad: -0.2963745338244258
iteration: 0 loss: 77.4055994549266 grad: 21.620862044869977
iteration: 10 loss: 46.02266911310298 grad: 6.7855512941012135
iteration: 20 loss: 34.38550724562777 grad: 1.5362710100388512
iteration: 30 loss: 27.80945272185671 grad: -0.1266303822730719
iteration: 40 loss: 23.433700812632484 grad: -0.7183279713023829
iteration: 50 loss: 20.271167127309976 grad: -0.9356908355202194
iteration: 60 loss: 17.865056242520144 grad: -1.0049557304823393
iteration: 70 loss: 15.967887328478954 grad: -1.0108866665035137
iteration: 80 loss: 14.43165993077297 grad: -0.9888327018953669
iteration: 90 loss: 13.161587656065235 grad: -0.9547082874500038
iteration: 100 loss: 12.09380282821334 grad: -0.9160532680898132
iteration: 110 loss: 11.1835424508281 grad: -0.8765386078764034
iteration: 120 loss: 10.39842692886226 grad: -0.8379525896233311
iteration: 130 loss: 9.714417989926377 grad: -0.8011329313010161
iteration: 140 loss: 9.11327669610397 grad: -0.7664263438227162
iteration: 150 loss: 8.580904731513412 grad: -0.7339239198292906
iteration: 160 loss: 8.106227904865426 grad: -0.7035851783060083
iteration: 170 loss: 7.680424408783396 grad: -0.6753047142440556
iteration: 180 loss: 7.296378987405702 grad: -0.648948376154425
iteration: 190 loss: 6.948289051918271 grad: -0.6243728900358293
iteration: 200 loss: 6.631375363477136 grad: -0.6014363375219236
iteration: 210 loss: 6.341666147336169 grad: -0.5800035255021418
iteration: 220 loss: 6.07583371001283 grad: -0.5599484902913738
iteration: 230 loss: 5.831069206603471 grad: -0.5411554015780888
iteration: 240 loss: 5.604985535358207 grad: -0.5235185877868545
iteration: 250 loss: 5.395541245258507 grad: -0.5069420972096454
iteration: 260 loss: 5.200980331669598 grad: -0.4913390331303064
iteration: 270 loss: 5.019784178068097 grad: -0.47663079908417244
iteration: 280 loss: 4.850632877666405 grad: -0.4627463307518728
iteration: 290 loss: 4.692373866766792 grad: -0.4496213559857357
iteration: 300 loss: 4.5439963072764264 grad: -0.4371977039334486
iteration: 310 loss: 4.404610026284652 grad: -0.42542267225783437
iteration: 320 loss: 4.273428095017565 grad: -0.41424845459147974
iteration: 330 loss: 4.1497523347607475 grad: -0.4036316265562221
iteration: 340 loss: 4.032961192331228 grad: -0.3935326866901169
iteration: 350 loss: 3.9224995457420433 grad: -0.3839156477165918
iteration: 360 loss: 3.81787009133425 grad: -0.3747476733125435
iteration: 370 loss: 3.7186260338042882 grad: -0.36599875561119655
iteration: 380 loss: 3.6243648552008794 grad: -0.35764142894627904
iteration: 390 loss: 3.5347229818589354 grad: -0.3496505157070031
iteration: 400 loss: 3.449371202099304 grad: -0.3420029005694878
iteration: 410 loss: 3.3680107144346865 grad: -0.3346773297655915
iteration: 420 loss: 3.2903697075153517 grad: -0.32765423242590563
iteration: 430 loss: 3.2162003903174745 grad: -0.32091556138046706
iteration: 440 loss: 3.1452764050449415 grad: -0.3144446511147906
iteration: 450 loss: 3.077390566507099 grad: -0.3082260908595885
iteration: 460 loss: 3.0123528809966627 grad: -0.30224561104135916
iteration: 470 loss: 2.949988805233261 grad: -0.2964899815402116
iteration: 480 loss: 2.89013771217833 grad: -0.29094692039357056
iteration: 490 loss: 2.8326515356433943 grad: -0.2856050117525891
iteration: 0 loss: 77.50826344919689 grad: 24.99360366800062
iteration: 10 loss: 46.357436546619425 grad: 6.478494720857565
iteration: 20 loss: 34.84381216484296 grad: 1.0996525378167128
iteration: 30 loss: 28.214054714766505 grad: -0.3776713710998333
iteration: 40 loss: 23.765180809805564 grad: -0.8480019013245272
iteration: 50 loss: 20.540261346793944 grad: -1.0007662804385387
iteration: 60 loss: 18.085294257081056 grad: -1.036735524026081
iteration: 70 loss: 16.150512654380794 grad: -1.025473559929603
iteration: 80 loss: 14.585236469870324 grad: -0.9944749306492536
iteration: 90 loss: 13.292500710582049 grad: -0.9556789549552082
iteration: 100 loss: 12.20681188251615 grad: -0.9145968422654722
iteration: 110 loss: 11.282222218557164 grad: -0.8738483320031238
iteration: 120 loss: 10.485491205303044 grad: -0.834668519163456
iteration: 130 loss: 9.79195152889098 grad: -0.7976000757578336
iteration: 140 loss: 9.182900447895173 grad: -0.762830577795844
iteration: 150 loss: 8.643894037014 grad: -0.7303644950060123
iteration: 160 loss: 8.163597683341674 grad: -0.700113840404628
iteration: 170 loss: 7.732991134811858 grad: -0.6719470054292805
iteration: 180 loss: 7.344806242536709 grad: -0.6457153355122713
iteration: 190 loss: 6.993121356728228 grad: -0.6212675328992864
iteration: 200 loss: 6.673063569953146 grad: -0.5984572728392803
iteration: 210 loss: 6.380586694299795 grad: -0.5771469919203819
iteration: 220 loss: 6.11230337174588 grad: -0.5572095111944029
iteration: 230 loss: 5.865356499096337 grad: -0.5385284446914129
iteration: 240 loss: 5.637319619632845 grad: -0.5209979433145324
iteration: 250 loss: 5.426118938639113 grad: -0.5045220942373144
iteration: 260 loss: 5.2299716756577 grad: -0.48901416195075537
iteration: 270 loss: 5.0473368953790585 grad: -0.47439577810759986
iteration: 280 loss: 4.876875967127986 grad: -0.4605961403822033
iteration: 290 loss: 4.717420523686252 grad: -0.44755125261112405
iteration: 300 loss: 4.5679463119780515 grad: -0.4352032218812498
iteration: 310 loss: 4.427551710252043 grad: -0.4234996184828033
iteration: 320 loss: 4.295439969199296 grad: -0.41239289904518606
iteration: 330 loss: 4.170904445871119 grad: -0.4018398900905543
iteration: 340 loss: 4.053316258763516 grad: -0.3918013276760915
iteration: 350 loss: 3.942113913830822 grad: -0.3822414481391697
iteration: 360 loss: 3.8367945443446017 grad: -0.37312762483330075
iteration: 370 loss: 3.736906479517984 grad: -0.36443004591534145
iteration: 380 loss: 3.642042912913935 grad: -0.3561214285740508
iteration: 390 loss: 3.5518364856174727 grad: -0.3481767654909993
iteration: 400 loss: 3.465954633860871 grad: -0.34057309974599725
iteration: 410 loss: 3.3840955783353737 grad: -0.33328932479139844
iteration: 420 loss: 3.305984854430707 grad: -0.3263060065069642
iteration: 430 loss: 3.2313723003030232 grad: -0.31960522470184033
iteration: 440 loss: 3.1600294339307102 grad: -0.31317043174993353
iteration: 450 loss: 3.091747161886112 grad: -0.30698632632976236
iteration: 460 loss: 3.0263337719803034 grad: -0.30103874049158014
iteration: 470 loss: 2.9636131696461234 grad: -0.29531453849580636
iteration: 480 loss: 2.9034233242803733 grad: -0.28980152606053616
iteration: 490 loss: 2.845614897004907 grad: -0.28448836882512896
iteration: 0 loss: 77.34786531020055 grad: 32.43912067318601
iteration: 10 loss: 46.290655642517216 grad: 6.493983619189856
iteration: 20 loss: 34.8329813173759 grad: 0.30165713866848676
iteration: 30 loss: 28.217689953335977 grad: -0.9771223303947016
iteration: 40 loss: 23.779130179006458 grad: -1.2585136534510066
iteration: 50 loss: 20.56210680807232 grad: -1.2904224083673301
iteration: 60 loss: 18.11256616574875 grad: -1.2507774426755343
iteration: 70 loss: 16.18117658475103 grad: -1.1907261467904071
iteration: 80 loss: 14.617798201134395 grad: -1.1269916101237007
iteration: 90 loss: 13.325923307069719 grad: -1.0653808304190933
iteration: 100 loss: 12.24039925937671 grad: -1.0078305913497452
iteration: 110 loss: 11.315518919698992 grad: -0.9548073038040249
iteration: 120 loss: 10.518207256019775 grad: -0.9062064916144077
iteration: 130 loss: 9.823909186294156 grad: -0.861712966820735
iteration: 140 loss: 9.213997439665905 grad: -0.8209504782125753
iteration: 150 loss: 8.674078490646934 grad: -0.7835440656181573
iteration: 160 loss: 8.192851116992209 grad: -0.7491447417498243
iteration: 170 loss: 7.761316936620708 grad: -0.717437598698406
iteration: 180 loss: 7.372221868247297 grad: -0.6881426661687391
iteration: 190 loss: 7.019653044970048 grad: -0.6610127491145232
iteration: 200 loss: 6.698742765423876 grad: -0.6358301800400095
iteration: 210 loss: 6.405447633068186 grad: -0.6124033630444253
iteration: 220 loss: 6.136381459998285 grad: -0.5905634901171498
iteration: 230 loss: 5.888687235316163 grad: -0.5701615760340191
iteration: 240 loss: 5.659937889878827 grad: -0.5510658490312876
iteration: 250 loss: 5.448058568277247 grad: -0.5331594854833687
iteration: 260 loss: 5.251265157293388 grad: -0.5163386569425155
iteration: 270 loss: 5.068015237511397 grad: -0.5005108519142292
iteration: 280 loss: 4.896968625039583 grad: -0.4855934351586596
iteration: 290 loss: 4.736955385785057 grad: -0.47151241043920433
iteration: 300 loss: 4.586949722921087 grad: -0.4582013566889874
iteration: 310 loss: 4.44604851782647 grad: -0.4456005116857717
iteration: 320 loss: 4.313453585897647 grad: -0.43365598114313497
iteration: 330 loss: 4.188456918858637 grad: -0.42231905450287344
iteration: 340 loss: 4.070428343903999 grad: -0.4115456116267963
iteration: 350 loss: 3.958805150807579 grad: -0.4012956070629764
iteration: 360 loss: 3.853083330885387 grad: -0.3915326206500868
iteration: 370 loss: 3.752810143428881 grad: -0.38222346497659254
iteration: 380 loss: 3.6575777811055805 grad: -0.37333784167987505
iteration: 390 loss: 3.567017949643328 grad: -0.36484803979933395
iteration: 400 loss: 3.480797211733096 grad: -0.3567286704264772
iteration: 410 loss: 3.398612972536863 grad: -0.3489564327574079
iteration: 420 loss: 3.32019000616211 grad: -0.3415099073769656
iteration: 430 loss: 3.2452774400628432 grad: -0.33436937321243965
iteration: 440 loss: 3.1736461285919506 grad: -0.32751664510755324
iteration: 450 loss: 3.105086358441316 grad: -0.32093492940036106
iteration: 460 loss: 3.039405838154711 grad: -0.3146086952550201
iteration: 470 loss: 2.976427931575968 grad: -0.30852355980804613
iteration: 480 loss: 2.915990101456704 grad: -0.30266618545368595
iteration: 490 loss: 2.8579425346762366 grad: -0.2970241878179457
iteration: 0 loss: 75.6663149677304 grad: 32.28245185255938
iteration: 10 loss: 44.86507557346572 grad: 6.078015119444904
iteration: 20 loss: 33.94073995211383 grad: -0.0014722477533364933
iteration: 30 loss: 27.581743491397322 grad: -1.1945977337554836
iteration: 40 loss: 23.289877558382862 grad: -1.4250369489750558
iteration: 50 loss: 20.16641816331259 grad: -1.4236507856170684
iteration: 60 loss: 17.781076158684755 grad: -1.3606730687657655
iteration: 70 loss: 15.89612322657829 grad: -1.283528574620029
iteration: 80 loss: 14.367713837306423 grad: -1.2068542721700632
iteration: 90 loss: 13.103027354522737 grad: -1.1351800083813126
iteration: 100 loss: 12.039200474117086 grad: -1.0696218566211004
iteration: 110 loss: 11.132014510331395 grad: -1.010100736547476
iteration: 120 loss: 10.349399299719524 grad: -0.956138819624933
iteration: 130 loss: 9.667499887176291 grad: -0.9071583283110405
iteration: 140 loss: 9.06819009234139 grad: -0.8625936534710132
iteration: 150 loss: 8.537443457698414 grad: -0.8219302056882445
iteration: 160 loss: 8.06423317701125 grad: -0.7847140023604593
iteration: 170 loss: 7.639769744011449 grad: -0.7505498217148392
iteration: 180 loss: 7.256960642464244 grad: -0.7190953289910915
iteration: 190 loss: 6.910019796690407 grad: -0.6900542518207355
iteration: 200 loss: 6.5941803191016 grad: -0.6631698354644897
iteration: 210 loss: 6.305479932953569 grad: -0.6382190122759233
iteration: 220 loss: 6.040598437836294 grad: -0.615007380339587
iteration: 230 loss: 5.796733038921337 grad: -0.5933649488100414
iteration: 240 loss: 5.571501621579997 grad: -0.5731425594599499
iteration: 250 loss: 5.362866921227826 grad: -0.5542088846549367
iteration: 260 loss: 5.169076503548777 grad: -0.5364479084229782
iteration: 270 loss: 4.988614838619882 grad: -0.5197568091196023
iteration: 280 loss: 4.820164719304144 grad: -0.5040441748789545
iteration: 290 loss: 4.662575966622378 grad: -0.4892284947500906
iteration: 300 loss: 4.514839866824373 grad: -0.4752368785411283
iteration: 310 loss: 4.3760681530054 grad: -0.46200396686344963
iteration: 320 loss: 4.245475616983268 grad: -0.44947099983269084
iteration: 330 loss: 4.122365641409653 grad: -0.4375850185594826
iteration: 340 loss: 4.006118096379148 grad: -0.426298178170666
iteration: 350 loss: 3.8961791623823956 grad: -0.41556715483833545
iteration: 360 loss: 3.7920527317617765 grad: -0.4053526323267729
iteration: 370 loss: 3.6932931107407554 grad: -0.3956188560336516
iteration: 380 loss: 3.599498798582152 grad: -0.3863332445130902
iteration: 390 loss: 3.510307163204371 grad: -0.3774660501133357
iteration: 400 loss: 3.425389866368691 grad: -0.3689900617122634
iteration: 410 loss: 3.344448918386578 grad: -0.36088034364620214
iteration: 420 loss: 3.2672132637470153 grad: -0.35311400584702873
iteration: 430 loss: 3.1934358163091265 grad: -0.3456700009651851
iteration: 440 loss: 3.1228908766249166 grad: -0.338528944891104
iteration: 450 loss: 3.055371875249007 grad: -0.3316729576177543
iteration: 460 loss: 2.9906893951228994 grad: -0.32508552183128536
iteration: 470 loss: 2.9286694336679333 grad: -0.31875135699020746
iteration: 480 loss: 2.869151871422508 grad: -0.31265630696841784
iteration: 490 loss: 2.8119891192052986 grad: -0.3067872396036691
iteration: 0 loss: 76.4330943885242 grad: 30.877050132350934
iteration: 10 loss: 44.739090095290024 grad: 6.640450579759968
iteration: 20 loss: 33.796416416717214 grad: 0.3680761846258523
iteration: 30 loss: 27.487648561112522 grad: -0.9888930979801003
iteration: 40 loss: 23.231429046059578 grad: -1.301256009713906
iteration: 50 loss: 20.131049939800235 grad: -1.3422593260509728
iteration: 60 loss: 17.76080502883108 grad: -1.302812950872896
iteration: 70 loss: 15.885910589140519 grad: -1.2395159213809712
iteration: 80 loss: 14.364298906698071 grad: -1.171396400905901
iteration: 90 loss: 13.104245003135413 grad: -1.1052523862637305
iteration: 100 loss: 12.043582200651786 grad: -1.0434373502722738
iteration: 110 loss: 11.138547559754153 grad: -0.98657306908951
iteration: 120 loss: 10.357376207876314 grad: -0.9345894578778113
iteration: 130 loss: 9.676421641352052 grad: -0.8871509687423185
iteration: 140 loss: 9.077702234947951 grad: -0.843838555689475
iteration: 150 loss: 8.547293065791502 grad: -0.8042279259043268
iteration: 160 loss: 8.074239412598052 grad: -0.767921824514961
iteration: 170 loss: 7.649803411801238 grad: -0.7345614662851101
iteration: 180 loss: 7.266929829335078 grad: -0.7038285231341176
iteration: 190 loss: 6.9198596769454275 grad: -0.675443003507002
iteration: 200 loss: 6.60384584889293 grad: -0.6491595516970172
iteration: 210 loss: 6.3149405707710935 grad: -0.6247633558669135
iteration: 220 loss: 6.049834300133046 grad: -0.6020662023581318
iteration: 230 loss: 5.805732081722371 grad: -0.5809028955619912
iteration: 240 loss: 5.58025756185812 grad: -0.5611281088191333
iteration: 250 loss: 5.371377695912625 grad: -0.5426136602444773
iteration: 260 loss: 5.177343122119385 grad: -0.5252461769175053
iteration: 270 loss: 4.996640525779722 grad: -0.5089251006793157
iteration: 280 loss: 4.827954272753107 grad: -0.4935609882069716
iteration: 290 loss: 4.6701352752284455 grad: -0.4790740616925725
iteration: 300 loss: 4.522175548982928 grad: -0.4653929716399784
iteration: 310 loss: 4.383187285460777 grad: -0.45245373870291616
iteration: 320 loss: 4.252385531993448 grad: -0.44019884653811736
iteration: 330 loss: 4.1290737756995775 grad: -0.4285764621122388
iteration: 340 loss: 4.012631879442117 grad: -0.4175397637349176
iteration: 350 loss: 3.9025059347281967 grad: -0.4070463603268381
iteration: 360 loss: 3.798199685979274 grad: -0.3970577881379778
iteration: 370 loss: 3.699267249942295 grad: -0.3875390733821594
iteration: 380 loss: 3.6053069080844167 grad: -0.378458351121596
iteration: 390 loss: 3.515955792264487 grad: -0.3697865322834252
iteration: 400 loss: 3.4308853175366907 grad: -0.3614970119744574
iteration: 410 loss: 3.349797242592889 grad: -0.35356541332688884
iteration: 420 loss: 3.2724202596818115 grad: -0.34596936199503703
iteration: 430 loss: 3.1985070329781347 grad: -0.3386882871629152
iteration: 440 loss: 3.127831618216243 grad: -0.3317032455406953
iteration: 450 loss: 3.060187207656368 grad: -0.32499676534603344
iteration: 460 loss: 2.9953841536122456 grad: -0.31855270770125715
iteration: 470 loss: 2.9332482312931707 grad: -0.31235614324377364
iteration: 480 loss: 2.8736191078977424 grad: -0.30639324205636426
iteration: 490 loss: 2.8163489900008987 grad: -0.3006511752858876
iteration: 0 loss: 77.84950303296188 grad: 25.889429681991068
iteration: 10 loss: 47.13235452253594 grad: 7.523023087172342
iteration: 20 loss: 35.30814438603383 grad: 1.3144603480076538
iteration: 30 loss: 28.543151545060628 grad: -0.45666710408560696
iteration: 40 loss: 24.02146557531298 grad: -1.0024919524644194
iteration: 50 loss: 20.750675149476944 grad: -1.1619351901201287
iteration: 60 loss: 18.264003047303152 grad: -1.1846782630232155
iteration: 70 loss: 16.3059494583908 grad: -1.1560737348261148
iteration: 80 loss: 14.722842355031018 grad: -1.1085712646768298
iteration: 90 loss: 13.415992309237067 grad: -1.055428298318715
iteration: 100 loss: 12.318840662474651 grad: -1.0022617901465685
iteration: 110 loss: 11.384746623420204 grad: -0.9514293934556368
iteration: 120 loss: 10.579999773002804 grad: -0.9038311994388469
iteration: 130 loss: 9.879603109938216 grad: -0.8596992340759761
iteration: 140 loss: 9.264615782191186 grad: -0.8189604570782099
iteration: 150 loss: 8.720417542177158 grad: -0.7814093754375986
iteration: 160 loss: 8.235540337094129 grad: -0.7467916467948275
iteration: 170 loss: 7.800860984246273 grad: -0.714844550766379
iteration: 180 loss: 7.409030594156175 grad: -0.6853160171785938
iteration: 190 loss: 7.054063236460396 grad: -0.6579728363382435
iteration: 200 loss: 6.731034149786038 grad: -0.6326034043637896
iteration: 210 loss: 6.435854823360759 grad: -0.6090177561285568
iteration: 220 loss: 6.165102991160893 grad: -0.5870463185910222
iteration: 230 loss: 5.915892484871413 grad: -0.5665381321094388
iteration: 240 loss: 5.685772440445445 grad: -0.5473589255907362
iteration: 250 loss: 5.472648408093764 grad: -0.5293892383262169
iteration: 260 loss: 5.274720004081618 grad: -0.5125226780872414
iteration: 270 loss: 5.09043119366288 grad: -0.4966643501544039
iteration: 280 loss: 4.9184303174922865 grad: -0.48172946338213984
iteration: 290 loss: 4.757537704919989 grad: -0.4676421052057219
iteration: 300 loss: 4.60671924658647 grad: -0.4543341711223468
iteration: 310 loss: 4.4650646859854115 grad: -0.44174443199584273
iteration: 320 loss: 4.33176967616092 grad: -0.42981772253218287
iteration: 330 loss: 4.206120861837255 grad: -0.4185042353350715
iteration: 340 loss: 4.0874834087625125 grad: -0.4077589064653562
iteration: 350 loss: 3.9752905249308075 grad: -0.39754088006623406
iteration: 360 loss: 3.8690346126137145 grad: -0.38781304120872584
iteration: 370 loss: 3.7682597629892287 grad: -0.37854160757844246
iteration: 380 loss: 3.672555361893129 grad: -0.36969577193386405
iteration: 390 loss: 3.581550619687195 grad: -0.36124738841315573
iteration: 400 loss: 3.4949098733401858 grad: -0.3531706967591282
iteration: 410 loss: 3.412328536661376 grad: -0.3454420793845416
iteration: 420 loss: 3.3335295968752026 grad: -0.3380398469288237
iteration: 430 loss: 3.258260573580463 grad: -0.3309440485786323
iteration: 440 loss: 3.186290870550155 grad: -0.32413630395359594
iteration: 450 loss: 3.1174094625155457 grad: -0.3175996538085642
iteration: 460 loss: 3.0514228685996883 grad: -0.31131842718661296
iteration: 470 loss: 2.9881533718746804 grad: -0.30527812298308954
iteration: 480 loss: 2.9274374509198178 grad: -0.2994653041589035
iteration: 490 loss: 2.8691243945512808 grad: -0.2938675030784594
iteration: 0 loss: 75.26941405736096 grad: 42.348176560928195
iteration: 10 loss: 44.413879252531025 grad: 5.648237240689328
iteration: 20 loss: 33.85347511631631 grad: -0.8586475281770924
iteration: 30 loss: 27.598695653308017 grad: -1.7006765523236465
iteration: 40 loss: 23.345662354446485 grad: -1.7370450922873897
iteration: 50 loss: 20.238259721431405 grad: -1.6380932285770795
iteration: 60 loss: 17.859096666505447 grad: -1.5219060048296027
iteration: 70 loss: 15.975551160051138 grad: -1.4126543142440893
iteration: 80 loss: 14.44613120693349 grad: -1.314786282053686
iteration: 90 loss: 13.179199811799064 grad: -1.228088720343574
iteration: 100 loss: 12.112522134781289 grad: -1.1512929702313324
iteration: 110 loss: 11.202225142239946 grad: -1.083023493444641
iteration: 120 loss: 10.416432785070832 grad: -1.0220440621632523
iteration: 130 loss: 9.731399784029774 grad: -0.967303377679389
iteration: 140 loss: 9.129061087629948 grad: -0.9179233311473348
iteration: 150 loss: 8.595422764290644 grad: -0.8731735104798989
iteration: 160 loss: 8.119473388921238 grad: -0.8324453581365779
iteration: 170 loss: 7.692428470545358 grad: -0.7952297078680883
iteration: 180 loss: 7.307194255379479 grad: -0.7610982834531299
iteration: 190 loss: 6.957979724603981 grad: -0.7296887784944979
iteration: 200 loss: 6.6400109434993775 grad: -0.7006929129809736
iteration: 210 loss: 6.3493175002862365 grad: -0.6738468820970619
iteration: 220 loss: 6.0825706173816965 grad: -0.6489237007245141
iteration: 230 loss: 5.8369588881708045 grad: -0.6257270427925585
iteration: 240 loss: 5.610091803937146 grad: -0.604086259095876
iteration: 250 loss: 5.3999240741124845 grad: -0.5838523262910842
iteration: 260 loss: 5.204695689809879 grad: -0.5648945344349778
iteration: 270 loss: 5.022884037212737 grad: -0.5470977629807243
iteration: 280 loss: 4.853165326589575 grad: -0.5303602280453095
iteration: 290 loss: 4.694383290026715 grad: -0.5145916091319042
iteration: 300 loss: 4.545523599573781 grad: -0.49971148306256397
iteration: 310 loss: 4.405692823377401 grad: -0.48564800800554087
iteration: 320 loss: 4.27410100866692 grad: -0.47233681221270096
iteration: 330 loss: 4.150047183664231 grad: -0.4597200512146148
iteration: 340 loss: 4.032907224041785 grad: -0.4477456043609956
iteration: 350 loss: 3.922123646645187 grad: -0.4363663872034298
iteration: 360 loss: 3.8171969831574972 grad: -0.42553976064587673
iteration: 370 loss: 3.7176784560598124 grad: -0.415227021302605
iteration: 380 loss: 3.623163733586559 grad: -0.4053929603062049
iteration: 390 loss: 3.5332875830254538 grad: -0.3960054800554718
iteration: 400 loss: 3.447719275437852 grad: -0.38703526020365053
iteration: 410 loss: 3.3661586216692494 grad: -0.37845546565378296
iteration: 420 loss: 3.2883325409501265 grad: -0.3702414905210123
iteration: 430 loss: 3.2139920806081053 grad: -0.3623707329970688
iteration: 440 loss: 3.1429098193440783 grad: -0.35482239685323513
iteration: 450 loss: 3.0748775978031517 grad: -0.34757731597881963
iteration: 460 loss: 3.009704529421428 grad: -0.3406177988995016
iteration: 470 loss: 2.947215252058744 grad: -0.33392749067512817
iteration: 480 loss: 2.8872483871702816 grad: -0.32749124995660084
iteration: 490 loss: 2.8296551783890376 grad: -0.32129503930003966
iteration: 0 loss: 75.54748129540963 grad: 34.63445384071596
iteration: 10 loss: 43.57623749075003 grad: 5.58895041277807
iteration: 20 loss: 33.1205835576368 grad: -0.2865335526880536
iteration: 30 loss: 27.032163030863558 grad: -1.2714190395714353
iteration: 40 loss: 22.900602557726415 grad: -1.4143756218243322
iteration: 50 loss: 19.87915115690304 grad: -1.3793080415086618
iteration: 60 loss: 17.562369784706437 grad: -1.3045954226580667
iteration: 70 loss: 15.725457316085711 grad: -1.2248267806245818
iteration: 80 loss: 14.231828331982156 grad: -1.1493746588204048
iteration: 90 loss: 12.992994664337154 grad: -1.080512120946036
iteration: 100 loss: 11.948807207122151 grad: -1.0183464924286754
iteration: 110 loss: 11.056816499382855 grad: -0.9623347197866553
iteration: 120 loss: 10.286144191286708 grad: -0.9117843190820804
iteration: 130 loss: 9.61376065733998 grad: -0.8660216381039252
iteration: 140 loss: 9.022124836375456 grad: -0.8244435437557049
iteration: 150 loss: 8.497633562233204 grad: -0.7865271019378322
iteration: 160 loss: 8.029571688220669 grad: -0.7518245569930078
iteration: 170 loss: 7.609382761272958 grad: -0.7199539416048623
iteration: 180 loss: 7.230150958204169 grad: -0.6905892131760207
iteration: 190 loss: 6.886225834500982 grad: -0.6634512895009086
iteration: 200 loss: 6.572945786454304 grad: -0.6383003575210369
iteration: 210 loss: 6.286431101627923 grad: -0.6149294441889916
iteration: 220 loss: 6.023426934669715 grad: -0.5931591053702178
iteration: 230 loss: 5.781182670375859 grad: -0.5728330579604474
iteration: 240 loss: 5.557358187106977 grad: -0.5538145892549524
iteration: 250 loss: 5.349950265748041 grad: -0.5359835996130358
iteration: 260 loss: 5.157234264571136 grad: -0.5192341586929667
iteration: 270 loss: 4.977717488048008 grad: -0.5034724777891261
iteration: 280 loss: 4.810101603063396 grad: -0.4886152197785164
iteration: 290 loss: 4.653252119613213 grad: -0.4745880837762131
iteration: 300 loss: 4.506173434925999 grad: -0.46132461416272325
iteration: 310 loss: 4.367988293798631 grad: -0.44876519366325074
iteration: 320 loss: 4.237920780537355 grad: -0.436856188104757
iteration: 330 loss: 4.11528215473345 grad: -0.4255492167684646
iteration: 340 loss: 3.9994589919501364 grad: -0.4148005272395869
iteration: 350 loss: 3.8899032039811243 grad: -0.40457045761206145
iteration: 360 loss: 3.7861236006603822 grad: -0.3948229720553663
iteration: 370 loss: 3.6876787228816283 grad: -0.385525258266866
iteration: 380 loss: 3.594170729280863 grad: -0.3766473773522053
iteration: 390 loss: 3.505240160529287 grad: -0.3681619583035311
iteration: 400 loss: 3.4205614379685274 grad: -0.3600439305629147
iteration: 410 loss: 3.339838979415204 grad: -0.35227028923022674
iteration: 420 loss: 3.262803835815248 grad: -0.3448198883507615
iteration: 430 loss: 3.189210769217549 grad: -0.3376732584372574
iteration: 440 loss: 3.118835706093829 grad: -0.3308124449742548
iteration: 450 loss: 3.0514735110489046 grad: -0.32422086514424664
iteration: 460 loss: 2.986936034967047 grad: -0.3178831804239649
iteration: 470 loss: 2.9250503989998666 grad: -0.311785183040691
iteration: 480 loss: 2.865657481877282 grad: -0.3059136945648616
iteration: 490 loss: 2.8086105830459265 grad: -0.3002564751562957
iteration: 0 loss: 76.03932774333069 grad: 30.903514180032126
iteration: 10 loss: 43.85065538941473 grad: 5.74553753608339
iteration: 20 loss: 32.98937788227777 grad: 0.5543721313712092
iteration: 30 loss: 26.78018508215223 grad: -0.6871388214267835
iteration: 40 loss: 22.623305486794607 grad: -1.0627160100177862
iteration: 50 loss: 19.608133205989155 grad: -1.1744589291310201
iteration: 60 loss: 17.308072399248534 grad: -1.1880501338570477
iteration: 70 loss: 15.490619430276205 grad: -1.161487969448988
iteration: 80 loss: 14.016243636474634 grad: -1.1184374781520072
iteration: 90 loss: 12.79535808479754 grad: -1.0694800840927687
iteration: 100 loss: 11.767473572778583 grad: -1.019575047962965
iteration: 110 loss: 10.89011806236437 grad: -0.9710649431655622
iteration: 120 loss: 10.132519088723381 grad: -0.92500990892539
iteration: 130 loss: 9.471797791542981 grad: -0.8818225007779146
iteration: 140 loss: 8.89057162472752 grad: -0.8415851097581358
iteration: 150 loss: 8.375387737485054 grad: -0.8042142732587092
iteration: 160 loss: 7.915666988856236 grad: -0.7695476470365414
iteration: 170 loss: 7.502973080047468 grad: -0.7373905177958996
iteration: 180 loss: 7.130495006631133 grad: -0.7075405902831516
iteration: 190 loss: 6.792673170757925 grad: -0.6798008942060236
iteration: 200 loss: 6.484924472195706 grad: -0.6539861264190135
iteration: 210 loss: 6.2034369805390845 grad: -0.6299253591541315
iteration: 220 loss: 5.945014406584423 grad: -0.6074627557430418
iteration: 230 loss: 5.706956791820755 grad: -0.5864572227091641
iteration: 240 loss: 5.486967923015775 grad: -0.5667815262384155
iteration: 250 loss: 5.28308272784873 grad: -0.5483211723576147
iteration: 260 loss: 5.09360978940454 grad: -0.5309732183736787
iteration: 270 loss: 4.917085426744382 grad: -0.5146451067380722
iteration: 280 loss: 4.752236713407952 grad: -0.49925356817333344
iteration: 290 loss: 4.597951467591429 grad: -0.48472361531216396
iteration: 300 loss: 4.453253727507342 grad: -0.470987633542695
iteration: 310 loss: 4.317283577178304 grad: -0.4579845677084555
iteration: 320 loss: 4.189280448634194 grad: -0.44565919910662544
iteration: 330 loss: 4.068569221595556 grad: -0.4339615052721007
iteration: 340 loss: 3.9545485891575978 grad: -0.4228460943755502
iteration: 350 loss: 3.846681270322361 grad: -0.4122717061306536
iteration: 360 loss: 3.74448573651985 grad: -0.4022007715581234
iteration: 370 loss: 3.647529186083623 grad: -0.3925990245899723
iteration: 380 loss: 3.5554215527255852 grad: -0.3834351591988978
iteration: 390 loss: 3.467810374952602 grad: -0.37468052643784794
iteration: 400 loss: 3.3843763856700453 grad: -0.3663088664378741
iteration: 410 loss: 3.304829706895072 grad: -0.35829607102080974
iteration: 420 loss: 3.2289065550335594 grad: -0.3506199731305623
iteration: 430 loss: 3.1563663786677436 grad: -0.3432601597725339
iteration: 440 loss: 3.0869893641358264 grad: -0.33619780557796014
iteration: 450 loss: 3.0205742550132197 grad: -0.3294155244835091
iteration: 460 loss: 2.956936440428969 grad: -0.3228972373417164
iteration: 470 loss: 2.8959062743894552 grad: -0.31662805356024765
iteration: 480 loss: 2.8373275942421143 grad: -0.3105941651127664
iteration: 490 loss: 2.7810564113249465 grad: -0.30478275147619527
iteration: 0 loss: 76.65205750760559 grad: 32.274271391688295
iteration: 10 loss: 45.093057741278955 grad: 6.885336910726153
iteration: 20 loss: 33.998094595109144 grad: 0.23998381089471416
iteration: 30 loss: 27.63885819891576 grad: -1.18276629360418
iteration: 40 loss: 23.356439846445173 grad: -1.4745431453375175
iteration: 50 loss: 20.237644909892403 grad: -1.4821529047263997
iteration: 60 loss: 17.852978349435656 grad: -1.4137699239859998
iteration: 70 loss: 15.966431101473558 grad: -1.3282935190941207
iteration: 80 loss: 14.435303711057237 grad: -1.2436822766860935
iteration: 90 loss: 13.167428578109014 grad: -1.1652749385299512
iteration: 100 loss: 12.100295274069595 grad: -1.0942310933524406
iteration: 110 loss: 11.189868910993793 grad: -1.0303050651720014
iteration: 120 loss: 10.404169893115625 grad: -0.9728194365061501
iteration: 130 loss: 9.719382776118085 grad: -0.9210168638729705
iteration: 140 loss: 9.117393161567572 grad: -0.87418398700249
iteration: 150 loss: 8.584171623669631 grad: -0.8316888913210252
iteration: 160 loss: 8.10868090785766 grad: -0.7929857780251421
iteration: 170 loss: 7.682117660705899 grad: -0.757607924420556
iteration: 180 loss: 7.297374394814844 grad: -0.7251573183478476
iteration: 190 loss: 6.948650179422244 grad: -0.6952942232933663
iteration: 200 loss: 6.631164031894101 grad: -0.6677278235279682
iteration: 210 loss: 6.340940641526314 grad: -0.6422082374539624
iteration: 220 loss: 6.074647941069708 grad: -0.6185198510348919
iteration: 230 loss: 5.829472433926557 grad: -0.5964758087779465
iteration: 240 loss: 5.603022409750834 grad: -0.575913477171481
iteration: 250 loss: 5.393252028186825 grad: -0.5566907084067343
iteration: 260 loss: 5.198401203084483 grad: -0.5386827564274521
iteration: 270 loss: 5.016947580350503 grad: -0.5217797228269062
iteration: 280 loss: 4.847567864906358 grad: -0.5058844330732919
iteration: 290 loss: 4.689106441928164 grad: -0.49091066291920565
iteration: 300 loss: 4.540549737995786 grad: -0.4767816506756024
iteration: 310 loss: 4.401005135050169 grad: -0.4634288437414327
iteration: 320 loss: 4.2696835224554555 grad: -0.45079083790648034
iteration: 330 loss: 4.145884776496339 grad: -0.4388124759859394
iteration: 340 loss: 4.028985610851206 grad: -0.42744407872875734
iteration: 350 loss: 3.9184293591640444 grad: -0.41664078601747834
iteration: 360 loss: 3.813717341176016 grad: -0.4063619904243886
iteration: 370 loss: 3.7144015338485175 grad: -0.3965708484268806
iteration: 380 loss: 3.6200783234806875 grad: -0.38723385718604025
iteration: 390 loss: 3.5303831576311495 grad: -0.3783204868905158
iteration: 400 loss: 3.4449859495250412 grad: -0.369802860367283
iteration: 410 loss: 3.363587114506903 grad: -0.36165547304355955
iteration: 420 loss: 3.2859141396140994 grad: -0.3538549474737621
iteration: 430 loss: 3.2117186046213066 grad: -0.3463798175721505
iteration: 440 loss: 3.140773586872621 grad: -0.3392103384552335
iteration: 450 loss: 3.0728713935492866 grad: -0.33232831842937693
iteration: 460 loss: 3.0078215742768077 grad: -0.3257169701832042
iteration: 470 loss: 2.9454491745310736 grad: -0.3193607786811335
iteration: 480 loss: 2.8855931965621164 grad: -0.313245383619625
iteration: 490 loss: 2.828105239684109 grad: -0.30735747461417023
iteration: 0 loss: 77.05997772370296 grad: 22.816660219628545
iteration: 10 loss: 45.73175124333561 grad: 6.4625146618253595
iteration: 20 loss: 34.41343576521436 grad: 1.5359354188491463
iteration: 30 loss: 27.892848033705707 grad: 0.000404408063443204
iteration: 40 loss: 23.515997451119265 grad: -0.5821147981796231
iteration: 50 loss: 20.341293708787404 grad: -0.8241967425473136
iteration: 60 loss: 17.922428580167477 grad: -0.9225100814141332
iteration: 70 loss: 16.01428010810849 grad: -0.953425999432355
iteration: 80 loss: 14.469120805729165 grad: -0.9508553019966064
iteration: 90 loss: 13.191911331468088 grad: -0.9313073334986551
iteration: 100 loss: 12.118442344882128 grad: -0.9033365149238913
iteration: 110 loss: 11.203641299507995 grad: -0.8715593030715683
iteration: 120 loss: 10.414876883778334 grad: -0.8385254514447713
iteration: 130 loss: 9.727914332443602 grad: -0.8056523711878802
iteration: 140 loss: 9.12436375223214 grad: -0.7737172983640728
iteration: 150 loss: 8.590011383884116 grad: -0.743127586186898
iteration: 160 loss: 8.113694632500007 grad: -0.7140741039832789
iteration: 170 loss: 7.686523681752657 grad: -0.6866205716158607
iteration: 180 loss: 7.301330619405937 grad: -0.6607566270895625
iteration: 190 loss: 6.952271784084545 grad: -0.636429806577189
iteration: 200 loss: 6.634535647001981 grad: -0.6135649886371651
iteration: 210 loss: 6.344124844597197 grad: -0.5920762504247997
iteration: 220 loss: 6.077691244382177 grad: -0.5718740644657934
iteration: 230 loss: 5.832409550281227 grad: -0.552869603485876
iteration: 240 loss: 5.60587932142289 grad: -0.5349772378335046
iteration: 250 loss: 5.3960482150371885 grad: -0.5181159002274638
iteration: 260 loss: 5.201151273932737 grad: -0.502209742329392
iteration: 270 loss: 5.019662476800642 grad: -0.48718835250438686
iteration: 280 loss: 4.850255756083201 grad: -0.4729867066821645
iteration: 290 loss: 4.69177339388311 grad: -0.45954496233252096
iteration: 300 loss: 4.5432002175368575 grad: -0.446808165891063
iteration: 310 loss: 4.403642390999597 grad: -0.43472591835512153
iteration: 320 loss: 4.272309875550656 grad: -0.42325202714879495
iteration: 330 loss: 4.148501840754514 grad: -0.41234416154259873
iteration: 340 loss: 4.031594463221981 grad: -0.4019635218781965
iteration: 350 loss: 3.921030669937078 grad: -0.3920745282872365
iteration: 360 loss: 3.8163114744670565 grad: -0.3826445316583892
iteration: 370 loss: 3.7169886251671085 grad: -0.3736435477441861
iteration: 380 loss: 3.6226583396572676 grad: -0.3650440141442801
iteration: 390 loss: 3.532955943137908 grad: -0.35682056921262695
iteration: 400 loss: 3.4475512622474196 grad: -0.34894985155220604
iteration: 410 loss: 3.3661446533263355 grad: -0.3414103185751158
iteration: 420 loss: 3.2884635656130907 grad: -0.33418208254676973
iteration: 430 loss: 3.2142595573136945 grad: -0.3272467625527955
iteration: 440 loss: 3.143305696548065 grad: -0.3205873508944679
iteration: 450 loss: 3.0753942905833935 grad: -0.31418809251215635
iteration: 460 loss: 3.0103348960602405 grad: -0.30803437614268747
iteration: 470 loss: 2.947952570548833 grad: -0.3021126360269165
iteration: 480 loss: 2.888086332021885 grad: -0.29641026309268353
iteration: 490 loss: 2.830587798012609 grad: -0.29091552464236614
iteration: 0 loss: 75.26727052167634 grad: 35.5334335438794
iteration: 10 loss: 43.665955077607094 grad: 6.003607296146267
iteration: 20 loss: 33.23726566218715 grad: -0.45116014579435804
iteration: 30 loss: 27.147238289012794 grad: -1.4937788639680547
iteration: 40 loss: 23.001456765727696 grad: -1.6050370945296186
iteration: 50 loss: 19.963290576248358 grad: -1.534553257920122
iteration: 60 loss: 17.630907296637506 grad: -1.4323364555312494
iteration: 70 loss: 15.780535343639837 grad: -1.3322259169596489
iteration: 80 loss: 14.27567311599982 grad: -1.2415451622171991
iteration: 90 loss: 13.027598274684578 grad: -1.1609692319530378
iteration: 100 loss: 11.975850674043965 grad: -1.0895360355650392
iteration: 110 loss: 11.077681831106103 grad: -1.0260061499880049
iteration: 120 loss: 10.301954594062325 grad: -0.9692286403711723
iteration: 130 loss: 9.625425254070334 grad: -0.9182202833337434
iteration: 140 loss: 9.030379345442704 grad: -0.8721624852112436
iteration: 150 loss: 8.503074558816127 grad: -0.8303776910039448
iteration: 160 loss: 8.032684456430367 grad: -0.7923039511771357
iteration: 170 loss: 7.6105635547351085 grad: -0.7574727535559451
iteration: 180 loss: 7.229724656471387 grad: -0.7254909617760137
iteration: 190 loss: 6.884459912606375 grad: -0.6960264524521882
iteration: 200 loss: 6.57006137220928 grad: -0.6687967751745272
iteration: 210 loss: 6.282611746558607 grad: -0.6435601942855279
iteration: 220 loss: 6.0188255933227826 grad: -0.620108582146205
iteration: 230 loss: 5.775927274775975 grad: -0.5982617471547946
iteration: 240 loss: 5.551556117301061 grad: -0.5778628757952307
iteration: 250 loss: 5.343691950269989 grad: -0.5587748437229179
iteration: 260 loss: 5.150596092657741 grad: -0.5408772088925233
iteration: 270 loss: 4.970764175314633 grad: -0.5240637436014699
iteration: 280 loss: 4.80288812138104 grad: -0.5082403953728275
iteration: 290 loss: 4.645825278046356 grad: -0.49332359151444294
iteration: 300 loss: 4.498573180095429 grad: -0.47923882103061843
iteration: 310 loss: 4.36024878370319 grad: -0.46591944187746964
iteration: 320 loss: 4.230071274707577 grad: -0.4533056724926815
iteration: 330 loss: 4.107347754865631 grad: -0.44134373494384116
iteration: 340 loss: 3.9914612603476844 grad: -0.4299851235533518
iteration: 350 loss: 3.881860681742968 grad: -0.4191859779355406
iteration: 360 loss: 3.778052243312438 grad: -0.40890654336650956
iteration: 370 loss: 3.679592267769584 grad: -0.3991107045557113
iteration: 380 loss: 3.5860810063667157 grad: -0.3897655813919884
iteration: 390 loss: 3.4971573560845752 grad: -0.3808411772404269
iteration: 400 loss: 3.412494318930385 grad: -0.37231007197943133
iteration: 410 loss: 3.3317950847929505 grad: -0.3641471532735636
iteration: 420 loss: 3.2547896404099776 grad: -0.3563293806410408
iteration: 430 loss: 3.1812318240038375 grad: -0.3488355777450204
iteration: 440 loss: 3.1108967588778467 grad: -0.34164624905347296
iteration: 450 loss: 3.0435786104101017 grad: -0.33474341760371934
iteration: 460 loss: 2.9790886199902245 grad: -0.328110481098404
iteration: 470 loss: 2.9172533769020363 grad: -0.3217320839685542
iteration: 480 loss: 2.8579132952982036 grad: -0.31559400338144306
iteration: 490 loss: 2.8009212684783877 grad: -0.30968304745822584
iteration: 0 loss: 77.41551987411486 grad: 29.530456478354147
iteration: 10 loss: 45.966473626683914 grad: 6.579172642446343
iteration: 20 loss: 34.532024730551555 grad: 0.42656898353037864
iteration: 30 loss: 27.98847029309778 grad: -0.9096521406674657
iteration: 40 loss: 23.604890422637315 grad: -1.2097855366825576
iteration: 50 loss: 20.427268169079028 grad: -1.2480336502628189
iteration: 60 loss: 18.00621870273852 grad: -1.2120346002358198
iteration: 70 loss: 16.095831756963147 grad: -1.1551372588054911
iteration: 80 loss: 14.548245812876985 grad: -1.0943624744597629
iteration: 90 loss: 13.268466064801652 grad: -1.0354709870530392
iteration: 100 loss: 12.192360028157736 grad: -0.9803516279335852
iteration: 110 loss: 11.27492223053301 grad: -0.9294646838792054
iteration: 120 loss: 10.483570135986941 grad: -0.8827274468008675
iteration: 130 loss: 9.794101053371916 grad: -0.8398581492625387
iteration: 140 loss: 9.18814433189885 grad: -0.800515750029332
iteration: 150 loss: 8.65149631523803 grad: -0.7643576343168152
iteration: 160 loss: 8.172998284870717 grad: -0.7310626612912514
iteration: 170 loss: 7.7437602032505515 grad: -0.7003391190017471
iteration: 180 loss: 7.3566113361688705 grad: -0.6719260491409889
iteration: 190 loss: 7.005703627918964 grad: -0.6455917377719156
iteration: 200 loss: 6.686220277856592 grad: -0.6211311141963678
iteration: 210 loss: 6.394158235375554 grad: -0.5983628617082164
iteration: 220 loss: 6.126163568048758 grad: -0.5771266029684723
iteration: 230 loss: 5.879405260041784 grad: -0.5572803118460602
iteration: 240 loss: 5.651477349945088 grad: -0.5386980023650485
iteration: 250 loss: 5.440322242918683 grad: -0.521267697676351
iteration: 260 loss: 5.244170034227616 grad: -0.5048896606841399
iteration: 270 loss: 5.061490073746385 grad: -0.48947485976813054
iteration: 280 loss: 4.890951983924011 grad: -0.47494364125453725
iteration: 290 loss: 4.731394046919666 grad: -0.4612245814999134
iteration: 300 loss: 4.581797386083053 grad: -0.4482534939331941
iteration: 310 loss: 4.441264740315133 grad: -0.43597256928172745
iteration: 320 loss: 4.309002906387542 grad: -0.42432963006277796
iteration: 330 loss: 4.1843081312001065 grad: -0.41327748306071926
iteration: 340 loss: 4.066553892167783 grad: -0.40277335585935187
iteration: 350 loss: 3.9551806229249618 grad: -0.39277840554589416
iteration: 360 loss: 3.8496870328783435 grad: -0.38325728946435966
iteration: 370 loss: 3.7496227398555675 grad: -0.37417778939949037
iteration: 380 loss: 3.654581990159634 grad: -0.36551048184993773
iteration: 390 loss: 3.564198283587385 grad: -0.3572284481321736
iteration: 400 loss: 3.4781397550853566 grad: -0.3493070189731962
iteration: 410 loss: 3.3961051918446503 grad: -0.3417235490257936
iteration: 420 loss: 3.317820586293072 grad: -0.33445721739682166
iteration: 430 loss: 3.2430361428628616 grad: -0.3274888508353265
iteration: 440 loss: 3.1715236704564465 grad: -0.3208007666992975
iteration: 450 loss: 3.1030743039598914 grad: -0.3143766332206824
iteration: 460 loss: 3.0374965074375226 grad: -0.30820134492929924
iteration: 470 loss: 2.9746143192860472 grad: -0.30226091138684064
iteration: 480 loss: 2.914265805875237 grad: -0.2965423576301568
iteration: 490 loss: 2.856301695395442 grad: -0.2910336349350846
iteration: 0 loss: 75.72563603471644 grad: 33.36339482779536
iteration: 10 loss: 43.47808509179895 grad: 7.254340893126377
iteration: 20 loss: 32.829853574245355 grad: -0.04917324905348199
iteration: 30 loss: 26.7762273496713 grad: -1.4738926324566504
iteration: 40 loss: 22.68917142979802 grad: -1.6896728020946545
iteration: 50 loss: 19.702718509888925 grad: -1.6371993850332673
iteration: 60 loss: 17.41237031854001 grad: -1.5293280625799348
iteration: 70 loss: 15.59572237996221 grad: -1.4186623515990293
iteration: 80 loss: 14.118005822429701 grad: -1.317801532585342
iteration: 90 loss: 12.891957343005226 grad: -1.2286244236437192
iteration: 100 loss: 11.858247804860309 grad: -1.1501891249763063
iteration: 110 loss: 10.974987904743887 grad: -1.080986043016725
iteration: 120 loss: 10.211692468872942 grad: -1.0195723004719897
iteration: 130 loss: 9.545615421062145 grad: -0.9647210531838546
iteration: 140 loss: 8.959425326590607 grad: -0.915426794825593
iteration: 150 loss: 8.439677436074279 grad: -0.8708731285240663
iteration: 160 loss: 7.975778637942974 grad: -0.8303960204848209
iteration: 170 loss: 7.559268069170932 grad: -0.793451808023476
iteration: 180 loss: 7.183305877564511 grad: -0.7595915945613837
iteration: 190 loss: 6.842302768662152 grad: -0.7284413883085041
iteration: 200 loss: 6.531646911839702 grad: -0.6996868486232132
iteration: 210 loss: 6.24749951000889 grad: -0.6730615688981564
iteration: 220 loss: 5.986639649833912 grad: -0.6483380278573916
iteration: 230 loss: 5.7463450808274334 grad: -0.6253205456925756
iteration: 240 loss: 5.524299563202765 grad: -0.6038397501958253
iteration: 250 loss: 5.318520117522811 grad: -0.5837481871913689
iteration: 260 loss: 5.127299358463915 grad: -0.5649168052165541
iteration: 270 loss: 4.94915938519245 grad: -0.5472321142572669
iteration: 280 loss: 4.782814614127553 grad: -0.5305938691478156
iteration: 290 loss: 4.627141595049249 grad: -0.5149131652493677
iteration: 300 loss: 4.481154327323979 grad: -0.5001108611024164
iteration: 310 loss: 4.343983942501299 grad: -0.4861162627032606
iteration: 320 loss: 4.214861878974881 grad: -0.472866018875344
iteration: 330 loss: 4.093105868851291 grad: -0.46030318830942063
iteration: 340 loss: 3.9781082042830103 grad: -0.4483764472388054
iteration: 350 loss: 3.8693258627438563 grad: -0.4370394131162208
iteration: 360 loss: 3.7662721570518856 grad: -0.42625006458299464
iteration: 370 loss: 3.6685096428197785 grad: -0.41597024184263787
iteration: 380 loss: 3.575644068220258 grad: -0.40616521454074833
iteration: 390 loss: 3.487319191959531 grad: -0.3968033066112041
iteration: 400 loss: 3.4032123277729625 grad: -0.38785556942228827
iteration: 410 loss: 3.323030499555857 grad: -0.37929549605583623
iteration: 420 loss: 3.246507111856648 grad: -0.37109877076056813
iteration: 430 loss: 3.173399057074097 grad: -0.3632430486002237
iteration: 440 loss: 3.1034841940819122 grad: -0.3557077611160073
iteration: 450 loss: 3.036559143940353 grad: -0.34847394447805635
iteration: 460 loss: 2.9724373572025384 grad: -0.34152408714088317
iteration: 470 loss: 2.91094741465651 grad: -0.3348419944653593
iteration: 480 loss: 2.8519315293141188 grad: -0.3284126681424161
iteration: 490 loss: 2.795244222445738 grad: -0.32222219856515044
iteration: 0 loss: 73.60292253832026 grad: 49.62672163355996
iteration: 10 loss: 42.855919291435605 grad: 4.096607410749478
iteration: 20 loss: 32.68929915834531 grad: -1.9419724074848213
iteration: 30 loss: 26.70770014665953 grad: -2.256206846912537
iteration: 40 loss: 22.63956247106053 grad: -2.064585763523795
iteration: 50 loss: 19.66056568915707 grad: -1.855361641388769
iteration: 60 loss: 17.37402772115977 grad: -1.6784388715826504
iteration: 70 loss: 15.559708284360216 grad: -1.5323659513612835
iteration: 80 loss: 14.083618725269183 grad: -1.4105482460956063
iteration: 90 loss: 12.858830842412688 grad: -1.3074063715876778
iteration: 100 loss: 11.826176171223757 grad: -1.2188160298319084
iteration: 110 loss: 10.943847734489365 grad: -1.1417740011704667
iteration: 120 loss: 10.181404304448982 grad: -1.0740651659306137
iteration: 130 loss: 9.516123928645955 grad: -1.0140234151399135
iteration: 140 loss: 8.930688714579384 grad: -0.9603702783540805
iteration: 150 loss: 8.411661681652442 grad: -0.9121064396866159
iteration: 160 loss: 7.94845429156852 grad: -0.8684377816941048
iteration: 170 loss: 7.5326084732467855 grad: -0.8287239724031319
iteration: 180 loss: 7.157286174970481 grad: -0.7924419843284092
iteration: 190 loss: 6.8168993579104775 grad: -0.7591596965530714
iteration: 200 loss: 6.5068371559123905 grad: -0.728516444917479
iteration: 210 loss: 6.223261588352381 grad: -0.7002084546305729
iteration: 220 loss: 5.962952489365886 grad: -0.6739777666604205
iteration: 230 loss: 5.723188328559403 grad: -0.6496037060411557
iteration: 240 loss: 5.501653578794173 grad: -0.6268962276462415
iteration: 250 loss: 5.296365973450478 grad: -0.6056906678104641
iteration: 260 loss: 5.105618841096202 grad: -0.5858435619427493
iteration: 270 loss: 4.92793499338367 grad: -0.5672292798227748
iteration: 280 loss: 4.7620295539129325 grad: -0.5497372948621078
iteration: 290 loss: 4.606779770122888 grad: -0.5332699498090319
iteration: 300 loss: 4.461200325574985 grad: -0.517740614839137
iteration: 310 loss: 4.3244230191687825 grad: -0.5030721584936803
iteration: 320 loss: 4.195679937083738 grad: -0.48919567008873155
iteration: 330 loss: 4.074289437593879 grad: -0.4760493858053297
iteration: 340 loss: 3.9596444159553603 grad: -0.4635777809333599
iteration: 350 loss: 3.851202428768333 grad: -0.4517307985634045
iteration: 360 loss: 3.7484773435400545 grad: -0.44046319103341597
iteration: 370 loss: 3.651032246037754 grad: -0.42973395509712214
iteration: 380 loss: 3.5584733902545107 grad: -0.4195058454211012
iteration: 390 loss: 3.4704450168084637 grad: -0.4097449538819653
iteration: 400 loss: 3.3866248980389586 grad: -0.40042034440530583
iteration: 410 loss: 3.306720493862237 grad: -0.39150373489935947
iteration: 420 loss: 3.2304656230817272 grad: -0.3829692192908192
iteration: 430 loss: 3.1576175714528953 grad: -0.37479302384517327
iteration: 440 loss: 3.087954571224145 grad: -0.36695329290859285
iteration: 450 loss: 3.021273597755639 grad: -0.35942989998823716
iteration: 460 loss: 2.957388437758048 grad: -0.3522042807281897
iteration: 470 loss: 2.8961279909342674 grad: -0.3452592848665754
iteration: 480 loss: 2.837334772862629 grad: -0.33857904469737
iteration: 490 loss: 2.780863591888812 grad: -0.33214885792503046
iteration: 0 loss: 77.28885339838597 grad: 25.73305433565944
iteration: 10 loss: 46.972311896598576 grad: 7.279735848897994
iteration: 20 loss: 35.34336794927697 grad: 1.4185045058384458
iteration: 30 loss: 28.606263000414227 grad: -0.32645332581814157
iteration: 40 loss: 24.08384470181702 grad: -0.9050660415057037
iteration: 50 loss: 20.806799858231464 grad: -1.095662664254743
iteration: 60 loss: 18.31330040292046 grad: -1.1404618832613191
iteration: 70 loss: 16.34905148696583 grad: -1.1265329994112974
iteration: 80 loss: 14.760621318035605 grad: -1.0887010033599698
iteration: 90 loss: 13.449284132429222 grad: -1.0419693530796792
iteration: 100 loss: 12.348371061773792 grad: -0.9930995779660506
iteration: 110 loss: 11.411122432704856 grad: -0.9451802669064078
iteration: 120 loss: 10.603720795505048 grad: -0.8995786281093114
iteration: 130 loss: 9.901078894085645 grad: -0.8568282413294093
iteration: 140 loss: 9.284181883082862 grad: -0.8170533204446148
iteration: 150 loss: 8.738349593001079 grad: -0.7801791769370272
iteration: 160 loss: 8.252065659295154 grad: -0.746039349271331
iteration: 170 loss: 7.8161679575510705 grad: -0.7144308741074767
iteration: 180 loss: 7.423276142802437 grad: -0.6851427580168398
iteration: 190 loss: 7.067378787876847 grad: -0.6579703018832836
iteration: 200 loss: 6.743530376430747 grad: -0.6327218609456219
iteration: 210 loss: 6.447625423291356 grad: -0.6092215488905578
iteration: 220 loss: 6.176227711639471 grad: -0.5873097900503639
iteration: 230 loss: 5.926439549942932 grad: -0.5668427655472971
iteration: 240 loss: 5.695800508404153 grad: -0.5476913307418488
iteration: 250 loss: 5.482208157078151 grad: -0.5297397215168178
iteration: 260 loss: 5.283855422452647 grad: -0.512884221192781
iteration: 270 loss: 5.099180635078607 grad: -0.49703187761689804
iteration: 280 loss: 4.926827367631921 grad: -0.4820993135395702
iteration: 290 loss: 4.765611896771258 grad: -0.4680116474004429
iteration: 300 loss: 4.6144966534396765 grad: -0.4547015273874236
iteration: 310 loss: 4.472568415220559 grad: -0.4421082741297618
iteration: 320 loss: 4.339020282208614 grad: -0.43017712375924405
iteration: 330 loss: 4.213136692995125 grad: -0.4188585616451898
iteration: 340 loss: 4.094280899653561 grad: -0.408107736898822
iteration: 350 loss: 3.9818844441016483 grad: -0.3978839481779175
iteration: 360 loss: 3.875438272961963 grad: -0.38815019207057966
iteration: 370 loss: 3.774485201267701 grad: -0.3788727662068233
iteration: 380 loss: 3.6786134923851352 grad: -0.3700209201318875
iteration: 390 loss: 3.587451366234539 grad: -0.36156654781911846
iteration: 400 loss: 3.5006622831550556 grad: -0.3534839164753937
iteration: 410 loss: 3.4179408787576904 grad: -0.345749426987804
iteration: 420 loss: 3.3390094474688423 grad: -0.33834140197554685
iteration: 430 loss: 3.2636148904128857 grad: -0.3312398979496803
iteration: 440 loss: 3.191526057762489 grad: -0.3244265385519398
iteration: 450 loss: 3.1225314274385956 grad: -0.3178843662495816
iteration: 460 loss: 3.056437071610121 grad: -0.3115977102136137
iteration: 470 loss: 2.993064870276984 grad: -0.30555206840983884
iteration: 480 loss: 2.9322509376744494 grad: -0.29973400219232177
iteration: 490 loss: 2.8738442325432683 grad: -0.2941310419129437
iteration: 0 loss: 77.0879598207812 grad: 25.730915163441576
iteration: 10 loss: 45.55366972815147 grad: 7.159591831820297
iteration: 20 loss: 34.16216516212296 grad: 1.0208598069931778
iteration: 30 loss: 27.68567589345355 grad: -0.6365750985487216
iteration: 40 loss: 23.350589645036713 grad: -1.1092602347326455
iteration: 50 loss: 20.207137959940074 grad: -1.2253618967571769
iteration: 60 loss: 17.81146126559063 grad: -1.2226273115185333
iteration: 70 loss: 15.92088642200995 grad: -1.178928974491914
iteration: 80 loss: 14.389360951574583 grad: -1.122384815087213
iteration: 90 loss: 13.122951016515076 grad: -1.0637704850338294
iteration: 100 loss: 12.058189329782573 grad: -1.0072693180147578
iteration: 110 loss: 11.150515055001858 grad: -0.9543990535606207
iteration: 120 loss: 10.3676594192129 grad: -0.905560048557387
iteration: 130 loss: 9.685650880645715 grad: -0.8606824947611674
iteration: 140 loss: 9.086294808694902 grad: -0.8195084686881798
iteration: 150 loss: 8.555525013872119 grad: -0.7817177527871196
iteration: 160 loss: 8.08229224102173 grad: -0.7469839625038992
iteration: 170 loss: 7.657794956164377 grad: -0.7149986448857801
iteration: 180 loss: 7.274934867045431 grad: -0.6854804173693434
iteration: 190 loss: 6.9279238208152325 grad: -0.6581771286717879
iteration: 200 loss: 6.611994983603878 grad: -0.632864858973488
iteration: 210 loss: 6.323187301535324 grad: -0.6093456070197675
iteration: 220 loss: 6.058182380221834 grad: -0.5874445570560634
iteration: 230 loss: 5.814179461827941 grad: -0.5670073474614
iteration: 240 loss: 5.588798493123084 grad: -0.5478975283040821
iteration: 250 loss: 5.380004179223306 grad: -0.5299942783058795
iteration: 260 loss: 5.186045903514241 grad: -0.5131903946294218
iteration: 270 loss: 5.005409775472841 grad: -0.49739054226930757
iteration: 280 loss: 4.836780043006498 grad: -0.4825097387097419
iteration: 290 loss: 4.679007803414524 grad: -0.46847204619795835
iteration: 300 loss: 4.531085452304813 grad: -0.4552094444585957
iteration: 310 loss: 4.3921256800090305 grad: -0.4426608588498838
iteration: 320 loss: 4.261344099206202 grad: -0.43077132176139965
iteration: 330 loss: 4.138044792557056 grad: -0.4194912479354763
iteration: 340 loss: 4.021608223960398 grad: -0.40877580710465455
iteration: 350 loss: 3.91148107495825 grad: -0.39858437977013295
iteration: 360 loss: 3.807167658319527 grad: -0.3888800840721678
iteration: 370 loss: 3.7082226308627715 grad: -0.37962936353208576
iteration: 380 loss: 3.6142447821374515 grad: -0.37080162700322233
iteration: 390 loss: 3.5248717183888156 grad: -0.36236893348704535
iteration: 400 loss: 3.439775295024984 grad: -0.35430571558370394
iteration: 410 loss: 3.3586576776519492 grad: -0.34658853628376884
iteration: 420 loss: 3.281247933187594 grad: -0.33919587459721845
iteration: 430 loss: 3.2072990698028745 grad: -0.3321079361803315
iteration: 440 loss: 3.136585458344598 grad: -0.3253064856812361
iteration: 450 loss: 3.0689005791866126 grad: -0.31877469799745106
iteration: 460 loss: 3.0040550476711276 grad: -0.3124970260380859
iteration: 470 loss: 2.941874878830028 grad: -0.3064590829214153
iteration: 480 loss: 2.8821999582938918 grad: -0.3006475368252251
iteration: 490 loss: 2.8248826914082614 grad: -0.29505001695093885
iteration: 0 loss: 77.38952674363004 grad: 26.342110032547286
iteration: 10 loss: 46.40027254609251 grad: 6.613041279598336
iteration: 20 loss: 34.77858067923103 grad: 0.8395762245853926
iteration: 30 loss: 28.133009513273556 grad: -0.5941443871194487
iteration: 40 loss: 23.69089872986609 grad: -0.9843876754722467
iteration: 50 loss: 20.477239860995724 grad: -1.0793351021007864
iteration: 60 loss: 18.033110435492002 grad: -1.0788266781441427
iteration: 70 loss: 16.107573588947762 grad: -1.045325256851089
iteration: 80 loss: 14.549875010030643 grad: -1.0009048900266737
iteration: 90 loss: 13.263268351104609 grad: -0.9540672037849708
iteration: 100 loss: 12.18252453792595 grad: -0.9082333456457191
iteration: 110 loss: 11.261933986968808 grad: -0.8647572186558165
iteration: 120 loss: 10.468452398571447 grad: -0.8241007509114228
iteration: 130 loss: 9.777568513960825 grad: -0.7863288947615503
iteration: 140 loss: 9.170701924559332 grad: -0.7513308956652283
iteration: 150 loss: 8.633504123348303 grad: -0.7189234351072462
iteration: 160 loss: 8.154714911770187 grad: -0.6888998161992538
iteration: 170 loss: 7.725372221919977 grad: -0.6610533967576548
iteration: 180 loss: 7.338253590831093 grad: -0.6351883466286785
iteration: 190 loss: 6.987473428317447 grad: -0.6111240593266514
iteration: 200 loss: 6.668187454168095 grad: -0.5886963849579537
iteration: 210 loss: 6.376372346993279 grad: -0.5677573055627545
iteration: 220 loss: 6.108659128762735 grad: -0.5481738931282611
iteration: 230 loss: 5.862205563118798 grad: -0.5298269861538867
iteration: 240 loss: 5.634597293281703 grad: -0.5126098076877665
iteration: 250 loss: 5.42377043224885 grad: -0.49642663458577974
iteration: 260 loss: 5.227950359846215 grad: -0.4811915675689019
iteration: 270 loss: 5.045602899764575 grad: -0.46682741996563304
iteration: 280 loss: 4.875395049952616 grad: -0.4532647267481864
iteration: 290 loss: 4.716163154691375 grad: -0.4404408675340379
iteration: 300 loss: 4.566886924119692 grad: -0.42829929377187903
iteration: 310 loss: 4.426668085863391 grad: -0.41678884924953585
iteration: 320 loss: 4.294712733814529 grad: -0.4058631731923384
iteration: 330 loss: 4.170316648745153 grad: -0.3954801759311641
iteration: 340 loss: 4.05285302356705 grad: -0.3856015780684784
iteration: 350 loss: 3.9417621464552792 grad: -0.37619250507783275
iteration: 360 loss: 3.8365426874028428 grad: -0.36722113024615133
iteration: 370 loss: 3.736744305224792 grad: -0.3586583597662156
iteration: 380 loss: 3.6419613476403523 grad: -0.35047755459293384
iteration: 390 loss: 3.5518274607157267 grad: -0.3426542843890036
iteration: 400 loss: 3.4660109583615353 grad: -0.3351661095081363
iteration: 410 loss: 3.3842108299349176 grad: -0.32799238750493537
iteration: 420 loss: 3.306153285828135 grad: -0.32111410112868055
iteration: 430 loss: 3.231588758472588 grad: -0.314513705162512
iteration: 440 loss: 3.1602892903293154 grad: -0.30817498981813013
iteration: 450 loss: 3.0920462519455794 grad: -0.30208295869662555
iteration: 460 loss: 3.0266683424929397 grad: -0.29622371958507776
iteration: 470 loss: 2.963979832902334 grad: -0.2905843865819334
iteration: 480 loss: 2.9038190179823933 grad: -0.28515299223691604
iteration: 490 loss: 2.8460368491487316 grad: -0.27991840855768835
iteration: 0 loss: 75.93316831264382 grad: 33.672056129317305
iteration: 10 loss: 44.14165315626172 grad: 7.439212275802956
iteration: 20 loss: 33.26766389915475 grad: 0.23541254675257783
iteration: 30 loss: 27.04474034344846 grad: -1.2702919199577176
iteration: 40 loss: 22.856411933520555 grad: -1.5739156731034238
iteration: 50 loss: 19.808906439510398 grad: -1.5825920365142094
iteration: 60 loss: 17.48058634753265 grad: -1.5123758228161155
iteration: 70 loss: 15.639598681524625 grad: -1.4235811828137535
iteration: 80 loss: 14.145877695407638 grad: -1.3347997891759438
iteration: 90 loss: 12.909085337598277 grad: -1.251811472880689
iteration: 100 loss: 11.868043906176148 grad: -1.1760788757264766
iteration: 110 loss: 10.97971717512897 grad: -1.1075486226658633
iteration: 120 loss: 10.212888664555232 grad: -1.0456554414386707
iteration: 130 loss: 9.544334884592693 grad: -0.9897004286212796
iteration: 140 loss: 8.956406297647101 grad: -0.9389945116512913
iteration: 150 loss: 8.43544236491485 grad: -0.8929092606966761
iteration: 160 loss: 7.970701242745256 grad: -0.8508902814405568
iteration: 170 loss: 7.553618085047012 grad: -0.8124557166364172
iteration: 180 loss: 7.177279405912776 grad: -0.7771893212120459
iteration: 190 loss: 6.836043165201318 grad: -0.744732112510842
iteration: 200 loss: 6.525259357362881 grad: -0.7147742387956226
iteration: 210 loss: 6.24106130058135 grad: -0.6870476748475318
iteration: 220 loss: 5.9802075468335465 grad: -0.6613199023184619
iteration: 230 loss: 5.7399606158893555 grad: -0.6373885433359372
iteration: 240 loss: 5.5179929038692785 grad: -0.6150768451457751
iteration: 250 loss: 5.312312908843648 grad: -0.5942298956792282
iteration: 260 loss: 5.121206828556421 grad: -0.5747114544856607
iteration: 270 loss: 4.943191916744788 grad: -0.5564012964472064
iteration: 280 loss: 4.776978924992983 grad: -0.5391929807323033
iteration: 290 loss: 4.621441630369521 grad: -0.5229919718239304
iteration: 300 loss: 4.475591937156299 grad: -0.5077140521542005
iteration: 310 loss: 4.338559398848619 grad: -0.493283976649342
iteration: 320 loss: 4.209574271785683 grad: -0.4796343284345862
iteration: 330 loss: 4.0879534102743325 grad: -0.4667045422892707
iteration: 340 loss: 3.973088462990249 grad: -0.4544400684288479
iteration: 350 loss: 3.864435944691236 grad: -0.4427916540585646
iteration: 360 loss: 3.761508845022449 grad: -0.4317147240996787
iteration: 370 loss: 3.663869504130203 grad: -0.42116884570653434
iteration: 380 loss: 3.5711235377434605 grad: -0.41111726381393976
iteration: 390 loss: 3.482914635948708 grad: -0.4015264970945829
iteration: 400 loss: 3.3989200927216685 grad: -0.3923659854585393
iteration: 410 loss: 3.3188469493625234 grad: -0.3836077816659027
iteration: 420 loss: 3.2424286558382183 grad: -0.37522628080882525
iteration: 430 loss: 3.1694221708097015 grad: -0.3671979823986855
iteration: 440 loss: 3.099605434643598 grad: -0.35950128060598213
iteration: 450 loss: 3.0327751607189217 grad: -0.3521162788757416
iteration: 460 loss: 2.968744899296036 grad: -0.3450246257044723
iteration: 470 loss: 2.907343335566979 grad: -0.33820936883604935
iteration: 480 loss: 2.8484127895578486 grad: -0.33165482552954983
iteration: 490 loss: 2.7918078905415977 grad: -0.32534646688513835
iteration: 0 loss: 74.91494762537863 grad: 34.87319633273353
iteration: 10 loss: 43.65824627357922 grad: 5.812331970191332
iteration: 20 loss: 33.29248539809551 grad: -0.22675658121784448
iteration: 30 loss: 27.182637016206957 grad: -1.258867028120878
iteration: 40 loss: 23.01799016028136 grad: -1.4159972277841535
iteration: 50 loss: 19.967430382146073 grad: -1.3868555829263345
iteration: 60 loss: 17.62765587757814 grad: -1.3152933906907363
iteration: 70 loss: 15.77316390955986 grad: -1.2373479314629912
iteration: 80 loss: 14.266207973941267 grad: -1.162934402454141
iteration: 90 loss: 13.017261168983747 grad: -1.0946037800350292
iteration: 100 loss: 11.965362772977084 grad: -1.0326306021778366
iteration: 110 loss: 11.06745759800978 grad: -0.9765792533575257
iteration: 120 loss: 10.292221823973394 grad: -0.9258287865108379
iteration: 130 loss: 9.616298627513082 grad: -0.879754721343097
iteration: 140 loss: 9.021905423402565 grad: -0.8377888449741241
iteration: 150 loss: 8.49525937050151 grad: -0.7994337871218526
iteration: 160 loss: 8.025510475182251 grad: -0.7642609561020359
iteration: 170 loss: 7.604000174690555 grad: -0.7319029570464985
iteration: 180 loss: 7.223734660304465 grad: -0.7020448713902754
iteration: 190 loss: 6.879003446072312 grad: -0.6744160565796373
iteration: 200 loss: 6.565098355741363 grad: -0.6487830028191865
iteration: 210 loss: 6.27810330083974 grad: -0.6249433246716354
iteration: 220 loss: 6.0147348407127454 grad: -0.6027207895693998
iteration: 230 loss: 5.772219746127741 grad: -0.5819612315663897
iteration: 240 loss: 5.548199911148913 grad: -0.562529196142897
iteration: 250 loss: 5.34065773918858 grad: -0.5443051788564868
iteration: 260 loss: 5.147857038149306 grad: -0.527183342966943
iteration: 270 loss: 4.968295790940291 grad: -0.5110696228455487
iteration: 280 loss: 4.80066810969075 grad: -0.49588013879050424
iteration: 290 loss: 4.643833357570491 grad: -0.48153986431720003
iteration: 300 loss: 4.496790912525078 grad: -0.4679814993065972
iteration: 310 loss: 4.358659407355018 grad: -0.45514451205915946
iteration: 320 loss: 4.228659547709371 grad: -0.44297432083144683
iteration: 330 loss: 4.1060998097953165 grad: -0.4314215912892117
iteration: 340 loss: 3.9903644709395185 grad: -0.4204416308756483
iteration: 350 loss: 3.8809035415961675 grad: -0.40999386466732135
iteration: 360 loss: 3.7772242561226945 grad: -0.40004138010537144
iteration: 370 loss: 3.6788838483666124 grad: -0.39055053022236685
iteration: 380 loss: 3.58548339173777 grad: -0.3814905867691153
iteration: 390 loss: 3.496662525507666 grad: -0.3728334360816232
iteration: 400 loss: 3.4120949223745565 grad: -0.36455331169236277
iteration: 410 loss: 3.331484378755952 grad: -0.35662655864006176
iteration: 420 loss: 3.2545614304396455 grad: -0.3490314252125588
iteration: 430 loss: 3.1810804131939054 grad: -0.34174787850204313
iteration: 440 loss: 3.1108169016976137 grad: -0.33475744068759583
iteration: 450 loss: 3.043565471297494 grad: -0.3280430434071334
iteration: 460 loss: 2.9791377361829374 grad: -0.32158889795585577
iteration: 470 loss: 2.917360625046306 grad: -0.3153803793641506
iteration: 480 loss: 2.858074861421177 grad: -0.3094039226748924
iteration: 490 loss: 2.8011336209629834 grad: -0.30364692996660825
iteration: 0 loss: 76.10432118592169 grad: 40.01293764017198
iteration: 10 loss: 44.72053499424371 grad: 6.24985523757001
iteration: 20 loss: 33.75670504939656 grad: -0.5902891539504633
iteration: 30 loss: 27.418308156423418 grad: -1.628469446503345
iteration: 40 loss: 23.153402304904294 grad: -1.7331951229294436
iteration: 50 loss: 20.053051685159787 grad: -1.6555444876077545
iteration: 60 loss: 17.685966330316226 grad: -1.5442466027872368
iteration: 70 loss: 15.815227869833818 grad: -1.4343923535945184
iteration: 80 loss: 14.297940420191026 grad: -1.334342379713978
iteration: 90 loss: 13.042055059956573 grad: -1.2452594141183924
iteration: 100 loss: 11.985285668741529 grad: -1.166315058513494
iteration: 110 loss: 11.08383597853084 grad: -1.0962318756242633
iteration: 120 loss: 10.305944467127864 grad: -1.0337545663098178
iteration: 130 loss: 9.627983085450152 grad: -0.9777805989788217
iteration: 140 loss: 9.031994066140875 grad: -0.9273790711202062
iteration: 150 loss: 8.504077332072855 grad: -0.8817746658608655
iteration: 160 loss: 8.033302016030161 grad: -0.8403233375499536
iteration: 170 loss: 7.610952290040007 grad: -0.8024887287521476
iteration: 180 loss: 7.229992864833052 grad: -0.7678219434505433
iteration: 190 loss: 6.884682600467758 grad: -0.7359450187789467
iteration: 200 loss: 6.5702902615486725 grad: -0.7065376892386176
iteration: 210 loss: 6.282882136733364 grad: -0.6793268625678008
iteration: 220 loss: 6.019161125444001 grad: -0.6540782546472657
iteration: 230 loss: 5.776343275985957 grad: -0.6305897185921869
iteration: 240 loss: 5.552061970688638 grad: -0.6086858969078311
iteration: 250 loss: 5.344292788231872 grad: -0.5882139074658735
iteration: 260 loss: 5.1512940154351865 grad: -0.5690398402018043
iteration: 270 loss: 4.971559133042526 grad: -0.5510458929586877
iteration: 280 loss: 4.803778555600027 grad: -0.5341280143523661
iteration: 290 loss: 4.64680858990691 grad: -0.5181939515205711
iteration: 300 loss: 4.499646072817293 grad: -0.5031616233792315
iteration: 310 loss: 4.361407513221696 grad: -0.48895775731892643
iteration: 320 loss: 4.231311832919221 grad: -0.47551674049160997
iteration: 330 loss: 4.108666003161488 grad: -0.4627796469835346
iteration: 340 loss: 3.992853026343751 grad: -0.4506934100053429
iteration: 350 loss: 3.8833218286743185 grad: -0.4392101143195865
iteration: 360 loss: 3.7795787190899235 grad: -0.42828638888939574
iteration: 370 loss: 3.6811801388908365 grad: -0.4178828834826618
iteration: 380 loss: 3.5877264805445437 grad: -0.40796381593822195
iteration: 390 loss: 3.498856796489649 grad: -0.3984965791703975
iteration: 400 loss: 3.414244252222778 grad: -0.38945139888928426
iteration: 410 loss: 3.333592204573878 grad: -0.3808010345479309
iteration: 420 loss: 3.2566308073304837 grad: -0.372520517271779
iteration: 430 loss: 3.183114063460752 grad: -0.36458691954042916
iteration: 440 loss: 3.1128172569982864 grad: -0.35697915222341664
iteration: 450 loss: 3.0455347088573035 grad: -0.34967778525649884
iteration: 460 loss: 2.9810778099929185 grad: -0.34266488881144835
iteration: 470 loss: 2.91927329279993 grad: -0.3359238922829496
iteration: 480 loss: 2.859961707835861 grad: -0.32943945880873915
iteration: 490 loss: 2.802996078011182 grad: -0.3231973733678458
iteration: 0 loss: 72.25426156727721 grad: 43.91356485537163
iteration: 10 loss: 41.34628849269707 grad: 4.549317242418834
iteration: 20 loss: 31.602863113120012 grad: -1.5599541332003004
iteration: 30 loss: 25.906471208875267 grad: -2.0306538476611014
iteration: 40 loss: 22.025252643307795 grad: -1.901358788495319
iteration: 50 loss: 19.174582371970907 grad: -1.7235893343127844
iteration: 60 loss: 16.97975407755503 grad: -1.5668019221996068
iteration: 70 loss: 15.233101781834916 grad: -1.43552241974262
iteration: 80 loss: 13.808248051931493 grad: -1.3253759307230628
iteration: 90 loss: 12.623098604513808 grad: -1.231800210069034
iteration: 100 loss: 11.621677648310135 grad: -1.1512273536122173
iteration: 110 loss: 10.764355672431458 grad: -1.0810018871556573
iteration: 120 loss: 10.022216695003904 grad: -1.019147417093392
iteration: 130 loss: 9.373631511741594 grad: -0.9641725324197485
iteration: 140 loss: 8.80208042344045 grad: -0.9149322599497647
iteration: 150 loss: 8.294719419008372 grad: -0.8705323749899609
iteration: 160 loss: 7.841407110701187 grad: -0.8302632317652531
iteration: 170 loss: 7.434027173166462 grad: -0.7935534169084449
iteration: 180 loss: 7.06600593814433 grad: -0.7599367502836076
iteration: 190 loss: 6.731962186076874 grad: -0.7290283900300621
iteration: 200 loss: 6.4274485003658235 grad: -0.7005072491993145
iteration: 210 loss: 6.148757299433286 grad: -0.674102860923246
iteration: 220 loss: 5.892773363435977 grad: -0.649585428579718
iteration: 230 loss: 5.656860314056592 grad: -0.6267581897534285
iteration: 240 loss: 5.438772243112205 grad: -0.6054514839548847
iteration: 250 loss: 5.236584210084154 grad: -0.5855180908637003
iteration: 260 loss: 5.04863706398948 grad: -0.5668295274184669
iteration: 270 loss: 4.87349325711805 grad: -0.5492730768120764
iteration: 280 loss: 4.709901177275904 grad: -0.5327493822603249
iteration: 290 loss: 4.556766142283157 grad: -0.5171704811097383
iteration: 300 loss: 4.413126649256108 grad: -0.5024581856605707
iteration: 310 loss: 4.278134801285929 grad: -0.4885427395414097
iteration: 320 loss: 4.151040079479737 grad: -0.4753616950089592
iteration: 330 loss: 4.031175812509983 grad: -0.46285896884142097
iteration: 340 loss: 3.9179478353042794 grad: -0.450984043722165
iteration: 350 loss: 3.8108249351095598 grad: -0.43969128900165294
iteration: 360 loss: 3.709330765213963 grad: -0.42893938007034077
iteration: 370 loss: 3.6130369703113585 grad: -0.4186907996972893
iteration: 380 loss: 3.521557317230955 grad: -0.408911407895116
iteration: 390 loss: 3.4345426639134446 grad: -0.39957006938557726
iteration: 400 loss: 3.3516766304767938 grad: -0.3906383297262185
iteration: 410 loss: 3.272671860898919 grad: -0.3820901327393571
iteration: 420 loss: 3.1972667835877306 grad: -0.37390157315165884
iteration: 430 loss: 3.125222795026151 grad: -0.36605067937466923
iteration: 440 loss: 3.056321803534295 grad: -0.3585172221863373
iteration: 450 loss: 2.990364080674061 grad: -0.35128254575090956
iteration: 460 loss: 2.9271663763546796 grad: -0.3443294179706379
iteration: 470 loss: 2.8665602607294947 grad: -0.33764189762154967
iteration: 480 loss: 2.8083906617379895 grad: -0.3312052161059671
iteration: 490 loss: 2.7525145719511617 grad: -0.32500567197130936
iteration: 0 loss: 73.45963409450569 grad: 37.326805495942864
iteration: 10 loss: 42.6833948469864 grad: 5.915822162314578
iteration: 20 loss: 32.40711386358077 grad: -0.7525968889053682
iteration: 30 loss: 26.423081402155812 grad: -1.7268837339060463
iteration: 40 loss: 22.370964192688067 grad: -1.7903633852962153
iteration: 50 loss: 19.41376696253366 grad: -1.6876206948186065
iteration: 60 loss: 17.149803186182247 grad: -1.5612105137069507
iteration: 70 loss: 15.35665912421078 grad: -1.4421116418462254
iteration: 80 loss: 13.899561727239519 grad: -1.3362966447040092
iteration: 90 loss: 12.691449661609939 grad: -1.243573343385199
iteration: 100 loss: 11.673300561703895 grad: -1.1623314157902453
iteration: 110 loss: 10.803553573332051 grad: -1.090826802097875
iteration: 120 loss: 10.052031339743177 grad: -1.0275144111783854
iteration: 130 loss: 9.396258557023037 grad: -0.9711043910143502
iteration: 140 loss: 8.819134288253508 grad: -0.9205403701104085
iteration: 150 loss: 8.30740664236867 grad: -0.8749605785688448
iteration: 160 loss: 7.850642501420804 grad: -0.8336601970283347
iteration: 170 loss: 7.440513289327555 grad: -0.7960596265141877
iteration: 180 loss: 7.070288520738276 grad: -0.7616790441552009
iteration: 190 loss: 6.734469496011446 grad: -0.7301183833921131
iteration: 200 loss: 6.428519675607979 grad: -0.7010416749312853
iteration: 210 loss: 6.148663092761705 grad: -0.6741647945410603
iteration: 220 loss: 5.8917315087957665 grad: -0.6492458429657141
iteration: 230 loss: 5.655047050927309 grad: -0.6260775534940457
iteration: 240 loss: 5.4363310557673685 grad: -0.6044812635573171
iteration: 250 loss: 5.233632522707674 grad: -0.5843020970478217
iteration: 260 loss: 5.0452714181439084 grad: -0.5654050884076925
iteration: 270 loss: 4.869793350392022 grad: -0.5476720433780731
iteration: 280 loss: 4.705933038873354 grad: -0.530998979437479
iteration: 290 loss: 4.552584648507698 grad: -0.5152940252580407
iteration: 300 loss: 4.408777529777498 grad: -0.5004756859382762
iteration: 310 loss: 4.2736562494547305 grad: -0.4864714015678825
iteration: 320 loss: 4.146464052464911 grad: -0.4732163425130975
iteration: 330 loss: 4.0265290867685675 grad: -0.4606523969216177
iteration: 340 loss: 3.9132528678084615 grad: -0.4487273152559027
iteration: 350 loss: 3.8061005694228864 grad: -0.43739398385862027
iteration: 360 loss: 3.704592812947877 grad: -0.4266098051450241
iteration: 370 loss: 3.6082986919385887 grad: -0.41633616638606186
iteration: 380 loss: 3.5168298212290874 grad: -0.40653798247949857
iteration: 390 loss: 3.4298352393099196 grad: -0.39718330081990166
iteration: 400 loss: 3.3469970248536978 grad: -0.3882429585350635
iteration: 410 loss: 3.2680265135432323 grad: -0.3796902840801425
iteration: 420 loss: 3.19266102160848 grad: -0.3715008365659763
iteration: 430 loss: 3.1206609987802536 grad: -0.3636521773171143
iteration: 440 loss: 3.0518075465322245 grad: -0.35612366906391735
iteration: 450 loss: 2.9859002481882517 grad: -0.3488962989148835
iteration: 460 loss: 2.922755266205637 grad: -0.3419525218637228
iteration: 470 loss: 2.862203669105466 grad: -0.3352761220870766
iteration: 480 loss: 2.8040899564213593 grad: -0.3288520897037309
iteration: 490 loss: 2.748270754906488 grad: -0.32266651101119126
iteration: 0 loss: 75.79951368834669 grad: 33.015271920733056
iteration: 10 loss: 43.798329946958184 grad: 6.708243746063974
iteration: 20 loss: 33.10396943045316 grad: 0.04684659798954571
iteration: 30 loss: 26.94970316545416 grad: -1.270834643966632
iteration: 40 loss: 22.791068493938052 grad: -1.510126342226981
iteration: 50 loss: 19.757946205166967 grad: -1.496321503336219
iteration: 60 loss: 17.43726111709128 grad: -1.4201150171829644
iteration: 70 loss: 15.600644317246013 grad: -1.332426254105865
iteration: 80 loss: 14.109602964700365 grad: -1.2478080350711942
iteration: 90 loss: 12.874571861854148 grad: -1.1701549106572666
iteration: 100 loss: 11.834777988847815 grad: -1.1000227169422354
iteration: 110 loss: 10.947406587818291 grad: -1.036927502871648
iteration: 120 loss: 10.181365772610723 grad: -0.9801131186520197
iteration: 130 loss: 9.513503110096405 grad: -0.9288115608802381
iteration: 140 loss: 8.926209633897212 grad: -0.8823257084638072
iteration: 150 loss: 8.40584770382698 grad: -0.840048343553426
iteration: 160 loss: 7.941688053064354 grad: -0.8014588455524907
iteration: 170 loss: 7.525172397847365 grad: -0.7661130174063533
iteration: 180 loss: 7.149390355236061 grad: -0.7336317940505102
iteration: 190 loss: 6.808701051391918 grad: -0.7036908426964985
iteration: 200 loss: 6.498454615345633 grad: -0.676011615935528
iteration: 210 loss: 6.214784000237862 grad: -0.6503538745851011
iteration: 220 loss: 5.954447199789436 grad: -0.6265095140802823
iteration: 230 loss: 5.714706152438835 grad: -0.6042974870188829
iteration: 240 loss: 5.493232738431785 grad: -0.583559625825076
iteration: 250 loss: 5.288035045820792 grad: -0.5641571971059709
iteration: 260 loss: 5.097398980924853 grad: -0.545968048910946
iteration: 270 loss: 4.919841622142692 grad: -0.5288842387069697
iteration: 280 loss: 4.754073651526968 grad: -0.512810052133486
iteration: 290 loss: 4.598968868771036 grad: -0.49766034061053366
iteration: 300 loss: 4.453539278444003 grad: -0.48335912022778266
iteration: 310 loss: 4.316914598023976 grad: -0.4698383857072665
iteration: 320 loss: 4.18832529879356 grad: -0.45703710221616667
iteration: 330 loss: 4.067088489735645 grad: -0.444900344915371
iteration: 340 loss: 3.9525961042711306 grad: -0.43337856177129014
iteration: 350 loss: 3.844304963800162 grad: -0.4224269396522395
iteration: 360 loss: 3.7417283796966827 grad: -0.41200485732519665
iteration: 370 loss: 3.6444290233066154 grad: -0.40207541185678125
iteration: 380 loss: 3.5520128464574388 grad: -0.3926050072532288
iteration: 390 loss: 3.4641238765437845 grad: -0.38356299606395744
iteration: 400 loss: 3.3804397431181727 grad: -0.374921366212294
iteration: 410 loss: 3.300667819008719 grad: -0.3666544665756626
iteration: 420 loss: 3.224541879866923 grad: -0.3587387658713194
iteration: 430 loss: 3.1518192028243406 grad: -0.35115264025625476
iteration: 440 loss: 3.082278038486865 grad: -0.34387618575563283
iteration: 450 loss: 3.015715401509872 grad: -0.3368910522206395
iteration: 460 loss: 2.951945133969771 grad: -0.3301802960057057
iteration: 470 loss: 2.8907962031044008 grad: -0.3237282489644168
iteration: 480 loss: 2.8321112010552167 grad: -0.31752040170712703
iteration: 490 loss: 2.7757450192413917 grad: -0.31154329935284264
iteration: 0 loss: 73.71113190927514 grad: 43.95152633647583
iteration: 10 loss: 43.08677334403083 grad: 5.315772505484996
iteration: 20 loss: 32.81768407241727 grad: -1.0692744944951693
iteration: 30 loss: 26.78650014036072 grad: -1.8940492846161057
iteration: 40 loss: 22.691558563760136 grad: -1.9121434811479807
iteration: 50 loss: 19.69736155732992 grad: -1.7869815482191689
iteration: 60 loss: 17.401993600543992 grad: -1.6451900631550036
iteration: 70 loss: 15.582420717886594 grad: -1.5143712666212457
iteration: 80 loss: 14.103114267697741 grad: -1.3993415972842715
iteration: 90 loss: 12.876281532384485 grad: -1.2992508833464547
iteration: 100 loss: 11.842261592027254 grad: -1.2120302393243287
iteration: 110 loss: 10.958963767508646 grad: -1.1356033446896088
iteration: 120 loss: 10.195782179559377 grad: -1.0681834798074497
iteration: 130 loss: 9.529899058339652 grad: -1.008304411173616
iteration: 140 loss: 8.943940921491917 grad: -0.9547797365153774
iteration: 150 loss: 8.424438872266467 grad: -0.9066503469019023
iteration: 160 loss: 7.960786419966305 grad: -0.8631371854804413
iteration: 170 loss: 7.544515746825007 grad: -0.8236029082706925
iteration: 180 loss: 7.168783819749106 grad: -0.7875219248850149
iteration: 190 loss: 6.828000321523922 grad: -0.7544572939378844
iteration: 200 loss: 6.517553577182711 grad: -0.7240429380648387
iteration: 210 loss: 6.233605536231111 grad: -0.6959699013135974
iteration: 220 loss: 5.972936277577859 grad: -0.6699756578380152
iteration: 230 loss: 5.732824592482178 grad: -0.6458357255714666
iteration: 240 loss: 5.51095522736399 grad: -0.6233570298398097
iteration: 250 loss: 5.305346083270954 grad: -0.602372605531627
iteration: 260 loss: 5.114290531648459 grad: -0.5827373322931897
iteration: 270 loss: 4.936311304834982 grad: -0.5643244745393587
iteration: 280 loss: 4.77012333840602 grad: -0.5470228544308067
iteration: 290 loss: 4.614603601111825 grad: -0.5307345271433779
iteration: 300 loss: 4.468766426125802 grad: -0.5153728580192779
iteration: 310 loss: 4.331743208182243 grad: -0.5008609236323205
iteration: 320 loss: 4.202765591430844 grad: -0.4871301756243921
iteration: 330 loss: 4.0811514678075165 grad: -0.47411931893500425
iteration: 340 loss: 3.96629325311183 grad: -0.461773365846624
iteration: 350 loss: 3.857648020366262 grad: -0.4500428348784098
iteration: 360 loss: 3.754729156431164 grad: -0.4388830695287259
iteration: 370 loss: 3.6570992747630373 grad: -0.428253656586292
iteration: 380 loss: 3.5643641693930643 grad: -0.41811792748813814
iteration: 390 loss: 3.4761676362104894 grad: -0.4084425292123622
iteration: 400 loss: 3.3921870200309225 grad: -0.3991970536152269
iteration: 410 loss: 3.312129371697169 grad: -0.39035371607795044
iteration: 420 loss: 3.23572812006916 grad: -0.38188707591350546
iteration: 430 loss: 3.162740180333623 grad: -0.3737737922721446
iteration: 440 loss: 3.092943433446387 grad: -0.36599241033481145
iteration: 450 loss: 3.0261345224210925 grad: -0.3585231734425043
iteration: 460 loss: 2.9621269200434983 grad: -0.35134785751407127
iteration: 470 loss: 2.9007492298731616 grad: -0.34444962468437673
iteration: 480 loss: 2.8418436883935514 grad: -0.33781289357295674
iteration: 490 loss: 2.7852648411254366 grad: -0.33142322398915797
iteration: 0 loss: 76.07207367442975 grad: 35.27312125100869
iteration: 10 loss: 44.29609976485506 grad: 5.1320684787984305
iteration: 20 loss: 33.517980943063414 grad: -0.32649156666155416
iteration: 30 loss: 27.28122229230621 grad: -1.2066637218987013
iteration: 40 loss: 23.076831955492604 grad: -1.3365079244857716
iteration: 50 loss: 20.014237530523566 grad: -1.3081041364929014
iteration: 60 loss: 17.671462866761967 grad: -1.2430040674313463
iteration: 70 loss: 15.81672446672424 grad: -1.1720368424450802
iteration: 80 loss: 14.310122755861977 grad: -1.103940544994964
iteration: 90 loss: 13.061430462924053 grad: -1.041086707378595
iteration: 100 loss: 12.009505573363475 grad: -0.9838150565512851
iteration: 110 loss: 11.111290323330401 grad: -0.9318034249106149
iteration: 120 loss: 10.335511405020126 grad: -0.8845413362510093
iteration: 130 loss: 9.658870465545823 grad: -0.8414998758816227
iteration: 140 loss: 9.06363668534455 grad: -0.8021902198122073
iteration: 150 loss: 8.536068905843505 grad: -0.7661797166926418
iteration: 160 loss: 8.065349239257124 grad: -0.7330919595992786
iteration: 170 loss: 7.6428432428289685 grad: -0.7026013649096019
iteration: 180 loss: 7.261574916763121 grad: -0.6744264368410603
iteration: 190 loss: 6.915846721682101 grad: -0.6483232946938124
iteration: 200 loss: 6.600959730975981 grad: -0.6240799717048966
iteration: 210 loss: 6.313004316954903 grad: -0.6015115703315941
iteration: 220 loss: 6.048701407844998 grad: -0.580456204913698
iteration: 230 loss: 5.805280582328578 grad: -0.5607716193468292
iteration: 240 loss: 5.580385384257666 grad: -0.5423323666626843
iteration: 250 loss: 5.3719990135926885 grad: -0.5250274509229989
iteration: 260 loss: 5.178385451864208 grad: -0.5087583482924931
iteration: 270 loss: 4.998042406373274 grad: -0.4934373394510778
iteration: 280 loss: 4.829663395119827 grad: -0.47898609845638007
iteration: 290 loss: 4.672106966765858 grad: -0.46533449371214114
iteration: 300 loss: 4.52437153783136 grad: -0.4524195651432248
iteration: 310 loss: 4.385574687528168 grad: -0.4401846484051088
iteration: 320 loss: 4.254936016356334 grad: -0.42857862231513516
iteration: 330 loss: 4.131762873685602 grad: -0.41755525997544674
iteration: 340 loss: 4.01543841008631 grad: -0.40707266749473603
iteration: 350 loss: 3.9054115249739634 grad: -0.39709279698820044
iteration: 360 loss: 3.8011883683965277 grad: -0.38758102278236983
iteration: 370 loss: 3.702325124157555 grad: -0.3785057715821656
iteration: 380 loss: 3.608421854804171 grad: -0.36983819885593006
iteration: 390 loss: 3.519117230898122 grad: -0.36155190492605593
iteration: 400 loss: 3.434084000104907 grad: -0.3536226852698956
iteration: 410 loss: 3.353025077943833 grad: -0.34602831037863147
iteration: 420 loss: 3.275670163118899 grad: -0.3387483312231736
iteration: 430 loss: 3.2017727972631125 grad: -0.33176390696172836
iteration: 440 loss: 3.131107802612736 grad: -0.32505765201427694
iteration: 450 loss: 3.063469042245494 grad: -0.31861350004146327
iteration: 460 loss: 2.998667456578005 grad: -0.31241658271296924
iteration: 470 loss: 2.9365293372527295 grad: -0.3064531214442674
iteration: 480 loss: 2.8768948056614465 grad: -0.30071033052979856
iteration: 490 loss: 2.8196164684070273 grad: -0.2951763303123896
iteration: 0 loss: 76.15446642712419 grad: 32.8723243426347
iteration: 10 loss: 43.686926461589195 grad: 7.560447764319687
iteration: 20 loss: 32.786219418979066 grad: 0.010777033655275645
iteration: 30 loss: 26.63449525935756 grad: -1.5169987026510432
iteration: 40 loss: 22.50795556743827 grad: -1.7617517522921669
iteration: 50 loss: 19.508791010321396 grad: -1.712350480730751
iteration: 60 loss: 17.218456914043063 grad: -1.5988427630752362
iteration: 70 loss: 15.407785249281027 grad: -1.4800457382390642
iteration: 80 loss: 13.938642640240397 grad: -1.3708861547965097
iteration: 90 loss: 12.72207016830095 grad: -1.274093940978611
iteration: 100 loss: 11.697887653039805 grad: -1.188984918427622
iteration: 110 loss: 10.823791189864158 grad: -1.11406436678074
iteration: 120 loss: 10.069109392885304 grad: -1.0478091926420414
iteration: 130 loss: 9.41102975468117 grad: -0.9888816554458231
iteration: 140 loss: 8.83221643446977 grad: -0.9361627933675452
iteration: 150 loss: 8.319251204875256 grad: -0.888731874740087
iteration: 160 loss: 7.861581578771517 grad: -0.8458337178246644
iteration: 170 loss: 7.4507923614024465 grad: -0.8068472509681066
iteration: 180 loss: 7.080089629613171 grad: -0.7712589141203781
iteration: 190 loss: 6.7439278627114785 grad: -0.738641221206638
iteration: 200 loss: 6.437735741514478 grad: -0.7086358153381478
iteration: 210 loss: 6.157711327524381 grad: -0.680940162475368
iteration: 220 loss: 5.9006669055001195 grad: -0.6552971029536618
iteration: 230 loss: 5.6639099496524485 grad: -0.6314866192457351
iteration: 240 loss: 5.445150747759933 grad: -0.6093193155010285
iteration: 250 loss: 5.242429957790798 grad: -0.5886312201785386
iteration: 260 loss: 5.054061247846221 grad: -0.5692796148656601
iteration: 270 loss: 4.878585475827549 grad: -0.5511396630304901
iteration: 280 loss: 4.714733787259692 grad: -0.534101666094186
iteration: 290 loss: 4.561397669773297 grad: -0.5180688146871295
iteration: 300 loss: 4.417604481214849 grad: -0.502955333454016
iteration: 310 loss: 4.282497319198725 grad: -0.4886849407896643
iteration: 320 loss: 4.155318359974237 grad: -0.4751895623126953
iteration: 330 loss: 4.035394989121207 grad: -0.46240825013604747
iteration: 340 loss: 3.922128193687182 grad: -0.45028627012498573
iteration: 350 loss: 3.8149827974457367 grad: -0.43877432712514175
iteration: 360 loss: 3.713479207080044 grad: -0.4278279041728037
iteration: 370 loss: 3.617186403767143 grad: -0.4174066963955836
iteration: 380 loss: 3.5257159666227627 grad: -0.4074741239923151
iteration: 390 loss: 3.4387169552746855 grad: -0.3979969115845204
iteration: 400 loss: 3.3558715110827753 grad: -0.3889447235365565
iteration: 410 loss: 3.27689106215158 grad: -0.3802898466822531
iteration: 420 loss: 3.2015130377577736 grad: -0.3720069133746497
iteration: 430 loss: 3.1294980143096094 grad: -0.3640726589702342
iteration: 440 loss: 3.0606272282374944 grad: -0.35646570882969475
iteration: 450 loss: 2.9947004020304204 grad: -0.3491663907098028
iteration: 460 loss: 2.9315338384508323 grad: -0.34215656907152336
iteration: 470 loss: 2.870958745173952 grad: -0.3354194983657796
iteration: 480 loss: 2.812819758053386 grad: -0.3289396928025694
iteration: 490 loss: 2.75697363611971 grad: -0.32270281047865645
iteration: 0 loss: 76.09280607413044 grad: 31.789185588849055
iteration: 10 loss: 44.360678290902634 grad: 6.386336719667561
iteration: 20 loss: 33.54792445167246 grad: 0.09105212181120675
iteration: 30 loss: 27.28953035089938 grad: -1.1621586398993602
iteration: 40 loss: 23.064290160084344 grad: -1.395799263723812
iteration: 50 loss: 19.987112197528816 grad: -1.3898523538840168
iteration: 60 loss: 17.635387215534447 grad: -1.3238756269529974
iteration: 70 loss: 15.775675261185928 grad: -1.2457952308802036
iteration: 80 loss: 14.266702640717313 grad: -1.169519698046698
iteration: 90 loss: 13.01728557706352 grad: -1.0989788629106374
iteration: 100 loss: 11.965652676466975 grad: -1.034922408840547
iteration: 110 loss: 11.068341094911023 grad: -0.9770626944279477
iteration: 120 loss: 10.293819351740778 grad: -0.9248046035207211
iteration: 130 loss: 9.618625428991477 grad: -0.8775042839267628
iteration: 140 loss: 9.024925243503752 grad: -0.8345591722990355
iteration: 150 loss: 8.498912935946981 grad: -0.7954347774765553
iteration: 160 loss: 8.029730581402667 grad: -0.7596675094535382
iteration: 170 loss: 7.608719520694484 grad: -0.7268588901384555
iteration: 180 loss: 7.228889714548628 grad: -0.6966672730537531
iteration: 190 loss: 6.884536139720157 grad: -0.6687994909812381
iteration: 200 loss: 6.570956602442369 grad: -0.6430033134043572
iteration: 210 loss: 6.284240903460531 grad: -0.6190609543496519
iteration: 220 loss: 6.021111099285173 grad: -0.5967836115560063
iteration: 230 loss: 5.778798941369256 grad: -0.5760069233030751
iteration: 240 loss: 5.5549507579506265 grad: -0.5565872055796899
iteration: 250 loss: 5.34755285883987 grad: -0.5383983377897343
iteration: 260 loss: 5.154872472302776 grad: -0.5213291812266013
iteration: 270 loss: 4.9754105660232115 grad: -0.5052814329081092
iteration: 280 loss: 4.807863852826689 grad: -0.4901678346546519
iteration: 290 loss: 4.651093961218893 grad: -0.475910672310322
iteration: 300 loss: 4.5041022433374565 grad: -0.4624405125248011
iteration: 310 loss: 4.366009054189859 grad: -0.449695134716077
iteration: 320 loss: 4.236036603835642 grad: -0.4376186240450419
iteration: 330 loss: 4.113494684666875 grad: -0.42616059779786336
iteration: 340 loss: 3.9977687274220197 grad: -0.41527554280617374
iteration: 350 loss: 3.8883097550360617 grad: -0.4049222457073952
iteration: 360 loss: 3.784625892136029 grad: -0.3950633011784377
iteration: 370 loss: 3.686275156680577 grad: -0.3856646859425353
iteration: 380 loss: 3.5928593137932694 grad: -0.3766953884912086
iteration: 390 loss: 3.504018613888382 grad: -0.3681270861914221
iteration: 400 loss: 3.4194272704067687 grad: -0.3599338628481062
iteration: 410 loss: 3.338789558876215 grad: -0.3520919609318045
iteration: 420 loss: 3.261836440129049 grad: -0.34457956361301545
iteration: 430 loss: 3.1883226274664014 grad: -0.3373766025102033
iteration: 440 loss: 3.118024031269959 grad: -0.33046458768999865
iteration: 450 loss: 3.050735525692248 grad: -0.3238264569814523
iteration: 460 loss: 2.9862689911339904 grad: -0.31744644210173034
iteration: 470 loss: 2.9244515936539672 grad: -0.31130994945450063
iteration: 480 loss: 2.8651242685872274 grad: -0.3054034537674547
iteration: 490 loss: 2.8081403806979157 grad: -0.2997144029923088
iteration: 0 loss: 75.8585212996897 grad: 31.30919008577399
iteration: 10 loss: 43.408686291827856 grad: 6.219596242179781
iteration: 20 loss: 32.88129519521615 grad: 0.15670607132159245
iteration: 30 loss: 26.8265068762356 grad: -1.0762802986037927
iteration: 40 loss: 22.728654634635713 grad: -1.3277854077535123
iteration: 50 loss: 19.733708980765808 grad: -1.3410897683359777
iteration: 60 loss: 17.437359675861135 grad: -1.2905104108379575
iteration: 70 loss: 15.616349090220432 grad: -1.2240498842829541
iteration: 80 loss: 14.13529166981793 grad: -1.1564172824975572
iteration: 90 loss: 12.90656499762254 grad: -1.0922784770018215
iteration: 100 loss: 11.87063708693009 grad: -1.0329543383666673
iteration: 110 loss: 10.985500109000936 grad: -0.9785843346429635
iteration: 120 loss: 10.220594925518299 grad: -0.9288903359068654
iteration: 130 loss: 9.553125585597234 grad: -0.8834636858774307
iteration: 140 loss: 8.965724488098328 grad: -0.8418765429507583
iteration: 150 loss: 8.444919088628096 grad: -0.803724393902966
iteration: 160 loss: 7.980094012654697 grad: -0.7686404820719032
iteration: 170 loss: 7.562770073740467 grad: -0.7362985871992782
iteration: 180 loss: 7.186092079126907 grad: -0.7064110833348609
iteration: 190 loss: 6.8444577552351635 grad: -0.6787252767796625
iteration: 200 loss: 6.533244218017568 grad: -0.6530193349688167
iteration: 210 loss: 6.2486032165678065 grad: -0.6290983641981607
iteration: 220 loss: 5.9873057272449675 grad: -0.6067908521064557
iteration: 230 loss: 5.746622525078734 grad: -0.5859455352130529
iteration: 240 loss: 5.52423135982169 grad: -0.5664286828739369
iteration: 250 loss: 5.31814406186285 grad: -0.5481217609418627
iteration: 260 loss: 5.126648754993229 grad: -0.5309194295577271
iteration: 270 loss: 4.948263644572845 grad: -0.5147278294411108
iteration: 280 loss: 4.7816997638105105 grad: -0.49946311472809324
iteration: 290 loss: 4.625830716609678 grad: -0.485050195379453
iteration: 300 loss: 4.479667931674654 grad: -0.4714216573024088
iteration: 310 loss: 4.342340292394717 grad: -0.45851683308797014
iteration: 320 loss: 4.213077266723551 grad: -0.4462810004754847
iteration: 330 loss: 4.09119485596408 grad: -0.43466468928060326
iteration: 340 loss: 3.9760838286636253 grad: -0.423623080596413
iteration: 350 loss: 3.867199818222368 grad: -0.4131154846609776
iteration: 360 loss: 3.7640549492751814 grad: -0.4031048859456602
iteration: 370 loss: 3.6662107249031495 grad: -0.3935575458223374
iteration: 380 loss: 3.573271959049056 grad: -0.3844426546719539
iteration: 390 loss: 3.4848815795704797 grad: -0.37573202655205085
iteration: 400 loss: 3.4007161598971667 grad: -0.3673998305892908
iteration: 410 loss: 3.320482063078833 grad: -0.35942235414009377
iteration: 420 loss: 3.2439121026988107 grad: -0.35177779349760524
iteration: 430 loss: 3.1707626417647092 grad: -0.3444460685406688
iteration: 440 loss: 3.100811064130167 grad: -0.33740865824020677
iteration: 450 loss: 3.0338535639298656 grad: -0.33064845437692336
iteration: 460 loss: 2.969703207419488 grad: -0.3241496311950658
iteration: 470 loss: 2.9081882289386325 grad: -0.31789752903132223
iteration: 480 loss: 2.8491505287210286 grad: -0.31187855022499744
iteration: 490 loss: 2.792444345258239 grad: -0.30608006584304814
iteration: 0 loss: 74.01747900324918 grad: 44.6570784587276
iteration: 10 loss: 42.80792036805787 grad: 5.183662779345731
iteration: 20 loss: 32.819707421768 grad: -1.313995385044438
iteration: 30 loss: 26.92118479498041 grad: -1.9341559276134932
iteration: 40 loss: 22.879409766165157 grad: -1.850184859969382
iteration: 50 loss: 19.902886239414183 grad: -1.6906129705659394
iteration: 60 loss: 17.608756924264775 grad: -1.5432567748602826
iteration: 70 loss: 15.782760884480446 grad: -1.4176859441937653
iteration: 80 loss: 14.293629879963254 grad: -1.3113049864519077
iteration: 90 loss: 13.055711069967153 grad: -1.2203425773001169
iteration: 100 loss: 12.010432441432242 grad: -1.1416458433544743
iteration: 110 loss: 11.116250152898353 grad: -1.0728013811546346
iteration: 120 loss: 10.342817741696535 grad: -1.0119838328356545
iteration: 130 loss: 9.667422151781011 grad: -0.9578004260540023
iteration: 140 loss: 9.072711335063662 grad: -0.9091727525021207
iteration: 150 loss: 8.545193910222865 grad: -0.8652526344408438
iteration: 160 loss: 8.074219009302107 grad: -0.8253629295121518
iteration: 170 loss: 7.651264944432899 grad: -0.7889555903435859
iteration: 180 loss: 7.269432260557468 grad: -0.7555815396884487
iteration: 190 loss: 6.923075464669554 grad: -0.7248686877299242
iteration: 200 loss: 6.607530924323535 grad: -0.6965056344953682
iteration: 210 loss: 6.3189127595513295 grad: -0.6702294048195019
iteration: 220 loss: 6.053957644330612 grad: -0.645816090348575
iteration: 230 loss: 5.809905340207851 grad: -0.623073619854134
iteration: 240 loss: 5.584405703833371 grad: -0.6018361099670256
iteration: 250 loss: 5.375445560775089 grad: -0.5819594045453285
iteration: 260 loss: 5.181290661828246 grad: -0.5633175183098529
iteration: 270 loss: 5.000439212954337 grad: -0.5457997755950541
iteration: 280 loss: 4.83158437407792 grad: -0.5293084885803173
iteration: 290 loss: 4.673583771659526 grad: -0.5137570579890369
iteration: 300 loss: 4.525434542541668 grad: -0.4990684074580646
iteration: 310 loss: 4.386252774249897 grad: -0.48517368360931235
iteration: 320 loss: 4.255256465380213 grad: -0.4720111693731042
iteration: 330 loss: 4.131751323736022 grad: -0.45952536976913105
iteration: 340 loss: 4.015118866857847 grad: -0.44766623817341855
iteration: 350 loss: 3.904806401866071 grad: -0.43638851782640314
iteration: 360 loss: 3.8003185480298187 grad: -0.42565117850245276
iteration: 370 loss: 3.7012100325396617 grad: -0.4154169322553495
iteration: 380 loss: 3.6070795424106605 grad: -0.40565181526450955
iteration: 390 loss: 3.5175644566408497 grad: -0.39632482524538626
iteration: 400 loss: 3.4323363154120754 grad: -0.38740760581277384
iteration: 410 loss: 3.35109690907513 grad: -0.37887417071587803
iteration: 420 loss: 3.2735748904833186 grad: -0.3707006620882004
iteration: 430 loss: 3.1995228309760106 grad: -0.3628651378409331
iteration: 440 loss: 3.128714653849453 grad: -0.3553473841270881
iteration: 450 loss: 3.0609433902010013 grad: -0.3481287494543695
iteration: 460 loss: 2.9960192109838317 grad: -0.34119199755820806
iteration: 470 loss: 2.9337676965319175 grad: -0.33452117658596037
iteration: 480 loss: 2.8740283108607567 grad: -0.32810150250740655
iteration: 490 loss: 2.8166530531099685 grad: -0.3219192549698034
iteration: 0 loss: 76.53747877086121 grad: 30.513493013982767
iteration: 10 loss: 44.29929315721501 grad: 7.392623223544732
iteration: 20 loss: 33.34576293650029 grad: 0.6357867430670996
iteration: 30 loss: 27.11873643563176 grad: -1.011030599044585
iteration: 40 loss: 22.930515263869687 grad: -1.4152574332750778
iteration: 50 loss: 19.88059613979565 grad: -1.4726624845838043
iteration: 60 loss: 17.548277901982065 grad: -1.425556571445788
iteration: 70 loss: 15.702710994968955 grad: -1.3486139196657456
iteration: 80 loss: 14.20438476418882 grad: -1.2667748856544825
iteration: 90 loss: 12.963225197670083 grad: -1.1885222471468222
iteration: 100 loss: 11.918162392682627 grad: -1.1164737435482701
iteration: 110 loss: 11.026188532691405 grad: -1.051078026548294
iteration: 120 loss: 10.256077075639702 grad: -0.9919929472110994
iteration: 130 loss: 9.584577397856286 grad: -0.9386215023070176
iteration: 140 loss: 8.994006768019103 grad: -0.8903228928032095
iteration: 150 loss: 8.470670883338823 grad: -0.846492931244178
iteration: 160 loss: 8.003796131851603 grad: -0.8065906893114492
iteration: 170 loss: 7.584788933466123 grad: -0.7701430593634944
iteration: 180 loss: 7.206710365231464 grad: -0.7367406294860007
iteration: 190 loss: 6.863896135087213 grad: -0.7060306078853905
iteration: 200 loss: 6.551676895835499 grad: -0.6777092299023384
iteration: 210 loss: 6.266169204995549 grad: -0.6515146324576364
iteration: 220 loss: 6.0041171046443855 grad: -0.6272205413216836
iteration: 230 loss: 5.762770548666305 grad: -0.604630837216612
iteration: 240 loss: 5.53979103742966 grad: -0.5835749498681964
iteration: 250 loss: 5.333177604239484 grad: -0.563903986094778
iteration: 260 loss: 5.141208206960403 grad: -0.545487488382106
iteration: 270 loss: 4.962392908105192 grad: -0.5282107253538513
iteration: 280 loss: 4.7954361666678444 grad: -0.5119724262609362
iteration: 290 loss: 4.63920623837368 grad: -0.4966828838994167
iteration: 300 loss: 4.492710169339923 grad: -0.4822623622943793
iteration: 310 loss: 4.355073226401725 grad: -0.46863975623129706
iteration: 320 loss: 4.225521872917522 grad: -0.45575145900368774
iteration: 330 loss: 4.103369597745355 grad: -0.4435404025751539
iteration: 340 loss: 3.988005055324555 grad: -0.43195524084838854
iteration: 350 loss: 3.878882089348653 grad: -0.42094965206509016
iteration: 360 loss: 3.7755113005008423 grad: -0.41048174070849136
iteration: 370 loss: 3.6774528868786134 grad: -0.40051352281229674
iteration: 380 loss: 3.584310538847686 grad: -0.39101048144425954
iteration: 390 loss: 3.495726211791035 grad: -0.38194118145511585
iteration: 400 loss: 3.411375633171329 grad: -0.37327693446850974
iteration: 410 loss: 3.330964426518015 grad: -0.36499150662019
iteration: 420 loss: 3.254224755901553 grad: -0.3570608628041055
iteration: 430 loss: 3.1809124112755853 grad: -0.34946294220450114
iteration: 440 loss: 3.1108042686939616 grad: -0.34217746073105204
iteration: 450 loss: 3.0436960704214884 grad: -0.33518573666389984
iteration: 460 loss: 2.9794004789975834 grad: -0.32847053638540713
iteration: 470 loss: 2.9177453666710047 grad: -0.3220159375481311
iteration: 480 loss: 2.858572307714191 grad: -0.31580720742197943
iteration: 490 loss: 2.801735246141685 grad: -0.309830694492218
iteration: 0 loss: 76.5675996306708 grad: 30.628169074783976
iteration: 10 loss: 45.30405926296774 grad: 6.402237460512412
iteration: 20 loss: 34.16991697725109 grad: 0.5112583373621753
iteration: 30 loss: 27.753196658986077 grad: -0.8350256409392267
iteration: 40 loss: 23.43956020381769 grad: -1.188627774867781
iteration: 50 loss: 20.304089497674568 grad: -1.264762820751562
iteration: 60 loss: 17.910183071831998 grad: -1.249770782673545
iteration: 70 loss: 16.018278793051795 grad: -1.2030594845893936
iteration: 80 loss: 14.483866711931444 grad: -1.1463270913504522
iteration: 90 loss: 13.213828909220348 grad: -1.0881602727859332
iteration: 100 loss: 12.14514460039746 grad: -1.032044531588427
iteration: 110 loss: 11.233500695469337 grad: -0.9793218268627182
iteration: 120 loss: 10.446764939240149 grad: -0.9303906448592563
iteration: 130 loss: 9.761038372510484 grad: -0.8852281957059129
iteration: 140 loss: 9.158161622298415 grad: -0.8436291912101531
iteration: 150 loss: 8.624081872895927 grad: -0.8053184365427075
iteration: 160 loss: 8.147750519870442 grad: -0.7700044673398969
iteration: 170 loss: 7.7203594706630945 grad: -0.7374046560533966
iteration: 180 loss: 7.334800008026377 grad: -0.7072561958048333
iteration: 190 loss: 6.985271714685413 grad: -0.6793200339668632
iteration: 200 loss: 6.666994872055883 grad: -0.6533813212432994
iteration: 210 loss: 6.375995640568404 grad: -0.6292482063977478
iteration: 220 loss: 6.108943348139292 grad: -0.6067499232110128
iteration: 230 loss: 5.863025683795513 grad: -0.5857346566554735
iteration: 240 loss: 5.635851863795034 grad: -0.5660674326542976
iteration: 250 loss: 5.425376711522817 grad: -0.5476281467841493
iteration: 260 loss: 5.229840561057864 grad: -0.5303097788360941
iteration: 270 loss: 5.047721264659685 grad: -0.5140168044062251
iteration: 280 loss: 4.877695552412413 grad: -0.4986637967232402
iteration: 290 loss: 4.718607685335635 grad: -0.4841742036199899
iteration: 300 loss: 4.56944384567338 grad: -0.47047928145695345
iteration: 310 loss: 4.429311076483166 grad: -0.457517167436615
iteration: 320 loss: 4.297419855665959 grad: -0.4452320727389188
iteration: 330 loss: 4.173069593942108 grad: -0.4335735805046338
iteration: 340 loss: 4.055636500651334 grad: -0.42249603448648426
iteration: 350 loss: 3.944563378878766 grad: -0.41195800596313203
iteration: 360 loss: 3.8393510017692525 grad: -0.4019218281637227
iteration: 370 loss: 3.7395507918303834 grad: -0.39235318893660465
iteration: 380 loss: 3.6447585795424193 grad: -0.3832207737039711
iteration: 390 loss: 3.5546092603882986 grad: -0.3744959518804638
iteration: 400 loss: 3.468772203222505 grad: -0.36615250091242235
iteration: 410 loss: 3.386947289745504 grad: -0.358166362932707
iteration: 420 loss: 3.3088614863385044 grad: -0.35051542974160943
iteration: 430 loss: 3.234265866750651 grad: -0.3431793524340754
iteration: 440 loss: 3.1629330180729665 grad: -0.33613937251250936
iteration: 450 loss: 3.094654773752774 grad: -0.3293781717662442
iteration: 460 loss: 3.0292402266241796 grad: -0.32287973857500035
iteration: 470 loss: 2.966513982490514 grad: -0.3166292486143114
iteration: 480 loss: 2.906314621024194 grad: -0.31061295821450297
iteration: 490 loss: 2.8484933358776785 grad: -0.3048181088585725
iteration: 0 loss: 76.93812935264799 grad: 22.994946194336148
iteration: 10 loss: 44.97624817252551 grad: 7.548921079037563
iteration: 20 loss: 33.62227466114785 grad: 1.703542543004386
iteration: 30 loss: 27.251812028264194 grad: -0.17544386156378264
iteration: 40 loss: 23.007564220966596 grad: -0.832988806938425
iteration: 50 loss: 19.932925189012973 grad: -1.0646252296338954
iteration: 60 loss: 17.588668590716473 grad: -1.1310546131131325
iteration: 70 loss: 15.736895173789414 grad: -1.1289577793657675
iteration: 80 loss: 14.235095070052095 grad: -1.0978328689395687
iteration: 90 loss: 12.991817380005779 grad: -1.0550039904568693
iteration: 100 loss: 11.945327806382382 grad: -1.0084340548383905
iteration: 110 loss: 11.052287783795093 grad: -0.9618618060201013
iteration: 120 loss: 10.28130319271399 grad: -0.9170211953859913
iteration: 130 loss: 9.609037900077654 grad: -0.874660500389217
iteration: 140 loss: 9.017765613910342 grad: -0.8350338543996124
iteration: 150 loss: 8.493770047490154 grad: -0.7981475994904201
iteration: 160 loss: 8.026266669995163 grad: -0.7638872775987855
iteration: 170 loss: 7.606656582599044 grad: -0.7320842398847134
iteration: 180 loss: 7.227998303043891 grad: -0.702550758713268
iteration: 190 loss: 6.884626275013323 grad: -0.6750983251752629
iteration: 200 loss: 6.571870441125074 grad: -0.6495468324549893
iteration: 210 loss: 6.2858468405375705 grad: -0.625728786586358
iteration: 220 loss: 6.023299022654886 grad: -0.6034908153880678
iteration: 230 loss: 5.781476407714101 grad: -0.5826937371143831
iteration: 240 loss: 5.558039903421724 grad: -0.5632118943549351
iteration: 250 loss: 5.3509878955479016 grad: -0.5449321474792896
iteration: 260 loss: 5.1585976524193855 grad: -0.5277527457534908
iteration: 270 loss: 4.97937851998532 grad: -0.5115821939195756
iteration: 280 loss: 4.812034227720812 grad: -0.4963381748295942
iteration: 290 loss: 4.6554323008329614 grad: -0.4819465562924484
iteration: 300 loss: 4.508579063481482 grad: -0.46834049215736173
iteration: 310 loss: 4.3705990763883165 grad: -0.4554596177935071
iteration: 320 loss: 4.24071811795875 grad: -0.443249335032342
iteration: 330 loss: 4.118249016927662 grad: -0.4316601792765631
iteration: 340 loss: 4.002579794812721 grad: -0.4206472606539385
iteration: 350 loss: 3.893163690928387 grad: -0.4101697711083287
iteration: 360 loss: 3.789510730682609 grad: -0.40019054976836477
iteration: 370 loss: 3.691180565974967 grad: -0.39067569958288334
iteration: 380 loss: 3.597776369595894 grad: -0.3815942489291494
iteration: 390 loss: 3.5089396072116457 grad: -0.3729178526127112
iteration: 400 loss: 3.424345543451471 grad: -0.3646205273489538
iteration: 410 loss: 3.3436993647861857 grad: -0.3566784174288876
iteration: 420 loss: 3.2667328228037396 grad: -0.34906958681972466
iteration: 430 loss: 3.193201318320999 grad: -0.3417738344349859
iteration: 440 loss: 3.122881360354809 grad: -0.3347725297332042
iteration: 450 loss: 3.055568345004055 grad: -0.32804846617407424
iteration: 460 loss: 2.9910746083115884 grad: -0.3215857303820835
iteration: 470 loss: 2.9292277145380634 grad: -0.3153695851460022
iteration: 480 loss: 2.8698689473683525 grad: -0.3093863646235434
iteration: 490 loss: 2.8128519765635 grad: -0.30362338032895714
iteration: 0 loss: 75.07048257801897 grad: 36.171675417291624
iteration: 10 loss: 42.90571597224314 grad: 6.4112807152606015
iteration: 20 loss: 32.70477334541898 grad: -0.22442763880139174
iteration: 30 loss: 26.747027801915547 grad: -1.4758331804811557
iteration: 40 loss: 22.685027656743863 grad: -1.667653932132223
iteration: 50 loss: 19.70512766229897 grad: -1.6190227966384958
iteration: 60 loss: 17.41565351178193 grad: -1.5171168102171273
iteration: 70 loss: 15.597994158351929 grad: -1.4104108983491268
iteration: 80 loss: 14.118663479973934 grad: -1.3116524059716146
iteration: 90 loss: 12.890877270928513 grad: -1.223412535514507
iteration: 100 loss: 11.855496132428387 grad: -1.1452870956460017
iteration: 110 loss: 10.970705283342774 grad: -1.076098617735925
iteration: 120 loss: 10.20604440243369 grad: -1.0145895746497358
iteration: 130 loss: 9.538769704151616 grad: -0.959628110097678
iteration: 140 loss: 8.951541619676867 grad: -0.910251700796556
iteration: 150 loss: 8.430902637857427 grad: -0.8656585625872741
iteration: 160 loss: 7.966245173447574 grad: -0.8251848580460057
iteration: 170 loss: 7.549093700034808 grad: -0.7882806815514806
iteration: 180 loss: 7.17259431779927 grad: -0.7544888223656241
iteration: 190 loss: 6.831144713470125 grad: -0.723427127428453
iteration: 200 loss: 6.520121252427791 grad: -0.6947742318028496
iteration: 210 loss: 6.235674598509335 grad: -0.6682581419516029
iteration: 220 loss: 5.97457453298462 grad: -0.6436471404289273
iteration: 230 loss: 5.734090655809144 grad: -0.6207425448868507
iteration: 240 loss: 5.511899631787194 grad: -0.5993729345091385
iteration: 250 loss: 5.306012329510147 grad: -0.5793895320628825
iteration: 260 loss: 5.11471604473474 grad: -0.5606624940465278
iteration: 270 loss: 4.9365282862081585 grad: -0.5430779143508018
iteration: 280 loss: 4.770159512647297 grad: -0.5265353894664949
iteration: 290 loss: 4.614482863045065 grad: -0.5109460270417693
iteration: 300 loss: 4.468509397227081 grad: -0.4962308060283914
iteration: 310 loss: 4.331367712451248 grad: -0.48231921716792714
iteration: 320 loss: 4.202287060944193 grad: -0.4691481283858606
iteration: 330 loss: 4.080583287587657 grad: -0.4566608318156599
iteration: 340 loss: 3.9656470540559727 grad: -0.444806238503772
iteration: 350 loss: 3.856933927972229 grad: -0.4335381940138017
iteration: 360 loss: 3.753956002046545 grad: -0.42281489366935676
iteration: 370 loss: 3.656274775134658 grad: -0.4125983804455038
iteration: 380 loss: 3.5634950794462217 grad: -0.4028541118387282
iteration: 390 loss: 3.4752598792291876 grad: -0.39355058464147236
iteration: 400 loss: 3.3912457987575677 grad: -0.3846590085904949
iteration: 410 loss: 3.3111592633189586 grad: -0.3761530214769583
iteration: 420 loss: 3.234733157576805 grad: -0.3680084395972225
iteration: 430 loss: 3.161723922343919 grad: -0.36020303845999935
iteration: 440 loss: 3.091909024249865 grad: -0.3527163595034545
iteration: 450 loss: 3.0250847437228003 grad: -0.3455295392574136
iteration: 460 loss: 2.9610642356393244 grad: -0.33862515794359943
iteration: 470 loss: 2.899675824304929 grad: -0.3319871049659465
iteration: 480 loss: 2.840761500464437 grad: -0.32560045912296565
iteration: 490 loss: 2.7841755930155823 grad: -0.31945138169021337
iteration: 0 loss: 75.72062302226058 grad: 35.63928591316366
iteration: 10 loss: 43.7122711148168 grad: 5.840082943877446
iteration: 20 loss: 33.111305881191555 grad: -0.15712703630658942
iteration: 30 loss: 26.988134996043982 grad: -1.2493722738547548
iteration: 40 loss: 22.84972408671249 grad: -1.4430930678589893
iteration: 50 loss: 19.828897692901013 grad: -1.429400253413991
iteration: 60 loss: 17.51487159494104 grad: -1.3624828410970244
iteration: 70 loss: 15.681222981002334 grad: -1.284192758907556
iteration: 80 loss: 14.190821644130232 grad: -1.207280296512128
iteration: 90 loss: 12.95499457414059 grad: -1.1356479890340756
iteration: 100 loss: 11.91354118082894 grad: -1.0702085957454015
iteration: 110 loss: 11.024012265252829 grad: -1.0108193902245644
iteration: 120 loss: 10.255549205849919 grad: -0.9569833257181709
iteration: 130 loss: 9.585148123500506 grad: -0.9081169074773507
iteration: 140 loss: 8.995294076420889 grad: -0.8636527962315903
iteration: 150 loss: 8.472408141714823 grad: -0.823075966403675
iteration: 160 loss: 8.005796796409435 grad: -0.7859325989581567
iteration: 170 loss: 7.58692250681369 grad: -0.7518280412492867
iteration: 180 loss: 7.208885850985305 grad: -0.7204208281883212
iteration: 190 loss: 6.866050547393983 grad: -0.6914157586978681
iteration: 200 loss: 6.553767211259707 grad: -0.6645572571271687
iteration: 210 loss: 6.268166680273722 grad: -0.6396234638096748
iteration: 220 loss: 6.00600323480355 grad: -0.6164211546309049
iteration: 230 loss: 5.76453417227706 grad: -0.594781446808836
iteration: 240 loss: 5.541426250779089 grad: -0.574556197264907
iteration: 250 loss: 5.334682250520638 grad: -0.5556149898388553
iteration: 260 loss: 5.142582777323738 grad: -0.5378426142943252
iteration: 270 loss: 4.963639739788547 grad: -0.5211369525713234
iteration: 280 loss: 4.7965588567934985 grad: -0.5054072011813227
iteration: 290 loss: 4.640209215199155 grad: -0.49057237101659656
iteration: 300 loss: 4.493598379037973 grad: -0.476560016499468
iteration: 310 loss: 4.355851904953727 grad: -0.4633051548626582
iteration: 320 loss: 4.226196380924731 grad: -0.4507493435983168
iteration: 330 loss: 4.1039453018577 grad: -0.43883988998258516
iteration: 340 loss: 3.9884872442690225 grad: -0.42752917131784673
iteration: 350 loss: 3.8792759156365473 grad: -0.41677404835229936
iteration: 360 loss: 3.77582174119086 grad: -0.4065353574178427
iteration: 370 loss: 3.6776847184396915 grad: -0.3967774693205928
iteration: 380 loss: 3.5844683224205998 grad: -0.387467905042362
iteration: 390 loss: 3.4958142860708574 grad: -0.37857699996052085
iteration: 400 loss: 3.411398112822636 grad: -0.3700776096425339
iteration: 410 loss: 3.3309252045621425 grad: -0.36194485137916066
iteration: 420 loss: 3.2541275088927004 grad: -0.3541558765336241
iteration: 430 loss: 3.180760606393464 grad: -0.3466896695400191
iteration: 440 loss: 3.1106011720857665 grad: -0.33952687001238346
iteration: 450 loss: 3.043444756308831 grad: -0.3326496149497294
iteration: 460 loss: 2.9791038391813345 grad: -0.32604139846079483
iteration: 470 loss: 2.917406120173837 grad: -0.31968694680052395
iteration: 480 loss: 2.8581930103614055 grad: -0.3135721068205697
iteration: 490 loss: 2.8013182999469874 grad: -0.3076837461984011
iteration: 0 loss: 75.7240832932458 grad: 33.70684725591647
iteration: 10 loss: 43.902144600229455 grad: 6.0480234722015185
iteration: 20 loss: 33.164940687751134 grad: 0.028719449563748167
iteration: 30 loss: 26.978825078002252 grad: -1.1451183429861327
iteration: 40 loss: 22.811528224412893 grad: -1.3823353585998812
iteration: 50 loss: 19.777612081394594 grad: -1.3915209806503024
iteration: 60 loss: 17.458233746836104 grad: -1.3374007986526197
iteration: 70 loss: 15.623174974732489 grad: -1.2667225738473675
iteration: 80 loss: 14.13340714174346 grad: -1.1945717644586407
iteration: 90 loss: 12.899261223200277 grad: -1.126039489229505
iteration: 100 loss: 11.859995966091633 grad: -1.062686109189969
iteration: 110 loss: 10.97286273842664 grad: -1.0047432143894275
iteration: 120 loss: 10.206836198758435 grad: -0.9519402180412184
iteration: 130 loss: 9.538820786113176 grad: -0.90383515989787
iteration: 140 loss: 8.951253068507247 grad: -0.8599511193625322
iteration: 150 loss: 8.430530716489143 grad: -0.8198316673490069
iteration: 160 loss: 7.965951363334708 grad: -0.7830613102490653
iteration: 170 loss: 7.548976989512002 grad: -0.7492706139856379
iteration: 180 loss: 7.172712351588829 grad: -0.7181347793381052
iteration: 190 loss: 6.831527803608052 grad: -0.6893696616377495
iteration: 200 loss: 6.52078173679671 grad: -0.6627270546914571
iteration: 210 loss: 6.236613127146814 grad: -0.6379900451416765
iteration: 220 loss: 5.975784304911485 grad: -0.614968765445964
iteration: 230 loss: 5.735560277880764 grad: -0.5934966495696038
iteration: 240 loss: 5.513615045369988 grad: -0.5734271936237891
iteration: 250 loss: 5.307958103565439 grad: -0.5546311808725282
iteration: 260 loss: 5.116876236681988 grad: -0.5369943159585093
iteration: 270 loss: 4.938887007224904 grad: -0.5204152117084204
iteration: 280 loss: 4.772701290615384 grad: -0.5048036760270939
iteration: 290 loss: 4.617192867021996 grad: -0.4900792525655797
iteration: 300 loss: 4.471373567401935 grad: -0.4761699753516295
iteration: 310 loss: 4.334372825937987 grad: -0.4630113036542414
iteration: 320 loss: 4.205420754430055 grad: -0.4505452087357814
iteration: 330 loss: 4.083834051413925 grad: -0.4387193887672784
iteration: 340 loss: 3.9690042078362406 grad: -0.42748659208608514
iteration: 350 loss: 3.8603875847580555 grad: -0.41680403224050744
iteration: 360 loss: 3.757497025873528 grad: -0.40663288098308753
iteration: 370 loss: 3.65989473526096 grad: -0.3969378276291197
iteration: 380 loss: 3.5671862035392636 grad: -0.3876866950665316
iteration: 390 loss: 3.4790150069919616 grad: -0.37885010425365184
iteration: 400 loss: 3.3950583369775233 grad: -0.3704011803283337
iteration: 410 loss: 3.315023142936256 grad: -0.362315294521758
iteration: 420 loss: 3.238642793132509 grad: -0.354569836961255
iteration: 430 loss: 3.1656741739726715 grad: -0.3471440161901166
iteration: 440 loss: 3.0958951622704842 grad: -0.3400186818543791
iteration: 450 loss: 3.029102415799447 grad: -0.3331761675280858
iteration: 460 loss: 2.965109436426701 grad: -0.32660015108685536
iteration: 470 loss: 2.903744867465858 grad: -0.32027553040889567
iteration: 480 loss: 2.8448509929208656 grad: -0.31418831249453977
iteration: 490 loss: 2.7882824112914686 grad: -0.30832551435950495
iteration: 0 loss: 74.88330227770497 grad: 39.11582712490163
iteration: 10 loss: 43.57484305575437 grad: 4.31435647004899
iteration: 20 loss: 33.031152892113575 grad: -0.2398557692286356
iteration: 30 loss: 26.888001663038782 grad: -1.053931199567709
iteration: 40 loss: 22.748067904500616 grad: -1.2560766070982847
iteration: 50 loss: 19.73364336938907 grad: -1.2880710188550777
iteration: 60 loss: 17.42804027056625 grad: -1.2603736834690813
iteration: 70 loss: 15.602643736566485 grad: -1.2107410014296383
iteration: 80 loss: 14.119636461770618 grad: -1.1541729721997125
iteration: 90 loss: 12.890221938017971 grad: -1.0970438645796272
iteration: 100 loss: 11.854272027643013 grad: -1.0420857973224538
iteration: 110 loss: 10.969464524073912 grad: -0.990392142642909
iteration: 120 loss: 10.20506978607649 grad: -0.9422940599948879
iteration: 130 loss: 9.538197475352966 grad: -0.8977667617127636
iteration: 140 loss: 8.951427020121152 grad: -0.8566251663353742
iteration: 150 loss: 8.431255977631178 grad: -0.8186201446598563
iteration: 160 loss: 7.967052194081609 grad: -0.7834860012945285
iteration: 170 loss: 7.55032729850977 grad: -0.7509633960371915
iteration: 180 loss: 7.174221376544789 grad: -0.7208097144930292
iteration: 190 loss: 6.833130083761344 grad: -0.6928030133927517
iteration: 200 loss: 6.522430046584304 grad: -0.6667427233283802
iteration: 210 loss: 6.238273462969366 grad: -0.6424487789947155
iteration: 220 loss: 5.9774323036088965 grad: -0.6197600540612417
iteration: 230 loss: 5.737178640012189 grad: -0.5985325567416107
iteration: 240 loss: 5.515191668525156 grad: -0.5786376170645113
iteration: 250 loss: 5.30948472099445 grad: -0.5599601765001618
iteration: 260 loss: 5.118347418153338 grad: -0.5423972266896253
iteration: 270 loss: 4.9402994214107245 grad: -0.5258564106822975
iteration: 280 loss: 4.774053157626192 grad: -0.5102547833051471
iteration: 290 loss: 4.618483550145563 grad: -0.49551771943744677
iteration: 300 loss: 4.472603267401692 grad: -0.4815779558763657
iteration: 310 loss: 4.335542351386142 grad: -0.4683747518696003
iteration: 320 loss: 4.206531348729269 grad: -0.45585315401496534
iteration: 330 loss: 4.084887262304371 grad: -0.4439633524080979
iteration: 340 loss: 3.9700017889053925 grad: -0.4326601162975055
iteration: 350 loss: 3.861331421150143 grad: -0.42190229889419983
iteration: 360 loss: 3.758389078375124 grad: -0.4116524022897191
iteration: 370 loss: 3.6607369983826175 grad: -0.40187619462180796
iteration: 380 loss: 3.567980674282229 grad: -0.39254237267976927
iteration: 390 loss: 3.4797636617952112 grad: -0.3836222640639777
iteration: 400 loss: 3.395763114927418 grad: -0.37508956381568365
iteration: 410 loss: 3.315685933779853 grad: -0.3669201011261943
iteration: 420 loss: 3.2392654289692886 grad: -0.35909163233163244
iteration: 430 loss: 3.1662584237587907 grad: -0.351583656912998
iteration: 440 loss: 3.0964427284734484 grad: -0.3443772536625169
iteration: 450 loss: 3.029614932679496 grad: -0.3374549345563124
iteration: 460 loss: 2.965588469552999 grad: -0.3308005141991048
iteration: 470 loss: 2.904191914152844 grad: -0.32439899298663777
iteration: 480 loss: 2.845267483342137 grad: -0.3182364523724175
iteration: 490 loss: 2.7886697100917774 grad: -0.31229996083282274
iteration: 0 loss: 76.26032433766756 grad: 31.830479668754883
iteration: 10 loss: 44.30575815527276 grad: 6.816733671433439
iteration: 20 loss: 33.35129119989544 grad: 0.11027256912560729
iteration: 30 loss: 27.086556289124143 grad: -1.260940428623755
iteration: 40 loss: 22.877602365406823 grad: -1.5209491170724136
iteration: 50 loss: 19.819098043732023 grad: -1.5145818524681836
iteration: 60 loss: 17.484402857024158 grad: -1.4405945057975698
iteration: 70 loss: 15.639475382634084 grad: -1.3529973617173936
iteration: 80 loss: 14.143217465543206 grad: -1.2676153156821366
iteration: 90 loss: 12.904758987506574 grad: -1.1888946060894032
iteration: 100 loss: 11.862623403180061 grad: -1.117633952140768
iteration: 110 loss: 10.97359332718181 grad: -1.0534533368583365
iteration: 120 loss: 10.206335378232904 grad: -0.9956367227177889
iteration: 130 loss: 9.537546955919982 grad: -0.9434270015275188
iteration: 140 loss: 8.949523135204448 grad: -0.8961249949350426
iteration: 150 loss: 8.428563693733146 grad: -0.8531161617293841
iteration: 160 loss: 7.96389792681436 grad: -0.8138708918150546
iteration: 170 loss: 7.546939804375887 grad: -0.7779359686641547
iteration: 180 loss: 7.170760212117495 grad: -0.7449239696416248
iteration: 190 loss: 6.829705556465838 grad: -0.7145031084422507
iteration: 200 loss: 6.5191172944860645 grad: -0.6863883216966893
iteration: 210 loss: 6.235122452746799 grad: -0.6603337341150003
iteration: 220 loss: 5.974474970458521 grad: -0.6361263927032925
iteration: 230 loss: 5.734434012794364 grad: -0.6135810894792477
iteration: 240 loss: 5.512669565453549 grad: -0.5925360887298559
iteration: 250 loss: 5.307188424868193 grad: -0.5728495952513197
iteration: 260 loss: 5.116275618868093 grad: -0.5543968261753418
iteration: 270 loss: 4.93844762946901 grad: -0.53706757398103
iteration: 280 loss: 4.772414733810075 grad: -0.5207641698569319
iteration: 290 loss: 4.61705045540256 grad: -0.5053997743551537
iteration: 300 loss: 4.471366607976258 grad: -0.4908969366163298
iteration: 310 loss: 4.334492773589855 grad: -0.4771863748886656
iteration: 320 loss: 4.205659322988553 grad: -0.4642059401644361
iteration: 330 loss: 4.084183285520629 grad: -0.4518997319913397
iteration: 340 loss: 3.9694565264734676 grad: -0.440217341277031
iteration: 350 loss: 3.860935804406505 grad: -0.4291131995067664
iteration: 360 loss: 3.7581343691535123 grad: -0.418546017483229
iteration: 370 loss: 3.6606148293689413 grad: -0.4084782996671228
iteration: 380 loss: 3.5679830716302687 grad: -0.39887592259743343
iteration: 390 loss: 3.4798830548381208 grad: -0.38970776781864735
iteration: 400 loss: 3.3959923365879066 grad: -0.38094540133039156
iteration: 410 loss: 3.3160182143831416 grad: -0.372562792874806
iteration: 420 loss: 3.239694385460511 grad: -0.3645360694450879
iteration: 430 loss: 3.166778045842379 grad: -0.3568432982797441
iteration: 440 loss: 3.097047362773372 grad: -0.3494642953366191
iteration: 450 loss: 3.0302992657630883 grad: -0.3423804558470171
iteration: 460 loss: 2.9663475104108246 grad: -0.33557460405576334
iteration: 470 loss: 2.9050209765825703 grad: -0.3290308596760321
iteration: 480 loss: 2.846162168553664 grad: -0.3227345189428438
iteration: 490 loss: 2.789625889750906 grad: -0.31667194844810836
iteration: 0 loss: 76.04993084558131 grad: 34.44266256914092
iteration: 10 loss: 44.39344458935318 grad: 6.9741715537781905
iteration: 20 loss: 33.47531615575353 grad: 0.4099157507288294
iteration: 30 loss: 27.1921244826891 grad: -1.1031337582099683
iteration: 40 loss: 22.965536555248676 grad: -1.4739139903599625
iteration: 50 loss: 19.89222484267161 grad: -1.5262420912486385
iteration: 60 loss: 17.545561140413486 grad: -1.4792556319347818
iteration: 70 loss: 15.691044517471566 grad: -1.4021314624364338
iteration: 80 loss: 14.187057222746985 grad: -1.3190689356481848
iteration: 90 loss: 12.94229433724759 grad: -1.2388356439067283
iteration: 100 loss: 11.894949305211123 grad: -1.1644144210851128
iteration: 110 loss: 11.001561816631238 grad: -1.0965134824419789
iteration: 120 loss: 10.230620045300983 grad: -1.0349450149830774
iteration: 130 loss: 9.558688867645845 grad: -0.9791938365528543
iteration: 140 loss: 8.967963296829339 grad: -0.928657068403981
iteration: 150 loss: 8.44466616916638 grad: -0.882743730553155
iteration: 160 loss: 7.977966555497522 grad: -0.8409129739827821
iteration: 170 loss: 7.559230512784035 grad: -0.8026852333571726
iteration: 180 loss: 7.181490258063852 grad: -0.7676416973274306
iteration: 190 loss: 6.8390606020702345 grad: -0.7354191259996646
iteration: 200 loss: 6.5272569125814615 grad: -0.7057032215540457
iteration: 210 loss: 6.242184480616867 grad: -0.6782219745053841
iteration: 220 loss: 5.980578998987519 grad: -0.6527395687570436
iteration: 230 loss: 5.739684215342095 grad: -0.6290510361058278
iteration: 240 loss: 5.517157014212651 grad: -0.6069776728096448
iteration: 250 loss: 5.310993003629971 grad: -0.5863631561008653
iteration: 260 loss: 5.119467613980429 grad: -0.5670702730832706
iteration: 270 loss: 4.94108906142806 grad: -0.5489781717701716
iteration: 280 loss: 4.774560477880806 grad: -0.531980051085672
iteration: 290 loss: 4.618749189278085 grad: -0.5159812171470486
iteration: 300 loss: 4.472661616663448 grad: -0.5008974441132678
iteration: 310 loss: 4.335422635746247 grad: -0.4866535880340569
iteration: 320 loss: 4.206258498306025 grad: -0.47318241099893527
iteration: 330 loss: 4.084482619126902 grad: -0.4604235803897601
iteration: 340 loss: 3.9694836834496527 grad: -0.4483228142796273
iteration: 350 loss: 3.860715645226099 grad: -0.43683114915531307
iteration: 360 loss: 3.757689274999269 grad: -0.4259043103415882
iteration: 370 loss: 3.65996498478266 grad: -0.41550216893409936
iteration: 380 loss: 3.5671467107289656 grad: -0.4055882718432531
iteration: 390 loss: 3.478876676328913 grad: -0.39612943383284543
iteration: 400 loss: 3.394830891982937 grad: -0.3870953823015868
iteration: 410 loss: 3.3147152731348384 grad: -0.37845844708269855
iteration: 420 loss: 3.2382622801740335 grad: -0.37019328879069835
iteration: 430 loss: 3.1652280002380064 grad: -0.3622766602771389
iteration: 440 loss: 3.0953896046877905 grad: -0.3546871966100879
iteration: 450 loss: 3.028543127132583 grad: -0.34740522969897825
iteration: 460 loss: 2.9645015159156753 grad: -0.3404126242740535
iteration: 470 loss: 2.9030929223800244 grad: -0.33369263241965474
iteration: 480 loss: 2.844159192341687 grad: -0.3272297642705473
iteration: 490 loss: 2.787554533220249 grad: -0.3210096728245205
iteration: 0 loss: 74.6573974751173 grad: 40.56379321307967
iteration: 10 loss: 42.750626683553534 grad: 4.598798573577976
iteration: 20 loss: 32.44539228426677 grad: -0.7415812431449237
iteration: 30 loss: 26.45208278536494 grad: -1.505212009804778
iteration: 40 loss: 22.397737372896867 grad: -1.5890229567909202
iteration: 50 loss: 19.438018673945056 grad: -1.5314326757313839
iteration: 60 loss: 17.17097189172498 grad: -1.442149235936924
iteration: 70 loss: 15.374735598779084 grad: -1.3499751817513717
iteration: 80 loss: 13.914877134837111 grad: -1.2632788775867696
iteration: 90 loss: 12.704456119668913 grad: -1.1842230457145235
iteration: 100 loss: 11.684450953912387 grad: -1.1129037604851366
iteration: 110 loss: 10.813249941466562 grad: -1.048733961268287
iteration: 120 loss: 10.060609427053164 grad: -0.9909443769833499
iteration: 130 loss: 9.40398904854913 grad: -0.9387690822590704
iteration: 140 loss: 8.826230656896955 grad: -0.8915102722892799
iteration: 150 loss: 8.314034757978147 grad: -0.8485552512772072
iteration: 160 loss: 7.856929861645685 grad: -0.8093747378683331
iteration: 170 loss: 7.446557058109689 grad: -0.7735145080918073
iteration: 180 loss: 7.076162182763958 grad: -0.740585376299008
iteration: 190 loss: 6.740228206279922 grad: -0.7102535256188476
iteration: 200 loss: 6.434204483107433 grad: -0.6822319065655508
iteration: 210 loss: 6.154304237251711 grad: -0.6562728683459387
iteration: 220 loss: 5.897350975286951 grad: -0.632161961591778
iteration: 230 loss: 5.660660539758613 grad: -0.6097127716416255
iteration: 240 loss: 5.441949497541083 grad: -0.5887626256148624
iteration: 250 loss: 5.239263241149771 grad: -0.5691690266484513
iteration: 260 loss: 5.050919021754495 grad: -0.550806687705536
iteration: 270 loss: 4.875460415656397 grad: -0.5335650578119766
iteration: 280 loss: 4.711620633456195 grad: -0.5173462523956918
iteration: 290 loss: 4.5582927316378035 grad: -0.5020633156004709
iteration: 300 loss: 4.414505258350883 grad: -0.48763875592243716
iteration: 310 loss: 4.279402211708037 grad: -0.4740033075310739
iteration: 320 loss: 4.152226445974231 grad: -0.4610948785539601
iteration: 330 loss: 4.032305853610482 grad: -0.4488576547925707
iteration: 340 loss: 3.919041796741071 grad: -0.43724133312304286
iteration: 350 loss: 3.81189937266754 grad: -0.42620046349619234
iteration: 360 loss: 3.710399183401395 grad: -0.4156938822106656
iteration: 370 loss: 3.6141103453195766 grad: -0.4056842221732389
iteration: 380 loss: 3.522644526626085 grad: -0.396137488325559
iteration: 390 loss: 3.4356508408241924 grad: -0.3870226884219453
iteration: 400 loss: 3.3528114564141602 grad: -0.378311510979419
iteration: 410 loss: 3.2738378085053808 grad: -0.36997804356115227
iteration: 420 loss: 3.1984673183894134 grad: -0.36199852565549634
iteration: 430 loss: 3.1264605435007566 grad: -0.35435113132038243
iteration: 440 loss: 3.0575986934166974 grad: -0.3470157775136351
iteration: 450 loss: 2.991681458308945 grad: -0.33997395465281866
iteration: 460 loss: 2.9285251050212704 grad: -0.33320857646702995
iteration: 470 loss: 2.8679608031481867 grad: -0.32670384663644425
iteration: 480 loss: 2.809833149393977 grad: -0.3204451400785726
iteration: 490 loss: 2.7539988634063195 grad: -0.31441889704547465
iteration: 0 loss: 75.77264720479754 grad: 31.217019262286282
iteration: 10 loss: 43.77230999746032 grad: 6.197658375823874
iteration: 20 loss: 33.12004890352659 grad: 0.3340006980374255
iteration: 30 loss: 26.98922631699487 grad: -0.9270096749180594
iteration: 40 loss: 22.848941868907517 grad: -1.2285078118703243
iteration: 50 loss: 19.82746413368014 grad: -1.2790359089029946
iteration: 60 loss: 17.513155754263916 grad: -1.2527164743913217
iteration: 70 loss: 15.679393018425117 grad: -1.201622023142587
iteration: 80 loss: 14.188986846722525 grad: -1.1437342846018126
iteration: 90 loss: 12.953233858621594 grad: -1.0858414122119382
iteration: 100 loss: 11.911908786788196 grad: -1.030571481381609
iteration: 110 loss: 11.022541246297722 grad: -0.9788635085313715
iteration: 120 loss: 10.254255740790335 grad: -0.9309326967877707
iteration: 130 loss: 9.584036063110117 grad: -0.8866792407791164
iteration: 140 loss: 8.994358980878133 grad: -0.8458718384851286
iteration: 150 loss: 8.471640472768202 grad: -0.8082331216020142
iteration: 160 loss: 8.005184243145964 grad: -0.7734799107267623
iteration: 170 loss: 7.586451575289257 grad: -0.7413417761875543
iteration: 180 loss: 7.208542889985686 grad: -0.7115688922471138
iteration: 190 loss: 6.86582237049908 grad: -0.6839345491260507
iteration: 200 loss: 6.553641443899158 grad: -0.6582350283511958
iteration: 210 loss: 6.268131927000178 grad: -0.6342882364554098
iteration: 220 loss: 6.006049133596579 grad: -0.6119318237403293
iteration: 230 loss: 5.764651380985717 grad: -0.5910211647661292
iteration: 240 loss: 5.541606395241987 grad: -0.5714273904459388
iteration: 250 loss: 5.3349178531891335 grad: -0.5530355610211607
iteration: 260 loss: 5.142867178046634 grad: -0.535743015123616
iteration: 270 loss: 4.963967015745187 grad: -0.5194579015781806
iteration: 280 loss: 4.796923745553406 grad: -0.5040978861546097
iteration: 290 loss: 4.6406070430085 grad: -0.48958901882505723
iteration: 300 loss: 4.4940249952667894 grad: -0.4758647446846224
iteration: 310 loss: 4.356303622936676 grad: -0.46286504154877395
iteration: 320 loss: 4.226669925040204 grad: -0.4505356682271596
iteration: 330 loss: 4.104437760459363 grad: -0.43882750895827893
iteration: 340 loss: 3.988996027982549 grad: -0.42769600112878015
iteration: 350 loss: 3.879798720510941 grad: -0.41710063501292716
iteration: 360 loss: 3.7763565161848502 grad: -0.40700451576312746
iteration: 370 loss: 3.6782296367571443 grad: -0.39737397922455897
iteration: 380 loss: 3.585021756250998 grad: -0.38817825432735986
iteration: 390 loss: 3.496374784323755 grad: -0.37938916583493065
iteration: 400 loss: 3.4119643814965457 grad: -0.3709808721098835
iteration: 410 loss: 3.3314960894269294 grad: -0.3629296333158981
iteration: 420 loss: 3.2547019801979578 grad: -0.3552136061203089
iteration: 430 loss: 3.181337745357062 grad: -0.3478126615138489
iteration: 440 loss: 3.111180158942881 grad: -0.34070822283423263
iteration: 450 loss: 3.0440248597319624 grad: -0.3338831214812622
iteration: 460 loss: 2.9796844069019244 grad: -0.32732146815329044
iteration: 470 loss: 2.917986570659154 grad: -0.3210085377271061
iteration: 480 loss: 2.858772825425314 grad: -0.3149306661532204
iteration: 490 loss: 2.8018970181765326 grad: -0.3090751579525848
iteration: 0 loss: 76.33214334873414 grad: 35.566595510514816
iteration: 10 loss: 45.98332097729236 grad: 5.611819521555825
iteration: 20 loss: 34.83266562696072 grad: -0.33059663679793255
iteration: 30 loss: 28.29551719574337 grad: -1.2926739061473618
iteration: 40 loss: 23.87989412335506 grad: -1.4356870938243032
iteration: 50 loss: 20.666727252766496 grad: -1.4040449086483027
iteration: 60 loss: 18.21374401105719 grad: -1.3307463076311357
iteration: 70 loss: 16.276217548507372 grad: -1.2504115780357739
iteration: 80 loss: 14.705940062031459 grad: -1.1732283896324487
iteration: 90 loss: 13.407229947689446 grad: -1.1021014402391027
iteration: 100 loss: 12.315275507373826 grad: -1.0375170957434197
iteration: 110 loss: 11.384490869472426 grad: -0.9791363856031172
iteration: 120 loss: 10.581822048143728 grad: -0.9263669084148933
iteration: 130 loss: 9.882690424351289 grad: -0.8785770093138381
iteration: 140 loss: 9.268427065765165 grad: -0.8351743563383563
iteration: 150 loss: 8.724591316618877 grad: -0.7956307474048949
iteration: 160 loss: 8.23983531252797 grad: -0.7594854307957413
iteration: 170 loss: 7.805117167023024 grad: -0.7263402486259405
iteration: 180 loss: 7.413143438075704 grad: -0.6958521951384173
iteration: 190 loss: 7.057966238118669 grad: -0.6677257082388566
iteration: 200 loss: 6.734686997244353 grad: -0.6417055912865087
iteration: 210 loss: 6.439235248367492 grad: -0.6175708435555447
iteration: 220 loss: 6.168201122257497 grad: -0.595129416014173
iteration: 230 loss: 5.918706908127247 grad: -0.5742138074732857
iteration: 240 loss: 5.688307437756563 grad: -0.5546773857777363
iteration: 250 loss: 5.474912014635596 grad: -0.5363913187631913
iteration: 260 loss: 5.276722639945588 grad: -0.5192420114762218
iteration: 270 loss: 5.092184700615524 grad: -0.5031289612355542
iteration: 280 loss: 4.919947283128832 grad: -0.48796295689385427
iteration: 290 loss: 4.758830991584959 grad: -0.4736645617756721
iteration: 300 loss: 4.60780166665526 grad: -0.4601628308652158
iteration: 310 loss: 4.465948781948307 grad: -0.4473942219766466
iteration: 320 loss: 4.332467575797678 grad: -0.4353016680983886
iteration: 330 loss: 4.206644187110711 grad: -0.4238337841365909
iteration: 340 loss: 4.08784322302381 grad: -0.41294418615041323
iteration: 350 loss: 3.9754973072810404 grad: -0.4025909050976445
iteration: 360 loss: 3.869098251334559 grad: -0.3927358802798019
iteration: 370 loss: 3.7681895621667123 grad: -0.3833445202425685
iteration: 380 loss: 3.672360056975611 grad: -0.3743853209714499
iteration: 390 loss: 3.581238398888176 grad: -0.36582953292037224
iteration: 400 loss: 3.494488402651024 grad: -0.3576508697990789
iteration: 410 loss: 3.4118049868712164 grad: -0.3498252531843491
iteration: 420 loss: 3.3329106714564163 grad: -0.34233058795832594
iteration: 430 loss: 3.2575525366335736 grad: -0.335146564352909
iteration: 440 loss: 3.185499574249951 grad: -0.3282544830227392
iteration: 450 loss: 3.116540373683661 grad: -0.3216371001051322
iteration: 460 loss: 3.0504810941641733 grad: -0.3152784896730231
iteration: 470 loss: 2.9871436830654106 grad: -0.30916392136224
iteration: 480 loss: 2.926364306126371 grad: -0.30327975127004925
iteration: 490 loss: 2.8679919608154125 grad: -0.29761332448812783
iteration: 0 loss: 76.17809308739494 grad: 30.73558820313876
iteration: 10 loss: 43.98413752722683 grad: 7.083057960708756
iteration: 20 loss: 33.21365422911299 grad: 0.6134413080426375
iteration: 30 loss: 27.064893892215732 grad: -0.9472495941385602
iteration: 40 loss: 22.91805983607998 grad: -1.3463994309492844
iteration: 50 loss: 19.891267997910106 grad: -1.4183483463245752
iteration: 60 loss: 17.572014982787593 grad: -1.3871180769519724
iteration: 70 loss: 15.733717030587968 grad: -1.3230854447338094
iteration: 80 loss: 14.239220837892018 grad: -1.2508862982550202
iteration: 90 loss: 12.999808989390845 grad: -1.1796302407252766
iteration: 100 loss: 11.955219432869626 grad: -1.112624431294837
iteration: 110 loss: 11.062939135436098 grad: -1.0508681748061939
iteration: 120 loss: 10.292051135777786 grad: -0.9944196738315163
iteration: 130 loss: 9.619501357865044 grad: -0.9429640223914163
iteration: 140 loss: 9.027732870659149 grad: -0.8960581262077253
iteration: 150 loss: 8.503132029068638 grad: -0.8532373170506988
iteration: 160 loss: 8.034976668697812 grad: -0.8140603511602551
iteration: 170 loss: 7.6147055763708105 grad: -0.7781263371442211
iteration: 180 loss: 7.235399661966105 grad: -0.7450788606269169
iteration: 190 loss: 6.891406215219831 grad: -0.7146044455866898
iteration: 200 loss: 6.578062047207161 grad: -0.686428737257764
iteration: 210 loss: 6.291486329060056 grad: -0.6603120058041021
iteration: 220 loss: 6.0284234259573335 grad: -0.6360447044612612
iteration: 230 loss: 5.786122163801875 grad: -0.6134433929199226
iteration: 240 loss: 5.562242026165429 grad: -0.5923471313229948
iteration: 250 loss: 5.354779516770081 grad: -0.5726143532175142
iteration: 260 loss: 5.16200980143185 grad: -0.55412018300138
iteration: 270 loss: 4.982440053305916 grad: -0.5367541475328056
iteration: 280 loss: 4.814771852058997 grad: -0.5204182287266579
iteration: 290 loss: 4.657870652163513 grad: -0.5050252070560065
iteration: 300 loss: 4.510740817933807 grad: -0.49049725132337346
iteration: 310 loss: 4.3725050771843765 grad: -0.4767647160948723
iteration: 320 loss: 4.242387508260022 grad: -0.463765113973181
iteration: 330 loss: 4.119699372192785 grad: -0.4514422350769936
iteration: 340 loss: 4.003827250739175 grad: -0.4397453905925404
iteration: 350 loss: 3.8942230646997897 grad: -0.4286287610804156
iteration: 360 loss: 3.790395634329508 grad: -0.41805083342202143
iteration: 370 loss: 3.691903511355618 grad: -0.4079739129551404
iteration: 380 loss: 3.598348864956816 grad: -0.3983636995586275
iteration: 390 loss: 3.5093722455767833 grad: -0.389188918276423
iteration: 400 loss: 3.4246480832474515 grad: -0.3804209965860534
iteration: 410 loss: 3.343880803201707 grad: -0.37203378167164664
iteration: 420 loss: 3.266801462427297 grad: -0.36400329210241766
iteration: 430 loss: 3.1931648276074402 grad: -0.35630749918260385
iteration: 440 loss: 3.12274682845009 grad: -0.34892613395928196
iteration: 450 loss: 3.055342331444246 grad: -0.3418405164759699
iteration: 460 loss: 2.9907631880717584 grad: -0.3350334043633292
iteration: 470 loss: 2.9288365188773984 grad: -0.3284888582807465
iteration: 480 loss: 2.86940320086908 grad: -0.32219212207802356
iteration: 490 loss: 2.812316530744903 grad: -0.3161295158462894
iteration: 0 loss: 75.55214193537913 grad: 41.815829988050226
iteration: 10 loss: 43.74093609311092 grad: 5.121921787078989
iteration: 20 loss: 33.15842177919731 grad: -0.8919974067322711
iteration: 30 loss: 27.003199244961582 grad: -1.668298602948063
iteration: 40 loss: 22.848132926176753 grad: -1.7017782049129822
iteration: 50 loss: 19.82055839610172 grad: -1.6071047888403807
iteration: 60 loss: 17.504319236120036 grad: -1.495070533759678
iteration: 70 loss: 15.67038859351605 grad: -1.3892321611192346
iteration: 80 loss: 14.180483839498617 grad: -1.2941999490232914
iteration: 90 loss: 12.945429927478058 grad: -1.209911414579837
iteration: 100 loss: 11.904808760511555 grad: -1.135186482216866
iteration: 110 loss: 11.016079233066586 grad: -1.0687042333776058
iteration: 120 loss: 10.24834717616781 grad: -1.0092684344957206
iteration: 130 loss: 9.578598693229692 grad: -0.9558612983324313
iteration: 140 loss: 8.98932024047039 grad: -0.9076343450533437
iteration: 150 loss: 8.46693908118083 grad: -0.8638834829195137
iteration: 160 loss: 8.000769511155465 grad: -0.8240232356199684
iteration: 170 loss: 7.5822819574111895 grad: -0.7875643599324686
iteration: 180 loss: 7.204584429965797 grad: -0.7540955446163095
iteration: 190 loss: 6.862047288910286 grad: -0.723268784594455
iteration: 200 loss: 6.5500269417719785 grad: -0.6947877757331324
iteration: 210 loss: 6.2646592066782265 grad: -0.6683987044057307
iteration: 220 loss: 6.002702611333709 grad: -0.6438829095456178
iteration: 230 loss: 5.761418059190998 grad: -0.6210510036254036
iteration: 240 loss: 5.53847536377502 grad: -0.59973813222021
iteration: 250 loss: 5.331879893379871 grad: -0.5798001260217154
iteration: 260 loss: 5.139914447864323 grad: -0.5611103563955078
iteration: 270 loss: 4.961092798863774 grad: -0.543557149113937
iteration: 280 loss: 4.794122250714446 grad: -0.527041643880005
iteration: 290 loss: 4.637873243015871 grad: -0.511476012257914
iteration: 300 loss: 4.491354497279975 grad: -0.49678196563667426
iteration: 310 loss: 4.353692563557946 grad: -0.4828894993790429
iteration: 320 loss: 4.224114885097896 grad: -0.4697358304700391
iteration: 330 loss: 4.101935695495616 grad: -0.4572644946089509
iteration: 340 loss: 3.9865442113145946 grad: -0.4454245754029347
iteration: 350 loss: 3.877394696383197 grad: -0.43417004357756
iteration: 360 loss: 3.773998061061651 grad: -0.42345918826213946
iteration: 370 loss: 3.6759147272030286 grad: -0.41325412569204256
iteration: 380 loss: 3.5827485421591105 grad: -0.40352037328982376
iteration: 390 loss: 3.4941415665068067 grad: -0.3942264791886084
iteration: 400 loss: 3.409769592851717 grad: -0.3853436989566134
iteration: 410 loss: 3.3293382790263255 grad: -0.3768457126567415
iteration: 420 loss: 3.2525797998041526 grad: -0.36870837649614474
iteration: 430 loss: 3.1792499379365093 grad: -0.3609095042388858
iteration: 440 loss: 3.109125548841803 grad: -0.3534286743105247
iteration: 450 loss: 3.0420023442372686 grad: -0.34624705914811593
iteration: 460 loss: 2.9776929489636874 grad: -0.3393472738676387
iteration: 470 loss: 2.916025192584088 grad: -0.3327132417530546
iteration: 480 loss: 2.856840603393101 grad: -0.32633007443272793
iteration: 490 loss: 2.799993077448727 grad: -0.32018396491256473
iteration: 0 loss: 76.36342393046363 grad: 35.68835835107622
iteration: 10 loss: 44.713925808303095 grad: 5.995523519218127
iteration: 20 loss: 33.79262435600487 grad: 0.07963965880628698
iteration: 30 loss: 27.455986832274448 grad: -1.0695076196409035
iteration: 40 loss: 23.190175189839195 grad: -1.3272419266074529
iteration: 50 loss: 20.089859073893194 grad: -1.3605190811609227
iteration: 60 loss: 17.723378235315355 grad: -1.3248611887278705
iteration: 70 loss: 15.85335948844456 grad: -1.2668488581038733
iteration: 80 loss: 14.33667810913014 grad: -1.2030166979308743
iteration: 90 loss: 13.081200210418398 grad: -1.1398024353968936
iteration: 100 loss: 12.024625240669552 grad: -1.0797469343454371
iteration: 110 loss: 11.123173199759632 grad: -1.0237490078991118
iteration: 120 loss: 10.345110101561824 grad: -0.9719839079089253
iteration: 130 loss: 9.666837662650117 grad: -0.924307645450459
iteration: 140 loss: 9.070426134349026 grad: -0.8804439020682839
iteration: 150 loss: 8.5420001690583 grad: -0.840072567756527
iteration: 160 loss: 8.070649953907234 grad: -0.8028716232344411
iteration: 170 loss: 7.647677184178809 grad: -0.7685362117084988
iteration: 180 loss: 7.266060927705894 grad: -0.7367863846351921
iteration: 190 loss: 6.920071668695403 grad: -0.7073692288852471
iteration: 200 loss: 6.604987491084268 grad: -0.6800582722775835
iteration: 210 loss: 6.316882088430578 grad: -0.6546516548787981
iteration: 220 loss: 6.052464191398155 grad: -0.6309698300985035
iteration: 230 loss: 5.808954395647271 grad: -0.6088531822885859
iteration: 240 loss: 5.583989589255742 grad: -0.5881597488175284
iteration: 250 loss: 5.375548015373872 grad: -0.5687631297127018
iteration: 260 loss: 5.181889948382005 grad: -0.5505506130951034
iteration: 270 loss: 5.001510313827691 grad: -0.5334215166486975
iteration: 280 loss: 4.833100537390816 grad: -0.5172857318853594
iteration: 290 loss: 4.675517591790925 grad: -0.502062452271828
iteration: 300 loss: 4.52775870619222 grad: -0.4876790647046505
iteration: 310 loss: 4.3889405660443535 grad: -0.4740701843359164
iteration: 320 loss: 4.258282100659347 grad: -0.46117681426041224
iteration: 330 loss: 4.135090157406714 grad: -0.44894561347300155
iteration: 340 loss: 4.018747513723611 grad: -0.4373282584748576
iteration: 350 loss: 3.9087027941698747 grad: -0.4262808857873171
iteration: 360 loss: 3.8044619489188665 grad: -0.41576360434862714
iteration: 370 loss: 3.705581019081571 grad: -0.40574006829551545
iteration: 380 loss: 3.611659968055433 grad: -0.3961771019674467
iteration: 390 loss: 3.522337400317073 grad: -0.387044370128403
iteration: 400 loss: 3.4372860224456088 grad: -0.3783140873963583
iteration: 410 loss: 3.3562087276622647 grad: -0.36996076172327474
iteration: 420 loss: 3.2788352063686363 grad: -0.3619609674970913
iteration: 430 loss: 3.2049190021958482 grad: -0.35429314445891175
iteration: 440 loss: 3.134234946831963 grad: -0.34693741915892795
iteration: 450 loss: 3.066576918070785 grad: -0.33987544612695164
iteration: 460 loss: 3.00175587462598 grad: -0.333090266319512
iteration: 470 loss: 2.939598128734939 grad: -0.3265661807352458
iteration: 480 loss: 2.8799438237042936 grad: -0.3202886373723537
iteration: 490 loss: 2.8226455886368638 grad: -0.3142441299434014
iteration: 0 loss: 77.38241648057183 grad: 24.5545061505715
iteration: 10 loss: 46.60549345189212 grad: 6.498965258300194
iteration: 20 loss: 34.96662018376217 grad: 1.3201986981135705
iteration: 30 loss: 28.288023790857043 grad: -0.18273611888737307
iteration: 40 loss: 23.822307004377514 grad: -0.6978813513664928
iteration: 50 loss: 20.589879280319646 grad: -0.8842424268478033
iteration: 60 loss: 18.130253560050278 grad: -0.9430345319344622
iteration: 70 loss: 16.19185508834471 grad: -0.947434471448079
iteration: 80 loss: 14.623478512816643 grad: -0.9276178310641988
iteration: 90 loss: 13.327991723952499 grad: -0.8971675341682006
iteration: 100 loss: 12.239835832174963 grad: -0.8625665115972115
iteration: 110 loss: 11.313026897979997 grad: -0.827016011188287
iteration: 120 loss: 10.514297106276466 grad: -0.7921107530928264
iteration: 130 loss: 9.818955303715814 grad: -0.7586253945726692
iteration: 140 loss: 9.208277131515285 grad: -0.7269040222430438
iteration: 150 loss: 8.667799070357681 grad: -0.6970613357662064
iteration: 160 loss: 8.186168910287073 grad: -0.669090011927605
iteration: 170 loss: 7.754350738902841 grad: -0.6429194024889974
iteration: 180 loss: 7.3650625669286205 grad: -0.618448138333336
iteration: 190 loss: 7.01237056993618 grad: -0.5955623616312931
iteration: 200 loss: 6.6913911615580135 grad: -0.5741458724154089
iteration: 210 loss: 6.3980688002500505 grad: -0.5540856542438678
iteration: 220 loss: 6.129007938190336 grad: -0.5352747328024297
iteration: 230 loss: 5.881344298421482 grad: -0.5176134903575778
iteration: 240 loss: 5.652645134226697 grad: -0.5010100912136806
iteration: 250 loss: 5.440831128005729 grad: -0.4853804046657327
iteration: 260 loss: 5.244114641669884 grad: -0.470647654964622
iteration: 270 loss: 5.060950459158283 grad: -0.45674193477584507
iteration: 280 loss: 4.889996169571077 grad: -0.44359966284204666
iteration: 290 loss: 4.73008006020051 grad: -0.43116303285154717
iteration: 300 loss: 4.5801749105878775 grad: -0.419379480045365
iteration: 310 loss: 4.43937646099692 grad: -0.4082011796592868
iteration: 320 loss: 4.306885611618244 grad: -0.3975845837804832
iteration: 330 loss: 4.181993620403897 grad: -0.38748999871225676
iteration: 340 loss: 4.064069727042724 grad: -0.37788120232899947
iteration: 350 loss: 3.952550752129355 grad: -0.36872509945104415
iteration: 360 loss: 3.8469323138100333 grad: -0.3599914125302527
iteration: 370 loss: 3.746761376304262 grad: -0.35165240463296965
iteration: 380 loss: 3.6516299008752124 grad: -0.34368263166221186
iteration: 390 loss: 3.5611694138367302 grad: -0.3360587208657517
iteration: 400 loss: 3.4750463409725607 grad: -0.32875917286234857
iteration: 410 loss: 3.392957985319795 grad: -0.3217641846421975
iteration: 420 loss: 3.3146290473241606 grad: -0.3150554912337136
iteration: 430 loss: 3.2398086040708964 grad: -0.3086162239616849
iteration: 440 loss: 3.168267478577863 grad: -0.302430783442872
iteration: 450 loss: 3.0997959417360024 grad: -0.2964847256698522
iteration: 460 loss: 3.034201698932148 grad: -0.2907646597203891
iteration: 470 loss: 2.9713081211052255 grad: -0.2852581557976086
iteration: 480 loss: 2.9109526863806177 grad: -0.27995366245636844
iteration: 490 loss: 2.8529856036472085 grad: -0.2748404320046171
iteration: 0 loss: 77.44862796522294 grad: 23.446542916103596
iteration: 10 loss: 46.44778293207046 grad: 6.8329144228320455
iteration: 20 loss: 34.95285627054218 grad: 1.308277073596952
iteration: 30 loss: 28.362791461532154 grad: -0.3181694249083573
iteration: 40 loss: 23.9332345546823 grad: -0.8437132446414709
iteration: 50 loss: 20.712834421501714 grad: -1.009880303271908
iteration: 60 loss: 18.254464449843887 grad: -1.0445795396347193
iteration: 70 loss: 16.312489282884908 grad: -1.0280206307455488
iteration: 80 loss: 14.738458625522567 grad: -0.9914680444708829
iteration: 90 loss: 13.436570481629504 grad: -0.948009801987865
iteration: 100 loss: 12.341919856026585 grad: -0.9033564463904324
iteration: 110 loss: 11.40884025984542 grad: -0.8600145393539272
iteration: 120 loss: 10.604208575260497 grad: -0.8190298701575589
iteration: 130 loss: 9.903388268145797 grad: -0.7807621610080233
iteration: 140 loss: 9.287662753539363 grad: -0.745246406185994
iteration: 150 loss: 8.742552585785832 grad: -0.7123675209212322
iteration: 160 loss: 8.256678481575467 grad: -0.6819467041512856
iteration: 170 loss: 7.820973128363138 grad: -0.6537844983223047
iteration: 180 loss: 7.428122469097058 grad: -0.6276820170778212
iteration: 190 loss: 7.07216184002424 grad: -0.6034509609046486
iteration: 200 loss: 6.748178952734137 grad: -0.5809178267722321
iteration: 210 loss: 6.4520920597723155 grad: -0.55992512529751
iteration: 220 loss: 6.180481961435817 grad: -0.5403310926301437
iteration: 230 loss: 5.930463182439822 grad: -0.5220086896459755
iteration: 240 loss: 5.699584053956394 grad: -0.5048443104858045
iteration: 250 loss: 5.485748404494354 grad: -0.4887364221080749
iteration: 260 loss: 5.287153597258731 grad: -0.4735942472990418
iteration: 270 loss: 5.102241068221784 grad: -0.45933654395409085
iteration: 280 loss: 4.929656520068792 grad: -0.445890501129673
iteration: 290 loss: 4.768217643975607 grad: -0.4331907552161018
iteration: 300 loss: 4.616887760787946 grad: -0.42117852087682994
iteration: 310 loss: 4.474754154226461 grad: -0.4098008273814523
iteration: 320 loss: 4.34101015109288 grad: -0.39900984950916396
iteration: 330 loss: 4.214940214776892 grad: -0.3887623221179981
iteration: 340 loss: 4.095907477959042 grad: -0.379019028084994
iteration: 350 loss: 3.983343262002522 grad: -0.36974435024312924
iteration: 360 loss: 3.8767382238791637 grad: -0.36090587896252596
iteration: 370 loss: 3.775634843751215 grad: -0.3524740680342267
iteration: 380 loss: 3.6796210226151 grad: -0.34442193245879066
iteration: 390 loss: 3.588324603612901 grad: -0.33672478259485805
iteration: 400 loss: 3.5014086655000938 grad: -0.3293599898779155
iteration: 410 loss: 3.4185674644646413 grad: -0.3223067799793769
iteration: 420 loss: 3.3395229226510477 grad: -0.31554604984790857
iteration: 430 loss: 3.2640215795209273 grad: -0.30906020556776487
iteration: 440 loss: 3.1918319365537915 grad: -0.30283301839220006
iteration: 450 loss: 3.1227421374466178 grad: -0.29684949667288735
iteration: 460 loss: 3.056557935479594 grad: -0.29109577171702383
iteration: 470 loss: 2.9931009074992456 grad: -0.2855589958698598
iteration: 480 loss: 2.9322068803681676 grad: -0.28022725134824905
iteration: 490 loss: 2.8737245410364434 grad: -0.27508946854610783
iteration: 0 loss: 76.24826361562437 grad: 36.31838939494129
iteration: 10 loss: 44.6065205617936 grad: 7.279423681716152
iteration: 20 loss: 33.71610758817051 grad: -0.10615678982833798
iteration: 30 loss: 27.425366419940143 grad: -1.5025772171316918
iteration: 40 loss: 23.17483593033347 grad: -1.7120720523110102
iteration: 50 loss: 20.076916098070402 grad: -1.660830913160873
iteration: 60 loss: 17.708458686271754 grad: -1.5544648515796464
iteration: 70 loss: 15.83536631530191 grad: -1.4440144479738553
iteration: 80 loss: 14.315708781491635 grad: -1.34222661569118
iteration: 90 loss: 13.05773790778552 grad: -1.2514221373442356
iteration: 100 loss: 11.999226265397324 grad: -1.1710274132966103
iteration: 110 loss: 11.096352048652555 grad: -1.0997762879945685
iteration: 120 loss: 10.31730428406265 grad: -1.0363692158356224
iteration: 130 loss: 9.63840414147353 grad: -0.9796553308924729
iteration: 140 loss: 9.04164934408738 grad: -0.9286632236016045
iteration: 150 loss: 8.513103271759435 grad: -0.8825859005936653
iteration: 160 loss: 8.041805983918927 grad: -0.840754946586789
iteration: 170 loss: 7.619018881769162 grad: -0.8026152129932316
iteration: 180 loss: 7.237688971146161 grad: -0.7677031881654255
iteration: 190 loss: 6.8920613801034385 grad: -0.7356294265508914
iteration: 200 loss: 6.57739421775407 grad: -0.7060645557939228
iteration: 210 loss: 6.289745485400048 grad: -0.67872820177747
iteration: 220 loss: 6.0258116142424365 grad: -0.6533802179742605
iteration: 230 loss: 5.7828035823321935 grad: -0.629813711340588
iteration: 240 loss: 5.558350778152214 grad: -0.6078494642695595
iteration: 250 loss: 5.3504256178228555 grad: -0.5873314433024998
iteration: 260 loss: 5.15728386992987 grad: -0.5681231576438206
iteration: 270 loss: 4.977416998447428 grad: -0.5501046861715319
iteration: 280 loss: 4.809513793147579 grad: -0.5331702338496087
iteration: 290 loss: 4.652429243890807 grad: -0.5172261103184952
iteration: 300 loss: 4.50515911348686 grad: -0.5021890475074111
iteration: 310 loss: 4.3668190293603235 grad: -0.4879847913466943
iteration: 320 loss: 4.236627185280213 grad: -0.4745469165433176
iteration: 330 loss: 4.113889947323184 grad: -0.46181582401973764
iteration: 340 loss: 3.997989811565979 grad: -0.4497378888187743
iteration: 350 loss: 3.888375277848478 grad: -0.43826473264564836
iteration: 360 loss: 3.784552293718951 grad: -0.42735260019656773
iteration: 370 loss: 3.6860769921704972 grad: -0.4169618223409839
iteration: 380 loss: 3.592549500946213 grad: -0.40705635232791754
iteration: 390 loss: 3.503608643724337 grad: -0.3976033636603755
iteration: 400 loss: 3.418927387070777 grad: -0.38857290026544095
iteration: 410 loss: 3.338208913753492 grad: -0.3799375711873696
iteration: 420 loss: 3.261183224337277 grad: -0.371672283328238
iteration: 430 loss: 3.1876041861181132 grad: -0.36375400681808095
iteration: 440 loss: 3.117246962318983 grad: -0.3561615684625347
iteration: 450 loss: 3.0499057656872783 grad: -0.34887546942877234
iteration: 460 loss: 2.9853918898270835 grad: -0.34187772391966376
iteration: 470 loss: 2.9235319790810452 grad: -0.3351517160751258
iteration: 480 loss: 2.86416650398893 grad: -0.3286820727472093
iteration: 490 loss: 2.807148414419275 grad: -0.3224545501364169
iteration: 0 loss: 77.56408825968317 grad: 23.847001264001104
iteration: 10 loss: 46.00158307869241 grad: 6.904803768032046
iteration: 20 loss: 34.47583148984199 grad: 1.1948204000969773
iteration: 30 loss: 27.931198281185733 grad: -0.415740293435945
iteration: 40 loss: 23.55142086995052 grad: -0.908161768260553
iteration: 50 loss: 20.37570520299722 grad: -1.0495786069507975
iteration: 60 loss: 17.955774715861892 grad: -1.0680002829089545
iteration: 70 loss: 16.0464797296917 grad: -1.0409407597483864
iteration: 80 loss: 14.500189411728508 grad: -0.9975504016503598
iteration: 90 loss: 13.221912601627185 grad: -0.9495628938315133
iteration: 100 loss: 12.147453066858382 grad: -0.9018571810022602
iteration: 110 loss: 11.23173577337159 grad: -0.8564287112857534
iteration: 120 loss: 10.442121696167327 grad: -0.8140039039277541
iteration: 130 loss: 9.754367592088556 grad: -0.7747383897381324
iteration: 140 loss: 9.150075938059155 grad: -0.7385335384336568
iteration: 150 loss: 8.61502658919337 grad: -0.7051846638289143
iteration: 160 loss: 8.138051745756105 grad: -0.6744515329608628
iteration: 170 loss: 7.710257368706565 grad: -0.6460917250924324
iteration: 180 loss: 7.324472097484844 grad: -0.6198757856041892
iteration: 190 loss: 6.974849420810435 grad: -0.5955933460096152
iteration: 200 loss: 6.656575421029381 grad: -0.57305477442303
iteration: 210 loss: 6.36565070313761 grad: -0.5520906729469308
iteration: 220 loss: 6.098725383393147 grad: -0.5325504098446189
iteration: 230 loss: 5.852972637040239 grad: -0.5143002947928489
iteration: 240 loss: 5.625990673808743 grad: -0.4972217030475969
iteration: 250 loss: 5.415725947942354 grad: -0.48120929512611244
iteration: 260 loss: 5.220412420438234 grad: -0.4661693948206522
iteration: 270 loss: 5.038523089743483 grad: -0.45201854484731463
iteration: 280 loss: 4.868730994231968 grad: -0.4386822375823558
iteration: 290 loss: 4.709877595898764 grad: -0.42609380813676667
iteration: 300 loss: 4.560946966134204 grad: -0.4141934730219173
iteration: 310 loss: 4.421044569179106 grad: -0.4029274968842598
iteration: 320 loss: 4.289379716332323 grad: -0.3922474706038753
iteration: 330 loss: 4.165250971543049 grad: -0.38210968557378716
iteration: 340 loss: 4.048033945679022 grad: -0.3724745907319219
iteration: 350 loss: 3.9371710360626575 grad: -0.3633063206626924
iteration: 360 loss: 3.8321627594385643 grad: -0.35457228470451196
iteration: 370 loss: 3.732560397383416 grad: -0.34624280844630284
iteration: 380 loss: 3.637959728351937 grad: -0.33829082025982865
iteration: 390 loss: 3.547995663847153 grad: -0.33069157660343224
iteration: 400 loss: 3.462337640383008 grad: -0.32342242076267347
iteration: 410 loss: 3.380685646050841 grad: -0.3164625704834756
iteration: 420 loss: 3.3027667821893374 grad: -0.3097929306226975
iteration: 430 loss: 3.2283322780737596 grad: -0.30339592750723376
iteration: 440 loss: 3.1571548906054803 grad: -0.2972553621714815
iteration: 450 loss: 3.0890266323940523 grad: -0.2913562800479965
iteration: 460 loss: 3.0237567809322585 grad: -0.2856848550290572
iteration: 470 loss: 2.9611701291782992 grad: -0.2802282861075676
iteration: 480 loss: 2.90110544413305 grad: -0.2749747050525967
iteration: 490 loss: 2.843414105167788 grad: -0.26991309378484973
iteration: 0 loss: 75.68652652262395 grad: 24.704934688746867
iteration: 10 loss: 43.94470603422142 grad: 6.678700370593474
iteration: 20 loss: 33.15931926620751 grad: 1.3532827123166653
iteration: 30 loss: 26.950193506974507 grad: -0.24038179131651535
iteration: 40 loss: 22.773873364723656 grad: -0.8028497830282881
iteration: 50 loss: 19.736289803378792 grad: -1.0098380530888393
iteration: 60 loss: 17.41565820749838 grad: -1.074975581328328
iteration: 70 loss: 15.580605379163114 grad: -1.0784782507799389
iteration: 80 loss: 14.09156014603251 grad: -1.0542628364526965
iteration: 90 loss: 12.858557650520321 grad: -1.0178722244126184
iteration: 100 loss: 11.82067789614627 grad: -0.9768103119531756
iteration: 110 loss: 10.935057609675699 grad: -0.9348029385691652
iteration: 120 loss: 10.170595882104884 grad: -0.89370471225495
iteration: 130 loss: 9.504147115877851 grad: -0.8544081151904455
iteration: 140 loss: 8.918115184556966 grad: -0.8172989243889485
iteration: 150 loss: 8.398877056277259 grad: -0.7824933472208092
iteration: 160 loss: 7.935717991575695 grad: -0.7499651909159737
iteration: 170 loss: 7.520093358662259 grad: -0.7196155099908867
iteration: 180 loss: 7.145105241045712 grad: -0.6913112283673986
iteration: 190 loss: 6.805123965115482 grad: -0.6649066130813827
iteration: 200 loss: 6.4955096235252805 grad: -0.6402550906758264
iteration: 210 loss: 6.2124039779025475 grad: -0.6172155541012263
iteration: 220 loss: 5.952572778641297 grad: -0.5956555054303209
iteration: 230 loss: 5.713284778339585 grad: -0.5754523829992234
iteration: 240 loss: 5.4922178357005995 grad: -0.5564938584032733
iteration: 250 loss: 5.287385281499197 grad: -0.5386775646474541
iteration: 260 loss: 5.097077620142789 grad: -0.5219105273314646
iteration: 270 loss: 4.919815965082432 grad: -0.506108458686701
iteration: 280 loss: 4.754314542596851 grad: -0.49119500737606026
iteration: 290 loss: 4.599450269163299 grad: -0.4771010167931652
iteration: 300 loss: 4.454237894004233 grad: -0.4637638204347595
iteration: 310 loss: 4.3178095552096405 grad: -0.4511265884375839
iteration: 320 loss: 4.189397862324478 grad: -0.43913773078132123
iteration: 330 loss: 4.0683218163463115 grad: -0.4277503576708206
iteration: 340 loss: 3.9539750276935592 grad: -0.416921794824523
iteration: 350 loss: 3.8458158067605424 grad: -0.4066131499511514
iteration: 360 loss: 3.743358789290679 grad: -0.39678892606100874
iteration: 370 loss: 3.6461678266167508 grad: -0.387416677101637
iteration: 380 loss: 3.5538499237120713 grad: -0.37846670152547357
iteration: 390 loss: 3.466050049501312 grad: -0.36991176966312983
iteration: 400 loss: 3.382446676679551 grad: -0.361726881112656
iteration: 410 loss: 3.3027479343375243 grad: -0.35388904871610216
iteration: 420 loss: 3.2266882775430035 grad: -0.34637710605253236
iteration: 430 loss: 3.1540255947522216 grad: -0.3391715357162615
iteration: 440 loss: 3.084538687460157 grad: -0.332254315962725
iteration: 450 loss: 3.0180250674810933 grad: -0.32560878358891293
iteration: 460 loss: 2.9542990262003017 grad: -0.3192195111703309
iteration: 470 loss: 2.893189937487436 grad: -0.3130721970030819
iteration: 480 loss: 2.834540761984458 grad: -0.30715356629990587
iteration: 490 loss: 2.778206725493171 grad: -0.3014512823652151
iteration: 0 loss: 76.80389583700627 grad: 23.797981799674346
iteration: 10 loss: 44.896013306684466 grad: 7.066347384418608
iteration: 20 loss: 33.72408073811814 grad: 1.37711411561229
iteration: 30 loss: 27.38445456176762 grad: -0.3381058297553818
iteration: 40 loss: 23.137169744804456 grad: -0.8990226668964001
iteration: 50 loss: 20.051589042635463 grad: -1.0756874581431708
iteration: 60 loss: 17.69523276994654 grad: -1.1105374945406785
iteration: 70 loss: 15.832128444442974 grad: -1.0902894788957587
iteration: 80 loss: 14.32026697142144 grad: -1.048971960705546
iteration: 90 loss: 13.068235193238934 grad: -1.0007784929458998
iteration: 100 loss: 12.014186172860905 grad: -0.9518096474974401
iteration: 110 loss: 11.114628915542736 grad: -0.9046641195318603
iteration: 120 loss: 10.338017561790263 grad: -0.8603644355424117
iteration: 130 loss: 9.66087908959699 grad: -0.8192096897549018
iteration: 140 loss: 9.065368962559846 grad: -0.7811687468502195
iteration: 150 loss: 8.537670964015888 grad: -0.746066622460605
iteration: 160 loss: 8.066917077206261 grad: -0.713674064929634
iteration: 170 loss: 7.6444390560076165 grad: -0.6837504671628003
iteration: 180 loss: 7.263237942757562 grad: -0.6560638109055321
iteration: 190 loss: 6.917600539362824 grad: -0.6303991880855941
iteration: 200 loss: 6.602817229372828 grad: -0.6065616519659894
iteration: 210 loss: 6.314971110652054 grad: -0.5843763109245018
iteration: 220 loss: 6.050778202853425 grad: -0.5636871522895923
iteration: 230 loss: 5.807464824349856 grad: -0.5443553551229692
iteration: 240 loss: 5.582672411061222 grad: -0.5262574736090826
iteration: 250 loss: 5.374382861603201 grad: -0.5092836759775683
iteration: 260 loss: 5.180859419795277 grad: -0.49333612128523974
iteration: 270 loss: 5.0005994471114885 grad: -0.4783275034186649
iteration: 280 loss: 4.832296385652461 grad: -0.46417976505014436
iteration: 290 loss: 4.674808891246977 grad: -0.4508229716369257
iteration: 300 loss: 4.527135608710978 grad: -0.4381943302837917
iteration: 310 loss: 4.388394422540056 grad: -0.42623733683266646
iteration: 320 loss: 4.257805284140762 grad: -0.414901034980217
iteration: 330 loss: 4.134675917251116 grad: -0.404139372525829
iteration: 340 loss: 4.018389854759252 grad: -0.3939106414731728
iteration: 350 loss: 3.9083963756421243 grad: -0.38417699036910985
iteration: 360 loss: 3.8042019995255445 grad: -0.37490399882849307
iteration: 370 loss: 3.7053632650897104 grad: -0.3660603056047777
iteration: 380 loss: 3.6114805721495857 grad: -0.35761728280751925
iteration: 390 loss: 3.5221929093121287 grad: -0.3495487499427601
iteration: 400 loss: 3.437173322363709 grad: -0.3418307223745256
iteration: 410 loss: 3.3561250049672875 grad: -0.33444118959237124
iteration: 420 loss: 3.278777914379837 grad: -0.3273599193388115
iteration: 430 loss: 3.204885831874311 grad: -0.32056828421825134
iteration: 440 loss: 3.1342238012853985 grad: -0.3140491078907085
iteration: 450 loss: 3.0665858902245664 grad: -0.30778652836234516
iteration: 460 loss: 3.001783227613363 grad: -0.30176587623191287
iteration: 470 loss: 2.9396422786202163 grad: -0.2959735660473143
iteration: 480 loss: 2.880003324222426 grad: -0.29039699917772743
iteration: 490 loss: 2.8227191176799598 grad: -0.285024476821015
iteration: 0 loss: 74.9650304687766 grad: 40.587364385250936
iteration: 10 loss: 43.78268827984437 grad: 5.2137006084622115
iteration: 20 loss: 33.11822234308112 grad: -0.6381664441049009
iteration: 30 loss: 26.927175620271246 grad: -1.4878138782514871
iteration: 40 loss: 22.757052462920413 grad: -1.5901970385952984
iteration: 50 loss: 19.72309614713206 grad: -1.538144525117105
iteration: 60 loss: 17.405045043397568 grad: -1.4511184426120691
iteration: 70 loss: 15.571915976116005 grad: -1.3601287607487027
iteration: 80 loss: 14.084302049466032 grad: -1.2741467835965397
iteration: 90 loss: 12.852345358995143 grad: -1.1955344722175147
iteration: 100 loss: 11.815210252139112 grad: -1.124472017950708
iteration: 110 loss: 10.930102559693234 grad: -1.0604193118379197
iteration: 120 loss: 10.165977450683302 grad: -1.0026395538992887
iteration: 130 loss: 9.499734006700486 grad: -0.9503918167300931
iteration: 140 loss: 8.913811090693985 grad: -0.9029982452539362
iteration: 150 loss: 8.394612677022058 grad: -0.8598621411414611
iteration: 160 loss: 7.9314446162693875 grad: -0.8204669785869867
iteration: 170 loss: 7.515777804854059 grad: -0.7843686226561178
iteration: 180 loss: 7.1407259166653185 grad: -0.7511858045771949
iteration: 190 loss: 6.800667838133904 grad: -0.7205908731964792
iteration: 200 loss: 6.490969910237461 grad: -0.6923015441687411
iteration: 210 loss: 6.207778392354405 grad: -0.6660738176392416
iteration: 220 loss: 5.947862215683224 grad: -0.6416960119118895
iteration: 230 loss: 5.708492329097639 grad: -0.6189837816203551
iteration: 240 loss: 5.487348056296262 grad: -0.5977759723436538
iteration: 250 loss: 5.282443653729553 grad: -0.5779311724681542
iteration: 260 loss: 5.0920701570459475 grad: -0.5593248407750744
iteration: 270 loss: 4.914748925443829 grad: -0.5418469074550334
iteration: 280 loss: 4.749194227071712 grad: -0.5253997640575179
iteration: 290 loss: 4.594282877307003 grad: -0.5098965732770157
iteration: 300 loss: 4.44902942657642 grad: -0.4952598423284263
iteration: 310 loss: 4.312565749951153 grad: -0.48142021419191217
iteration: 320 loss: 4.184124154337703 grad: -0.46831543954472304
iteration: 330 loss: 4.063023316405923 grad: -0.45588949908607096
iteration: 340 loss: 3.9486565135033413 grad: -0.4440918515089381
iteration: 350 loss: 3.84048172341235 grad: -0.4328767868412203
iteration: 360 loss: 3.738013256142246 grad: -0.42220286848306043
iteration: 370 loss: 3.640814648520187 grad: -0.412032450181436
iteration: 380 loss: 3.5484926050614622 grad: -0.40233125654594704
iteration: 390 loss: 3.460691809979215 grad: -0.39306801763184773
iteration: 400 loss: 3.3770904678781335 grad: -0.3842141496853099
iteration: 410 loss: 3.2973964566702887 grad: -0.3757434754311548
iteration: 420 loss: 3.221343997025294 grad: -0.36763197833994454
iteration: 430 loss: 3.148690759359066 grad: -0.35985758618325775
iteration: 440 loss: 3.0792153428707767 grad: -0.35239997990794925
iteration: 450 loss: 3.012715072082632 grad: -0.34524042446024983
iteration: 460 loss: 2.949004065278049 grad: -0.3383616186908834
iteration: 470 loss: 2.8879115365675303 grad: -0.3317475618910382
iteration: 480 loss: 2.8292802993209163 grad: -0.3253834348604525
iteration: 490 loss: 2.7729654437118616 grad: -0.3192554937048253
iteration: 0 loss: 76.08884556158984 grad: 29.568728595127634
iteration: 10 loss: 44.15980036762256 grad: 6.6006092745693685
iteration: 20 loss: 33.40240680253485 grad: 0.6101536475123572
iteration: 30 loss: 27.19630377176244 grad: -0.792449337583872
iteration: 40 loss: 23.00303897791173 grad: -1.170457007124786
iteration: 50 loss: 19.944838166926647 grad: -1.2605702353258348
iteration: 60 loss: 17.604621680888986 grad: -1.254728214984114
iteration: 70 loss: 15.752094856246686 grad: -1.2141246471283336
iteration: 80 loss: 14.247743586883189 grad: -1.1613967989071652
iteration: 90 loss: 13.001388191236133 grad: -1.1057688031439497
iteration: 100 loss: 11.95184426562078 grad: -1.0511774097976063
iteration: 110 loss: 11.056000314346564 grad: -0.9992808751181098
iteration: 120 loss: 10.282539990608567 grad: -0.95069703028576
iteration: 130 loss: 9.608135930527252 grad: -0.9055546411536575
iteration: 140 loss: 9.01504014150524 grad: -0.8637533587447982
iteration: 150 loss: 8.489502941523039 grad: -0.8250910422341525
iteration: 160 loss: 8.020703784503707 grad: -0.7893274148883055
iteration: 170 loss: 7.600009345112002 grad: -0.7562159290775978
iteration: 180 loss: 7.2204470553560895 grad: -0.7255193168859078
iteration: 190 loss: 6.876324139529433 grad: -0.6970166331871955
iteration: 200 loss: 6.562947122655177 grad: -0.6705058453445564
iteration: 210 loss: 6.276412103016438 grad: -0.6458041197580677
iteration: 220 loss: 6.013445748791252 grad: -0.6227469612509123
iteration: 230 loss: 5.771283232985628 grad: -0.6011868301559354
iteration: 240 loss: 5.54757345428371 grad: -0.5809915732644131
iteration: 250 loss: 5.340304676790518 grad: -0.5620428459463696
iteration: 260 loss: 5.147745631824777 grad: -0.5442346147694056
iteration: 270 loss: 4.968398456027486 grad: -0.5274717812309899
iteration: 280 loss: 4.800960781253252 grad: -0.5116689405185508
iteration: 290 loss: 4.64429496623089 grad: -0.4967492749398404
iteration: 300 loss: 4.497402949359722 grad: -0.4826435744286287
iteration: 310 loss: 4.35940556116818 grad: -0.4692893732575092
iteration: 320 loss: 4.229525401327175 grad: -0.456630191019212
iteration: 330 loss: 4.107072584653617 grad: -0.4446148660558521
iteration: 340 loss: 3.9914328113525865 grad: -0.43319697023821657
iteration: 350 loss: 3.882057331769953 grad: -0.42233429499106107
iteration: 360 loss: 3.7784544642943896 grad: -0.41198839953825583
iteration: 370 loss: 3.680182393526841 grad: -0.4021242134019076
iteration: 380 loss: 3.5868430292233495 grad: -0.39270968618053004
iteration: 390 loss: 3.4980767484360267 grad: -0.38371547852999266
iteration: 400 loss: 3.413557876427438 grad: -0.37511468907069867
iteration: 410 loss: 3.332990788263363 grad: -0.36688261264737443
iteration: 420 loss: 3.256106534053784 grad: -0.3589965259807453
iteration: 430 loss: 3.1826599077487363 grad: -0.35143549728186374
iteration: 440 loss: 3.112426893068749 grad: -0.3441802168591934
iteration: 450 loss: 3.0452024312677084 grad: -0.33721284614464986
iteration: 460 loss: 2.9807984644881302 grad: -0.33051688290600256
iteration: 470 loss: 2.919042215892246 grad: -0.32407704070680393
iteration: 480 loss: 2.859774673876183 grad: -0.31787914092794994
iteration: 490 loss: 2.8028492527177646 grad: -0.3119100158828692
iteration: 0 loss: 73.96289285060485 grad: 43.36477075854749
iteration: 10 loss: 43.80962917866039 grad: 5.36216715085944
iteration: 20 loss: 33.31774622522421 grad: -1.1733429070831767
iteration: 30 loss: 27.147353628875802 grad: -1.9029423914641281
iteration: 40 loss: 22.96265689109683 grad: -1.8892794425304063
iteration: 50 loss: 19.907725732016512 grad: -1.7609943463763815
iteration: 60 loss: 17.569486746992343 grad: -1.623702499736688
iteration: 70 loss: 15.718610690125656 grad: -1.4980893406366205
iteration: 80 loss: 14.215807498904683 grad: -1.3872171415062158
iteration: 90 loss: 12.970927329494565 grad: -1.290074314811251
iteration: 100 loss: 11.922773799090486 grad: -1.2048311084423655
iteration: 110 loss: 11.02822268517399 grad: -1.1296879688162373
iteration: 120 loss: 10.255952326235274 grad: -1.063080955782732
iteration: 130 loss: 9.582636696887034 grad: -1.003703725438283
iteration: 140 loss: 8.990533406157573 grad: -0.9504790341679703
iteration: 150 loss: 8.465900579757193 grad: -0.9025204979643655
iteration: 160 loss: 7.9979262892971255 grad: -0.8590969176818823
iteration: 170 loss: 7.577985794360715 grad: -0.8196022537550617
iteration: 180 loss: 7.199114617461749 grad: -0.7835313372096114
iteration: 190 loss: 6.855627357791987 grad: -0.7504605296492257
iteration: 200 loss: 6.542837122197977 grad: -0.720032400812371
iteration: 210 loss: 6.256845802120014 grad: -0.6919435839563484
iteration: 220 loss: 5.994385118344739 grad: -0.6659351185064483
iteration: 230 loss: 5.752694624774862 grad: -0.6417847341512641
iteration: 240 loss: 5.529427005249297 grad: -0.6193006525494795
iteration: 250 loss: 5.322573788650474 grad: -0.5983165797678789
iteration: 260 loss: 5.130406521284964 grad: -0.578687637560832
iteration: 270 loss: 4.951429768797797 grad: -0.5602870389058573
iteration: 280 loss: 4.7843432624255335 grad: -0.5430033568307886
iteration: 290 loss: 4.6280111796245365 grad: -0.5267382688021285
iteration: 300 loss: 4.481437038942286 grad: -0.5114046843553671
iteration: 310 loss: 4.343743048328592 grad: -0.4969251831746042
iteration: 320 loss: 4.2141530125341395 grad: -0.4832307059174203
iteration: 330 loss: 4.091978104762265 grad: -0.4702594518065459
iteration: 340 loss: 3.976604958519064 grad: -0.4579559461724272
iteration: 350 loss: 3.8674856505351936 grad: -0.4462702483280126
iteration: 360 loss: 3.7641292339566546 grad: -0.4351572758384871
iteration: 370 loss: 3.666094549374107 grad: -0.42457622575434195
iteration: 380 loss: 3.572984094598434 grad: -0.414490076965245
iteration: 390 loss: 3.4844387759422633 grad: -0.40486516070333145
iteration: 400 loss: 3.4001333968587293 grad: -0.395670788530818
iteration: 410 loss: 3.3197727660800327 grad: -0.38687892900696064
iteration: 420 loss: 3.2430883284210226 grad: -0.378463925735567
iteration: 430 loss: 3.169835238308389 grad: -0.37040225071876265
iteration: 440 loss: 3.0997898097606265 grad: -0.36267228794202944
iteration: 450 loss: 3.032747287618804 grad: -0.3552541429343084
iteration: 460 loss: 2.9685198938813384 grad: -0.34812947472032435
iteration: 470 loss: 2.9069351104125087 grad: -0.34128134713823743
iteration: 480 loss: 2.8478341653789854 grad: -0.33469409695637553
iteration: 490 loss: 2.7910706958342746 grad: -0.3283532166059709
iteration: 0 loss: 75.21484219671612 grad: 39.81836795805346
iteration: 10 loss: 44.717926901467195 grad: 5.11911725941061
iteration: 20 loss: 33.930820591027235 grad: -0.5434729975012067
iteration: 30 loss: 27.60942904989435 grad: -1.4291437332886807
iteration: 40 loss: 23.339890751834158 grad: -1.5571800122684003
iteration: 50 loss: 20.23047363624623 grad: -1.5172378030208487
iteration: 60 loss: 17.853601784904082 grad: -1.4356283308329711
iteration: 70 loss: 15.973358733625941 grad: -1.3470271146519561
iteration: 80 loss: 14.447139004171607 grad: -1.2620761743781403
iteration: 90 loss: 13.182963874153096 grad: -1.1838929190908631
iteration: 100 loss: 12.118540975564285 grad: -1.1130044231254699
iteration: 110 loss: 11.210039806725677 grad: -1.0490310119110124
iteration: 120 loss: 10.4256549614778 grad: -0.991308868788844
iteration: 130 loss: 9.741712410847619 grad: -0.9391283087004403
iteration: 140 loss: 9.140209584404436 grad: -0.891822796261192
iteration: 150 loss: 8.607204203544638 grad: -0.848797681256793
iteration: 160 loss: 8.131726373425325 grad: -0.8095344556073956
iteration: 170 loss: 7.705024569214981 grad: -0.7735855267827347
iteration: 180 loss: 7.3200310817848155 grad: -0.7405658987478858
iteration: 190 loss: 6.97097546607996 grad: -0.7101444524058584
iteration: 200 loss: 6.653100080249922 grad: -0.6820358902095384
iteration: 210 loss: 6.362447466317421 grad: -0.6559936910075879
iteration: 220 loss: 6.095699193723993 grad: -0.6318041113470043
iteration: 230 loss: 5.850052161014818 grad: -0.6092811456387024
iteration: 240 loss: 5.623122557694511 grad: -0.5882623179235256
iteration: 250 loss: 5.412870519868772 grad: -0.5686051751199259
iteration: 260 loss: 5.217540453391683 grad: -0.5501843635282477
iteration: 270 loss: 5.035613349241388 grad: -0.5328891868670567
iteration: 280 loss: 4.865768370603498 grad: -0.516621560750977
iteration: 290 loss: 4.706851675051191 grad: -0.5012942935013016
iteration: 300 loss: 4.55785093128361 grad: -0.48682963597312945
iteration: 310 loss: 4.417874353824862 grad: -0.47315805370106334
iteration: 320 loss: 4.286133348992528 grad: -0.46021718334331796
iteration: 330 loss: 4.161928067557628 grad: -0.44795094243721767
iteration: 340 loss: 4.0446353123288645 grad: -0.43630876715704975
iteration: 350 loss: 3.9336983653380146 grad: -0.42524495734379075
iteration: 360 loss: 3.8286183888744745 grad: -0.4147181117706862
iteration: 370 loss: 3.7289471239134233 grad: -0.4046906395951544
iteration: 380 loss: 3.634280663588718 grad: -0.3951283363679473
iteration: 390 loss: 3.5442541218100234 grad: -0.38600001493829095
iteration: 400 loss: 3.4585370506955853 grad: -0.37727718319885606
iteration: 410 loss: 3.3768294871599456 grad: -0.36893376192849603
iteration: 420 loss: 3.2988585303468096 grad: -0.36094583707026895
iteration: 430 loss: 3.224375368731795 grad: -0.35329144167257065
iteration: 440 loss: 3.153152689593381 grad: -0.3459503634579677
iteration: 450 loss: 3.0849824147979814 grad: -0.33890397459626054
iteration: 460 loss: 3.0196737160314857 grad: -0.33213508076832404
iteration: 470 loss: 2.9570512701347154 grad: -0.3256277870337638
iteration: 480 loss: 2.896953721404977 grad: -0.319367378373293
iteration: 490 loss: 2.8392323228216894 grad: -0.31334021307792326
iteration: 0 loss: 74.76792181002197 grad: 45.30901468042513
iteration: 10 loss: 44.86784045657941 grad: 5.074398190214792
iteration: 20 loss: 34.06451328460883 grad: -1.375366778387233
iteration: 30 loss: 27.70829031555296 grad: -1.9751358093855398
iteration: 40 loss: 23.412295046129223 grad: -1.8989873922736473
iteration: 50 loss: 20.284604614334636 grad: -1.7424533895940497
iteration: 60 loss: 17.89496367798382 grad: -1.5928581802971236
iteration: 70 loss: 16.005564757090973 grad: -1.4626524545078208
iteration: 80 loss: 14.472608629036047 grad: -1.35102106034434
iteration: 90 loss: 13.20335867571897 grad: -1.255012388033891
iteration: 100 loss: 12.135030202091663 grad: -1.1717845806314662
iteration: 110 loss: 11.223465289152871 grad: -1.0989993787682861
iteration: 120 loss: 10.436636001179316 grad: -1.0348063239030871
iteration: 130 loss: 9.75071395605844 grad: -0.9777505503973876
iteration: 140 loss: 9.147587396593446 grad: -0.9266829303509272
iteration: 150 loss: 8.613235304741528 grad: -0.8806879555019987
iteration: 160 loss: 8.136629918602631 grad: -0.8390289893316984
iteration: 170 loss: 7.708976398355844 grad: -0.8011073400666261
iteration: 180 loss: 7.3231740421198 grad: -0.7664316698921485
iteration: 190 loss: 6.973426863802615 grad: -0.7345949917859268
iteration: 200 loss: 6.654957155358798 grad: -0.7052572270535877
iteration: 210 loss: 6.363791484580485 grad: -0.6781318601935402
iteration: 220 loss: 6.096598555268139 grad: -0.6529756372909556
iteration: 230 loss: 5.850564797763251 grad: -0.6295805449464646
iteration: 240 loss: 5.623297807828914 grad: -0.607767512608525
iteration: 250 loss: 5.412750611422656 grad: -0.5873814278002947
iteration: 260 loss: 5.217161691225969 grad: -0.5682871591625006
iteration: 270 loss: 5.035007073764188 grad: -0.5503663588147435
iteration: 280 loss: 4.864961738593634 grad: -0.5335148716819721
iteration: 290 loss: 4.705868300283038 grad: -0.5176406209267337
iteration: 300 loss: 4.5567114135474185 grad: -0.5026618695057661
iteration: 310 loss: 4.416596718327913 grad: -0.4885057809862987
iteration: 320 loss: 4.284733413238428 grad: -0.475107220159054
iteration: 330 loss: 4.160419749134979 grad: -0.46240774715349764
iteration: 340 loss: 4.04303088825604 grad: -0.45035476877862346
iteration: 350 loss: 3.9320086914816867 grad: -0.4389008184745021
iteration: 360 loss: 3.8268530862870977 grad: -0.4280029421528833
iteration: 370 loss: 3.727114737644942 grad: -0.41762217176412153
iteration: 380 loss: 3.632388798493759 grad: -0.40772307197593066
iteration: 390 loss: 3.542309559053716 grad: -0.39827334812820603
iteration: 400 loss: 3.456545848006127 grad: -0.3892435058179676
iteration: 410 loss: 3.374797065348792 grad: -0.38060655420518397
iteration: 420 loss: 3.2967897481832384 grad: -0.3723377465162764
iteration: 430 loss: 3.2222745879158454 grad: -0.36441435233518416
iteration: 440 loss: 3.151023831279996 grad: -0.3568154571712444
iteration: 450 loss: 3.0828290088953425 grad: -0.3495217855240744
iteration: 460 loss: 3.017498944298454 grad: -0.3425155442631602
iteration: 470 loss: 2.9548580039477614 grad: -0.3357802836309184
iteration: 480 loss: 2.894744554919436 grad: -0.3293007735837249
iteration: 490 loss: 2.8370096021650086 grad: -0.32306289352228834
iteration: 0 loss: 77.402649875842 grad: 28.87183169968074
iteration: 10 loss: 46.307796641343515 grad: 7.037430900954121
iteration: 20 loss: 34.7896280426379 grad: 0.7627775823092886
iteration: 30 loss: 28.193439445826865 grad: -0.7527878009536234
iteration: 40 loss: 23.77124631766649 grad: -1.147104044136055
iteration: 50 loss: 20.563358387368638 grad: -1.2302983993411754
iteration: 60 loss: 18.11831243452124 grad: -1.2155206029587398
iteration: 70 loss: 16.188818647142128 grad: -1.1684961768029076
iteration: 80 loss: 14.62593734604562 grad: -1.1120537042368406
iteration: 90 loss: 13.333829228043193 grad: -1.054740182051542
iteration: 100 loss: 12.24770955517647 grad: -0.9998230108290085
iteration: 110 loss: 11.32207294700104 grad: -0.9484633995511449
iteration: 120 loss: 10.523954260777744 grad: -0.9009420087518871
iteration: 130 loss: 9.828857209348804 grad: -0.8571671402381258
iteration: 140 loss: 9.218184731155237 grad: -0.8168966512327326
iteration: 150 loss: 8.677557474785523 grad: -0.7798380042400151
iteration: 160 loss: 8.19567943197107 grad: -0.7456937312310621
iteration: 170 loss: 7.763552502993207 grad: -0.7141815089644904
iteration: 180 loss: 7.373920267766152 grad: -0.6850421425028501
iteration: 190 loss: 7.020866228191277 grad: -0.658041751124621
iteration: 200 loss: 6.699518520457042 grad: -0.6329712239021088
iteration: 210 loss: 6.405829484155427 grad: -0.6096444680428851
iteration: 220 loss: 6.136408801369229 grad: -0.58789621115398
iteration: 230 loss: 5.888395585921899 grad: -0.5675797336838168
iteration: 240 loss: 5.6593592020553425 grad: -0.548564710280572
iteration: 250 loss: 5.447221551710424 grad: -0.530735236803479
iteration: 260 loss: 5.25019559660545 grad: -0.5139880673741228
iteration: 270 loss: 5.066736291955873 grad: -0.49823105967031384
iteration: 280 loss: 4.895501104837711 grad: -0.48338181435677907
iteration: 290 loss: 4.73531800318762 grad: -0.4693664895753448
iteration: 300 loss: 4.58515931812264 grad: -0.45611877026026026
iteration: 310 loss: 4.44412026096585 grad: -0.4435789727926407
iteration: 320 loss: 4.31140115693283 grad: -0.4316932671275021
iteration: 330 loss: 4.186292667331062 grad: -0.4204130004634855
iteration: 340 loss: 4.068163430623454 grad: -0.40969410849006543
iteration: 350 loss: 3.956449673417286 grad: -0.39949660209721816
iteration: 360 loss: 3.8506464351280143 grad: -0.3897841191059387
iteration: 370 loss: 3.7503001217652456 grad: -0.3805235320557132
iteration: 380 loss: 3.6550021601766605 grad: -0.37168460437063877
iteration: 390 loss: 3.5643835679032403 grad: -0.36323968833371856
iteration: 400 loss: 3.4781102884194297 grad: -0.35516345924780235
iteration: 410 loss: 3.3958791690195294 grad: -0.34743268097139246
iteration: 420 loss: 3.3174144805720145 grad: -0.3400259987070525
iteration: 430 loss: 3.2424648960013656 grad: -0.33292375550663933
iteration: 440 loss: 3.1708008586146703 grad: -0.3261078294563069
iteration: 450 loss: 3.102212282932049 grad: -0.31956148892847774
iteration: 460 loss: 3.0365065401185594 grad: -0.313269263649109
iteration: 470 loss: 2.973506687825193 grad: -0.30721682963634933
iteration: 480 loss: 2.9130499106020333 grad: -0.30139090632927173
iteration: 490 loss: 2.8549861422755964 grad: -0.2957791644497788
iteration: 0 loss: 77.29760933907716 grad: 28.948679349682518
iteration: 10 loss: 45.93845225088482 grad: 6.961528557684082
iteration: 20 loss: 34.57625552314595 grad: 0.7228470750188256
iteration: 30 loss: 28.051638724404544 grad: -0.7700829864374723
iteration: 40 loss: 23.66488424961147 grad: -1.1545599075387183
iteration: 50 loss: 20.476731738503698 grad: -1.2326839916216168
iteration: 60 loss: 18.04388779667488 grad: -1.2147631075703151
iteration: 70 loss: 16.122650339831168 grad: -1.165621790346325
iteration: 80 loss: 14.565790257046418 grad: -1.1077541748963204
iteration: 90 loss: 13.278342979492994 grad: -1.0495292702784487
iteration: 100 loss: 12.19600006090435 grad: -0.9940862183615644
iteration: 110 loss: 11.273530926388917 grad: -0.9424835184870156
iteration: 120 loss: 10.47813774723419 grad: -0.8949196438309116
iteration: 130 loss: 9.785431295044212 grad: -0.8512390923001819
iteration: 140 loss: 9.176886208040493 grad: -0.8111518210850166
iteration: 150 loss: 8.638173190460913 grad: -0.7743304188328941
iteration: 160 loss: 8.158032514624045 grad: -0.7404527246669703
iteration: 170 loss: 7.727493272317133 grad: -0.7092193947194455
iteration: 180 loss: 7.339320032747723 grad: -0.6803598339688397
iteration: 190 loss: 6.9876129199336985 grad: -0.6536327873315881
iteration: 200 loss: 6.667513526048217 grad: -0.628824598562859
iteration: 210 loss: 6.3749852886711444 grad: -0.6057465831374009
iteration: 220 loss: 6.106647188164226 grad: -0.5842322019870105
iteration: 230 loss: 5.859646232422702 grad: -0.5641343492534456
iteration: 240 loss: 5.631558562509946 grad: -0.545322882828342
iteration: 250 loss: 5.420311952856206 grad: -0.5276824365109093
iteration: 260 loss: 5.224124494662647 grad: -0.511110510208126
iteration: 270 loss: 5.041455654211362 grad: -0.49551581613818513
iteration: 280 loss: 4.8709668891352855 grad: -0.4808168524678337
iteration: 290 loss: 4.7114897155757225 grad: -0.4669406750655747
iteration: 300 loss: 4.561999633758412 grad: -0.4538218399336188
iteration: 310 loss: 4.4215946968552915 grad: -0.4414014917920209
iteration: 320 loss: 4.289477787614198 grad: -0.42962657743587745
iteration: 330 loss: 4.164941876471651 grad: -0.41844916549748606
iteration: 340 loss: 4.0473576929158215 grad: -0.40782585696293794
iteration: 350 loss: 3.9361633622210817 grad: -0.3977172731690963
iteration: 360 loss: 3.830855652135969 grad: -0.3880876100473979
iteration: 370 loss: 3.7309825456277808 grad: -0.37890424911418674
iteration: 380 loss: 3.6361369115235735 grad: -0.3701374171703522
iteration: 390 loss: 3.545951088618949 grad: -0.36175988790376273
iteration: 400 loss: 3.460092233367794 grad: -0.35374671962166254
iteration: 410 loss: 3.3782583086717337 grad: -0.34607502420805714
iteration: 420 loss: 3.3001746132311136 grad: -0.3387237631301464
iteration: 430 loss: 3.225590768494633 grad: -0.3316735669309341
iteration: 440 loss: 3.154278094480064 grad: -0.3249065751614621
iteration: 450 loss: 3.0860273172603145 grad: -0.31840629414173954
iteration: 460 loss: 3.020646560317759 grad: -0.31215747030766267
iteration: 470 loss: 2.9579595796738185 grad: -0.3061459772131495
iteration: 480 loss: 2.897804209024881 grad: -0.3003587145214892
iteration: 490 loss: 2.840030986362804 grad: -0.29478351754518883
iteration: 0 loss: 75.66172639742825 grad: 36.11378586420687
iteration: 10 loss: 42.930525473026734 grad: 6.296103529603679
iteration: 20 loss: 32.33091900615603 grad: -0.5026521523249448
iteration: 30 loss: 26.301850089214813 grad: -1.6076407950726206
iteration: 40 loss: 22.25096659924948 grad: -1.7164034379381155
iteration: 50 loss: 19.304134895543278 grad: -1.6320688013479012
iteration: 60 loss: 17.05155379185637 grad: -1.5162327132453974
iteration: 70 loss: 15.26888642850062 grad: -1.404814794249795
iteration: 80 loss: 13.821008056604093 grad: -1.305094863469225
iteration: 90 loss: 12.620919412184652 grad: -1.2173111207445786
iteration: 100 loss: 11.60975300523021 grad: -1.14007550772198
iteration: 110 loss: 10.746105195296671 grad: -1.0718105774272004
iteration: 120 loss: 9.999935507657586 grad: -1.0111125111188712
iteration: 130 loss: 9.348882553660815 grad: -0.9568118709052937
iteration: 140 loss: 8.77593908649357 grad: -0.9079527786847001
iteration: 150 loss: 8.267930316466783 grad: -0.8637556047357089
iteration: 160 loss: 7.814486910411853 grad: -0.8235816357468884
iteration: 170 loss: 7.407333419616035 grad: -0.7869039253114607
iteration: 180 loss: 7.039783919382899 grad: -0.7532843201183089
iteration: 190 loss: 6.706377334694728 grad: -0.7223556160041914
iteration: 200 loss: 6.402609089693794 grad: -0.693807723555215
iteration: 210 loss: 6.1247305243375445 grad: -0.6673769009119266
iteration: 220 loss: 5.869596847066284 grad: -0.6428373199745648
iteration: 230 loss: 5.634550411537765 grad: -0.6199944106841233
iteration: 240 loss: 5.417330076567148 grad: -0.5986795669943724
iteration: 250 loss: 5.21600008033304 grad: -0.5787459025182835
iteration: 260 loss: 5.028893690065698 grad: -0.5600648212280984
iteration: 270 loss: 4.854568162484569 grad: -0.5425232258230073
iteration: 280 loss: 4.691768450420981 grad: -0.5260212287878906
iteration: 290 loss: 4.539397735758815 grad: -0.5104702627265932
iteration: 300 loss: 4.396493336372012 grad: -0.49579151016835243
iteration: 310 loss: 4.2622068777484605 grad: -0.481914590824438
iteration: 320 loss: 4.135787874329873 grad: -0.4687764577428613
iteration: 330 loss: 4.016570056096911 grad: -0.4563204640839107
iteration: 340 loss: 3.9039599199094153 grad: -0.4444955701294626
iteration: 350 loss: 3.7974270949138456 grad: -0.4332556662392142
iteration: 360 loss: 3.6964961957047224 grad: -0.4225589922151889
iteration: 370 loss: 3.600739902313349 grad: -0.41236763725656184
iteration: 380 loss: 3.50977305706899 grad: -0.4026471076215312
iteration: 390 loss: 3.423247608445515 grad: -0.3933659514426425
iteration: 400 loss: 3.340848263650018 grad: -0.38449543200284736
iteration: 410 loss: 3.2622887368904476 grad: -0.3760092422750032
iteration: 420 loss: 3.1873085003890496 grad: -0.36788325473623407
iteration: 430 loss: 3.1156699614038685 grad: -0.360095301450934
iteration: 440 loss: 3.0471560015955053 grad: -0.352624980218895
iteration: 450 loss: 2.9815678257217026 grad: -0.34545348324421493
iteration: 460 loss: 2.918723075304239 grad: -0.33856344532458704
iteration: 470 loss: 2.858454170032887 grad: -0.33193880901147665
iteration: 480 loss: 2.8006068455184834 grad: -0.32556470456711517
iteration: 490 loss: 2.7450388608600735 grad: -0.3194273428580908
iteration: 0 loss: 75.54662562814636 grad: 35.528709884748324
iteration: 10 loss: 43.728875126924805 grad: 5.941776280265604
iteration: 20 loss: 33.16144050982409 grad: -0.5305285481284765
iteration: 30 loss: 27.023485561215196 grad: -1.507068517013296
iteration: 40 loss: 22.865752448029692 grad: -1.586159965771997
iteration: 50 loss: 19.83056601521488 grad: -1.5035698670568474
iteration: 60 loss: 17.507053850750914 grad: -1.3979506945753746
iteration: 70 loss: 15.667442703058338 grad: -1.297995045019377
iteration: 80 loss: 14.17344676472294 grad: -1.2088667554014207
iteration: 90 loss: 12.93557841632074 grad: -1.1303325462278546
iteration: 100 loss: 11.893091890197246 grad: -1.0610407411916822
iteration: 110 loss: 11.003178891547536 grad: -0.999583557729603
iteration: 120 loss: 10.234745250316832 grad: -0.9447402004744
iteration: 130 loss: 9.564632608372222 grad: -0.895503328503181
iteration: 140 loss: 8.97522464655425 grad: -0.8510520240792973
iteration: 150 loss: 8.452875921217913 grad: -0.8107166949445732
iteration: 160 loss: 7.98684935179296 grad: -0.7739478467253005
iteration: 170 loss: 7.568579034573056 grad: -0.7402909169716028
iteration: 180 loss: 7.191147258662155 grad: -0.7093666646100243
iteration: 190 loss: 6.848906163354224 grad: -0.6808560395963474
iteration: 200 loss: 6.537199256664115 grad: -0.6544885052465532
iteration: 210 loss: 6.252153247798891 grad: -0.6300329780928202
iteration: 220 loss: 5.990520266160894 grad: -0.607290744788767
iteration: 230 loss: 5.7495567617310455 grad: -0.5860898761625477
iteration: 240 loss: 5.526929493566678 grad: -0.5662807817278506
iteration: 250 loss: 5.32064178347239 grad: -0.547732639668216
iteration: 260 loss: 5.128975111330254 grad: -0.5303305047056635
iteration: 270 loss: 4.950442451824859 grad: -0.5139729456088687
iteration: 280 loss: 4.783750687698847 grad: -0.49857010026897297
iteration: 290 loss: 4.627770104808778 grad: -0.4840420629019734
iteration: 300 loss: 4.481509460318813 grad: -0.4703175376641612
iteration: 310 loss: 4.344095471973395 grad: -0.45733270768919715
iteration: 320 loss: 4.214755840802065 grad: -0.4450302796316834
iteration: 330 loss: 4.0928051176038025 grad: -0.43335867220660607
iteration: 340 loss: 3.9776328731839965 grad: -0.4222713236464892
iteration: 350 loss: 3.8686937463869735 grad: -0.41172609796368154
iteration: 360 loss: 3.7654990316081323 grad: -0.40168477377057876
iteration: 370 loss: 3.6676095353422666 grad: -0.39211260244395957
iteration: 380 loss: 3.574629484245923 grad: -0.3829779248184645
iteration: 390 loss: 3.4862013087571624 grad: -0.3742518375052625
iteration: 400 loss: 3.402001159136944 grad: -0.3659079014648518
iteration: 410 loss: 3.32173503691361 grad: -0.35792188670055364
iteration: 420 loss: 3.2451354455659964 grad: -0.35027154794458326
iteration: 430 loss: 3.1719584810733648 grad: -0.3429364270298348
iteration: 440 loss: 3.1019812964972187 grad: -0.335897678315054
iteration: 450 loss: 3.034999885788321 grad: -0.32913791408782084
iteration: 460 loss: 2.9708271409708273 grad: -0.32264106733150777
iteration: 470 loss: 2.9092911442426157 grad: -0.3163922696269629
iteration: 480 loss: 2.850233662561591 grad: -0.3103777422813736
iteration: 490 loss: 2.7935088173119103 grad: -0.3045846990468757
iteration: 0 loss: 72.96474762337473 grad: 47.88885678197981
iteration: 10 loss: 42.74175797529351 grad: 3.4994231987393505
iteration: 20 loss: 32.63006023403229 grad: -1.6411570631559462
iteration: 30 loss: 26.64648564080899 grad: -1.9437514431292144
iteration: 40 loss: 22.575172477538196 grad: -1.8139412768683743
iteration: 50 loss: 19.595628575146232 grad: -1.6552208909454165
iteration: 60 loss: 17.31046702183719 grad: -1.5146741700023263
iteration: 70 loss: 15.49851148506803 grad: -1.3950245421363894
iteration: 80 loss: 14.025152355343629 grad: -1.2930029622110217
iteration: 90 loss: 12.803118124691393 grad: -1.2051741659853628
iteration: 100 loss: 11.773073922116902 grad: -1.1287635769904623
iteration: 110 loss: 10.89314663090799 grad: -1.0616328988456716
iteration: 120 loss: 10.132881363686073 grad: -1.0021384851563835
iteration: 130 loss: 9.469567571021033 grad: -0.9490049635335923
iteration: 140 loss: 8.88590894261502 grad: -0.9012302688310061
iteration: 150 loss: 8.36849261622404 grad: -0.8580169259971232
iteration: 160 loss: 7.90675307886055 grad: -0.818722457122976
iteration: 170 loss: 7.4922527049882115 grad: -0.7828232845389118
iteration: 180 loss: 7.118170913099427 grad: -0.7498881466467069
iteration: 190 loss: 6.778934280176889 grad: -0.7195582803708727
iteration: 200 loss: 6.469944034513736 grad: -0.6915324758934257
iteration: 210 loss: 6.187372159333749 grad: -0.6655556832324299
iteration: 220 loss: 5.928006697675342 grad: -0.6414102378262013
iteration: 230 loss: 5.689132903806157 grad: -0.6189090372710646
iteration: 240 loss: 5.468440888635459 grad: -0.5978901853348517
iteration: 250 loss: 5.263953103637298 grad: -0.5782127490533371
iteration: 260 loss: 5.073966857446356 grad: -0.5597533673530981
iteration: 270 loss: 4.897008348312531 grad: -0.5424035165377089
iteration: 280 loss: 4.731795607192453 grad: -0.5260672867004582
iteration: 290 loss: 4.577208399698024 grad: -0.5106595588745977
iteration: 300 loss: 4.432263609383798 grad: -0.4961044991227975
iteration: 310 loss: 4.296094973050871 grad: -0.48233430536537425
iteration: 320 loss: 4.167936297143817 grad: -0.46928815738653046
iteration: 330 loss: 4.047107477956326 grad: -0.4569113314611651
iteration: 340 loss: 3.933002794840873 grad: -0.44515444936881576
iteration: 350 loss: 3.825081057382704 grad: -0.43397283790165625
iteration: 360 loss: 3.722857273451609 grad: -0.423325979837477
iteration: 370 loss: 3.625895571653845 grad: -0.41317704110760506
iteration: 380 loss: 3.5338031637156746 grad: -0.40349246181656695
iteration: 390 loss: 3.44622517317744 grad: -0.39424160106634987
iteration: 400 loss: 3.3628401890896797 grad: -0.38539642735276225
iteration: 410 loss: 3.283356429109641 grad: -0.37693124774592646
iteration: 420 loss: 3.2075084169575594 grad: -0.3688224702250637
iteration: 430 loss: 3.135054095734507 grad: -0.3610483944724425
iteration: 440 loss: 3.065772311984264 grad: -0.3535890271906713
iteration: 450 loss: 2.999460616241942 grad: -0.3464259186281131
iteration: 460 loss: 2.9359333346937175 grad: -0.3395420175073762
iteration: 470 loss: 2.8750198738329016 grad: -0.332921541973479
iteration: 480 loss: 2.8165632259998263 grad: -0.3265498645286155
iteration: 490 loss: 2.760418648640759 grad: -0.3204134092128578
iteration: 0 loss: 76.41182250778284 grad: 32.6706642195377
iteration: 10 loss: 44.83800282926813 grad: 7.31842949266006
iteration: 20 loss: 33.843478104450995 grad: 0.1562970213675065
iteration: 30 loss: 27.544468126211108 grad: -1.3108786017112595
iteration: 40 loss: 23.293647784065218 grad: -1.5727228300940777
iteration: 50 loss: 20.193787294050193 grad: -1.5519546301292364
iteration: 60 loss: 17.821756115189807 grad: -1.4643177648824188
iteration: 70 loss: 15.94421953920069 grad: -1.3665473518538547
iteration: 80 loss: 14.419775185348271 grad: -1.2741255991308242
iteration: 90 loss: 13.156975922335256 grad: -1.1906963445060246
iteration: 100 loss: 12.093756013793525 grad: -1.116361361610279
iteration: 110 loss: 11.186376503908997 grad: -1.0502265110942945
iteration: 120 loss: 10.403065233964877 grad: -0.9912155051824717
iteration: 130 loss: 9.720157438836127 grad: -0.9383231938443863
iteration: 140 loss: 9.119650899949441 grad: -0.890680519001365
iteration: 150 loss: 8.587601168870172 grad: -0.8475567387355516
iteration: 160 loss: 8.113036175677196 grad: -0.8083428242760095
iteration: 170 loss: 7.687203045378779 grad: -0.7725313914393317
iteration: 180 loss: 7.303033676438232 grad: -0.7396982289052152
iteration: 190 loss: 6.954758064095503 grad: -0.7094867431400184
iteration: 200 loss: 6.637619644976067 grad: -0.6815953261989302
iteration: 210 loss: 6.347662488855928 grad: -0.6557672486718371
iteration: 220 loss: 6.081569984125426 grad: -0.6317826103339516
iteration: 230 loss: 5.83654101652122 grad: -0.6094519247839936
iteration: 240 loss: 5.610193839593111 grad: -0.5886109874353376
iteration: 250 loss: 5.4004906647023745 grad: -0.5691167481937048
iteration: 260 loss: 5.205677938508333 grad: -0.5508439715158664
iteration: 270 loss: 5.024238627719487 grad: -0.533682515784541
iteration: 280 loss: 4.8548537866348935 grad: -0.5175351023119632
iteration: 290 loss: 4.696371367832646 grad: -0.5023154737689848
iteration: 300 loss: 4.547780733149714 grad: -0.48794686434636514
iteration: 310 loss: 4.408191686668984 grad: -0.47436072111314076
iteration: 320 loss: 4.276817121752381 grad: -0.46149562913459813
iteration: 330 loss: 4.152958576635353 grad: -0.4492964029439832
iteration: 340 loss: 4.035994146135388 grad: -0.4377133146810244
iteration: 350 loss: 3.9253683136968323 grad: -0.4267014351822825
iteration: 360 loss: 3.820583357655282 grad: -0.4162200689562982
iteration: 370 loss: 3.721192055046934 grad: -0.4062322676166867
iteration: 380 loss: 3.626791460432452 grad: -0.39670440921540506
iteration: 390 loss: 3.5370175797381553 grad: -0.38760583319415826
iteration: 400 loss: 3.451540792696795 grad: -0.37890852248833373
iteration: 410 loss: 3.3700619041995776 grad: -0.37058682577648394
iteration: 420 loss: 3.29230872621098 grad: -0.36261721404663505
iteration: 430 loss: 3.218033109068239 grad: -0.3549780666078422
iteration: 440 loss: 3.1470083548634977 grad: -0.34764948245712196
iteration: 450 loss: 3.0790269568516866 grad: -0.34061311355365953
iteration: 460 loss: 3.0138986180428136 grad: -0.3338520170815948
iteration: 470 loss: 2.9514485096355285 grad: -0.32735052422147287
iteration: 480 loss: 2.8915157361727157 grad: -0.3210941233157014
iteration: 490 loss: 2.8339519793979973 grad: -0.31506935561868754
iteration: 0 loss: 73.00660818339382 grad: 43.19799565870545
iteration: 10 loss: 41.13510195376463 grad: 4.618613186867798
iteration: 20 loss: 31.396187423116825 grad: -1.5040926611243541
iteration: 30 loss: 25.731629679157262 grad: -2.0748461246887566
iteration: 40 loss: 21.878078356626578 grad: -1.9771911759895993
iteration: 50 loss: 19.048269725113723 grad: -1.8038128246093184
iteration: 60 loss: 16.86881349700624 grad: -1.6421764969687014
iteration: 70 loss: 15.133613934760763 grad: -1.5033677461621742
iteration: 80 loss: 13.717499308064259 grad: -1.3854213422195798
iteration: 90 loss: 12.539213473401189 grad: -1.2846317672352447
iteration: 100 loss: 11.543345829407723 grad: -1.197681163641112
iteration: 110 loss: 10.69064509263692 grad: -1.1219261847086832
iteration: 120 loss: 9.952448996322367 grad: -1.0553155056656873
iteration: 130 loss: 9.307299966395435 grad: -0.9962583717470328
iteration: 140 loss: 8.738795926555833 grad: -0.9435113218182232
iteration: 150 loss: 8.234174843620911 grad: -0.8960916651137913
iteration: 160 loss: 7.783353613637163 grad: -0.8532136390731175
iteration: 170 loss: 7.378258278528185 grad: -0.8142416427701962
iteration: 180 loss: 7.0123467017896965 grad: -0.7786558602489176
iteration: 190 loss: 6.680261702977615 grad: -0.7460267868364898
iteration: 200 loss: 6.3775746327167075 grad: -0.715996161660365
iteration: 210 loss: 6.100592897403906 grad: -0.6882625395294415
iteration: 220 loss: 5.846213502469047 grad: -0.6625702524482602
iteration: 230 loss: 5.611810234373521 grad: -0.6387008724397656
iteration: 240 loss: 5.395145781602216 grad: -0.6164665394161123
iteration: 250 loss: 5.194302582863904 grad: -0.5957046942650095
iteration: 260 loss: 5.00762790273794 grad: -0.5762738816113002
iteration: 270 loss: 4.833689832108534 grad: -0.5580503749850838
iteration: 280 loss: 4.671241760070064 grad: -0.5409254403759935
iteration: 290 loss: 4.519193474684264 grad: -0.5248030998921379
iteration: 300 loss: 4.376587494474816 grad: -0.5095982906289046
iteration: 310 loss: 4.242579559783073 grad: -0.4952353384500793
iteration: 320 loss: 4.116422456522204 grad: -0.4816466846712252
iteration: 330 loss: 3.997452527703734 grad: -0.46877181735308554
iteration: 340 loss: 3.8850783667140933 grad: -0.4565563692919739
iteration: 350 loss: 3.7787712922442798 grad: -0.44495135271320135
iteration: 360 loss: 3.678057286409016 grad: -0.4339125067645614
iteration: 370 loss: 3.5825101409497866 grad: -0.4233997386282986
iteration: 380 loss: 3.4917456059461074 grad: -0.4133766427571063
iteration: 390 loss: 3.4054163744302226 grad: -0.4038100856398129
iteration: 400 loss: 3.323207767164769 grad: -0.3946698457990411
iteration: 410 loss: 3.244834006419048 grad: -0.38592830055383703
iteration: 420 loss: 3.1700349872735387 grad: -0.377560152548599
iteration: 430 loss: 3.0985734708279282 grad: -0.3695421902343783
iteration: 440 loss: 3.030232636528953 grad: -0.3618530774498909
iteration: 450 loss: 2.9648139412696435 grad: -0.35447316803375906
iteration: 460 loss: 2.902135241431758 grad: -0.34738434204245033
iteration: 470 loss: 2.8420291410503027 grad: -0.34056986067810735
iteration: 480 loss: 2.784341535039361 grad: -0.3340142374689106
iteration: 490 loss: 2.72893032119344 grad: -0.32770312360910503
iteration: 0 loss: 77.25927825512055 grad: 31.04774339866348
iteration: 10 loss: 46.656276810145556 grad: 6.47252710940913
iteration: 20 loss: 35.1888722724793 grad: 0.37935056862009253
iteration: 30 loss: 28.53929586209603 grad: -0.9099679335661384
iteration: 40 loss: 24.064553603285834 grad: -1.2070617840743136
iteration: 50 loss: 20.814330067264624 grad: -1.2509128493968897
iteration: 60 loss: 18.335983203163075 grad: -1.2196111453674032
iteration: 70 loss: 16.380155085545766 grad: -1.1653742135538616
iteration: 80 loss: 14.796179179740731 grad: -1.1057854458468959
iteration: 90 loss: 13.486922174979037 grad: -1.0472284501185074
iteration: 100 loss: 12.386654959071683 grad: -0.9920093935940411
iteration: 110 loss: 11.449184640543391 grad: -0.9408289207765825
iteration: 120 loss: 10.641042009907757 grad: -0.893731309787385
iteration: 130 loss: 9.937356980506799 grad: -0.850496577889619
iteration: 140 loss: 9.3192515931372 grad: -0.810810265572417
iteration: 150 loss: 8.772132468386149 grad: -0.7743383846638991
iteration: 160 loss: 8.28453827134645 grad: -0.7407599226567945
iteration: 170 loss: 7.847341505369906 grad: -0.7097797750778443
iteration: 180 loss: 7.453183213069803 grad: -0.6811325644595283
iteration: 190 loss: 7.096064701083378 grad: -0.6545822895041228
iteration: 200 loss: 6.771047510268338 grad: -0.6299201877124125
iteration: 210 loss: 6.47402949090842 grad: -0.606961965390526
iteration: 220 loss: 6.201575335605619 grad: -0.5855449445137407
iteration: 230 loss: 5.950786701281066 grad: -0.5655253747958322
iteration: 240 loss: 5.719201526031653 grad: -0.5467760091415176
iteration: 250 loss: 5.504715157594877 grad: -0.5291839669295125
iteration: 260 loss: 5.305517972270784 grad: -0.512648874892951
iteration: 270 loss: 5.120045598087927 grad: -0.49708126060843405
iteration: 280 loss: 4.946938869207191 grad: -0.482401168869275
iteration: 290 loss: 4.785011363618651 grad: -0.46853697134259553
iteration: 300 loss: 4.633222901517167 grad: -0.4554243422125953
iteration: 310 loss: 4.490657766709526 grad: -0.44300537561924686
iteration: 320 loss: 4.356506698560762 grad: -0.43122782392999354
iteration: 330 loss: 4.23005191524515 grad: -0.42004443891349796
iteration: 340 loss: 4.11065459010735 grad: -0.4094124005906616
iteration: 350 loss: 3.9977443255311713 grad: -0.39929282088668955
iteration: 360 loss: 3.8908102628360295 grad: -0.38965031121289145
iteration: 370 loss: 3.7893935395311265 grad: -0.38045260480183785
iteration: 380 loss: 3.6930808619741087 grad: -0.37167022604446687
iteration: 390 loss: 3.6014990059648384 grad: -0.36327620027236873
iteration: 400 loss: 3.51431009294447 grad: -0.35524579842884707
iteration: 410 loss: 3.431207517332206 grad: -0.34755631191023495
iteration: 420 loss: 3.3519124228531094 grad: -0.34018685356133804
iteration: 430 loss: 3.2761706435611484 grad: -0.3331181813985707
iteration: 440 loss: 3.2037500397479395 grad: -0.3263325421303763
iteration: 450 loss: 3.1344381706278246 grad: -0.31981353196259343
iteration: 460 loss: 3.068040255246863 grad: -0.3135459725296346
iteration: 470 loss: 3.0043773808989003 grad: -0.3075158000914155
iteration: 480 loss: 2.9432849247641983 grad: -0.30170996638983305
iteration: 490 loss: 2.884611159788247 grad: -0.29611634977458345
iteration: 0 loss: 77.73622120416366 grad: 28.240546141009254
iteration: 10 loss: 46.843942647263816 grad: 7.640113342616486
iteration: 20 loss: 35.12633228587192 grad: 0.8775454477774309
iteration: 30 loss: 28.42354954319906 grad: -0.8133786888313617
iteration: 40 loss: 23.933426125785612 grad: -1.2388684313242049
iteration: 50 loss: 20.68094865774953 grad: -1.3142959363446713
iteration: 60 loss: 18.206154586430145 grad: -1.284509720164693
iteration: 70 loss: 16.256435715618657 grad: -1.2235412765642404
iteration: 80 loss: 14.679521441137302 grad: -1.1558471659199483
iteration: 90 loss: 13.37747642442086 grad: -1.0898311973676282
iteration: 100 loss: 12.284180132794704 grad: -1.0282536360457795
iteration: 110 loss: 11.353261765898464 grad: -0.9717856759189628
iteration: 120 loss: 10.551185493706457 grad: -0.9203166346641662
iteration: 130 loss: 9.853071975847673 grad: -0.8734608416234249
iteration: 140 loss: 9.2400625951718 grad: -0.8307598913983024
iteration: 150 loss: 8.69759646907501 grad: -0.7917628693931948
iteration: 160 loss: 8.21424964769087 grad: -0.7560559821520374
iteration: 170 loss: 7.780932551297982 grad: -0.7232709119762089
iteration: 180 loss: 7.3903224641131935 grad: -0.6930843707649502
iteration: 190 loss: 7.036454235927687 grad: -0.665214280742138
iteration: 200 loss: 6.714419877380558 grad: -0.6394149542861212
iteration: 210 loss: 6.42014460792052 grad: -0.6154722858007203
iteration: 220 loss: 6.150217539756423 grad: -0.5931993553760095
iteration: 230 loss: 5.9017620338555306 grad: -0.5724325679521565
iteration: 240 loss: 5.672335280619915 grad: -0.5530283308594894
iteration: 250 loss: 5.459849693119353 grad: -0.5348602237719067
iteration: 260 loss: 5.262510776801471 grad: -0.5178165994521552
iteration: 270 loss: 5.078767582530376 grad: -0.5017985528275578
iteration: 280 loss: 4.9072728674987465 grad: -0.486718201203285
iteration: 290 loss: 4.746850816117496 grad: -0.4724972257075807
iteration: 300 loss: 4.596470699572075 grad: -0.45906563151239593
iteration: 310 loss: 4.455225238328115 grad: -0.4463606912051825
iteration: 320 loss: 4.322312717193902 grad: -0.4343260416390736
iteration: 330 loss: 4.197022115825803 grad: -0.42291090963201505
iteration: 340 loss: 4.078720678444411 grad: -0.4120694460940653
iteration: 350 loss: 3.9668434689655525 grad: -0.40176015164445245
iteration: 360 loss: 3.860884551669518 grad: -0.39194537964874243
iteration: 370 loss: 3.7603895101446225 grad: -0.3825909049655475
iteration: 380 loss: 3.6649490737830672 grad: -0.37366554863183854
iteration: 390 loss: 3.5741936654299438 grad: -0.36514085031227694
iteration: 400 loss: 3.4877887187603434 grad: -0.3569907816540184
iteration: 410 loss: 3.405430641732106 grad: -0.34919149477575795
iteration: 420 loss: 3.3268433246156004 grad: -0.3417211010203717
iteration: 430 loss: 3.2517751089271356 grad: -0.3345594758484427
iteration: 440 loss: 3.1799961479270924 grad: -0.327688086372861
iteration: 450 loss: 3.1112961010196405 grad: -0.3210898385549373
iteration: 460 loss: 3.045482113874765 grad: -0.31474894151826266
iteration: 470 loss: 2.9823770438707387 grad: -0.30865078680261615
iteration: 480 loss: 2.921817896847601 grad: -0.302781840688618
iteration: 490 loss: 2.8636544464331397 grad: -0.2971295479843645
iteration: 0 loss: 75.89709715677081 grad: 40.319848044857245
iteration: 10 loss: 44.77236125270613 grad: 5.033618840051673
iteration: 20 loss: 34.02296700789645 grad: -0.7033811868717075
iteration: 30 loss: 27.719435228403118 grad: -1.460729724228873
iteration: 40 loss: 23.445288335897718 grad: -1.5167571084097151
iteration: 50 loss: 20.324349848330236 grad: -1.447629480608657
iteration: 60 loss: 17.935188857640366 grad: -1.3574636920599703
iteration: 70 loss: 16.043894998889257 grad: -1.2693498572023079
iteration: 80 loss: 14.508318105110304 grad: -1.1885793423606692
iteration: 90 loss: 13.236396879449536 grad: -1.1158499149099292
iteration: 100 loss: 12.16560152976924 grad: -1.05060850396293
iteration: 110 loss: 11.251848800828817 grad: -0.9920122387170405
iteration: 120 loss: 10.463110808442913 grad: -0.9392205436289348
iteration: 130 loss: 9.775531333041044 grad: -0.8914791617605913
iteration: 140 loss: 9.170963292838595 grad: -0.8481359898742593
iteration: 150 loss: 8.635351345328052 grad: -0.8086348695511947
iteration: 160 loss: 8.15763750243923 grad: -0.7725032966207129
iteration: 170 loss: 7.729001512333732 grad: -0.739339598110352
iteration: 180 loss: 7.342321803403096 grad: -0.7088013765857268
iteration: 190 loss: 6.9917854437450675 grad: -0.6805956532469003
iteration: 200 loss: 6.672601023630374 grad: -0.6544706562873851
iteration: 210 loss: 6.38078402889505 grad: -0.6302090497896804
iteration: 220 loss: 6.112994170701603 grad: -0.6076223719013879
iteration: 230 loss: 5.866410543493078 grad: -0.5865464684445326
iteration: 240 loss: 5.638634719042371 grad: -0.5668377382450441
iteration: 250 loss: 5.42761473981752 grad: -0.5483700375752671
iteration: 260 loss: 5.231584933303401 grad: -0.5310321190970566
iteration: 270 loss: 5.049017833597645 grad: -0.5147255044936735
iteration: 280 loss: 4.878585461448231 grad: -0.49936270964315477
iteration: 290 loss: 4.719127905211198 grad: -0.4848657571705871
iteration: 300 loss: 4.569627646659435 grad: -0.4711649240810584
iteration: 310 loss: 4.429188443455794 grad: -0.4581976824631642
iteration: 320 loss: 4.297017852904694 grad: -0.44590779945019776
iteration: 330 loss: 4.1724126858221515 grad: -0.4342445691458523
iteration: 340 loss: 4.054746833746363 grad: -0.42316215440583255
iteration: 350 loss: 3.943461030341744 grad: -0.4126190204941673
iteration: 360 loss: 3.8380541982608687 grad: -0.40257744592600075
iteration: 370 loss: 3.738076102732893 grad: -0.3930030984437648
iteration: 380 loss: 3.643121087704499 grad: -0.3838646661900822
iteration: 390 loss: 3.5528227132332506 grad: -0.37513353584753184
iteration: 400 loss: 3.466849146670525 grad: -0.3667835108979375
iteration: 410 loss: 3.3848991870880294 grad: -0.3587905642788287
iteration: 420 loss: 3.3066988239175275 grad: -0.3511326206343954
iteration: 430 loss: 3.2319982480438822 grad: -0.3437893641135874
iteration: 440 loss: 3.160569247593561 grad: -0.33674206829116315
iteration: 450 loss: 3.0922029319779707 grad: -0.32997344530390005
iteration: 460 loss: 3.026707737009102 grad: -0.32346751172394117
iteration: 470 loss: 2.9639076714981702 grad: -0.3172094690503715
iteration: 480 loss: 2.903640771973848 grad: -0.3111855970014036
iteration: 490 loss: 2.845757737320913 grad: -0.3053831580432406
iteration: 0 loss: 75.62461744755966 grad: 28.812331788343215
iteration: 10 loss: 43.05979496627287 grad: 6.995384868599052
iteration: 20 loss: 32.60898953881129 grad: 0.6500299827331313
iteration: 30 loss: 26.649891765163623 grad: -0.8989689371193718
iteration: 40 loss: 22.612745147158957 grad: -1.2822955853767684
iteration: 50 loss: 19.653836198034863 grad: -1.3421545605822478
iteration: 60 loss: 17.379257269069264 grad: -1.3049626813280253
iteration: 70 loss: 15.571873276574768 grad: -1.2397357400920133
iteration: 80 loss: 14.099685738242318 grad: -1.169313865919223
iteration: 90 loss: 12.876961165924865 grad: -1.101458086905654
iteration: 100 loss: 11.845248141905047 grad: -1.038611984768928
iteration: 110 loss: 10.963175779705642 grad: -0.9812617565906495
iteration: 120 loss: 10.200571951795878 grad: -0.9291767016257647
iteration: 130 loss: 9.534882818821892 grad: -0.881889113858863
iteration: 140 loss: 8.948896455439023 grad: -0.8388833074230585
iteration: 150 loss: 8.429243004367374 grad: -0.7996681321080876
iteration: 160 loss: 7.965376448883779 grad: -0.7638015601568988
iteration: 170 loss: 7.548865489485913 grad: -0.7308955448445211
iteration: 180 loss: 7.172888679261343 grad: -0.7006130963346334
iteration: 190 loss: 6.831867984524127 grad: -0.6726626964703131
iteration: 200 loss: 6.521198249002944 grad: -0.646792221286879
iteration: 210 loss: 6.237044409411944 grad: -0.6227832399066204
iteration: 220 loss: 5.976187412162992 grad: -0.6004459853638318
iteration: 230 loss: 5.735905686793393 grad: -0.5796150459329775
iteration: 240 loss: 5.513882946837012 grad: -0.5601457260267847
iteration: 250 loss: 5.30813573485346 grad: -0.5419109924276375
iteration: 260 loss: 5.116955948002256 grad: -0.5247989171261063
iteration: 270 loss: 4.938864851919169 grad: -0.508710535148204
iteration: 280 loss: 4.772575991833326 grad: -0.49355804663822167
iteration: 290 loss: 4.616965057196756 grad: -0.47926330370730413
iteration: 300 loss: 4.471045226763305 grad: -0.46575653279162993
iteration: 310 loss: 4.333946867156514 grad: -0.4529752520594061
iteration: 320 loss: 4.204900715138468 grad: -0.4408633507448995
iteration: 330 loss: 4.083223866753832 grad: -0.4293703033152137
iteration: 340 loss: 3.9683080426168336 grad: -0.4184504962847153
iteration: 350 loss: 3.8596097101672884 grad: -0.4080626494747529
iteration: 360 loss: 3.7566417295764207 grad: -0.3981693167436511
iteration: 370 loss: 3.6589662565696597 grad: -0.388736453828611
iteration: 380 loss: 3.566188687417004 grad: -0.379733043066225
iteration: 390 loss: 3.4779524722190134 grad: -0.37113076648832627
iteration: 400 loss: 3.3939346549366083 grad: -0.36290372020233275
iteration: 410 loss: 3.313842024359846 grad: -0.3550281641220768
iteration: 420 loss: 3.2374077807757287 grad: -0.3474823020657569
iteration: 430 loss: 3.1643886396784846 grad: -0.34024608802154255
iteration: 440 loss: 3.09456230725825 grad: -0.3333010550300279
iteration: 450 loss: 3.027725273285796 grad: -0.3266301636713079
iteration: 460 loss: 2.9636908759032186 grad: -0.32021766759321174
iteration: 470 loss: 2.902287600119797 grad: -0.3140489938923738
iteration: 480 loss: 2.8433575778096216 grad: -0.30811063647447623
iteration: 490 loss: 2.7867552619773877 grad: -0.30239006078477226
iteration: 0 loss: 72.84903773435458 grad: 44.757097287144255
iteration: 10 loss: 42.43916402906296 grad: 3.740344329471994
iteration: 20 loss: 32.469944893425016 grad: -1.4645176078463655
iteration: 30 loss: 26.542387743819166 grad: -1.8576201072215666
iteration: 40 loss: 22.500770312773284 grad: -1.7649201073709189
iteration: 50 loss: 19.540176448923727 grad: -1.622495148073481
iteration: 60 loss: 17.268149443066644 grad: -1.4897287771861345
iteration: 70 loss: 15.4656857909302 grad: -1.3741356072483966
iteration: 80 loss: 13.999375336292731 grad: -1.2744663575497084
iteration: 90 loss: 12.782688780407335 grad: -1.1881806881734982
iteration: 100 loss: 11.756776355715374 grad: -1.1129139845221008
iteration: 110 loss: 10.880092065713205 grad: -1.0467204146008122
iteration: 120 loss: 10.122406851531878 grad: -0.9880482063764461
iteration: 130 loss: 9.46116941398972 grad: -0.9356661314341814
iteration: 140 loss: 8.879198062855762 grad: -0.8885931927612565
iteration: 150 loss: 8.363164066912924 grad: -0.8460417525612207
iteration: 160 loss: 7.9025648092522465 grad: -0.8073737776021801
iteration: 170 loss: 7.489010478673166 grad: -0.772067668437101
iteration: 180 loss: 7.115717388950915 grad: -0.7396931435477941
iteration: 190 loss: 6.777140961953716 grad: -0.7098921409287677
iteration: 200 loss: 6.468705235403459 grad: -0.6823642010379114
iteration: 210 loss: 6.186600408763835 grad: -0.656855198029682
iteration: 220 loss: 5.927629199209108 grad: -0.6331485869924316
iteration: 230 loss: 5.6890887718442915 grad: -0.6110585540329618
iteration: 240 loss: 5.468678970855018 grad: -0.5904246149387504
iteration: 250 loss: 5.264430249794507 grad: -0.5711073237726962
iteration: 260 loss: 5.074646532291395 grad: -0.5529848374326454
iteration: 270 loss: 4.897859512475241 grad: -0.5359501446339243
iteration: 280 loss: 4.732791808602335 grad: -0.5199088140706813
iteration: 290 loss: 4.578327031710945 grad: -0.5047771510261996
iteration: 300 loss: 4.433485301839028 grad: -0.49048067755115615
iteration: 310 loss: 4.297403090001973 grad: -0.47695287077148224
iteration: 320 loss: 4.169316520681274 grad: -0.4641341085749576
iteration: 330 loss: 4.048547461887623 grad: -0.45197078307103167
iteration: 340 loss: 3.934491875339931 grad: -0.440414550716639
iteration: 350 loss: 3.826610010309196 grad: -0.4294216945142211
iteration: 360 loss: 3.7244181100737808 grad: -0.41895257870793245
iteration: 370 loss: 3.627481366108317 grad: -0.40897118029522217
iteration: 380 loss: 3.5354079067909963 grad: -0.3994446847056551
iteration: 390 loss: 3.4478436480232464 grad: -0.3903431353803999
iteration: 400 loss: 3.3644678652477853 grad: -0.38163912886657136
iteration: 410 loss: 3.2849893719107555 grad: -0.3733075485354437
iteration: 420 loss: 3.2091432098468844 grad: -0.3653253312290792
iteration: 430 loss: 3.136687773510692 grad: -0.35767126210205646
iteration: 440 loss: 3.0674023032774627 grad: -0.35032579370396744
iteration: 450 loss: 3.0010846938376248 grad: -0.34327088598276007
iteration: 460 loss: 2.9375495725363097 grad: -0.3364898644085942
iteration: 470 loss: 2.8766266097308786 grad: -0.3299672938457129
iteration: 480 loss: 2.818159029215911 grad: -0.3236888661539754
iteration: 490 loss: 2.7620022916684817 grad: -0.31764129979622685
iteration: 0 loss: 76.38781887694108 grad: 25.877858887783432
iteration: 10 loss: 44.34123487107116 grad: 7.148504096128981
iteration: 20 loss: 33.45729142538044 grad: 1.214383524789177
iteration: 30 loss: 27.2567163579084 grad: -0.47020502076051846
iteration: 40 loss: 23.077478174092388 grad: -0.9956948619036616
iteration: 50 loss: 20.027173401696885 grad: -1.1509777839162132
iteration: 60 loss: 17.689779099766323 grad: -1.1739330800297716
iteration: 70 loss: 15.83700553946553 grad: -1.1469547030709588
iteration: 80 loss: 14.330700972831478 grad: -1.1015709746622577
iteration: 90 loss: 13.081501881222577 grad: -1.0506596364270753
iteration: 100 loss: 12.028699638799274 grad: -0.9996484466392137
iteration: 110 loss: 11.129457728232286 grad: -0.9507976760062842
iteration: 120 loss: 10.352616874927394 grad: -0.9049632347455594
iteration: 130 loss: 9.674937099158694 grad: -0.8623651985981406
iteration: 140 loss: 9.07871634671859 grad: -0.8229374593047603
iteration: 150 loss: 8.550226187688292 grad: -0.7864915954896701
iteration: 160 loss: 8.078652710762753 grad: -0.7527948925047869
iteration: 170 loss: 7.6553607140752895 grad: -0.7216074117479161
iteration: 180 loss: 7.273370964884469 grad: -0.692699078027627
iteration: 190 loss: 6.926981506997436 grad: -0.6658568897492303
iteration: 200 loss: 6.6114885503772545 grad: -0.6408872442275567
iteration: 210 loss: 6.322977573610512 grad: -0.617615890562528
iteration: 220 loss: 6.058164808280832 grad: -0.5958867888798883
iteration: 230 loss: 5.81427544899324 grad: -0.5755605283363355
iteration: 240 loss: 5.588949017505663 grad: -0.556512633181364
iteration: 250 loss: 5.380165064621409 grad: -0.5386319177773122
iteration: 260 loss: 5.1861842849098965 grad: -0.5218189634066404
iteration: 270 loss: 5.005501438634052 grad: -0.5059847438997359
iteration: 280 loss: 4.836807409035001 grad: -0.49104940378949874
iteration: 290 loss: 4.678958392926109 grad: -0.4769411813667747
iteration: 300 loss: 4.530950708922774 grad: -0.4635954640656086
iteration: 310 loss: 4.3919000648627184 grad: -0.4509539620191231
iteration: 320 loss: 4.261024391129922 grad: -0.43896398574993006
iteration: 330 loss: 4.137629545320537 grad: -0.4275778148997496
iteration: 340 loss: 4.021097344033148 grad: -0.41675214618002454
iteration: 350 loss: 3.91087549224184 grad: -0.40644761008466695
iteration: 360 loss: 3.8064690689161993 grad: -0.3966283472189284
iteration: 370 loss: 3.7074332958919842 grad: -0.38726163630494614
iteration: 380 loss: 3.613367370320826 grad: -0.37831756700232677
iteration: 390 loss: 3.5239091829276648 grad: -0.369768751628457
iteration: 400 loss: 3.4387307774271574 grad: -0.36159007068518656
iteration: 410 loss: 3.357534432788353 grad: -0.353758447807638
iteration: 420 loss: 3.2800492711130573 grad: -0.34625265036007413
iteration: 430 loss: 3.2060283108299203 grad: -0.33905311242581404
iteration: 440 loss: 3.135245898612125 grad: -0.3321417773850287
iteration: 450 loss: 3.067495464545945 grad: -0.32550195765654344
iteration: 460 loss: 3.002587554161313 grad: -0.3191182095069318
iteration: 470 loss: 2.9403480983759107 grad: -0.3129762211103216
iteration: 480 loss: 2.8806168885355627 grad: -0.3070627122824374
iteration: 490 loss: 2.8232462287919176 grad: -0.3013653445184495
iteration: 0 loss: 75.64322183689701 grad: 38.331483311785874
iteration: 10 loss: 44.98756626835334 grad: 4.853199606751478
iteration: 20 loss: 34.192034865220016 grad: -0.6125760575334882
iteration: 30 loss: 27.809423477798813 grad: -1.3828031652386472
iteration: 40 loss: 23.48061425776797 grad: -1.4676251355043555
iteration: 50 loss: 20.32491588765887 grad: -1.4182957834476304
iteration: 60 loss: 17.913769288140017 grad: -1.34012298711309
iteration: 70 loss: 16.008485546444323 grad: -1.2590066526191286
iteration: 80 loss: 14.463993606460939 grad: -1.1822280145814412
iteration: 90 loss: 13.186449236387075 grad: -1.1117125481633523
iteration: 100 loss: 12.112212037450366 grad: -1.0476390819271233
iteration: 110 loss: 11.19648929792224 grad: -0.9895982011701709
iteration: 120 loss: 10.406788518624849 grad: -0.9370067972557286
iteration: 130 loss: 9.718942214738284 grad: -0.8892643864307147
iteration: 140 loss: 9.114590567992433 grad: -0.8458110884921697
iteration: 150 loss: 8.579530332452494 grad: -0.8061460527299454
iteration: 160 loss: 8.102599211367368 grad: -0.7698297144234185
iteration: 170 loss: 7.674902530644886 grad: -0.7364796763662333
iteration: 180 loss: 7.289265138365408 grad: -0.7057644326531229
iteration: 190 loss: 6.939835276058272 grad: -0.6773967523191415
iteration: 200 loss: 6.6217932878200205 grad: -0.6511274753016364
iteration: 210 loss: 6.331134085664251 grad: -0.6267399926631914
iteration: 220 loss: 6.064502422893756 grad: -0.6040454667210142
iteration: 230 loss: 5.819066578692648 grad: -0.5828787527036687
iteration: 240 loss: 5.592420384204285 grad: -0.5630949469745119
iteration: 250 loss: 5.382506433841669 grad: -0.54456647731561
iteration: 260 loss: 5.18755532182112 grad: -0.5271806536294318
iteration: 270 loss: 5.006037133611286 grad: -0.5108376055060585
iteration: 280 loss: 4.836622403650588 grad: -0.49544854286615725
iteration: 290 loss: 4.678150453463497 grad: -0.4809342855956986
iteration: 300 loss: 4.529603533688425 grad: -0.46722401695256977
iteration: 310 loss: 4.390085566984205 grad: -0.45425422325536624
iteration: 320 loss: 4.258804565504885 grad: -0.44196778892097655
iteration: 330 loss: 4.135058003721497 grad: -0.4303132213818881
iteration: 340 loss: 4.018220583769397 grad: -0.4192439849185853
iteration: 350 loss: 3.907733949657458 grad: -0.4087179261311955
iteration: 360 loss: 3.8030979981849256 grad: -0.3986967767848226
iteration: 370 loss: 3.7038635052254407 grad: -0.38914572221656285
iteration: 380 loss: 3.60962584123978 grad: -0.380033025492728
iteration: 390 loss: 3.5200195931814218 grad: -0.3713296991384988
iteration: 400 loss: 3.434713944177764 grad: -0.36300921759958527
iteration: 410 loss: 3.3534086895279787 grad: -0.35504726469339515
iteration: 420 loss: 3.275830789291707 grad: -0.347421511211655
iteration: 430 loss: 3.2017313751731638 grad: -0.34011141858404803
iteration: 440 loss: 3.130883143515968 grad: -0.3330980651327052
iteration: 450 loss: 3.0630780776306312 grad: -0.3263639919638714
iteration: 460 loss: 2.9981254520379976 grad: -0.31989306597469075
iteration: 470 loss: 2.9358500788163258 grad: -0.3136703578150209
iteration: 480 loss: 2.8760907625448286 grad: -0.30768203294880714
iteration: 490 loss: 2.8186989355202225 grad: -0.3019152542167168
iteration: 0 loss: 77.60648380930124 grad: 24.793912540059367
iteration: 10 loss: 46.63715283802472 grad: 6.867530268502034
iteration: 20 loss: 34.991224027212745 grad: 1.2072212175964685
iteration: 30 loss: 28.338454350404163 grad: -0.3626268890758729
iteration: 40 loss: 23.886563811088536 grad: -0.8505253816475251
iteration: 50 loss: 20.659912420076022 grad: -1.002432048286599
iteration: 60 loss: 18.20160057302707 grad: -1.0347182772098336
iteration: 70 loss: 16.262082548259308 grad: -1.0206112428602232
iteration: 80 loss: 14.691307303527765 grad: -0.9880851788080954
iteration: 90 loss: 13.392809571605374 grad: -0.9488122116141768
iteration: 100 loss: 12.30141478556867 grad: -0.9079490981995508
iteration: 110 loss: 11.37135641183361 grad: -0.867830463703138
iteration: 120 loss: 10.569484994170374 grad: -0.8294927033526921
iteration: 130 loss: 9.87116921550759 grad: -0.7933496081591758
iteration: 140 loss: 9.257710356666704 grad: -0.7595095167154906
iteration: 150 loss: 8.714650943669664 grad: -0.7279309454457956
iteration: 160 loss: 8.230633956306333 grad: -0.698501489035547
iteration: 170 loss: 7.796613014196385 grad: -0.6710786591705343
iteration: 180 loss: 7.405293062159217 grad: -0.6455111747624052
iteration: 190 loss: 7.050726382046088 grad: -0.6216499388111935
iteration: 200 loss: 6.728015660096554 grad: -0.5993534703182383
iteration: 210 loss: 6.433092323329344 grad: -0.5784903245380275
iteration: 220 loss: 6.162548742507583 grad: -0.5589398785837243
iteration: 230 loss: 5.913509602454186 grad: -0.5405922439443851
iteration: 240 loss: 5.683532163124792 grad: -0.5233477318738013
iteration: 250 loss: 5.470528110590914 grad: -0.5071161109168852
iteration: 260 loss: 5.272701734985855 grad: -0.491815790316931
iteration: 270 loss: 5.088500590768697 grad: -0.4773730027097326
iteration: 280 loss: 4.916575796285046 grad: -0.4637210247781029
iteration: 290 loss: 4.755749846562387 grad: -0.45079945453205006
iteration: 300 loss: 4.6049903328499076 grad: -0.4385535524363563
iteration: 310 loss: 4.463388343277405 grad: -0.42693364718530025
iteration: 320 loss: 4.330140601177556 grad: -0.4158946034575606
iteration: 330 loss: 4.204534608720058 grad: -0.40539534725440546
iteration: 340 loss: 4.085936222922555 grad: -0.3953984437064624
iteration: 350 loss: 3.973779212519069 grad: -0.3858697220953431
iteration: 360 loss: 3.867556437383676 grad: -0.37677794301654477
iteration: 370 loss: 3.7668123643452005 grad: -0.36809450295430285
iteration: 380 loss: 3.671136689408823 grad: -0.3597931719555904
iteration: 390 loss: 3.580158880509647 grad: -0.3518498605258676
iteration: 400 loss: 3.493543489722759 grad: -0.34424241229351527
iteration: 410 loss: 3.4109861114990796 grad: -0.33695041938725867
iteration: 420 loss: 3.332209885599406 grad: -0.3299550578339973
iteration: 430 loss: 3.2569624611266423 grad: -0.3232389406109596
iteration: 440 loss: 3.1850133523946416 grad: -0.31678598627652893
iteration: 450 loss: 3.1161516289875264 grad: -0.31058130136054807
iteration: 460 loss: 3.050183891845377 grad: -0.3046110749202115
iteration: 470 loss: 2.9869324949755454 grad: -0.2988624838649322
iteration: 480 loss: 2.9262339787669176 grad: -0.29332360782590433
iteration: 490 loss: 2.867937686163259 grad: -0.2879833524964412
iteration: 0 loss: 76.59807876061323 grad: 36.27256515712277
iteration: 10 loss: 44.68607429442907 grad: 6.459703008705426
iteration: 20 loss: 33.74288384835257 grad: -0.22442571702716205
iteration: 30 loss: 27.439111085659967 grad: -1.4397011175253434
iteration: 40 loss: 23.19227544224755 grad: -1.6311691898252607
iteration: 50 loss: 20.101138239267645 grad: -1.5913940219511005
iteration: 60 loss: 17.738819960783015 grad: -1.4975525973595147
iteration: 70 loss: 15.870463290996812 grad: -1.3964366907236876
iteration: 80 loss: 14.35417851252032 grad: -1.3011948804313618
iteration: 90 loss: 13.098446864824878 grad: -1.2150541859498303
iteration: 100 loss: 12.041284940737569 grad: -1.1381249998423095
iteration: 110 loss: 11.139082551403403 grad: -1.0695758976358425
iteration: 120 loss: 10.36019971196261 grad: -1.0083705493918123
iteration: 130 loss: 9.681091395971986 grad: -0.9535163822665635
iteration: 140 loss: 9.08385826468723 grad: -0.9041384432680528
iteration: 150 loss: 8.554642080346577 grad: -0.8594900262003
iteration: 160 loss: 8.08254216594783 grad: -0.8189417122546444
iteration: 170 loss: 7.658864478404832 grad: -0.7819646376805107
iteration: 180 loss: 7.276589344810418 grad: -0.748113833895474
iteration: 190 loss: 6.929986670653059 grad: -0.717013563294125
iteration: 200 loss: 6.614332863158862 grad: -0.688345047309873
iteration: 210 loss: 6.325699317612858 grad: -0.6618364252750941
iteration: 220 loss: 6.060792153012699 grad: -0.6372546156115593
iteration: 230 loss: 5.816829238473929 grad: -0.6143987361261436
iteration: 240 loss: 5.591444746718108 grad: -0.5930947787105985
iteration: 250 loss: 5.382614294568446 grad: -0.5731912853487384
iteration: 260 loss: 5.188595664761826 grad: -0.5545558216909032
iteration: 270 loss: 5.007881450168994 grad: -0.5370720866427212
iteration: 280 loss: 4.839160912944573 grad: -0.520637530699724
iteration: 290 loss: 4.681289032522975 grad: -0.5051613829353729
iteration: 300 loss: 4.533261210371578 grad: -0.4905630078284088
iteration: 310 loss: 4.394192461740617 grad: -0.4767705296876473
iteration: 320 loss: 4.2633001932575265 grad: -0.46371967532339464
iteration: 330 loss: 4.139889866273196 grad: -0.4513527956549548
iteration: 340 loss: 4.023342997823957 grad: -0.4396180347857707
iteration: 350 loss: 3.913107066878183 grad: -0.42846862122619933
iteration: 360 loss: 3.808686982513088 grad: -0.41786226078577793
iteration: 370 loss: 3.709637839561641 grad: -0.40776061448793655
iteration: 380 loss: 3.615558741005564 grad: -0.3981288479064733
iteration: 390 loss: 3.5260875085429935 grad: -0.3889352407576266
iteration: 400 loss: 3.440896136111405 grad: -0.38015084753690004
iteration: 410 loss: 3.3596868676217624 grad: -0.37174920156803537
iteration: 420 loss: 3.282188801343926 grad: -0.3637060561115285
iteration: 430 loss: 3.2081549404004117 grad: -0.35599915722316655
iteration: 440 loss: 3.1373596226012936 grad: -0.34860804390691125
iteration: 450 loss: 3.0695962739956815 grad: -0.3415138718085159
iteration: 460 loss: 3.00467543965318 grad: -0.3346992572759772
iteration: 470 loss: 2.9424230526389112 grad: -0.3281481390935662
iteration: 480 loss: 2.8826789083070916 grad: -0.32184565559627115
iteration: 490 loss: 2.8252953161025456 grad: -0.31577803520575715
iteration: 0 loss: 76.49578908734378 grad: 29.447234514647512
iteration: 10 loss: 44.96213274515117 grad: 7.774713878487933
iteration: 20 loss: 33.89108406523706 grad: 0.7599376333695762
iteration: 30 loss: 27.549040705969734 grad: -1.0022215481478391
iteration: 40 loss: 23.27729395885616 grad: -1.4400734756022389
iteration: 50 loss: 20.167657338934283 grad: -1.5057912927323085
iteration: 60 loss: 17.79162577710597 grad: -1.4591736492088736
iteration: 70 loss: 15.913064804241065 grad: -1.3803711761948314
iteration: 80 loss: 14.3890927955421 grad: -1.2961538632249585
iteration: 90 loss: 13.12747923420294 grad: -1.2156215353480486
iteration: 100 loss: 12.065744095776546 grad: -1.1415732931707887
iteration: 110 loss: 11.15993119565676 grad: -1.074478624767902
iteration: 120 loss: 10.378157141871084 grad: -1.0139610697033312
iteration: 130 loss: 9.696703283320652 grad: -0.959375897292383
iteration: 140 loss: 9.097544568411752 grad: -0.9100371387342531
iteration: 150 loss: 8.566730382930762 grad: -0.8653031223591365
iteration: 160 loss: 8.093291204864293 grad: -0.8246037361762794
iteration: 170 loss: 7.668480995050541 grad: -0.7874439048278006
iteration: 180 loss: 7.285240314594788 grad: -0.7533979343560984
iteration: 190 loss: 6.9378082960561605 grad: -0.722100987590353
iteration: 200 loss: 6.621437268012827 grad: -0.693240306622914
iteration: 210 loss: 6.332179588404994 grad: -0.6665471933277528
iteration: 220 loss: 6.066726177472984 grad: -0.6417900555614163
iteration: 230 loss: 5.822282659183987 grad: -0.6187685289796416
iteration: 240 loss: 5.59647325650237 grad: -0.5973085697696247
iteration: 250 loss: 5.387265437252733 grad: -0.5772583804478846
iteration: 260 loss: 5.192910260533815 grad: -0.5584850324091539
iteration: 270 loss: 5.011894733303796 grad: -0.5408716636464235
iteration: 280 loss: 4.842903447145049 grad: -0.5243151483410851
iteration: 290 loss: 4.684787452815644 grad: -0.508724152735922
iteration: 300 loss: 4.536538828617392 grad: -0.4940175073191655
iteration: 310 loss: 4.3972697640701055 grad: -0.48012283849472404
iteration: 320 loss: 4.266195251220407 grad: -0.466975413714249
iteration: 330 loss: 4.1426186786308214 grad: -0.45451716280200805
iteration: 340 loss: 4.02591977622011 grad: -0.4426958452516295
iteration: 350 loss: 3.9155444758224474 grad: -0.4314643389300633
iteration: 360 loss: 3.8109963419469337 grad: -0.42078003016063853
iteration: 370 loss: 3.711829296626299 grad: -0.410604288799153
iteration: 380 loss: 3.617641416305808 grad: -0.4009020148468557
iteration: 390 loss: 3.5280696212149367 grad: -0.39164124550769863
iteration: 400 loss: 3.4427851111675576 grad: -0.3827928135099614
iteration: 410 loss: 3.361489428429195 grad: -0.37433004906569256
iteration: 420 loss: 3.2839110495675743 grad: -0.3662285191074985
iteration: 430 loss: 3.209802425347665 grad: -0.35846579847809124
iteration: 440 loss: 3.1389374015563836 grad: -0.35102126859870897
iteration: 450 loss: 3.0711089648806262 grad: -0.34387593984382925
iteration: 460 loss: 3.0061272671171704 grad: -0.3370122944298791
iteration: 470 loss: 2.943817888518011 grad: -0.33041414710756656
iteration: 480 loss: 2.8840203072264474 grad: -0.3240665213491277
iteration: 490 loss: 2.8265865468918245 grad: -0.31795553905773555
iteration: 0 loss: 75.71715487157236 grad: 35.979228375309404
iteration: 10 loss: 44.46068228527866 grad: 6.904158974421701
iteration: 20 loss: 33.60237812010626 grad: -0.38342409234944386
iteration: 30 loss: 27.328203862377055 grad: -1.5990576111555346
iteration: 40 loss: 23.094063396089453 grad: -1.7352734476495042
iteration: 50 loss: 20.01078424810862 grad: -1.6571369530838218
iteration: 60 loss: 17.654577065810745 grad: -1.5406116287032416
iteration: 70 loss: 15.791441760745949 grad: -1.4262183894072278
iteration: 80 loss: 14.279766873240863 grad: -1.3229911041259452
iteration: 90 loss: 13.028153203323056 grad: -1.2318603689322183
iteration: 100 loss: 11.974690181163417 grad: -1.1516850678023656
iteration: 110 loss: 11.075823897265225 grad: -1.0809412963195455
iteration: 120 loss: 10.299962010828205 grad: -1.0181978312101396
iteration: 130 loss: 9.62360059407361 grad: -0.9622287233844502
iteration: 140 loss: 9.02887563385744 grad: -0.9120160758089991
iteration: 150 loss: 8.501959041152883 grad: -0.8667227606855319
iteration: 160 loss: 8.031975738894578 grad: -0.8256606129863439
iteration: 170 loss: 7.610253377539596 grad: -0.7882618037699849
iteration: 180 loss: 7.229790681142731 grad: -0.7540551220382965
iteration: 190 loss: 6.884873201451104 grad: -0.7226469606248065
iteration: 200 loss: 6.570790710587682 grad: -0.6937062732429884
iteration: 210 loss: 6.283626080339218 grad: -0.6669527208050627
iteration: 220 loss: 6.02009534138109 grad: -0.6421473237472407
iteration: 230 loss: 5.777424973559556 grad: -0.6190850654381816
iteration: 240 loss: 5.553256673604872 grad: -0.5975890104382384
iteration: 250 loss: 5.345572669398652 grad: -0.5775055999032266
iteration: 260 loss: 5.152636583026875 grad: -0.5587008644748298
iteration: 270 loss: 4.972946190222046 grad: -0.5410573554311956
iteration: 280 loss: 4.8051953740108395 grad: -0.5244716411269904
iteration: 290 loss: 4.648243250658404 grad: -0.5088522509653897
iteration: 300 loss: 4.501088939139705 grad: -0.4941179759019537
iteration: 310 loss: 4.3628508069798295 grad: -0.48019645482276097
iteration: 320 loss: 4.232749293346429 grad: -0.4670229916375266
iteration: 330 loss: 4.1100926109184 grad: -0.4545395597755602
iteration: 340 loss: 3.9942647796629402 grad: -0.44269395986681004
iteration: 350 loss: 3.8847155611925204 grad: -0.4314391034057564
iteration: 360 loss: 3.7809519511619825 grad: -0.4207324006369084
iteration: 370 loss: 3.6825309558840176 grad: -0.4105352351447191
iteration: 380 loss: 3.5890534329576194 grad: -0.4008125109608428
iteration: 390 loss: 3.5001588177742633 grad: -0.39153226063040303
iteration: 400 loss: 3.4155205910188586 grad: -0.3826653047666792
iteration: 410 loss: 3.334842368709691 grad: -0.3741849552915779
iteration: 420 loss: 3.2578545174582443 grad: -0.3660667558997724
iteration: 430 loss: 3.1843112146029027 grad: -0.358288254367843
iteration: 440 loss: 3.1139878866079016 grad: -0.35082880221036866
iteration: 450 loss: 3.0466789702555532 grad: -0.34366937790443575
iteration: 460 loss: 2.982195950252638 grad: -0.33679243049495594
iteration: 470 loss: 2.920365634322066 grad: -0.3301817408808538
iteration: 480 loss: 2.86102863298691 grad: -0.32382229848653554
iteration: 490 loss: 2.804038016316719 grad: -0.3177001913597687
iteration: 0 loss: 76.39821887877753 grad: 31.40653227077373
iteration: 10 loss: 44.06933471214768 grad: 6.0870250924082665
iteration: 20 loss: 33.23287148480163 grad: 0.39903355116133465
iteration: 30 loss: 27.029865435114846 grad: -0.880344551968798
iteration: 40 loss: 22.85437112708469 grad: -1.2127134807494713
iteration: 50 loss: 19.812517610042196 grad: -1.280546200509534
iteration: 60 loss: 17.485515267367706 grad: -1.2618267058572494
iteration: 70 loss: 15.643643973129285 grad: -1.2134059732205944
iteration: 80 loss: 14.148068967538558 grad: -1.1559852336955136
iteration: 90 loss: 12.909108640635809 grad: -1.097673952746469
iteration: 100 loss: 11.865917985054445 grad: -1.0416978715008285
iteration: 110 loss: 10.975617225643047 grad: -0.9892487448170102
iteration: 120 loss: 10.207052072679538 grad: -0.9406370529044562
iteration: 130 loss: 9.537012045739774 grad: -0.8957895444736612
iteration: 140 loss: 8.947836175352966 grad: -0.8544736382673217
iteration: 150 loss: 8.425841544392927 grad: -0.8164011038417076
iteration: 160 loss: 7.960260222360478 grad: -0.7812760373568068
iteration: 170 loss: 7.542501314549224 grad: -0.7488163605402847
iteration: 180 loss: 7.165627131646033 grad: -0.7187625283104992
iteration: 190 loss: 6.823974019092723 grad: -0.6908800525409915
iteration: 200 loss: 6.512873122755082 grad: -0.6649591015946567
iteration: 210 loss: 6.2284415677257705 grad: -0.6408128034076426
iteration: 220 loss: 5.967424126747615 grad: -0.618275065760106
iteration: 230 loss: 5.727071666243443 grad: -0.5971983142795356
iteration: 240 loss: 5.5050467651307455 grad: -0.5774513370126579
iteration: 250 loss: 5.299349670644915 grad: -0.5589173157285859
iteration: 260 loss: 5.108259655351173 grad: -0.5414920689069994
iteration: 270 loss: 4.930288164121149 grad: -0.5250825041441722
iteration: 280 loss: 4.764141076838231 grad: -0.5096052651734888
iteration: 290 loss: 4.608688084309464 grad: -0.49498555381340964
iteration: 300 loss: 4.462937662363567 grad: -0.4811561061770072
iteration: 310 loss: 4.326016486923725 grad: -0.46805630340013205
iteration: 320 loss: 4.197152398295946 grad: -0.45563139890391763
iteration: 330 loss: 4.075660221729322 grad: -0.44383184624268207
iteration: 340 loss: 3.9609299016178325 grad: -0.4326127136167014
iteration: 350 loss: 3.852416521315966 grad: -0.4219331730163639
iteration: 360 loss: 3.749631868617185 grad: -0.4117560536553818
iteration: 370 loss: 3.6521372751541836 grad: -0.40204745083347543
iteration: 380 loss: 3.559537511181175 grad: -0.3927763826515466
iteration: 390 loss: 3.4714755589632413 grad: -0.3839144881026503
iteration: 400 loss: 3.3876281210030523 grad: -0.37543576100113984
iteration: 410 loss: 3.3077017455709457 grad: -0.36731631501149786
iteration: 420 loss: 3.231429472979223 grad: -0.3595341757173443
iteration: 430 loss: 3.158567922904612 grad: -0.3520690962477928
iteration: 440 loss: 3.0888947566681138 grad: -0.34490239346814505
iteration: 450 loss: 3.0222064594728923 grad: -0.33801680215832874
iteration: 460 loss: 2.9583163965844093 grad: -0.3313963449568426
iteration: 470 loss: 2.897053104869056 grad: -0.3250262161499335
iteration: 480 loss: 2.8382587871575526 grad: -0.31889267764346074
iteration: 490 loss: 2.781787981950484 grad: -0.3129829656752214
iteration: 0 loss: 77.78275525851548 grad: 23.70122752111937
iteration: 10 loss: 46.49136501142189 grad: 6.930952920824838
iteration: 20 loss: 34.86203024427043 grad: 1.2421925346174585
iteration: 30 loss: 28.21721114870353 grad: -0.3886039700239461
iteration: 40 loss: 23.767512340198003 grad: -0.8973920042482841
iteration: 50 loss: 20.54309407562689 grad: -1.0503797501646497
iteration: 60 loss: 18.08819142152544 grad: -1.0769461774211249
iteration: 70 loss: 16.153013968885737 grad: -1.0556185046176991
iteration: 80 loss: 14.587077358090442 grad: -1.0162285395728936
iteration: 90 loss: 13.293576312788135 grad: -0.9709809184020542
iteration: 100 loss: 12.207116709966943 grad: -0.9250897945284365
iteration: 110 loss: 11.281804878438937 grad: -0.8807941882650234
iteration: 120 loss: 10.484425672703559 grad: -0.838999932170078
iteration: 130 loss: 9.790320278219207 grad: -0.7999959826426908
iteration: 140 loss: 9.180785138564344 grad: -0.7637833306653394
iteration: 150 loss: 8.641370762554681 grad: -0.7302319513238018
iteration: 160 loss: 8.160734882864022 grad: -0.6991576262890035
iteration: 170 loss: 7.729849004851303 grad: -0.6703599374766557
iteration: 180 loss: 7.341436954914157 grad: -0.6436408653886199
iteration: 190 loss: 6.98956967507477 grad: -0.6188134915369954
iteration: 200 loss: 6.669367631018014 grad: -0.5957055963318243
iteration: 210 loss: 6.376778823881358 grad: -0.5741606301806643
iteration: 220 loss: 6.1084108664531 grad: -0.554037362867492
iteration: 230 loss: 5.861402340932852 grad: -0.5352089062460403
iteration: 240 loss: 5.633323110840468 grad: -0.5175614811327556
iteration: 250 loss: 5.422096255490652 grad: -0.5009931242033647
iteration: 260 loss: 5.225936345759527 grad: -0.48541243508898535
iteration: 270 loss: 5.04330020572847 grad: -0.4707374114233083
iteration: 280 loss: 4.872847311092466 grad: -0.4568943909417123
iteration: 290 loss: 4.71340769500462 grad: -0.44381710434067184
iteration: 300 loss: 4.563955753320318 grad: -0.43144583464510566
iteration: 310 loss: 4.423588723095525 grad: -0.4197266750360129
iteration: 320 loss: 4.291508890945434 grad: -0.4086108756102679
iteration: 330 loss: 4.167008799308033 grad: -0.398054269323153
iteration: 340 loss: 4.049458878232723 grad: -0.3880167677985509
iteration: 350 loss: 3.9382970517924396 grad: -0.3784619184378268
iteration: 360 loss: 3.833019961455896 grad: -0.36935651512272216
iteration: 370 loss: 3.7331755208455104 grad: -0.36067025568381345
iteration: 380 loss: 3.6383565724768645 grad: -0.3523754401387047
iteration: 390 loss: 3.5481954611019235 grad: -0.3444467044668106
iteration: 400 loss: 3.4623593730533355 grad: -0.33686078537070013
iteration: 410 loss: 3.380546318564856 grad: -0.3295963120770273
iteration: 420 loss: 3.3024816561039434 grad: -0.32263362175740207
iteration: 430 loss: 3.227915075445129 grad: -0.3159545956078127
iteration: 440 loss: 3.156617970491342 grad: -0.30954251302174574
iteration: 450 loss: 3.088381144457716 grad: -0.3033819216344442
iteration: 460 loss: 3.0230127994649263 grad: -0.2974585213107344
iteration: 470 loss: 2.9603367703287984 grad: -0.2917590604029035
iteration: 480 loss: 2.9001909686950387 grad: -0.28627124282388516
iteration: 490 loss: 2.8424260089149853 grad: -0.28098364466944653
iteration: 0 loss: 76.00957013544901 grad: 34.64125848244281
iteration: 10 loss: 45.13817339015689 grad: 5.755331664680973
iteration: 20 loss: 34.08077028557613 grad: 0.03424296693710756
iteration: 30 loss: 27.661980949478963 grad: -1.063453598143391
iteration: 40 loss: 23.345317002823364 grad: -1.3067587908098095
iteration: 50 loss: 20.210403055747168 grad: -1.3354842079842395
iteration: 60 loss: 17.819199039304127 grad: -1.2986436855197334
iteration: 70 loss: 15.930948039954915 grad: -1.2408587925879784
iteration: 80 loss: 14.4004809933375 grad: -1.177897044044986
iteration: 90 loss: 13.134352553262136 grad: -1.1158394561645306
iteration: 100 loss: 12.069392164293937 grad: -1.057046962507619
iteration: 110 loss: 11.161228057968525 grad: -1.0023247437101208
iteration: 120 loss: 10.377714621915318 grad: -0.9517992231463221
iteration: 130 loss: 9.694961059618311 grad: -0.9053017233483331
iteration: 140 loss: 9.094825431722194 grad: -0.8625446311820069
iteration: 150 loss: 8.563275436410425 grad: -0.8232041408199251
iteration: 160 loss: 8.089283046760226 grad: -0.7869588819825524
iteration: 170 loss: 7.66405944673513 grad: -0.7535070868432523
iteration: 180 loss: 7.280513375559704 grad: -0.722573182461274
iteration: 190 loss: 6.932859951971915 grad: -0.6939092071236466
iteration: 200 loss: 6.6163331671378645 grad: -0.6672937789224987
iteration: 210 loss: 6.326971238221094 grad: -0.6425300053619556
iteration: 220 loss: 6.061454091440241 grad: -0.6194430364775081
iteration: 230 loss: 5.816978744377841 grad: -0.5978776079356489
iteration: 240 loss: 5.59116264340388 grad: -0.5776957351644658
iteration: 250 loss: 5.381967893938756 grad: -0.5587746235341673
iteration: 260 loss: 5.187641293641016 grad: -0.5410048107982223
iteration: 270 loss: 5.006666450600691 grad: -0.5242885342055977
iteration: 280 loss: 4.837725237131673 grad: -0.5085383040790659
iteration: 290 loss: 4.679666522866228 grad: -0.49367566189351947
iteration: 300 loss: 4.531480633050953 grad: -0.4796301005865309
iteration: 310 loss: 4.392278346075355 grad: -0.4663381261757328
iteration: 320 loss: 4.261273517009283 grad: -0.4537424417831578
iteration: 330 loss: 4.13776861801249 grad: -0.4417912373776904
iteration: 340 loss: 4.021142640631669 grad: -0.4304375706959854
iteration: 350 loss: 3.910840922426401 grad: -0.41963882677676906
iteration: 360 loss: 3.8063665505717172 grad: -0.40935624530185594
iteration: 370 loss: 3.707273064874037 grad: -0.3995545064744605
iteration: 380 loss: 3.6131582370659006 grad: -0.3902013674941279
iteration: 390 loss: 3.523658745932525 grad: -0.38126734282819785
iteration: 400 loss: 3.4384456015625138 grad: -0.37272542245464413
iteration: 410 loss: 3.3572201988062345 grad: -0.36455082308245335
iteration: 420 loss: 3.2797109014571273 grad: -0.3567207680637112
iteration: 430 loss: 3.2056700758657413 grad: -0.3492142923143663
iteration: 440 loss: 3.134871506618269 grad: -0.3420120690739594
iteration: 450 loss: 3.067108138185962 grad: -0.33509625577204916
iteration: 460 loss: 3.002190095654524 grad: -0.3284503566421753
iteration: 470 loss: 2.9399429451976755 grad: -0.3220591000428503
iteration: 480 loss: 2.8802061611439225 grad: -0.31590832871756946
iteration: 490 loss: 2.82283177163419 grad: -0.3099849014591878
iteration: 0 loss: 74.53650005679968 grad: 36.9451387317309
iteration: 10 loss: 42.39425370469128 grad: 5.433869280510551
iteration: 20 loss: 32.12284453172979 grad: -0.6818288138026314
iteration: 30 loss: 26.22380626880764 grad: -1.5860555765135544
iteration: 40 loss: 22.236270392451658 grad: -1.6646553099498376
iteration: 50 loss: 19.32085197484796 grad: -1.585395437767764
iteration: 60 loss: 17.08345161844512 grad: -1.4791320000039878
iteration: 70 loss: 15.307528919443898 grad: -1.3758714430202728
iteration: 80 loss: 13.861959675063025 grad: -1.2823094353903215
iteration: 90 loss: 12.66185070663724 grad: -1.199080117800174
iteration: 100 loss: 11.64946696474013 grad: -1.1252429168033493
iteration: 110 loss: 10.784023038808671 grad: -1.05955979611694
iteration: 120 loss: 10.035818441003192 grad: -1.0008621946707348
iteration: 130 loss: 9.38267811353752 grad: -0.9481406538923357
iteration: 140 loss: 8.807694941068947 grad: -0.9005488184336758
iteration: 150 loss: 8.297745406396622 grad: -0.8573833446109738
iteration: 160 loss: 7.842483650696274 grad: -0.8180597799392271
iteration: 170 loss: 7.433641962067585 grad: -0.7820905996594131
iteration: 180 loss: 7.0645334005971145 grad: -0.7490668642304175
iteration: 190 loss: 6.729691198418798 grad: -0.7186434083168558
iteration: 200 loss: 6.4246027895025115 grad: -0.6905270341950155
iteration: 210 loss: 6.145510609191991 grad: -0.6644671303746452
iteration: 220 loss: 5.8892608374414355 grad: -0.6402482049709783
iteration: 230 loss: 5.653187112655112 grad: -0.6176839179542143
iteration: 240 loss: 5.43502011771567 grad: -0.5966122847994255
iteration: 250 loss: 5.232816555049836 grad: -0.5768917975243386
iteration: 260 loss: 5.044902823930773 grad: -0.5583982672043173
iteration: 270 loss: 4.869829966901059 grad: -0.5410222369548447
iteration: 280 loss: 4.706337339970767 grad: -0.524666848707636
iteration: 290 loss: 4.55332309832792 grad: -0.5092460732640999
iteration: 300 loss: 4.4098200521698345 grad: -0.4946832330395118
iteration: 310 loss: 4.274975787351692 grad: -0.48090976213377357
iteration: 320 loss: 4.14803619813599 grad: -0.46786416003813824
iteration: 330 loss: 4.0283317686977504 grad: -0.4554911042834475
iteration: 340 loss: 3.9152660833831097 grad: -0.44374069430659197
iteration: 350 loss: 3.808306155105997 grad: -0.4325678042455593
iteration: 360 loss: 3.706974245437364 grad: -0.4219315266342686
iteration: 370 loss: 3.6108409151961496 grad: -0.41179469232985566
iteration: 380 loss: 3.5195190952714173 grad: -0.4021234546730281
iteration: 390 loss: 3.432659007458121 grad: -0.39288692801217406
iteration: 400 loss: 3.349943796738113 grad: -0.38405687243238673
iteration: 410 loss: 3.271085761635773 grad: -0.37560741791175367
iteration: 420 loss: 3.1958230894405957 grad: -0.36751482224858045
iteration: 430 loss: 3.1239170192864303 grad: -0.35975725801827796
iteration: 440 loss: 3.055149369226993 grad: -0.35231462456906554
iteration: 450 loss: 2.989320374058562 grad: -0.3451683816839729
iteration: 460 loss: 2.92624678937963 grad: -0.3383014020484115
iteration: 470 loss: 2.865760224487117 grad: -0.3316978400880225
iteration: 480 loss: 2.807705672587252 grad: -0.3253430150965796
iteration: 490 loss: 2.7519402116677156 grad: -0.319223306871303
